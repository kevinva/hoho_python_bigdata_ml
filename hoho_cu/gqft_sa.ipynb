{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6795c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import hanlp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "    \n",
    "%matplotlib inline\n",
    "mpl.rcParams['font.family'] = 'SimHei'\n",
    "\n",
    "RANDOM_SEED = 666\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dc97d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>情感</th>\n",
       "      <th>语句</th>\n",
       "      <th>来源</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100699</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>想了半天还真没有什么吐槽的地方，车身高度不够，后排乘客身子长的会顶头，后备箱空间略小（还是那...</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99496</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>没有mkx的持钥匙走近亮灯</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15042</th>\n",
       "      <td>投诉</td>\n",
       "      <td>一汽丰田皇冠仪表台老化反光影响视线</td>\n",
       "      <td>车质网</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106097</th>\n",
       "      <td>表扬</td>\n",
       "      <td>我和媳妇都是两厢控，最满意的就是320的外观了，适合年轻人时尚又不失大气，白颜色非常靓丽配合...</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115342</th>\n",
       "      <td>表扬</td>\n",
       "      <td>空间大，相对来说舒适性较好，性比价好些。</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80088</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>隔音不是很好，中控加装的十寸大屏有点斜，导致后排看屏幕有色彩不对。</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18793</th>\n",
       "      <td>投诉</td>\n",
       "      <td>长安CS75 PLUS涡轮增压器漏油维修未解决</td>\n",
       "      <td>车质网</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96265</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>油耗偏高，噪音偏大</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114096</th>\n",
       "      <td>表扬</td>\n",
       "      <td>外观好看，底盘扎实，内饰做工精细，乘坐空间和储物空间布局合理，特别后排中间凸起不高不影响中间...</td>\n",
       "      <td>开源数据集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69909</th>\n",
       "      <td>中性</td>\n",
       "      <td>看到最后一句快看哭了......(我慢慢变成了我喜欢的男孩子的样子......</td>\n",
       "      <td>COLDataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        情感                                                 语句          来源\n",
       "100699  抱怨  想了半天还真没有什么吐槽的地方，车身高度不够，后排乘客身子长的会顶头，后备箱空间略小（还是那...       开源数据集\n",
       "99496   抱怨                                      没有mkx的持钥匙走近亮灯       开源数据集\n",
       "15042   投诉                                  一汽丰田皇冠仪表台老化反光影响视线         车质网\n",
       "106097  表扬  我和媳妇都是两厢控，最满意的就是320的外观了，适合年轻人时尚又不失大气，白颜色非常靓丽配合...       开源数据集\n",
       "115342  表扬                               空间大，相对来说舒适性较好，性比价好些。       开源数据集\n",
       "80088   抱怨                  隔音不是很好，中控加装的十寸大屏有点斜，导致后排看屏幕有色彩不对。       开源数据集\n",
       "18793   投诉                            长安CS75 PLUS涡轮增压器漏油维修未解决         车质网\n",
       "96265   抱怨                                          油耗偏高，噪音偏大       开源数据集\n",
       "114096  表扬  外观好看，底盘扎实，内饰做工精细，乘坐空间和储物空间布局合理，特别后排中间凸起不高不影响中间...       开源数据集\n",
       "69909   中性            看到最后一句快看哭了......(我慢慢变成了我喜欢的男孩子的样子......  COLDataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('./data/data_train.xlsx')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca678b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数:(142834, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'样本数:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cb2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别：\n",
      "抱怨    37153\n",
      "表扬    35101\n",
      "投诉    30508\n",
      "中性    21881\n",
      "辱骂    18191\n",
      "Name: 情感, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '类别分布')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"类别：\\n{df['情感'].value_counts()}\")\n",
    "\n",
    "plt.figure(figsize = (6, 6), dpi = 60)\n",
    "df['情感'].value_counts().plot.pie(autopct = '%1.1f%%', textprops = {'fontsize': 16}).set_title('类别分布')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3df67ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值分布：\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"缺失值分布：\\n {df['语句'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6acb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>情感</th>\n",
       "      <th>语句</th>\n",
       "      <th>来源</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36723</th>\n",
       "      <td>中性</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLDataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52766</th>\n",
       "      <td>辱骂</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLDataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62669</th>\n",
       "      <td>辱骂</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLDataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       情感   语句          来源\n",
       "36723  中性  NaN  COLDataset\n",
       "52766  辱骂  NaN  COLDataset\n",
       "62669  辱骂  NaN  COLDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['语句'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1012b81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值分布：\n",
      " 0\n",
      "样本数:(142831, 3)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(f\"缺失值分布：\\n {df['语句'].isna().sum()}\")\n",
    "print(f'样本数:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7b4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['语句'].iloc[36723]\n",
    "# # df['语句'][36723]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d7eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重复值：\n",
      " 10743，比例：7.521%\n"
     ]
    }
   ],
   "source": [
    "print(f\"重复值：\\n {df['语句'].duplicated().sum()}，比例：{np.round(100 * df['语句'].duplicated().sum() / len(df), 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4580bc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88                      价格要是能再低点就更好了！京东应该多搞特价活动!快递的服务态度不是很好\n",
       "134          国产代工，电池部分设计不合理，装上电池后，与键盘平行的那一面有一点点高起来，有可能与屏接触。\n",
       "194       我在买之前就看到很多人说装系统麻烦，果然不只一点点烦啊，一开始硬盘没找到。。。找了半天，不过...\n",
       "262       六心电池装在后面突出一大块影响美观，速度慢机器有点卡，放歌一停顿一停顿，重量比一般的上网本重...\n",
       "274       发热是普遍的，不过从07年我就开始使用的6515b的情况看，这个热是没有带来任何问题的，你不...\n",
       "                                ...                        \n",
       "142042                      动力稍稍有点不足，油耗有点小高，不知道是不是我不太会开的缘故。\n",
       "142048    感觉北汽幻速车噪音还是有点的，不过慢慢习惯了就好了，特别是车子起步时候，踩上油门时候有明显感...\n",
       "142056    最不满意的就是车子的动力还是有点不那么够用，一般家用的话应该是可以了，主要我是用来拉客偶尔也...\n",
       "142179    虽然我是新手，但是油耗对于1.5l排量的车确实高，百公里差不多9个油，还有就是挂2档的时候，...\n",
       "142552                                                 暂时没有\n",
       "Name: 语句, Length: 10743, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['语句'].duplicated()]['语句']  # 好奇怪？为啥会重复？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a81c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理“6666”纯数字的文本\n",
    "def handle_int(content):\n",
    "    if type(content) == int:\n",
    "        return str(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90dde84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(content):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b1563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['语句'] = df['语句'].apply(lambda x: handle_int(x))\n",
    "df['语句'] = df['语句'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a01ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    37153\n",
       "3    35101\n",
       "1    30508\n",
       "0    21880\n",
       "4    18189\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = LabelEncoder().fit(df['情感'])\n",
    "df['label'] = labels.transform(df['情感'])\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bec8f3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIGHAN2005_PKU_CONVSEG': 'https://file.hankcs.com/hanlp/tok/sighan2005-pku-convseg_20200110_153722.zip',\n",
       " 'SIGHAN2005_MSR_CONVSEG': 'https://file.hankcs.com/hanlp/tok/convseg-msr-nocrf-noembed_20200110_153524.zip',\n",
       " 'CTB6_CONVSEG': 'https://file.hankcs.com/hanlp/tok/ctb6_convseg_nowe_nocrf_20200110_004046.zip',\n",
       " 'PKU_NAME_MERGED_SIX_MONTHS_CONVSEG': 'https://file.hankcs.com/hanlp/tok/pku98_6m_conv_ngram_20200110_134736.zip',\n",
       " 'LARGE_ALBERT_BASE': 'https://file.hankcs.com/hanlp/tok/large_corpus_cws_albert_base_20211228_160926.zip',\n",
       " 'SIGHAN2005_PKU_BERT_BASE_ZH': 'https://file.hankcs.com/hanlp/tok/sighan2005_pku_bert_base_zh_20201231_141130.zip',\n",
       " 'COARSE_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/tok/coarse_electra_small_20220616_012050.zip',\n",
       " 'FINE_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip',\n",
       " 'CTB9_TOK_ELECTRA_SMALL': 'https://file.hankcs.com/hanlp/tok/ctb9_electra_small_20220215_205427.zip',\n",
       " 'CTB9_TOK_ELECTRA_BASE': 'http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_20220426_111949.zip',\n",
       " 'CTB9_TOK_ELECTRA_BASE_CRF': 'http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_crf_20220426_161255.zip',\n",
       " 'MSR_TOK_ELECTRA_BASE_CRF': 'http://download.hanlp.com/tok/extra/msra_crf_electra_base_20220507_113936.zip',\n",
       " 'UD_TOK_MMINILMV2L6': 'https://file.hankcs.com/hanlp/tok/ud_tok_mMiniLMv2L6_no_space_mul_20220619_091824.zip',\n",
       " 'UD_TOK_MMINILMV2L12': 'https://file.hankcs.com/hanlp/tok/ud_tok_mMiniLMv2L12_no_space_mul_20220619_091159.zip'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanlp.pretrained.tok.ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f693d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8975a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(content):\n",
    "    seg_list = tok(content)\n",
    "    return ' '.join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：这里会运行很久\n",
    "df['text_splited'] = df['语句'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2122a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "text_tfidf = tfidf.fit_transform(df['text_splited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8438a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "89218908",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(random_state = RANDOM_SEED),\n",
    "    'RidgeClassifier': RidgeClassifier(random_state = RANDOM_SEED)\n",
    "}\n",
    "\n",
    "lr_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.25, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
    "    'max_iter': [50, 100, 200]\n",
    "}\n",
    "\n",
    "rd_grid = {\n",
    "    'alpha': [0.25, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
    "    'max_iter': [50, 100, 200]\n",
    "}\n",
    "\n",
    "grid_search_paramters = {\n",
    "    'LogisticRegression': lr_grid,\n",
    "    'RidgeClassifier': rd_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a3ff5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_tfidf\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2)\n",
    "X_train_grid, X_val_grid, y_train_grid, y_val_grid = train_test_split(X_train, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0d3d1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_test = LogisticRegression()\n",
    "# clf_test.fit(X_test[:100], y_test[:100])\n",
    "# clf_test.predict(X_test[100: 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bde468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "clf_best_params = classifiers.copy()\n",
    "valid_scores = pd.DataFrame({'Classifier': classifiers.keys(), 'Validation accuracy': np.zeros(len(classifiers)), 'Training time (mins):': np.zeros(len(classifiers))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e346df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.89486273        nan 0.89721598        nan 0.89725487\n",
      "        nan 0.89843148        nan 0.90452853        nan 0.90479107\n",
      "        nan 0.90104727        nan 0.90738743        nan 0.90782501\n",
      "        nan 0.90240865        nan 0.90911832        nan 0.9096337\n",
      "        nan 0.90228228        nan 0.91003239        nan 0.91020741\n",
      "        nan 0.90186412        nan 0.91030466        nan 0.91090755]\n",
      "  warnings.warn(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Training time (mins): 5.79\n",
      "\n",
      "Model: RidgeClassifier\n",
      "Training time (mins): 0.968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, classifier in classifiers.items():\n",
    "    start = time.time()\n",
    "    \n",
    "    clf = GridSearchCV(estimator = classifier, param_grid = grid_search_paramters[key], n_jobs = -1, cv = None)\n",
    "    clf.fit(X_train_grid, y_train_grid)\n",
    "    valid_scores.iloc[i, 1] = clf.score(X_val_grid, y_val_grid)\n",
    "    \n",
    "    clf_best_params[key] = clf.best_params_\n",
    "    \n",
    "    stop = time.time()\n",
    "    valid_scores.iloc[i, 2] = np.round((stop - start) / 60, 3)\n",
    "    \n",
    "    print(f'Model: {key}')\n",
    "    print(f'Training time (mins): {valid_scores.iloc[i, 2]}')\n",
    "    print('')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5021dfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Validation accuracy</th>\n",
       "      <th>Training time (mins):</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.911875</td>\n",
       "      <td>5.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.908375</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Classifier  Validation accuracy  Training time (mins):\n",
       "0  LogisticRegression             0.911875                  5.790\n",
       "1     RidgeClassifier             0.908375                  0.968"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "de7a4cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'C': 1.5, 'max_iter': 200, 'penalty': 'l2'},\n",
       " 'RidgeClassifier': {'alpha': 1.5, 'max_iter': 50}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f049c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(**clf_best_params['LogisticRegression'], random_state = RANDOM_SEED)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5b6ef3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Average validation accuracy: 91.313%\n",
      "Training time (mins): (3.0, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 10\n",
    "# preds = np.zeros(len(X_test))\n",
    "X_final = X_train\n",
    "y_final = np.array(y_train)\n",
    "for key, classifier in best_classifiers.items():\n",
    "    start = time.time()\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = RANDOM_SEED)\n",
    "    score = 0\n",
    "    for fold , (train_idx, val_idx) in enumerate(cv.split(X_final, y_final)):\n",
    "        X_train_cv, X_valid_cv = X_final[train_idx], X_final[val_idx]\n",
    "        y_train_cv, y_valid_cv = y_final[train_idx], y_final[val_idx]\n",
    "        \n",
    "        classifier.fit(X_train_cv, y_train_cv)\n",
    "        score += classifier.score(X_valid_cv, y_valid_cv)\n",
    "#         preds += classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    score = score / FOLDS\n",
    "    stop = time.time()\n",
    "    \n",
    "    print(f'Model: {key}')\n",
    "    print(f'Average validation accuracy: {np.round(100 * score, 3)}%')\n",
    "    print(f'Training time (mins): {np.round((stop - start) / 60, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69c9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5014dfeb",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5899a2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x216ca69c370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf8bb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>情感</th>\n",
       "      <th>语句</th>\n",
       "      <th>来源</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>唯一感觉不足的是硬盘就分了两个驱，每个100多G，要是能分成3个或更多就好了，呵呵</td>\n",
       "      <td>总部</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>六芯电池突出一块，影响美观；重量比一般上网本略重；触摸板反应不灵敏，左右键不太适应；没有配件...</td>\n",
       "      <td>总部</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>XP的驱动不好找！我的17号提的货，现在就降价了100元，而且还送杀毒软件！</td>\n",
       "      <td>总部</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>内存是2条，若升级，必须废除至少一条，有点浪费，该为用户着想一点。USB接口有点紧，内存藏盖...</td>\n",
       "      <td>总部</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>抱怨</td>\n",
       "      <td>摄像头不清楚，散热一般，说是商务型的，散热还是不怎么样！华硕屏的亮度都不好，声音小，音质比以...</td>\n",
       "      <td>总部</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   情感                                                 语句  来源  label\n",
       "0  抱怨          唯一感觉不足的是硬盘就分了两个驱，每个100多G，要是能分成3个或更多就好了，呵呵  总部      2\n",
       "1  抱怨  六芯电池突出一块，影响美观；重量比一般上网本略重；触摸板反应不灵敏，左右键不太适应；没有配件...  总部      2\n",
       "2  抱怨             XP的驱动不好找！我的17号提的货，现在就降价了100元，而且还送杀毒软件！  总部      2\n",
       "3  抱怨  内存是2条，若升级，必须废除至少一条，有点浪费，该为用户着想一点。USB接口有点紧，内存藏盖...  总部      2\n",
       "4  抱怨  摄像头不清楚，散热一般，说是商务型的，散热还是不怎么样！华硕屏的亮度都不好，声音小，音质比以...  总部      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a28b3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['语句'], \n",
    "                                                    df['label'], \n",
    "                                                    stratify = df['label'], \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = RANDOM_SEED)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, \n",
    "                                                y_test, \n",
    "                                                stratify = y_test, \n",
    "                                                test_size = 0.5,\n",
    "                                                random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87fa2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len_list = [len(x) for x in X_train]\n",
    "\n",
    "# plt.title('句子长度分布')\n",
    "# plt.ylabel('句子长度')\n",
    "# plt.bar(range(len(X_train)), sentenct_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "672a6766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.0000e+00, 2.4980e+03, 2.8600e+03, 5.0180e+03, 5.2970e+03,\n",
       "        5.2350e+03, 1.7601e+04, 1.4342e+04, 4.5890e+03, 4.8670e+03,\n",
       "        4.1930e+03, 2.5800e+03, 3.6830e+03, 2.4100e+03, 3.2240e+03,\n",
       "        3.0240e+03, 1.9110e+03, 2.6590e+03, 2.4390e+03, 1.4840e+03,\n",
       "        2.0930e+03, 1.9350e+03, 1.2060e+03, 1.6890e+03, 1.0720e+03,\n",
       "        1.5390e+03, 1.4630e+03, 9.4100e+02, 1.2970e+03, 1.2520e+03,\n",
       "        6.9800e+02, 1.0940e+03, 1.0430e+03, 6.6000e+02, 9.1300e+02,\n",
       "        9.2400e+02, 6.5400e+02, 9.6600e+02, 2.5400e+02, 4.3100e+02,\n",
       "        3.7700e+02, 2.3600e+02, 3.4300e+02, 3.1900e+02, 2.0300e+02,\n",
       "        2.6700e+02, 1.5300e+02, 9.8000e+01, 8.5000e+01, 6.0000e+00,\n",
       "        1.1000e+01, 5.0000e+00, 6.0000e+00, 8.0000e+00, 4.0000e+00,\n",
       "        2.0000e+00, 6.0000e+00, 1.0000e+01, 5.0000e+00, 2.2000e+01,\n",
       "        1.2000e+01, 5.0000e+00, 7.0000e+00, 7.0000e+00, 5.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "        4.0000e+00, 1.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  0.  ,   2.64,   5.28,   7.92,  10.56,  13.2 ,  15.84,  18.48,\n",
       "         21.12,  23.76,  26.4 ,  29.04,  31.68,  34.32,  36.96,  39.6 ,\n",
       "         42.24,  44.88,  47.52,  50.16,  52.8 ,  55.44,  58.08,  60.72,\n",
       "         63.36,  66.  ,  68.64,  71.28,  73.92,  76.56,  79.2 ,  81.84,\n",
       "         84.48,  87.12,  89.76,  92.4 ,  95.04,  97.68, 100.32, 102.96,\n",
       "        105.6 , 108.24, 110.88, 113.52, 116.16, 118.8 , 121.44, 124.08,\n",
       "        126.72, 129.36, 132.  , 134.64, 137.28, 139.92, 142.56, 145.2 ,\n",
       "        147.84, 150.48, 153.12, 155.76, 158.4 , 161.04, 163.68, 166.32,\n",
       "        168.96, 171.6 , 174.24, 176.88, 179.52, 182.16, 184.8 , 187.44,\n",
       "        190.08, 192.72, 195.36, 198.  , 200.64, 203.28, 205.92, 208.56,\n",
       "        211.2 , 213.84, 216.48, 219.12, 221.76, 224.4 , 227.04, 229.68,\n",
       "        232.32, 234.96, 237.6 , 240.24, 242.88, 245.52, 248.16, 250.8 ,\n",
       "        253.44, 256.08, 258.72, 261.36, 264.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('句子长度分布2')\n",
    "plt.xlabel('句子长度')\n",
    "plt.hist(sentence_len_list, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea68a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "train_encoding = tokenizer(list(X_train), truncation = True, padding = True, max_length = 100)\n",
    "val_encoding = tokenizer(list(X_val), truncation = True, padding = True, max_length = 100)\n",
    "test_encoding = tokenizer(list(X_test), truncation = True, padding = True, max_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a0d4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1290, 3247, 2140, 7716, 126, 5143, 6756, 3322, 5143, 5320, 1216, 5543, 680, 2146, 837, 679, 5016, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "tokenizer(X_train.iloc[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b0b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a64af48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TextDataset(train_encoding, y_train)\n",
    "val_dataset = TextDataset(val_encoding, y_val)\n",
    "test_dataset = TextDataset(test_encoding, y_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb0a951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 1506, 2472, 6619, 1052, 6756, 6716, 4024, 7481, 7961, 1259, 6206,\n",
       "         3724, 1322, 2157, 6237, 1104,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'labels': tensor(1, dtype=torch.int32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1afdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbcea478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(df['label'].value_counts())\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels = num_labels)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ecdd23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78157dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, batch_size, epoch_idx):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    n_examples = batch_size * len(data_loader)\n",
    "    \n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).to(torch.int64)  # 要转为int64，不然报错\n",
    "        outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        \n",
    "#         print(outputs)\n",
    "    \n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "#         print(f'loss: {outputs.loss}, {loss}')\n",
    "        \n",
    "        _, pred_idxs = torch.max(outputs.logits, dim = 1)\n",
    "        correct_predictions += torch.sum(pred_idxs == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        print(f'epoch: {epoch_idx} | {idx * batch_size} / {n_examples} | training loss: {loss}')\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39ec89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, data_loader, loss_fn, device, batch_size, epoch_idx):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    n_examples = batch_size * len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).to(torch.int64)  # 要转为int64，不然报错\n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            _, pred_idxs = torch.max(outputs.logits, dim = 1)\n",
    "            \n",
    "            correct_predictions += torch.sum(pred_idxs == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7571315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 0 | 0 / 114272 | training loss: 1.5561431646347046\n",
      "epoch: 0 | 32 / 114272 | training loss: 1.4505271911621094\n",
      "epoch: 0 | 64 / 114272 | training loss: 1.4607775211334229\n",
      "epoch: 0 | 96 / 114272 | training loss: 1.2168679237365723\n",
      "epoch: 0 | 128 / 114272 | training loss: 1.228115439414978\n",
      "epoch: 0 | 160 / 114272 | training loss: 1.0814472436904907\n",
      "epoch: 0 | 192 / 114272 | training loss: 0.895542562007904\n",
      "epoch: 0 | 224 / 114272 | training loss: 1.0388500690460205\n",
      "epoch: 0 | 256 / 114272 | training loss: 1.0344936847686768\n",
      "epoch: 0 | 288 / 114272 | training loss: 0.8923239707946777\n",
      "epoch: 0 | 320 / 114272 | training loss: 0.8056987524032593\n",
      "epoch: 0 | 352 / 114272 | training loss: 1.023438811302185\n",
      "epoch: 0 | 384 / 114272 | training loss: 0.8719208240509033\n",
      "epoch: 0 | 416 / 114272 | training loss: 0.8704617619514465\n",
      "epoch: 0 | 448 / 114272 | training loss: 0.8060319423675537\n",
      "epoch: 0 | 480 / 114272 | training loss: 0.8347871899604797\n",
      "epoch: 0 | 512 / 114272 | training loss: 0.7447221279144287\n",
      "epoch: 0 | 544 / 114272 | training loss: 0.6268383264541626\n",
      "epoch: 0 | 576 / 114272 | training loss: 0.5631864070892334\n",
      "epoch: 0 | 608 / 114272 | training loss: 0.5691554546356201\n",
      "epoch: 0 | 640 / 114272 | training loss: 0.5265389680862427\n",
      "epoch: 0 | 672 / 114272 | training loss: 0.4101247489452362\n",
      "epoch: 0 | 704 / 114272 | training loss: 0.3934984803199768\n",
      "epoch: 0 | 736 / 114272 | training loss: 0.5001625418663025\n",
      "epoch: 0 | 768 / 114272 | training loss: 0.240715891122818\n",
      "epoch: 0 | 800 / 114272 | training loss: 0.4865850508213043\n",
      "epoch: 0 | 832 / 114272 | training loss: 0.4230497479438782\n",
      "epoch: 0 | 864 / 114272 | training loss: 0.45514070987701416\n",
      "epoch: 0 | 896 / 114272 | training loss: 0.32493671774864197\n",
      "epoch: 0 | 928 / 114272 | training loss: 0.43259188532829285\n",
      "epoch: 0 | 960 / 114272 | training loss: 0.315229207277298\n",
      "epoch: 0 | 992 / 114272 | training loss: 0.350919634103775\n",
      "epoch: 0 | 1024 / 114272 | training loss: 0.25406932830810547\n",
      "epoch: 0 | 1056 / 114272 | training loss: 0.45931002497673035\n",
      "epoch: 0 | 1088 / 114272 | training loss: 0.4546251595020294\n",
      "epoch: 0 | 1120 / 114272 | training loss: 0.3119088411331177\n",
      "epoch: 0 | 1152 / 114272 | training loss: 0.3152097761631012\n",
      "epoch: 0 | 1184 / 114272 | training loss: 0.444124698638916\n",
      "epoch: 0 | 1216 / 114272 | training loss: 0.27156251668930054\n",
      "epoch: 0 | 1248 / 114272 | training loss: 0.5171365141868591\n",
      "epoch: 0 | 1280 / 114272 | training loss: 0.29847559332847595\n",
      "epoch: 0 | 1312 / 114272 | training loss: 0.3708817660808563\n",
      "epoch: 0 | 1344 / 114272 | training loss: 0.2829516530036926\n",
      "epoch: 0 | 1376 / 114272 | training loss: 0.5037484169006348\n",
      "epoch: 0 | 1408 / 114272 | training loss: 0.5187615752220154\n",
      "epoch: 0 | 1440 / 114272 | training loss: 0.27548909187316895\n",
      "epoch: 0 | 1472 / 114272 | training loss: 0.2981158494949341\n",
      "epoch: 0 | 1504 / 114272 | training loss: 0.23710918426513672\n",
      "epoch: 0 | 1536 / 114272 | training loss: 0.41828444600105286\n",
      "epoch: 0 | 1568 / 114272 | training loss: 0.22604098916053772\n",
      "epoch: 0 | 1600 / 114272 | training loss: 0.5856419801712036\n",
      "epoch: 0 | 1632 / 114272 | training loss: 0.14626093208789825\n",
      "epoch: 0 | 1664 / 114272 | training loss: 0.3034290671348572\n",
      "epoch: 0 | 1696 / 114272 | training loss: 0.20405980944633484\n",
      "epoch: 0 | 1728 / 114272 | training loss: 0.19179700314998627\n",
      "epoch: 0 | 1760 / 114272 | training loss: 0.42236611247062683\n",
      "epoch: 0 | 1792 / 114272 | training loss: 0.3280089795589447\n",
      "epoch: 0 | 1824 / 114272 | training loss: 0.1704026609659195\n",
      "epoch: 0 | 1856 / 114272 | training loss: 0.27884820103645325\n",
      "epoch: 0 | 1888 / 114272 | training loss: 0.4424005448818207\n",
      "epoch: 0 | 1920 / 114272 | training loss: 0.5466729998588562\n",
      "epoch: 0 | 1952 / 114272 | training loss: 0.375227689743042\n",
      "epoch: 0 | 1984 / 114272 | training loss: 0.2782808244228363\n",
      "epoch: 0 | 2016 / 114272 | training loss: 0.24173377454280853\n",
      "epoch: 0 | 2048 / 114272 | training loss: 0.19962362945079803\n",
      "epoch: 0 | 2080 / 114272 | training loss: 0.3690853416919708\n",
      "epoch: 0 | 2112 / 114272 | training loss: 0.2541489899158478\n",
      "epoch: 0 | 2144 / 114272 | training loss: 0.44288307428359985\n",
      "epoch: 0 | 2176 / 114272 | training loss: 0.22380657494068146\n",
      "epoch: 0 | 2208 / 114272 | training loss: 0.2681240439414978\n",
      "epoch: 0 | 2240 / 114272 | training loss: 0.18091653287410736\n",
      "epoch: 0 | 2272 / 114272 | training loss: 0.29085052013397217\n",
      "epoch: 0 | 2304 / 114272 | training loss: 0.2350098043680191\n",
      "epoch: 0 | 2336 / 114272 | training loss: 0.22716040909290314\n",
      "epoch: 0 | 2368 / 114272 | training loss: 0.16997352242469788\n",
      "epoch: 0 | 2400 / 114272 | training loss: 0.39327752590179443\n",
      "epoch: 0 | 2432 / 114272 | training loss: 0.1274944245815277\n",
      "epoch: 0 | 2464 / 114272 | training loss: 0.3430054485797882\n",
      "epoch: 0 | 2496 / 114272 | training loss: 0.3709805905818939\n",
      "epoch: 0 | 2528 / 114272 | training loss: 0.13706226646900177\n",
      "epoch: 0 | 2560 / 114272 | training loss: 0.3479559123516083\n",
      "epoch: 0 | 2592 / 114272 | training loss: 0.2674407362937927\n",
      "epoch: 0 | 2624 / 114272 | training loss: 0.41813063621520996\n",
      "epoch: 0 | 2656 / 114272 | training loss: 0.16128993034362793\n",
      "epoch: 0 | 2688 / 114272 | training loss: 0.20483677089214325\n",
      "epoch: 0 | 2720 / 114272 | training loss: 0.37044504284858704\n",
      "epoch: 0 | 2752 / 114272 | training loss: 0.41758906841278076\n",
      "epoch: 0 | 2784 / 114272 | training loss: 0.4936010241508484\n",
      "epoch: 0 | 2816 / 114272 | training loss: 0.10689610242843628\n",
      "epoch: 0 | 2848 / 114272 | training loss: 0.32701176404953003\n",
      "epoch: 0 | 2880 / 114272 | training loss: 0.5438087582588196\n",
      "epoch: 0 | 2912 / 114272 | training loss: 0.3618302345275879\n",
      "epoch: 0 | 2944 / 114272 | training loss: 0.2090000957250595\n",
      "epoch: 0 | 2976 / 114272 | training loss: 0.523995578289032\n",
      "epoch: 0 | 3008 / 114272 | training loss: 0.37028151750564575\n",
      "epoch: 0 | 3040 / 114272 | training loss: 0.37412530183792114\n",
      "epoch: 0 | 3072 / 114272 | training loss: 0.1964099407196045\n",
      "epoch: 0 | 3104 / 114272 | training loss: 0.40907955169677734\n",
      "epoch: 0 | 3136 / 114272 | training loss: 0.30585944652557373\n",
      "epoch: 0 | 3168 / 114272 | training loss: 0.19110442698001862\n",
      "epoch: 0 | 3200 / 114272 | training loss: 0.3608190715312958\n",
      "epoch: 0 | 3232 / 114272 | training loss: 0.17415474355220795\n",
      "epoch: 0 | 3264 / 114272 | training loss: 0.08624113351106644\n",
      "epoch: 0 | 3296 / 114272 | training loss: 0.24022254347801208\n",
      "epoch: 0 | 3328 / 114272 | training loss: 0.11209951341152191\n",
      "epoch: 0 | 3360 / 114272 | training loss: 0.25760066509246826\n",
      "epoch: 0 | 3392 / 114272 | training loss: 0.26663726568222046\n",
      "epoch: 0 | 3424 / 114272 | training loss: 0.21078546345233917\n",
      "epoch: 0 | 3456 / 114272 | training loss: 0.07286157459020615\n",
      "epoch: 0 | 3488 / 114272 | training loss: 0.325336217880249\n",
      "epoch: 0 | 3520 / 114272 | training loss: 0.21068204939365387\n",
      "epoch: 0 | 3552 / 114272 | training loss: 0.25802308320999146\n",
      "epoch: 0 | 3584 / 114272 | training loss: 0.23846706748008728\n",
      "epoch: 0 | 3616 / 114272 | training loss: 0.15393659472465515\n",
      "epoch: 0 | 3648 / 114272 | training loss: 0.4241839647293091\n",
      "epoch: 0 | 3680 / 114272 | training loss: 0.2874973714351654\n",
      "epoch: 0 | 3712 / 114272 | training loss: 0.19507119059562683\n",
      "epoch: 0 | 3744 / 114272 | training loss: 0.46255970001220703\n",
      "epoch: 0 | 3776 / 114272 | training loss: 0.3565513491630554\n",
      "epoch: 0 | 3808 / 114272 | training loss: 0.27089008688926697\n",
      "epoch: 0 | 3840 / 114272 | training loss: 0.3201946020126343\n",
      "epoch: 0 | 3872 / 114272 | training loss: 0.3791275918483734\n",
      "epoch: 0 | 3904 / 114272 | training loss: 0.24748267233371735\n",
      "epoch: 0 | 3936 / 114272 | training loss: 0.26247429847717285\n",
      "epoch: 0 | 3968 / 114272 | training loss: 0.10318249464035034\n",
      "epoch: 0 | 4000 / 114272 | training loss: 0.2657666504383087\n",
      "epoch: 0 | 4032 / 114272 | training loss: 0.25947338342666626\n",
      "epoch: 0 | 4064 / 114272 | training loss: 0.16448435187339783\n",
      "epoch: 0 | 4096 / 114272 | training loss: 0.11935814470052719\n",
      "epoch: 0 | 4128 / 114272 | training loss: 0.08324785530567169\n",
      "epoch: 0 | 4160 / 114272 | training loss: 0.21787554025650024\n",
      "epoch: 0 | 4192 / 114272 | training loss: 0.17997685074806213\n",
      "epoch: 0 | 4224 / 114272 | training loss: 0.1438744217157364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 4256 / 114272 | training loss: 0.04267065227031708\n",
      "epoch: 0 | 4288 / 114272 | training loss: 0.12842155992984772\n",
      "epoch: 0 | 4320 / 114272 | training loss: 0.32888442277908325\n",
      "epoch: 0 | 4352 / 114272 | training loss: 0.516974151134491\n",
      "epoch: 0 | 4384 / 114272 | training loss: 0.2270207554101944\n",
      "epoch: 0 | 4416 / 114272 | training loss: 0.20605988800525665\n",
      "epoch: 0 | 4448 / 114272 | training loss: 0.4096188247203827\n",
      "epoch: 0 | 4480 / 114272 | training loss: 0.22234629094600677\n",
      "epoch: 0 | 4512 / 114272 | training loss: 0.1151776984333992\n",
      "epoch: 0 | 4544 / 114272 | training loss: 0.1563706398010254\n",
      "epoch: 0 | 4576 / 114272 | training loss: 0.07679738104343414\n",
      "epoch: 0 | 4608 / 114272 | training loss: 0.26543208956718445\n",
      "epoch: 0 | 4640 / 114272 | training loss: 0.7574506998062134\n",
      "epoch: 0 | 4672 / 114272 | training loss: 0.13656292855739594\n",
      "epoch: 0 | 4704 / 114272 | training loss: 0.242642343044281\n",
      "epoch: 0 | 4736 / 114272 | training loss: 0.26994583010673523\n",
      "epoch: 0 | 4768 / 114272 | training loss: 0.13319295644760132\n",
      "epoch: 0 | 4800 / 114272 | training loss: 0.36629870533943176\n",
      "epoch: 0 | 4832 / 114272 | training loss: 0.25341302156448364\n",
      "epoch: 0 | 4864 / 114272 | training loss: 0.2381654679775238\n",
      "epoch: 0 | 4896 / 114272 | training loss: 0.1247933879494667\n",
      "epoch: 0 | 4928 / 114272 | training loss: 0.38782209157943726\n",
      "epoch: 0 | 4960 / 114272 | training loss: 0.08025980740785599\n",
      "epoch: 0 | 4992 / 114272 | training loss: 0.2648138999938965\n",
      "epoch: 0 | 5024 / 114272 | training loss: 0.09375191479921341\n",
      "epoch: 0 | 5056 / 114272 | training loss: 0.15258707106113434\n",
      "epoch: 0 | 5088 / 114272 | training loss: 0.16586361825466156\n",
      "epoch: 0 | 5120 / 114272 | training loss: 0.3428564667701721\n",
      "epoch: 0 | 5152 / 114272 | training loss: 0.3543463945388794\n",
      "epoch: 0 | 5184 / 114272 | training loss: 0.18401497602462769\n",
      "epoch: 0 | 5216 / 114272 | training loss: 0.5921173691749573\n",
      "epoch: 0 | 5248 / 114272 | training loss: 0.2344064563512802\n",
      "epoch: 0 | 5280 / 114272 | training loss: 0.039569683372974396\n",
      "epoch: 0 | 5312 / 114272 | training loss: 0.21500124037265778\n",
      "epoch: 0 | 5344 / 114272 | training loss: 0.332923024892807\n",
      "epoch: 0 | 5376 / 114272 | training loss: 0.4579930901527405\n",
      "epoch: 0 | 5408 / 114272 | training loss: 0.1573224663734436\n",
      "epoch: 0 | 5440 / 114272 | training loss: 0.1576327532529831\n",
      "epoch: 0 | 5472 / 114272 | training loss: 0.21684706211090088\n",
      "epoch: 0 | 5504 / 114272 | training loss: 0.25658464431762695\n",
      "epoch: 0 | 5536 / 114272 | training loss: 0.20298804342746735\n",
      "epoch: 0 | 5568 / 114272 | training loss: 0.10628966242074966\n",
      "epoch: 0 | 5600 / 114272 | training loss: 0.233209028840065\n",
      "epoch: 0 | 5632 / 114272 | training loss: 0.10304644703865051\n",
      "epoch: 0 | 5664 / 114272 | training loss: 0.42509257793426514\n",
      "epoch: 0 | 5696 / 114272 | training loss: 0.26572510600090027\n",
      "epoch: 0 | 5728 / 114272 | training loss: 0.16788722574710846\n",
      "epoch: 0 | 5760 / 114272 | training loss: 0.20782262086868286\n",
      "epoch: 0 | 5792 / 114272 | training loss: 0.24499627947807312\n",
      "epoch: 0 | 5824 / 114272 | training loss: 0.1116391122341156\n",
      "epoch: 0 | 5856 / 114272 | training loss: 0.2270730882883072\n",
      "epoch: 0 | 5888 / 114272 | training loss: 0.07687291502952576\n",
      "epoch: 0 | 5920 / 114272 | training loss: 0.08796949684619904\n",
      "epoch: 0 | 5952 / 114272 | training loss: 0.16163983941078186\n",
      "epoch: 0 | 5984 / 114272 | training loss: 0.11720813810825348\n",
      "epoch: 0 | 6016 / 114272 | training loss: 0.33786553144454956\n",
      "epoch: 0 | 6048 / 114272 | training loss: 0.2272827923297882\n",
      "epoch: 0 | 6080 / 114272 | training loss: 0.06356881558895111\n",
      "epoch: 0 | 6112 / 114272 | training loss: 0.24597151577472687\n",
      "epoch: 0 | 6144 / 114272 | training loss: 0.17226994037628174\n",
      "epoch: 0 | 6176 / 114272 | training loss: 0.3651038408279419\n",
      "epoch: 0 | 6208 / 114272 | training loss: 0.07966434955596924\n",
      "epoch: 0 | 6240 / 114272 | training loss: 0.31673896312713623\n",
      "epoch: 0 | 6272 / 114272 | training loss: 0.16114933788776398\n",
      "epoch: 0 | 6304 / 114272 | training loss: 0.022848961874842644\n",
      "epoch: 0 | 6336 / 114272 | training loss: 0.12016349285840988\n",
      "epoch: 0 | 6368 / 114272 | training loss: 0.08700823038816452\n",
      "epoch: 0 | 6400 / 114272 | training loss: 0.22630855441093445\n",
      "epoch: 0 | 6432 / 114272 | training loss: 0.1978529542684555\n",
      "epoch: 0 | 6464 / 114272 | training loss: 0.3448704481124878\n",
      "epoch: 0 | 6496 / 114272 | training loss: 0.1829967200756073\n",
      "epoch: 0 | 6528 / 114272 | training loss: 0.051278844475746155\n",
      "epoch: 0 | 6560 / 114272 | training loss: 0.22046300768852234\n",
      "epoch: 0 | 6592 / 114272 | training loss: 0.40426570177078247\n",
      "epoch: 0 | 6624 / 114272 | training loss: 0.3360689878463745\n",
      "epoch: 0 | 6656 / 114272 | training loss: 0.10740480571985245\n",
      "epoch: 0 | 6688 / 114272 | training loss: 0.3589174747467041\n",
      "epoch: 0 | 6720 / 114272 | training loss: 0.12388470768928528\n",
      "epoch: 0 | 6752 / 114272 | training loss: 0.33122268319129944\n",
      "epoch: 0 | 6784 / 114272 | training loss: 0.2651347815990448\n",
      "epoch: 0 | 6816 / 114272 | training loss: 0.12219445407390594\n",
      "epoch: 0 | 6848 / 114272 | training loss: 0.0676511749625206\n",
      "epoch: 0 | 6880 / 114272 | training loss: 0.20951229333877563\n",
      "epoch: 0 | 6912 / 114272 | training loss: 0.20406070351600647\n",
      "epoch: 0 | 6944 / 114272 | training loss: 0.15690076351165771\n",
      "epoch: 0 | 6976 / 114272 | training loss: 0.38230475783348083\n",
      "epoch: 0 | 7008 / 114272 | training loss: 0.09614353626966476\n",
      "epoch: 0 | 7040 / 114272 | training loss: 0.24640902876853943\n",
      "epoch: 0 | 7072 / 114272 | training loss: 0.16388757526874542\n",
      "epoch: 0 | 7104 / 114272 | training loss: 0.33122965693473816\n",
      "epoch: 0 | 7136 / 114272 | training loss: 0.10747936367988586\n",
      "epoch: 0 | 7168 / 114272 | training loss: 0.26397231221199036\n",
      "epoch: 0 | 7200 / 114272 | training loss: 0.3236415982246399\n",
      "epoch: 0 | 7232 / 114272 | training loss: 0.3983610272407532\n",
      "epoch: 0 | 7264 / 114272 | training loss: 0.23985309898853302\n",
      "epoch: 0 | 7296 / 114272 | training loss: 0.05350792407989502\n",
      "epoch: 0 | 7328 / 114272 | training loss: 0.4407007694244385\n",
      "epoch: 0 | 7360 / 114272 | training loss: 0.09614810347557068\n",
      "epoch: 0 | 7392 / 114272 | training loss: 0.2039075791835785\n",
      "epoch: 0 | 7424 / 114272 | training loss: 0.20493756234645844\n",
      "epoch: 0 | 7456 / 114272 | training loss: 0.43497514724731445\n",
      "epoch: 0 | 7488 / 114272 | training loss: 0.3374643623828888\n",
      "epoch: 0 | 7520 / 114272 | training loss: 0.1678936630487442\n",
      "epoch: 0 | 7552 / 114272 | training loss: 0.2602348327636719\n",
      "epoch: 0 | 7584 / 114272 | training loss: 0.1718742549419403\n",
      "epoch: 0 | 7616 / 114272 | training loss: 0.2201417237520218\n",
      "epoch: 0 | 7648 / 114272 | training loss: 0.35119226574897766\n",
      "epoch: 0 | 7680 / 114272 | training loss: 0.2665966749191284\n",
      "epoch: 0 | 7712 / 114272 | training loss: 0.6850837469100952\n",
      "epoch: 0 | 7744 / 114272 | training loss: 0.3991929590702057\n",
      "epoch: 0 | 7776 / 114272 | training loss: 0.3878847062587738\n",
      "epoch: 0 | 7808 / 114272 | training loss: 0.33496248722076416\n",
      "epoch: 0 | 7840 / 114272 | training loss: 0.3024185001850128\n",
      "epoch: 0 | 7872 / 114272 | training loss: 0.11300718039274216\n",
      "epoch: 0 | 7904 / 114272 | training loss: 0.375350683927536\n",
      "epoch: 0 | 7936 / 114272 | training loss: 0.11795312166213989\n",
      "epoch: 0 | 7968 / 114272 | training loss: 0.31415730714797974\n",
      "epoch: 0 | 8000 / 114272 | training loss: 0.3775079548358917\n",
      "epoch: 0 | 8032 / 114272 | training loss: 0.032725632190704346\n",
      "epoch: 0 | 8064 / 114272 | training loss: 0.29548344016075134\n",
      "epoch: 0 | 8096 / 114272 | training loss: 0.1231590211391449\n",
      "epoch: 0 | 8128 / 114272 | training loss: 0.23056794703006744\n",
      "epoch: 0 | 8160 / 114272 | training loss: 0.2611043453216553\n",
      "epoch: 0 | 8192 / 114272 | training loss: 0.2509211003780365\n",
      "epoch: 0 | 8224 / 114272 | training loss: 0.4050293564796448\n",
      "epoch: 0 | 8256 / 114272 | training loss: 0.20215368270874023\n",
      "epoch: 0 | 8288 / 114272 | training loss: 0.17409341037273407\n",
      "epoch: 0 | 8320 / 114272 | training loss: 0.09530620276927948\n",
      "epoch: 0 | 8352 / 114272 | training loss: 0.19527626037597656\n",
      "epoch: 0 | 8384 / 114272 | training loss: 0.228750541806221\n",
      "epoch: 0 | 8416 / 114272 | training loss: 0.17582187056541443\n",
      "epoch: 0 | 8448 / 114272 | training loss: 0.18365037441253662\n",
      "epoch: 0 | 8480 / 114272 | training loss: 0.272765189409256\n",
      "epoch: 0 | 8512 / 114272 | training loss: 0.2908758223056793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 8544 / 114272 | training loss: 0.26445651054382324\n",
      "epoch: 0 | 8576 / 114272 | training loss: 0.09239322692155838\n",
      "epoch: 0 | 8608 / 114272 | training loss: 0.2497396171092987\n",
      "epoch: 0 | 8640 / 114272 | training loss: 0.11092303693294525\n",
      "epoch: 0 | 8672 / 114272 | training loss: 0.17842723429203033\n",
      "epoch: 0 | 8704 / 114272 | training loss: 0.03436705097556114\n",
      "epoch: 0 | 8736 / 114272 | training loss: 0.5036190152168274\n",
      "epoch: 0 | 8768 / 114272 | training loss: 0.3586677610874176\n",
      "epoch: 0 | 8800 / 114272 | training loss: 0.2697771191596985\n",
      "epoch: 0 | 8832 / 114272 | training loss: 0.14258591830730438\n",
      "epoch: 0 | 8864 / 114272 | training loss: 0.05544507876038551\n",
      "epoch: 0 | 8896 / 114272 | training loss: 0.13139231503009796\n",
      "epoch: 0 | 8928 / 114272 | training loss: 0.09750055521726608\n",
      "epoch: 0 | 8960 / 114272 | training loss: 0.28331440687179565\n",
      "epoch: 0 | 8992 / 114272 | training loss: 0.19838294386863708\n",
      "epoch: 0 | 9024 / 114272 | training loss: 0.08339394629001617\n",
      "epoch: 0 | 9056 / 114272 | training loss: 0.43092969059944153\n",
      "epoch: 0 | 9088 / 114272 | training loss: 0.15489642322063446\n",
      "epoch: 0 | 9120 / 114272 | training loss: 0.2235146462917328\n",
      "epoch: 0 | 9152 / 114272 | training loss: 0.14827492833137512\n",
      "epoch: 0 | 9184 / 114272 | training loss: 0.18915030360221863\n",
      "epoch: 0 | 9216 / 114272 | training loss: 0.07286972552537918\n",
      "epoch: 0 | 9248 / 114272 | training loss: 0.15639503300189972\n",
      "epoch: 0 | 9280 / 114272 | training loss: 0.09907733649015427\n",
      "epoch: 0 | 9312 / 114272 | training loss: 0.09449886530637741\n",
      "epoch: 0 | 9344 / 114272 | training loss: 0.22041988372802734\n",
      "epoch: 0 | 9376 / 114272 | training loss: 0.18257910013198853\n",
      "epoch: 0 | 9408 / 114272 | training loss: 0.09293219447135925\n",
      "epoch: 0 | 9440 / 114272 | training loss: 0.03999155014753342\n",
      "epoch: 0 | 9472 / 114272 | training loss: 0.12731213867664337\n",
      "epoch: 0 | 9504 / 114272 | training loss: 0.1667458415031433\n",
      "epoch: 0 | 9536 / 114272 | training loss: 0.36137381196022034\n",
      "epoch: 0 | 9568 / 114272 | training loss: 0.023946251720190048\n",
      "epoch: 0 | 9600 / 114272 | training loss: 0.30258867144584656\n",
      "epoch: 0 | 9632 / 114272 | training loss: 0.16341571509838104\n",
      "epoch: 0 | 9664 / 114272 | training loss: 0.24569986760616302\n",
      "epoch: 0 | 9696 / 114272 | training loss: 0.20765691995620728\n",
      "epoch: 0 | 9728 / 114272 | training loss: 0.2590094804763794\n",
      "epoch: 0 | 9760 / 114272 | training loss: 0.49133503437042236\n",
      "epoch: 0 | 9792 / 114272 | training loss: 0.31304502487182617\n",
      "epoch: 0 | 9824 / 114272 | training loss: 0.36851057410240173\n",
      "epoch: 0 | 9856 / 114272 | training loss: 0.24357086420059204\n",
      "epoch: 0 | 9888 / 114272 | training loss: 0.2384067177772522\n",
      "epoch: 0 | 9920 / 114272 | training loss: 0.16087062656879425\n",
      "epoch: 0 | 9952 / 114272 | training loss: 0.213227316737175\n",
      "epoch: 0 | 9984 / 114272 | training loss: 0.17982132732868195\n",
      "epoch: 0 | 10016 / 114272 | training loss: 0.05050639435648918\n",
      "epoch: 0 | 10048 / 114272 | training loss: 0.18436861038208008\n",
      "epoch: 0 | 10080 / 114272 | training loss: 0.5086317658424377\n",
      "epoch: 0 | 10112 / 114272 | training loss: 0.292537122964859\n",
      "epoch: 0 | 10144 / 114272 | training loss: 0.25377652049064636\n",
      "epoch: 0 | 10176 / 114272 | training loss: 0.6148038506507874\n",
      "epoch: 0 | 10208 / 114272 | training loss: 0.2699104845523834\n",
      "epoch: 0 | 10240 / 114272 | training loss: 0.06229938194155693\n",
      "epoch: 0 | 10272 / 114272 | training loss: 0.1633956879377365\n",
      "epoch: 0 | 10304 / 114272 | training loss: 0.37204810976982117\n",
      "epoch: 0 | 10336 / 114272 | training loss: 0.2478426694869995\n",
      "epoch: 0 | 10368 / 114272 | training loss: 0.38124361634254456\n",
      "epoch: 0 | 10400 / 114272 | training loss: 0.08357526361942291\n",
      "epoch: 0 | 10432 / 114272 | training loss: 0.1545073539018631\n",
      "epoch: 0 | 10464 / 114272 | training loss: 0.29334062337875366\n",
      "epoch: 0 | 10496 / 114272 | training loss: 0.15270891785621643\n",
      "epoch: 0 | 10528 / 114272 | training loss: 0.3965865671634674\n",
      "epoch: 0 | 10560 / 114272 | training loss: 0.19806461036205292\n",
      "epoch: 0 | 10592 / 114272 | training loss: 0.05411846935749054\n",
      "epoch: 0 | 10624 / 114272 | training loss: 0.12287292629480362\n",
      "epoch: 0 | 10656 / 114272 | training loss: 0.14379288256168365\n",
      "epoch: 0 | 10688 / 114272 | training loss: 0.10292712599039078\n",
      "epoch: 0 | 10720 / 114272 | training loss: 0.16680839657783508\n",
      "epoch: 0 | 10752 / 114272 | training loss: 0.11548028141260147\n",
      "epoch: 0 | 10784 / 114272 | training loss: 0.25472593307495117\n",
      "epoch: 0 | 10816 / 114272 | training loss: 0.31212180852890015\n",
      "epoch: 0 | 10848 / 114272 | training loss: 0.06263059377670288\n",
      "epoch: 0 | 10880 / 114272 | training loss: 0.149882972240448\n",
      "epoch: 0 | 10912 / 114272 | training loss: 0.30741748213768005\n",
      "epoch: 0 | 10944 / 114272 | training loss: 0.39209216833114624\n",
      "epoch: 0 | 10976 / 114272 | training loss: 0.24994154274463654\n",
      "epoch: 0 | 11008 / 114272 | training loss: 0.09123041480779648\n",
      "epoch: 0 | 11040 / 114272 | training loss: 0.533093273639679\n",
      "epoch: 0 | 11072 / 114272 | training loss: 0.23049159348011017\n",
      "epoch: 0 | 11104 / 114272 | training loss: 0.2535610795021057\n",
      "epoch: 0 | 11136 / 114272 | training loss: 0.32119521498680115\n",
      "epoch: 0 | 11168 / 114272 | training loss: 0.262109637260437\n",
      "epoch: 0 | 11200 / 114272 | training loss: 0.15224191546440125\n",
      "epoch: 0 | 11232 / 114272 | training loss: 0.24089311063289642\n",
      "epoch: 0 | 11264 / 114272 | training loss: 0.3216778039932251\n",
      "epoch: 0 | 11296 / 114272 | training loss: 0.19659759104251862\n",
      "epoch: 0 | 11328 / 114272 | training loss: 0.0545174814760685\n",
      "epoch: 0 | 11360 / 114272 | training loss: 0.25768566131591797\n",
      "epoch: 0 | 11392 / 114272 | training loss: 0.07266465574502945\n",
      "epoch: 0 | 11424 / 114272 | training loss: 0.3384661078453064\n",
      "epoch: 0 | 11456 / 114272 | training loss: 0.519639253616333\n",
      "epoch: 0 | 11488 / 114272 | training loss: 0.2817249298095703\n",
      "epoch: 0 | 11520 / 114272 | training loss: 0.09639120101928711\n",
      "epoch: 0 | 11552 / 114272 | training loss: 0.2342837154865265\n",
      "epoch: 0 | 11584 / 114272 | training loss: 0.5964635014533997\n",
      "epoch: 0 | 11616 / 114272 | training loss: 0.22580550611019135\n",
      "epoch: 0 | 11648 / 114272 | training loss: 0.1473105400800705\n",
      "epoch: 0 | 11680 / 114272 | training loss: 0.3175753951072693\n",
      "epoch: 0 | 11712 / 114272 | training loss: 0.3196835219860077\n",
      "epoch: 0 | 11744 / 114272 | training loss: 0.18959301710128784\n",
      "epoch: 0 | 11776 / 114272 | training loss: 0.2750187814235687\n",
      "epoch: 0 | 11808 / 114272 | training loss: 0.10144731402397156\n",
      "epoch: 0 | 11840 / 114272 | training loss: 0.27608320116996765\n",
      "epoch: 0 | 11872 / 114272 | training loss: 0.3036825358867645\n",
      "epoch: 0 | 11904 / 114272 | training loss: 0.32734549045562744\n",
      "epoch: 0 | 11936 / 114272 | training loss: 0.07817034423351288\n",
      "epoch: 0 | 11968 / 114272 | training loss: 0.2473183274269104\n",
      "epoch: 0 | 12000 / 114272 | training loss: 0.15922759473323822\n",
      "epoch: 0 | 12032 / 114272 | training loss: 0.24824628233909607\n",
      "epoch: 0 | 12064 / 114272 | training loss: 0.08236304670572281\n",
      "epoch: 0 | 12096 / 114272 | training loss: 0.25134703516960144\n",
      "epoch: 0 | 12128 / 114272 | training loss: 0.44470328092575073\n",
      "epoch: 0 | 12160 / 114272 | training loss: 0.08925291895866394\n",
      "epoch: 0 | 12192 / 114272 | training loss: 0.29620230197906494\n",
      "epoch: 0 | 12224 / 114272 | training loss: 0.18933317065238953\n",
      "epoch: 0 | 12256 / 114272 | training loss: 0.25716814398765564\n",
      "epoch: 0 | 12288 / 114272 | training loss: 0.19928647577762604\n",
      "epoch: 0 | 12320 / 114272 | training loss: 0.14539365470409393\n",
      "epoch: 0 | 12352 / 114272 | training loss: 0.175367072224617\n",
      "epoch: 0 | 12384 / 114272 | training loss: 0.3623407185077667\n",
      "epoch: 0 | 12416 / 114272 | training loss: 0.12144328653812408\n",
      "epoch: 0 | 12448 / 114272 | training loss: 0.32758408784866333\n",
      "epoch: 0 | 12480 / 114272 | training loss: 0.31390300393104553\n",
      "epoch: 0 | 12512 / 114272 | training loss: 0.23448292911052704\n",
      "epoch: 0 | 12544 / 114272 | training loss: 0.038899920880794525\n",
      "epoch: 0 | 12576 / 114272 | training loss: 0.21540533006191254\n",
      "epoch: 0 | 12608 / 114272 | training loss: 0.055838730186223984\n",
      "epoch: 0 | 12640 / 114272 | training loss: 0.23704145848751068\n",
      "epoch: 0 | 12672 / 114272 | training loss: 0.23639148473739624\n",
      "epoch: 0 | 12704 / 114272 | training loss: 0.22423116862773895\n",
      "epoch: 0 | 12736 / 114272 | training loss: 0.13560739159584045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 12768 / 114272 | training loss: 0.37068668007850647\n",
      "epoch: 0 | 12800 / 114272 | training loss: 0.26181796193122864\n",
      "epoch: 0 | 12832 / 114272 | training loss: 0.05912419408559799\n",
      "epoch: 0 | 12864 / 114272 | training loss: 0.1212489902973175\n",
      "epoch: 0 | 12896 / 114272 | training loss: 0.06074461340904236\n",
      "epoch: 0 | 12928 / 114272 | training loss: 0.30498006939888\n",
      "epoch: 0 | 12960 / 114272 | training loss: 0.17435859143733978\n",
      "epoch: 0 | 12992 / 114272 | training loss: 0.05886908248066902\n",
      "epoch: 0 | 13024 / 114272 | training loss: 0.3908451795578003\n",
      "epoch: 0 | 13056 / 114272 | training loss: 0.26371046900749207\n",
      "epoch: 0 | 13088 / 114272 | training loss: 0.5762686729431152\n",
      "epoch: 0 | 13120 / 114272 | training loss: 0.21261677145957947\n",
      "epoch: 0 | 13152 / 114272 | training loss: 0.1911945939064026\n",
      "epoch: 0 | 13184 / 114272 | training loss: 0.044801805168390274\n",
      "epoch: 0 | 13216 / 114272 | training loss: 0.2305530309677124\n",
      "epoch: 0 | 13248 / 114272 | training loss: 0.40734851360321045\n",
      "epoch: 0 | 13280 / 114272 | training loss: 0.030541691929101944\n",
      "epoch: 0 | 13312 / 114272 | training loss: 0.16377870738506317\n",
      "epoch: 0 | 13344 / 114272 | training loss: 0.1649046689271927\n",
      "epoch: 0 | 13376 / 114272 | training loss: 0.03071993961930275\n",
      "epoch: 0 | 13408 / 114272 | training loss: 0.33176153898239136\n",
      "epoch: 0 | 13440 / 114272 | training loss: 0.13701209425926208\n",
      "epoch: 0 | 13472 / 114272 | training loss: 0.173468679189682\n",
      "epoch: 0 | 13504 / 114272 | training loss: 0.14925026893615723\n",
      "epoch: 0 | 13536 / 114272 | training loss: 0.07169132679700851\n",
      "epoch: 0 | 13568 / 114272 | training loss: 0.2631953954696655\n",
      "epoch: 0 | 13600 / 114272 | training loss: 0.03733029589056969\n",
      "epoch: 0 | 13632 / 114272 | training loss: 0.3181350827217102\n",
      "epoch: 0 | 13664 / 114272 | training loss: 0.2355855107307434\n",
      "epoch: 0 | 13696 / 114272 | training loss: 0.133992001414299\n",
      "epoch: 0 | 13728 / 114272 | training loss: 0.06822052597999573\n",
      "epoch: 0 | 13760 / 114272 | training loss: 0.1619647592306137\n",
      "epoch: 0 | 13792 / 114272 | training loss: 0.07560635358095169\n",
      "epoch: 0 | 13824 / 114272 | training loss: 0.4895687699317932\n",
      "epoch: 0 | 13856 / 114272 | training loss: 0.17917218804359436\n",
      "epoch: 0 | 13888 / 114272 | training loss: 0.27011725306510925\n",
      "epoch: 0 | 13920 / 114272 | training loss: 0.0632365420460701\n",
      "epoch: 0 | 13952 / 114272 | training loss: 0.12282665818929672\n",
      "epoch: 0 | 13984 / 114272 | training loss: 0.2965715825557709\n",
      "epoch: 0 | 14016 / 114272 | training loss: 0.12820309400558472\n",
      "epoch: 0 | 14048 / 114272 | training loss: 0.15812043845653534\n",
      "epoch: 0 | 14080 / 114272 | training loss: 0.1279434859752655\n",
      "epoch: 0 | 14112 / 114272 | training loss: 0.3306918144226074\n",
      "epoch: 0 | 14144 / 114272 | training loss: 0.15049047768115997\n",
      "epoch: 0 | 14176 / 114272 | training loss: 0.16349314153194427\n",
      "epoch: 0 | 14208 / 114272 | training loss: 0.17620103061199188\n",
      "epoch: 0 | 14240 / 114272 | training loss: 0.04677015542984009\n",
      "epoch: 0 | 14272 / 114272 | training loss: 0.19432993233203888\n",
      "epoch: 0 | 14304 / 114272 | training loss: 0.2827839255332947\n",
      "epoch: 0 | 14336 / 114272 | training loss: 0.1118747740983963\n",
      "epoch: 0 | 14368 / 114272 | training loss: 0.35926172137260437\n",
      "epoch: 0 | 14400 / 114272 | training loss: 0.09548230469226837\n",
      "epoch: 0 | 14432 / 114272 | training loss: 0.3449382483959198\n",
      "epoch: 0 | 14464 / 114272 | training loss: 0.1911180019378662\n",
      "epoch: 0 | 14496 / 114272 | training loss: 0.277437299489975\n",
      "epoch: 0 | 14528 / 114272 | training loss: 0.27239516377449036\n",
      "epoch: 0 | 14560 / 114272 | training loss: 0.04348985105752945\n",
      "epoch: 0 | 14592 / 114272 | training loss: 0.048710498958826065\n",
      "epoch: 0 | 14624 / 114272 | training loss: 0.10900142788887024\n",
      "epoch: 0 | 14656 / 114272 | training loss: 0.0421275869011879\n",
      "epoch: 0 | 14688 / 114272 | training loss: 0.32302728295326233\n",
      "epoch: 0 | 14720 / 114272 | training loss: 0.2197684347629547\n",
      "epoch: 0 | 14752 / 114272 | training loss: 0.0797286108136177\n",
      "epoch: 0 | 14784 / 114272 | training loss: 0.18386082351207733\n",
      "epoch: 0 | 14816 / 114272 | training loss: 0.25260818004608154\n",
      "epoch: 0 | 14848 / 114272 | training loss: 0.03288107365369797\n",
      "epoch: 0 | 14880 / 114272 | training loss: 0.19357918202877045\n",
      "epoch: 0 | 14912 / 114272 | training loss: 0.32500705122947693\n",
      "epoch: 0 | 14944 / 114272 | training loss: 0.2760981321334839\n",
      "epoch: 0 | 14976 / 114272 | training loss: 0.11327003687620163\n",
      "epoch: 0 | 15008 / 114272 | training loss: 0.3493906557559967\n",
      "epoch: 0 | 15040 / 114272 | training loss: 0.0305107943713665\n",
      "epoch: 0 | 15072 / 114272 | training loss: 0.2855958342552185\n",
      "epoch: 0 | 15104 / 114272 | training loss: 0.22843582928180695\n",
      "epoch: 0 | 15136 / 114272 | training loss: 0.31163647770881653\n",
      "epoch: 0 | 15168 / 114272 | training loss: 0.3104322552680969\n",
      "epoch: 0 | 15200 / 114272 | training loss: 0.21185359358787537\n",
      "epoch: 0 | 15232 / 114272 | training loss: 0.049288541078567505\n",
      "epoch: 0 | 15264 / 114272 | training loss: 0.061957962810993195\n",
      "epoch: 0 | 15296 / 114272 | training loss: 0.25982123613357544\n",
      "epoch: 0 | 15328 / 114272 | training loss: 0.03841670975089073\n",
      "epoch: 0 | 15360 / 114272 | training loss: 0.08264674991369247\n",
      "epoch: 0 | 15392 / 114272 | training loss: 0.021109741181135178\n",
      "epoch: 0 | 15424 / 114272 | training loss: 0.27447640895843506\n",
      "epoch: 0 | 15456 / 114272 | training loss: 0.24134084582328796\n",
      "epoch: 0 | 15488 / 114272 | training loss: 0.24170014262199402\n",
      "epoch: 0 | 15520 / 114272 | training loss: 0.22492893040180206\n",
      "epoch: 0 | 15552 / 114272 | training loss: 0.2831190228462219\n",
      "epoch: 0 | 15584 / 114272 | training loss: 0.2900737524032593\n",
      "epoch: 0 | 15616 / 114272 | training loss: 0.13891619443893433\n",
      "epoch: 0 | 15648 / 114272 | training loss: 0.14761513471603394\n",
      "epoch: 0 | 15680 / 114272 | training loss: 0.3058106601238251\n",
      "epoch: 0 | 15712 / 114272 | training loss: 0.4861711859703064\n",
      "epoch: 0 | 15744 / 114272 | training loss: 0.14857469499111176\n",
      "epoch: 0 | 15776 / 114272 | training loss: 0.1602461338043213\n",
      "epoch: 0 | 15808 / 114272 | training loss: 0.2804487943649292\n",
      "epoch: 0 | 15840 / 114272 | training loss: 0.3506799042224884\n",
      "epoch: 0 | 15872 / 114272 | training loss: 0.18904148042201996\n",
      "epoch: 0 | 15904 / 114272 | training loss: 0.280289888381958\n",
      "epoch: 0 | 15936 / 114272 | training loss: 0.17880897223949432\n",
      "epoch: 0 | 15968 / 114272 | training loss: 0.4537540674209595\n",
      "epoch: 0 | 16000 / 114272 | training loss: 0.16344760358333588\n",
      "epoch: 0 | 16032 / 114272 | training loss: 0.3574483096599579\n",
      "epoch: 0 | 16064 / 114272 | training loss: 0.2278020828962326\n",
      "epoch: 0 | 16096 / 114272 | training loss: 0.3214893639087677\n",
      "epoch: 0 | 16128 / 114272 | training loss: 0.1801038682460785\n",
      "epoch: 0 | 16160 / 114272 | training loss: 0.08721040189266205\n",
      "epoch: 0 | 16192 / 114272 | training loss: 0.23841896653175354\n",
      "epoch: 0 | 16224 / 114272 | training loss: 0.157399982213974\n",
      "epoch: 0 | 16256 / 114272 | training loss: 0.13221660256385803\n",
      "epoch: 0 | 16288 / 114272 | training loss: 0.23962128162384033\n",
      "epoch: 0 | 16320 / 114272 | training loss: 0.1955896019935608\n",
      "epoch: 0 | 16352 / 114272 | training loss: 0.06453457474708557\n",
      "epoch: 0 | 16384 / 114272 | training loss: 0.09150761365890503\n",
      "epoch: 0 | 16416 / 114272 | training loss: 0.12260839343070984\n",
      "epoch: 0 | 16448 / 114272 | training loss: 0.3881668448448181\n",
      "epoch: 0 | 16480 / 114272 | training loss: 0.2179565131664276\n",
      "epoch: 0 | 16512 / 114272 | training loss: 0.1266428828239441\n",
      "epoch: 0 | 16544 / 114272 | training loss: 0.20003587007522583\n",
      "epoch: 0 | 16576 / 114272 | training loss: 0.10242821276187897\n",
      "epoch: 0 | 16608 / 114272 | training loss: 0.1613580584526062\n",
      "epoch: 0 | 16640 / 114272 | training loss: 0.17222289741039276\n",
      "epoch: 0 | 16672 / 114272 | training loss: 0.11193359643220901\n",
      "epoch: 0 | 16704 / 114272 | training loss: 0.14195072650909424\n",
      "epoch: 0 | 16736 / 114272 | training loss: 0.08982226252555847\n",
      "epoch: 0 | 16768 / 114272 | training loss: 0.22010213136672974\n",
      "epoch: 0 | 16800 / 114272 | training loss: 0.24962586164474487\n",
      "epoch: 0 | 16832 / 114272 | training loss: 0.07300114631652832\n",
      "epoch: 0 | 16864 / 114272 | training loss: 0.24815739691257477\n",
      "epoch: 0 | 16896 / 114272 | training loss: 0.1794585883617401\n",
      "epoch: 0 | 16928 / 114272 | training loss: 0.17107197642326355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 16960 / 114272 | training loss: 0.21688397228717804\n",
      "epoch: 0 | 16992 / 114272 | training loss: 0.2980193495750427\n",
      "epoch: 0 | 17024 / 114272 | training loss: 0.06663262844085693\n",
      "epoch: 0 | 17056 / 114272 | training loss: 0.10445386916399002\n",
      "epoch: 0 | 17088 / 114272 | training loss: 0.1289837807416916\n",
      "epoch: 0 | 17120 / 114272 | training loss: 0.1386079639196396\n",
      "epoch: 0 | 17152 / 114272 | training loss: 0.2799355685710907\n",
      "epoch: 0 | 17184 / 114272 | training loss: 0.22770912945270538\n",
      "epoch: 0 | 17216 / 114272 | training loss: 0.40306082367897034\n",
      "epoch: 0 | 17248 / 114272 | training loss: 0.1703464686870575\n",
      "epoch: 0 | 17280 / 114272 | training loss: 0.13734102249145508\n",
      "epoch: 0 | 17312 / 114272 | training loss: 0.15192285180091858\n",
      "epoch: 0 | 17344 / 114272 | training loss: 0.14281825721263885\n",
      "epoch: 0 | 17376 / 114272 | training loss: 0.060792721807956696\n",
      "epoch: 0 | 17408 / 114272 | training loss: 0.27206721901893616\n",
      "epoch: 0 | 17440 / 114272 | training loss: 0.27075615525245667\n",
      "epoch: 0 | 17472 / 114272 | training loss: 0.20740695297718048\n",
      "epoch: 0 | 17504 / 114272 | training loss: 0.05561869218945503\n",
      "epoch: 0 | 17536 / 114272 | training loss: 0.19193321466445923\n",
      "epoch: 0 | 17568 / 114272 | training loss: 0.12249773740768433\n",
      "epoch: 0 | 17600 / 114272 | training loss: 0.08233900368213654\n",
      "epoch: 0 | 17632 / 114272 | training loss: 0.21223020553588867\n",
      "epoch: 0 | 17664 / 114272 | training loss: 0.17184123396873474\n",
      "epoch: 0 | 17696 / 114272 | training loss: 0.10412698984146118\n",
      "epoch: 0 | 17728 / 114272 | training loss: 0.9806949496269226\n",
      "epoch: 0 | 17760 / 114272 | training loss: 0.3290086090564728\n",
      "epoch: 0 | 17792 / 114272 | training loss: 0.10601215064525604\n",
      "epoch: 0 | 17824 / 114272 | training loss: 0.21292264759540558\n",
      "epoch: 0 | 17856 / 114272 | training loss: 0.23435284197330475\n",
      "epoch: 0 | 17888 / 114272 | training loss: 0.24470648169517517\n",
      "epoch: 0 | 17920 / 114272 | training loss: 0.10327311605215073\n",
      "epoch: 0 | 17952 / 114272 | training loss: 0.1929505318403244\n",
      "epoch: 0 | 17984 / 114272 | training loss: 0.2772822976112366\n",
      "epoch: 0 | 18016 / 114272 | training loss: 0.04899730533361435\n",
      "epoch: 0 | 18048 / 114272 | training loss: 0.12413258850574493\n",
      "epoch: 0 | 18080 / 114272 | training loss: 0.04644324630498886\n",
      "epoch: 0 | 18112 / 114272 | training loss: 0.3440474569797516\n",
      "epoch: 0 | 18144 / 114272 | training loss: 0.06002843752503395\n",
      "epoch: 0 | 18176 / 114272 | training loss: 0.1593734622001648\n",
      "epoch: 0 | 18208 / 114272 | training loss: 0.12196885049343109\n",
      "epoch: 0 | 18240 / 114272 | training loss: 0.5221254825592041\n",
      "epoch: 0 | 18272 / 114272 | training loss: 0.4065493047237396\n",
      "epoch: 0 | 18304 / 114272 | training loss: 0.15548938512802124\n",
      "epoch: 0 | 18336 / 114272 | training loss: 0.22195900976657867\n",
      "epoch: 0 | 18368 / 114272 | training loss: 0.22633413970470428\n",
      "epoch: 0 | 18400 / 114272 | training loss: 0.24921248853206635\n",
      "epoch: 0 | 18432 / 114272 | training loss: 0.2882523536682129\n",
      "epoch: 0 | 18464 / 114272 | training loss: 0.13751286268234253\n",
      "epoch: 0 | 18496 / 114272 | training loss: 0.11562924832105637\n",
      "epoch: 0 | 18528 / 114272 | training loss: 0.49004262685775757\n",
      "epoch: 0 | 18560 / 114272 | training loss: 0.3770892322063446\n",
      "epoch: 0 | 18592 / 114272 | training loss: 0.06973058730363846\n",
      "epoch: 0 | 18624 / 114272 | training loss: 0.14833804965019226\n",
      "epoch: 0 | 18656 / 114272 | training loss: 0.1284700334072113\n",
      "epoch: 0 | 18688 / 114272 | training loss: 0.04148969054222107\n",
      "epoch: 0 | 18720 / 114272 | training loss: 0.08772262185811996\n",
      "epoch: 0 | 18752 / 114272 | training loss: 0.30146265029907227\n",
      "epoch: 0 | 18784 / 114272 | training loss: 0.12898190319538116\n",
      "epoch: 0 | 18816 / 114272 | training loss: 0.4388640820980072\n",
      "epoch: 0 | 18848 / 114272 | training loss: 0.09842924028635025\n",
      "epoch: 0 | 18880 / 114272 | training loss: 0.03654029592871666\n",
      "epoch: 0 | 18912 / 114272 | training loss: 0.22466374933719635\n",
      "epoch: 0 | 18944 / 114272 | training loss: 0.22699125111103058\n",
      "epoch: 0 | 18976 / 114272 | training loss: 0.11654789745807648\n",
      "epoch: 0 | 19008 / 114272 | training loss: 0.19597390294075012\n",
      "epoch: 0 | 19040 / 114272 | training loss: 0.10160258412361145\n",
      "epoch: 0 | 19072 / 114272 | training loss: 0.21620771288871765\n",
      "epoch: 0 | 19104 / 114272 | training loss: 0.26811081171035767\n",
      "epoch: 0 | 19136 / 114272 | training loss: 0.20672623813152313\n",
      "epoch: 0 | 19168 / 114272 | training loss: 0.09524068981409073\n",
      "epoch: 0 | 19200 / 114272 | training loss: 0.3035402297973633\n",
      "epoch: 0 | 19232 / 114272 | training loss: 0.533946692943573\n",
      "epoch: 0 | 19264 / 114272 | training loss: 0.20074228942394257\n",
      "epoch: 0 | 19296 / 114272 | training loss: 0.13815635442733765\n",
      "epoch: 0 | 19328 / 114272 | training loss: 0.2230217158794403\n",
      "epoch: 0 | 19360 / 114272 | training loss: 0.19955198466777802\n",
      "epoch: 0 | 19392 / 114272 | training loss: 0.04715265333652496\n",
      "epoch: 0 | 19424 / 114272 | training loss: 0.3481678068637848\n",
      "epoch: 0 | 19456 / 114272 | training loss: 0.4357287585735321\n",
      "epoch: 0 | 19488 / 114272 | training loss: 0.16658151149749756\n",
      "epoch: 0 | 19520 / 114272 | training loss: 0.2492223083972931\n",
      "epoch: 0 | 19552 / 114272 | training loss: 0.19155360758304596\n",
      "epoch: 0 | 19584 / 114272 | training loss: 0.12085839360952377\n",
      "epoch: 0 | 19616 / 114272 | training loss: 0.13075654208660126\n",
      "epoch: 0 | 19648 / 114272 | training loss: 0.15657728910446167\n",
      "epoch: 0 | 19680 / 114272 | training loss: 0.3710700571537018\n",
      "epoch: 0 | 19712 / 114272 | training loss: 0.1729327142238617\n",
      "epoch: 0 | 19744 / 114272 | training loss: 0.23991909623146057\n",
      "epoch: 0 | 19776 / 114272 | training loss: 0.17666378617286682\n",
      "epoch: 0 | 19808 / 114272 | training loss: 0.2864662706851959\n",
      "epoch: 0 | 19840 / 114272 | training loss: 0.07735390216112137\n",
      "epoch: 0 | 19872 / 114272 | training loss: 0.0793825089931488\n",
      "epoch: 0 | 19904 / 114272 | training loss: 0.02436155453324318\n",
      "epoch: 0 | 19936 / 114272 | training loss: 0.042442042380571365\n",
      "epoch: 0 | 19968 / 114272 | training loss: 0.14985282719135284\n",
      "epoch: 0 | 20000 / 114272 | training loss: 0.3404272794723511\n",
      "epoch: 0 | 20032 / 114272 | training loss: 0.20008784532546997\n",
      "epoch: 0 | 20064 / 114272 | training loss: 0.11760329455137253\n",
      "epoch: 0 | 20096 / 114272 | training loss: 0.30837392807006836\n",
      "epoch: 0 | 20128 / 114272 | training loss: 0.1089305654168129\n",
      "epoch: 0 | 20160 / 114272 | training loss: 0.6059499979019165\n",
      "epoch: 0 | 20192 / 114272 | training loss: 0.1917668730020523\n",
      "epoch: 0 | 20224 / 114272 | training loss: 0.03491133451461792\n",
      "epoch: 0 | 20256 / 114272 | training loss: 0.14294308423995972\n",
      "epoch: 0 | 20288 / 114272 | training loss: 0.040508974343538284\n",
      "epoch: 0 | 20320 / 114272 | training loss: 0.404548317193985\n",
      "epoch: 0 | 20352 / 114272 | training loss: 0.11444711685180664\n",
      "epoch: 0 | 20384 / 114272 | training loss: 0.017867999151349068\n",
      "epoch: 0 | 20416 / 114272 | training loss: 0.1503075361251831\n",
      "epoch: 0 | 20448 / 114272 | training loss: 0.08264495432376862\n",
      "epoch: 0 | 20480 / 114272 | training loss: 0.06779153645038605\n",
      "epoch: 0 | 20512 / 114272 | training loss: 0.37803202867507935\n",
      "epoch: 0 | 20544 / 114272 | training loss: 0.297960102558136\n",
      "epoch: 0 | 20576 / 114272 | training loss: 0.27159708738327026\n",
      "epoch: 0 | 20608 / 114272 | training loss: 0.10562163591384888\n",
      "epoch: 0 | 20640 / 114272 | training loss: 0.12816143035888672\n",
      "epoch: 0 | 20672 / 114272 | training loss: 0.025818411260843277\n",
      "epoch: 0 | 20704 / 114272 | training loss: 0.4668201506137848\n",
      "epoch: 0 | 20736 / 114272 | training loss: 0.20734558999538422\n",
      "epoch: 0 | 20768 / 114272 | training loss: 0.02253853715956211\n",
      "epoch: 0 | 20800 / 114272 | training loss: 0.2404525876045227\n",
      "epoch: 0 | 20832 / 114272 | training loss: 0.1692325323820114\n",
      "epoch: 0 | 20864 / 114272 | training loss: 0.1944606602191925\n",
      "epoch: 0 | 20896 / 114272 | training loss: 0.39766690135002136\n",
      "epoch: 0 | 20928 / 114272 | training loss: 0.179533913731575\n",
      "epoch: 0 | 20960 / 114272 | training loss: 0.1711125522851944\n",
      "epoch: 0 | 20992 / 114272 | training loss: 0.3025273382663727\n",
      "epoch: 0 | 21024 / 114272 | training loss: 0.14568929374217987\n",
      "epoch: 0 | 21056 / 114272 | training loss: 0.21055515110492706\n",
      "epoch: 0 | 21088 / 114272 | training loss: 0.09671588242053986\n",
      "epoch: 0 | 21120 / 114272 | training loss: 0.5724727511405945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 21152 / 114272 | training loss: 0.17575539648532867\n",
      "epoch: 0 | 21184 / 114272 | training loss: 0.15120093524456024\n",
      "epoch: 0 | 21216 / 114272 | training loss: 0.1498817801475525\n",
      "epoch: 0 | 21248 / 114272 | training loss: 0.35105443000793457\n",
      "epoch: 0 | 21280 / 114272 | training loss: 0.10606196522712708\n",
      "epoch: 0 | 21312 / 114272 | training loss: 0.04049985110759735\n",
      "epoch: 0 | 21344 / 114272 | training loss: 0.06469959020614624\n",
      "epoch: 0 | 21376 / 114272 | training loss: 0.17029279470443726\n",
      "epoch: 0 | 21408 / 114272 | training loss: 0.198335662484169\n",
      "epoch: 0 | 21440 / 114272 | training loss: 0.1838924139738083\n",
      "epoch: 0 | 21472 / 114272 | training loss: 0.1500725895166397\n",
      "epoch: 0 | 21504 / 114272 | training loss: 0.21906079351902008\n",
      "epoch: 0 | 21536 / 114272 | training loss: 0.1732594072818756\n",
      "epoch: 0 | 21568 / 114272 | training loss: 0.10206479579210281\n",
      "epoch: 0 | 21600 / 114272 | training loss: 0.22506336867809296\n",
      "epoch: 0 | 21632 / 114272 | training loss: 0.29561886191368103\n",
      "epoch: 0 | 21664 / 114272 | training loss: 0.7192341089248657\n",
      "epoch: 0 | 21696 / 114272 | training loss: 0.41183653473854065\n",
      "epoch: 0 | 21728 / 114272 | training loss: 0.13149970769882202\n",
      "epoch: 0 | 21760 / 114272 | training loss: 0.490056574344635\n",
      "epoch: 0 | 21792 / 114272 | training loss: 0.1148490458726883\n",
      "epoch: 0 | 21824 / 114272 | training loss: 0.10464008897542953\n",
      "epoch: 0 | 21856 / 114272 | training loss: 0.11306366324424744\n",
      "epoch: 0 | 21888 / 114272 | training loss: 0.084262914955616\n",
      "epoch: 0 | 21920 / 114272 | training loss: 0.38694751262664795\n",
      "epoch: 0 | 21952 / 114272 | training loss: 0.22042632102966309\n",
      "epoch: 0 | 21984 / 114272 | training loss: 0.17987093329429626\n",
      "epoch: 0 | 22016 / 114272 | training loss: 0.15853536128997803\n",
      "epoch: 0 | 22048 / 114272 | training loss: 0.12389764189720154\n",
      "epoch: 0 | 22080 / 114272 | training loss: 0.14661990106105804\n",
      "epoch: 0 | 22112 / 114272 | training loss: 0.10113608837127686\n",
      "epoch: 0 | 22144 / 114272 | training loss: 0.258279949426651\n",
      "epoch: 0 | 22176 / 114272 | training loss: 0.24609088897705078\n",
      "epoch: 0 | 22208 / 114272 | training loss: 0.04984109476208687\n",
      "epoch: 0 | 22240 / 114272 | training loss: 0.08921375125646591\n",
      "epoch: 0 | 22272 / 114272 | training loss: 0.11188990622758865\n",
      "epoch: 0 | 22304 / 114272 | training loss: 0.16742049157619476\n",
      "epoch: 0 | 22336 / 114272 | training loss: 0.06591808795928955\n",
      "epoch: 0 | 22368 / 114272 | training loss: 0.34238073229789734\n",
      "epoch: 0 | 22400 / 114272 | training loss: 0.31123000383377075\n",
      "epoch: 0 | 22432 / 114272 | training loss: 0.11653407663106918\n",
      "epoch: 0 | 22464 / 114272 | training loss: 0.30154162645339966\n",
      "epoch: 0 | 22496 / 114272 | training loss: 0.05134452134370804\n",
      "epoch: 0 | 22528 / 114272 | training loss: 0.10452526062726974\n",
      "epoch: 0 | 22560 / 114272 | training loss: 0.20794938504695892\n",
      "epoch: 0 | 22592 / 114272 | training loss: 0.17646516859531403\n",
      "epoch: 0 | 22624 / 114272 | training loss: 0.33641517162323\n",
      "epoch: 0 | 22656 / 114272 | training loss: 0.12906868755817413\n",
      "epoch: 0 | 22688 / 114272 | training loss: 0.26746445894241333\n",
      "epoch: 0 | 22720 / 114272 | training loss: 0.10708034783601761\n",
      "epoch: 0 | 22752 / 114272 | training loss: 0.0539390966296196\n",
      "epoch: 0 | 22784 / 114272 | training loss: 0.7464140057563782\n",
      "epoch: 0 | 22816 / 114272 | training loss: 0.3833278715610504\n",
      "epoch: 0 | 22848 / 114272 | training loss: 0.08436813950538635\n",
      "epoch: 0 | 22880 / 114272 | training loss: 0.14031606912612915\n",
      "epoch: 0 | 22912 / 114272 | training loss: 0.14787563681602478\n",
      "epoch: 0 | 22944 / 114272 | training loss: 0.024969033896923065\n",
      "epoch: 0 | 22976 / 114272 | training loss: 0.1677178293466568\n",
      "epoch: 0 | 23008 / 114272 | training loss: 0.16249322891235352\n",
      "epoch: 0 | 23040 / 114272 | training loss: 0.356997013092041\n",
      "epoch: 0 | 23072 / 114272 | training loss: 0.031032243743538857\n",
      "epoch: 0 | 23104 / 114272 | training loss: 0.046419642865657806\n",
      "epoch: 0 | 23136 / 114272 | training loss: 0.5617279410362244\n",
      "epoch: 0 | 23168 / 114272 | training loss: 0.46499791741371155\n",
      "epoch: 0 | 23200 / 114272 | training loss: 0.07901569455862045\n",
      "epoch: 0 | 23232 / 114272 | training loss: 0.09346015751361847\n",
      "epoch: 0 | 23264 / 114272 | training loss: 0.17450593411922455\n",
      "epoch: 0 | 23296 / 114272 | training loss: 0.09711288660764694\n",
      "epoch: 0 | 23328 / 114272 | training loss: 0.26548030972480774\n",
      "epoch: 0 | 23360 / 114272 | training loss: 0.09303071349859238\n",
      "epoch: 0 | 23392 / 114272 | training loss: 0.1174638643860817\n",
      "epoch: 0 | 23424 / 114272 | training loss: 0.1137019470334053\n",
      "epoch: 0 | 23456 / 114272 | training loss: 0.2581157982349396\n",
      "epoch: 0 | 23488 / 114272 | training loss: 0.7337510585784912\n",
      "epoch: 0 | 23520 / 114272 | training loss: 0.03708112984895706\n",
      "epoch: 0 | 23552 / 114272 | training loss: 0.46244773268699646\n",
      "epoch: 0 | 23584 / 114272 | training loss: 0.2574555277824402\n",
      "epoch: 0 | 23616 / 114272 | training loss: 0.19413258135318756\n",
      "epoch: 0 | 23648 / 114272 | training loss: 0.5215995907783508\n",
      "epoch: 0 | 23680 / 114272 | training loss: 0.2805708050727844\n",
      "epoch: 0 | 23712 / 114272 | training loss: 0.18713940680027008\n",
      "epoch: 0 | 23744 / 114272 | training loss: 0.2995290756225586\n",
      "epoch: 0 | 23776 / 114272 | training loss: 0.24984362721443176\n",
      "epoch: 0 | 23808 / 114272 | training loss: 0.17134109139442444\n",
      "epoch: 0 | 23840 / 114272 | training loss: 0.3260353207588196\n",
      "epoch: 0 | 23872 / 114272 | training loss: 0.1217229887843132\n",
      "epoch: 0 | 23904 / 114272 | training loss: 0.28144440054893494\n",
      "epoch: 0 | 23936 / 114272 | training loss: 0.12114725261926651\n",
      "epoch: 0 | 23968 / 114272 | training loss: 0.10672532021999359\n",
      "epoch: 0 | 24000 / 114272 | training loss: 0.11005867272615433\n",
      "epoch: 0 | 24032 / 114272 | training loss: 0.05555138736963272\n",
      "epoch: 0 | 24064 / 114272 | training loss: 0.12820552289485931\n",
      "epoch: 0 | 24096 / 114272 | training loss: 0.3017580509185791\n",
      "epoch: 0 | 24128 / 114272 | training loss: 0.2605763375759125\n",
      "epoch: 0 | 24160 / 114272 | training loss: 0.392382949590683\n",
      "epoch: 0 | 24192 / 114272 | training loss: 0.15841341018676758\n",
      "epoch: 0 | 24224 / 114272 | training loss: 0.2144155502319336\n",
      "epoch: 0 | 24256 / 114272 | training loss: 0.14587320387363434\n",
      "epoch: 0 | 24288 / 114272 | training loss: 0.17194968461990356\n",
      "epoch: 0 | 24320 / 114272 | training loss: 0.2581336498260498\n",
      "epoch: 0 | 24352 / 114272 | training loss: 0.2541479766368866\n",
      "epoch: 0 | 24384 / 114272 | training loss: 0.13835541903972626\n",
      "epoch: 0 | 24416 / 114272 | training loss: 0.13391105830669403\n",
      "epoch: 0 | 24448 / 114272 | training loss: 0.23555323481559753\n",
      "epoch: 0 | 24480 / 114272 | training loss: 0.3670751750469208\n",
      "epoch: 0 | 24512 / 114272 | training loss: 0.04320267587900162\n",
      "epoch: 0 | 24544 / 114272 | training loss: 0.3955897092819214\n",
      "epoch: 0 | 24576 / 114272 | training loss: 0.1795700043439865\n",
      "epoch: 0 | 24608 / 114272 | training loss: 0.03840355947613716\n",
      "epoch: 0 | 24640 / 114272 | training loss: 0.2691093683242798\n",
      "epoch: 0 | 24672 / 114272 | training loss: 0.454129695892334\n",
      "epoch: 0 | 24704 / 114272 | training loss: 0.07376692444086075\n",
      "epoch: 0 | 24736 / 114272 | training loss: 0.19240975379943848\n",
      "epoch: 0 | 24768 / 114272 | training loss: 0.054352905601263046\n",
      "epoch: 0 | 24800 / 114272 | training loss: 0.2758711278438568\n",
      "epoch: 0 | 24832 / 114272 | training loss: 0.03717279061675072\n",
      "epoch: 0 | 24864 / 114272 | training loss: 0.13151875138282776\n",
      "epoch: 0 | 24896 / 114272 | training loss: 0.03347090259194374\n",
      "epoch: 0 | 24928 / 114272 | training loss: 0.24072344601154327\n",
      "epoch: 0 | 24960 / 114272 | training loss: 0.031864095479249954\n",
      "epoch: 0 | 24992 / 114272 | training loss: 0.33212193846702576\n",
      "epoch: 0 | 25024 / 114272 | training loss: 0.10792012512683868\n",
      "epoch: 0 | 25056 / 114272 | training loss: 0.1168844923377037\n",
      "epoch: 0 | 25088 / 114272 | training loss: 0.1457231491804123\n",
      "epoch: 0 | 25120 / 114272 | training loss: 0.11797792464494705\n",
      "epoch: 0 | 25152 / 114272 | training loss: 0.0224374458193779\n",
      "epoch: 0 | 25184 / 114272 | training loss: 0.195664644241333\n",
      "epoch: 0 | 25216 / 114272 | training loss: 0.06121109053492546\n",
      "epoch: 0 | 25248 / 114272 | training loss: 0.3555067479610443\n",
      "epoch: 0 | 25280 / 114272 | training loss: 0.12879502773284912\n",
      "epoch: 0 | 25312 / 114272 | training loss: 0.10461138933897018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 25344 / 114272 | training loss: 0.05215661972761154\n",
      "epoch: 0 | 25376 / 114272 | training loss: 0.015967611223459244\n",
      "epoch: 0 | 25408 / 114272 | training loss: 0.14701025187969208\n",
      "epoch: 0 | 25440 / 114272 | training loss: 0.20614756643772125\n",
      "epoch: 0 | 25472 / 114272 | training loss: 0.2502681612968445\n",
      "epoch: 0 | 25504 / 114272 | training loss: 0.020176297053694725\n",
      "epoch: 0 | 25536 / 114272 | training loss: 0.1905515342950821\n",
      "epoch: 0 | 25568 / 114272 | training loss: 0.1253126859664917\n",
      "epoch: 0 | 25600 / 114272 | training loss: 0.2739475965499878\n",
      "epoch: 0 | 25632 / 114272 | training loss: 0.6287651658058167\n",
      "epoch: 0 | 25664 / 114272 | training loss: 0.4057594835758209\n",
      "epoch: 0 | 25696 / 114272 | training loss: 0.29243114590644836\n",
      "epoch: 0 | 25728 / 114272 | training loss: 0.3843831419944763\n",
      "epoch: 0 | 25760 / 114272 | training loss: 0.5759756565093994\n",
      "epoch: 0 | 25792 / 114272 | training loss: 0.22193360328674316\n",
      "epoch: 0 | 25824 / 114272 | training loss: 0.20010608434677124\n",
      "epoch: 0 | 25856 / 114272 | training loss: 0.0909724235534668\n",
      "epoch: 0 | 25888 / 114272 | training loss: 0.04289759695529938\n",
      "epoch: 0 | 25920 / 114272 | training loss: 0.14051373302936554\n",
      "epoch: 0 | 25952 / 114272 | training loss: 0.345472127199173\n",
      "epoch: 0 | 25984 / 114272 | training loss: 0.2605893313884735\n",
      "epoch: 0 | 26016 / 114272 | training loss: 0.16868199408054352\n",
      "epoch: 0 | 26048 / 114272 | training loss: 0.18988320231437683\n",
      "epoch: 0 | 26080 / 114272 | training loss: 0.5118106007575989\n",
      "epoch: 0 | 26112 / 114272 | training loss: 0.1539503037929535\n",
      "epoch: 0 | 26144 / 114272 | training loss: 0.2251780480146408\n",
      "epoch: 0 | 26176 / 114272 | training loss: 0.2325829714536667\n",
      "epoch: 0 | 26208 / 114272 | training loss: 0.1951630860567093\n",
      "epoch: 0 | 26240 / 114272 | training loss: 0.2217227965593338\n",
      "epoch: 0 | 26272 / 114272 | training loss: 0.2437419295310974\n",
      "epoch: 0 | 26304 / 114272 | training loss: 0.14476732909679413\n",
      "epoch: 0 | 26336 / 114272 | training loss: 0.17234966158866882\n",
      "epoch: 0 | 26368 / 114272 | training loss: 0.24300791323184967\n",
      "epoch: 0 | 26400 / 114272 | training loss: 0.224056214094162\n",
      "epoch: 0 | 26432 / 114272 | training loss: 0.09186853468418121\n",
      "epoch: 0 | 26464 / 114272 | training loss: 0.04659534990787506\n",
      "epoch: 0 | 26496 / 114272 | training loss: 0.397488534450531\n",
      "epoch: 0 | 26528 / 114272 | training loss: 0.16279910504817963\n",
      "epoch: 0 | 26560 / 114272 | training loss: 0.09516775608062744\n",
      "epoch: 0 | 26592 / 114272 | training loss: 0.3584389090538025\n",
      "epoch: 0 | 26624 / 114272 | training loss: 0.1924935281276703\n",
      "epoch: 0 | 26656 / 114272 | training loss: 0.05190657824277878\n",
      "epoch: 0 | 26688 / 114272 | training loss: 0.04375171288847923\n",
      "epoch: 0 | 26720 / 114272 | training loss: 0.40147751569747925\n",
      "epoch: 0 | 26752 / 114272 | training loss: 0.21167977154254913\n",
      "epoch: 0 | 26784 / 114272 | training loss: 0.17973023653030396\n",
      "epoch: 0 | 26816 / 114272 | training loss: 0.21905122697353363\n",
      "epoch: 0 | 26848 / 114272 | training loss: 0.022249022498726845\n",
      "epoch: 0 | 26880 / 114272 | training loss: 0.18200336396694183\n",
      "epoch: 0 | 26912 / 114272 | training loss: 0.22013457119464874\n",
      "epoch: 0 | 26944 / 114272 | training loss: 0.45258209109306335\n",
      "epoch: 0 | 26976 / 114272 | training loss: 0.1279994696378708\n",
      "epoch: 0 | 27008 / 114272 | training loss: 0.07466280460357666\n",
      "epoch: 0 | 27040 / 114272 | training loss: 0.13277728855609894\n",
      "epoch: 0 | 27072 / 114272 | training loss: 0.2109394073486328\n",
      "epoch: 0 | 27104 / 114272 | training loss: 0.1525823324918747\n",
      "epoch: 0 | 27136 / 114272 | training loss: 0.31575149297714233\n",
      "epoch: 0 | 27168 / 114272 | training loss: 0.6524510979652405\n",
      "epoch: 0 | 27200 / 114272 | training loss: 0.47420021891593933\n",
      "epoch: 0 | 27232 / 114272 | training loss: 0.5846247673034668\n",
      "epoch: 0 | 27264 / 114272 | training loss: 0.29349371790885925\n",
      "epoch: 0 | 27296 / 114272 | training loss: 0.13082410395145416\n",
      "epoch: 0 | 27328 / 114272 | training loss: 0.28494957089424133\n",
      "epoch: 0 | 27360 / 114272 | training loss: 0.46548938751220703\n",
      "epoch: 0 | 27392 / 114272 | training loss: 0.156624436378479\n",
      "epoch: 0 | 27424 / 114272 | training loss: 0.3355346918106079\n",
      "epoch: 0 | 27456 / 114272 | training loss: 0.29717665910720825\n",
      "epoch: 0 | 27488 / 114272 | training loss: 0.03650471940636635\n",
      "epoch: 0 | 27520 / 114272 | training loss: 0.2789160907268524\n",
      "epoch: 0 | 27552 / 114272 | training loss: 0.30258265137672424\n",
      "epoch: 0 | 27584 / 114272 | training loss: 0.04666599631309509\n",
      "epoch: 0 | 27616 / 114272 | training loss: 0.09038731455802917\n",
      "epoch: 0 | 27648 / 114272 | training loss: 0.02464308775961399\n",
      "epoch: 0 | 27680 / 114272 | training loss: 0.39091232419013977\n",
      "epoch: 0 | 27712 / 114272 | training loss: 0.1934453696012497\n",
      "epoch: 0 | 27744 / 114272 | training loss: 0.08476884663105011\n",
      "epoch: 0 | 27776 / 114272 | training loss: 0.3260438144207001\n",
      "epoch: 0 | 27808 / 114272 | training loss: 0.23756441473960876\n",
      "epoch: 0 | 27840 / 114272 | training loss: 0.027395853772759438\n",
      "epoch: 0 | 27872 / 114272 | training loss: 0.27714264392852783\n",
      "epoch: 0 | 27904 / 114272 | training loss: 0.398153692483902\n",
      "epoch: 0 | 27936 / 114272 | training loss: 0.11139378696680069\n",
      "epoch: 0 | 27968 / 114272 | training loss: 0.14342309534549713\n",
      "epoch: 0 | 28000 / 114272 | training loss: 0.07441412657499313\n",
      "epoch: 0 | 28032 / 114272 | training loss: 0.14756469428539276\n",
      "epoch: 0 | 28064 / 114272 | training loss: 0.2656460106372833\n",
      "epoch: 0 | 28096 / 114272 | training loss: 0.0764179527759552\n",
      "epoch: 0 | 28128 / 114272 | training loss: 0.11132357269525528\n",
      "epoch: 0 | 28160 / 114272 | training loss: 0.7122442126274109\n",
      "epoch: 0 | 28192 / 114272 | training loss: 0.2904224395751953\n",
      "epoch: 0 | 28224 / 114272 | training loss: 0.05374307185411453\n",
      "epoch: 0 | 28256 / 114272 | training loss: 0.3377356231212616\n",
      "epoch: 0 | 28288 / 114272 | training loss: 0.058611467480659485\n",
      "epoch: 0 | 28320 / 114272 | training loss: 0.17477445304393768\n",
      "epoch: 0 | 28352 / 114272 | training loss: 0.30207133293151855\n",
      "epoch: 0 | 28384 / 114272 | training loss: 0.03730197250843048\n",
      "epoch: 0 | 28416 / 114272 | training loss: 0.06292416155338287\n",
      "epoch: 0 | 28448 / 114272 | training loss: 0.16670896112918854\n",
      "epoch: 0 | 28480 / 114272 | training loss: 0.03353951871395111\n",
      "epoch: 0 | 28512 / 114272 | training loss: 0.19199112057685852\n",
      "epoch: 0 | 28544 / 114272 | training loss: 0.6374038457870483\n",
      "epoch: 0 | 28576 / 114272 | training loss: 0.1994459331035614\n",
      "epoch: 0 | 28608 / 114272 | training loss: 0.1593901515007019\n",
      "epoch: 0 | 28640 / 114272 | training loss: 0.3096645176410675\n",
      "epoch: 0 | 28672 / 114272 | training loss: 0.03064923733472824\n",
      "epoch: 0 | 28704 / 114272 | training loss: 0.06566030532121658\n",
      "epoch: 0 | 28736 / 114272 | training loss: 0.14519990980625153\n",
      "epoch: 0 | 28768 / 114272 | training loss: 0.2694374918937683\n",
      "epoch: 0 | 28800 / 114272 | training loss: 0.06937068700790405\n",
      "epoch: 0 | 28832 / 114272 | training loss: 0.22337304055690765\n",
      "epoch: 0 | 28864 / 114272 | training loss: 0.10658901929855347\n",
      "epoch: 0 | 28896 / 114272 | training loss: 0.3017582297325134\n",
      "epoch: 0 | 28928 / 114272 | training loss: 0.0855926051735878\n",
      "epoch: 0 | 28960 / 114272 | training loss: 0.12015406042337418\n",
      "epoch: 0 | 28992 / 114272 | training loss: 0.1927800178527832\n",
      "epoch: 0 | 29024 / 114272 | training loss: 0.05368607118725777\n",
      "epoch: 0 | 29056 / 114272 | training loss: 0.19616281986236572\n",
      "epoch: 0 | 29088 / 114272 | training loss: 0.06213434785604477\n",
      "epoch: 0 | 29120 / 114272 | training loss: 0.13838255405426025\n",
      "epoch: 0 | 29152 / 114272 | training loss: 0.14058460295200348\n",
      "epoch: 0 | 29184 / 114272 | training loss: 0.06144842505455017\n",
      "epoch: 0 | 29216 / 114272 | training loss: 0.06815531104803085\n",
      "epoch: 0 | 29248 / 114272 | training loss: 0.08145269751548767\n",
      "epoch: 0 | 29280 / 114272 | training loss: 0.35353797674179077\n",
      "epoch: 0 | 29312 / 114272 | training loss: 0.047273315489292145\n",
      "epoch: 0 | 29344 / 114272 | training loss: 0.19672906398773193\n",
      "epoch: 0 | 29376 / 114272 | training loss: 0.0624186210334301\n",
      "epoch: 0 | 29408 / 114272 | training loss: 0.14596179127693176\n",
      "epoch: 0 | 29440 / 114272 | training loss: 0.1304573118686676\n",
      "epoch: 0 | 29472 / 114272 | training loss: 0.022247640416026115\n",
      "epoch: 0 | 29504 / 114272 | training loss: 0.15615059435367584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 29536 / 114272 | training loss: 0.2906245291233063\n",
      "epoch: 0 | 29568 / 114272 | training loss: 0.05492512136697769\n",
      "epoch: 0 | 29600 / 114272 | training loss: 0.04341956973075867\n",
      "epoch: 0 | 29632 / 114272 | training loss: 0.08813440799713135\n",
      "epoch: 0 | 29664 / 114272 | training loss: 0.5737851858139038\n",
      "epoch: 0 | 29696 / 114272 | training loss: 0.053578272461891174\n",
      "epoch: 0 | 29728 / 114272 | training loss: 0.06314364075660706\n",
      "epoch: 0 | 29760 / 114272 | training loss: 0.16994313895702362\n",
      "epoch: 0 | 29792 / 114272 | training loss: 0.14521805942058563\n",
      "epoch: 0 | 29824 / 114272 | training loss: 0.417492151260376\n",
      "epoch: 0 | 29856 / 114272 | training loss: 0.5626689791679382\n",
      "epoch: 0 | 29888 / 114272 | training loss: 0.1718904674053192\n",
      "epoch: 0 | 29920 / 114272 | training loss: 0.09314148873090744\n",
      "epoch: 0 | 29952 / 114272 | training loss: 0.1907242089509964\n",
      "epoch: 0 | 29984 / 114272 | training loss: 0.05757495015859604\n",
      "epoch: 0 | 30016 / 114272 | training loss: 0.2037055939435959\n",
      "epoch: 0 | 30048 / 114272 | training loss: 0.12704895436763763\n",
      "epoch: 0 | 30080 / 114272 | training loss: 0.12549768388271332\n",
      "epoch: 0 | 30112 / 114272 | training loss: 0.15008476376533508\n",
      "epoch: 0 | 30144 / 114272 | training loss: 0.2899375259876251\n",
      "epoch: 0 | 30176 / 114272 | training loss: 0.2325645238161087\n",
      "epoch: 0 | 30208 / 114272 | training loss: 0.40740424394607544\n",
      "epoch: 0 | 30240 / 114272 | training loss: 0.3797922730445862\n",
      "epoch: 0 | 30272 / 114272 | training loss: 0.15458130836486816\n",
      "epoch: 0 | 30304 / 114272 | training loss: 0.259240984916687\n",
      "epoch: 0 | 30336 / 114272 | training loss: 0.25281569361686707\n",
      "epoch: 0 | 30368 / 114272 | training loss: 0.09007620066404343\n",
      "epoch: 0 | 30400 / 114272 | training loss: 0.5420920848846436\n",
      "epoch: 0 | 30432 / 114272 | training loss: 0.019515199586749077\n",
      "epoch: 0 | 30464 / 114272 | training loss: 0.04905103147029877\n",
      "epoch: 0 | 30496 / 114272 | training loss: 0.4421415328979492\n",
      "epoch: 0 | 30528 / 114272 | training loss: 0.21036836504936218\n",
      "epoch: 0 | 30560 / 114272 | training loss: 0.1822352558374405\n",
      "epoch: 0 | 30592 / 114272 | training loss: 0.09998850524425507\n",
      "epoch: 0 | 30624 / 114272 | training loss: 0.36330074071884155\n",
      "epoch: 0 | 30656 / 114272 | training loss: 0.1196155697107315\n",
      "epoch: 0 | 30688 / 114272 | training loss: 0.19404475390911102\n",
      "epoch: 0 | 30720 / 114272 | training loss: 0.23094454407691956\n",
      "epoch: 0 | 30752 / 114272 | training loss: 0.1851966232061386\n",
      "epoch: 0 | 30784 / 114272 | training loss: 0.6851506233215332\n",
      "epoch: 0 | 30816 / 114272 | training loss: 0.18181289732456207\n",
      "epoch: 0 | 30848 / 114272 | training loss: 0.3129326403141022\n",
      "epoch: 0 | 30880 / 114272 | training loss: 0.24596890807151794\n",
      "epoch: 0 | 30912 / 114272 | training loss: 0.060728661715984344\n",
      "epoch: 0 | 30944 / 114272 | training loss: 0.32090193033218384\n",
      "epoch: 0 | 30976 / 114272 | training loss: 0.46618637442588806\n",
      "epoch: 0 | 31008 / 114272 | training loss: 0.22787804901599884\n",
      "epoch: 0 | 31040 / 114272 | training loss: 0.12149244546890259\n",
      "epoch: 0 | 31072 / 114272 | training loss: 0.5746665596961975\n",
      "epoch: 0 | 31104 / 114272 | training loss: 0.08535641431808472\n",
      "epoch: 0 | 31136 / 114272 | training loss: 0.10833922028541565\n",
      "epoch: 0 | 31168 / 114272 | training loss: 0.16079822182655334\n",
      "epoch: 0 | 31200 / 114272 | training loss: 0.07751742005348206\n",
      "epoch: 0 | 31232 / 114272 | training loss: 0.1059635579586029\n",
      "epoch: 0 | 31264 / 114272 | training loss: 0.13121309876441956\n",
      "epoch: 0 | 31296 / 114272 | training loss: 0.1515679955482483\n",
      "epoch: 0 | 31328 / 114272 | training loss: 0.39740896224975586\n",
      "epoch: 0 | 31360 / 114272 | training loss: 0.17295801639556885\n",
      "epoch: 0 | 31392 / 114272 | training loss: 0.19488558173179626\n",
      "epoch: 0 | 31424 / 114272 | training loss: 0.08381383121013641\n",
      "epoch: 0 | 31456 / 114272 | training loss: 0.16112206876277924\n",
      "epoch: 0 | 31488 / 114272 | training loss: 0.18392327427864075\n",
      "epoch: 0 | 31520 / 114272 | training loss: 0.09261519461870193\n",
      "epoch: 0 | 31552 / 114272 | training loss: 0.045110125094652176\n",
      "epoch: 0 | 31584 / 114272 | training loss: 0.3619919419288635\n",
      "epoch: 0 | 31616 / 114272 | training loss: 0.18289516866207123\n",
      "epoch: 0 | 31648 / 114272 | training loss: 0.022408805787563324\n",
      "epoch: 0 | 31680 / 114272 | training loss: 0.08219205588102341\n",
      "epoch: 0 | 31712 / 114272 | training loss: 0.10638396441936493\n",
      "epoch: 0 | 31744 / 114272 | training loss: 0.3247225880622864\n",
      "epoch: 0 | 31776 / 114272 | training loss: 0.26288893818855286\n",
      "epoch: 0 | 31808 / 114272 | training loss: 0.020462103188037872\n",
      "epoch: 0 | 31840 / 114272 | training loss: 0.07780250161886215\n",
      "epoch: 0 | 31872 / 114272 | training loss: 0.11833544075489044\n",
      "epoch: 0 | 31904 / 114272 | training loss: 0.2857917845249176\n",
      "epoch: 0 | 31936 / 114272 | training loss: 0.34517550468444824\n",
      "epoch: 0 | 31968 / 114272 | training loss: 0.18338724970817566\n",
      "epoch: 0 | 32000 / 114272 | training loss: 0.1345089077949524\n",
      "epoch: 0 | 32032 / 114272 | training loss: 0.0707843005657196\n",
      "epoch: 0 | 32064 / 114272 | training loss: 0.06369618326425552\n",
      "epoch: 0 | 32096 / 114272 | training loss: 0.15639275312423706\n",
      "epoch: 0 | 32128 / 114272 | training loss: 0.14330469071865082\n",
      "epoch: 0 | 32160 / 114272 | training loss: 0.359041303396225\n",
      "epoch: 0 | 32192 / 114272 | training loss: 0.03490518778562546\n",
      "epoch: 0 | 32224 / 114272 | training loss: 0.05704423785209656\n",
      "epoch: 0 | 32256 / 114272 | training loss: 0.4775174856185913\n",
      "epoch: 0 | 32288 / 114272 | training loss: 0.209990993142128\n",
      "epoch: 0 | 32320 / 114272 | training loss: 0.06895457208156586\n",
      "epoch: 0 | 32352 / 114272 | training loss: 0.31932294368743896\n",
      "epoch: 0 | 32384 / 114272 | training loss: 0.1580740362405777\n",
      "epoch: 0 | 32416 / 114272 | training loss: 0.025284502655267715\n",
      "epoch: 0 | 32448 / 114272 | training loss: 0.12566612660884857\n",
      "epoch: 0 | 32480 / 114272 | training loss: 0.23409034311771393\n",
      "epoch: 0 | 32512 / 114272 | training loss: 0.1460810899734497\n",
      "epoch: 0 | 32544 / 114272 | training loss: 0.028906147927045822\n",
      "epoch: 0 | 32576 / 114272 | training loss: 0.24088095128536224\n",
      "epoch: 0 | 32608 / 114272 | training loss: 0.13418418169021606\n",
      "epoch: 0 | 32640 / 114272 | training loss: 0.10279656946659088\n",
      "epoch: 0 | 32672 / 114272 | training loss: 0.026719113811850548\n",
      "epoch: 0 | 32704 / 114272 | training loss: 0.24201537668704987\n",
      "epoch: 0 | 32736 / 114272 | training loss: 0.3228830397129059\n",
      "epoch: 0 | 32768 / 114272 | training loss: 0.1671488881111145\n",
      "epoch: 0 | 32800 / 114272 | training loss: 0.2497623711824417\n",
      "epoch: 0 | 32832 / 114272 | training loss: 0.20463868975639343\n",
      "epoch: 0 | 32864 / 114272 | training loss: 0.08752197027206421\n",
      "epoch: 0 | 32896 / 114272 | training loss: 0.25561755895614624\n",
      "epoch: 0 | 32928 / 114272 | training loss: 0.3071003556251526\n",
      "epoch: 0 | 32960 / 114272 | training loss: 0.13389131426811218\n",
      "epoch: 0 | 32992 / 114272 | training loss: 0.08796118199825287\n",
      "epoch: 0 | 33024 / 114272 | training loss: 0.17415037751197815\n",
      "epoch: 0 | 33056 / 114272 | training loss: 0.17783759534358978\n",
      "epoch: 0 | 33088 / 114272 | training loss: 0.17174296081066132\n",
      "epoch: 0 | 33120 / 114272 | training loss: 0.10133828222751617\n",
      "epoch: 0 | 33152 / 114272 | training loss: 0.10019005089998245\n",
      "epoch: 0 | 33184 / 114272 | training loss: 0.2309258133172989\n",
      "epoch: 0 | 33216 / 114272 | training loss: 0.2308114767074585\n",
      "epoch: 0 | 33248 / 114272 | training loss: 0.1691351681947708\n",
      "epoch: 0 | 33280 / 114272 | training loss: 0.3804478347301483\n",
      "epoch: 0 | 33312 / 114272 | training loss: 0.2027297019958496\n",
      "epoch: 0 | 33344 / 114272 | training loss: 0.09653978049755096\n",
      "epoch: 0 | 33376 / 114272 | training loss: 0.13849453628063202\n",
      "epoch: 0 | 33408 / 114272 | training loss: 0.10095690935850143\n",
      "epoch: 0 | 33440 / 114272 | training loss: 0.1494050770998001\n",
      "epoch: 0 | 33472 / 114272 | training loss: 0.2877221703529358\n",
      "epoch: 0 | 33504 / 114272 | training loss: 0.09948345273733139\n",
      "epoch: 0 | 33536 / 114272 | training loss: 0.15099716186523438\n",
      "epoch: 0 | 33568 / 114272 | training loss: 0.13973961770534515\n",
      "epoch: 0 | 33600 / 114272 | training loss: 0.07445073872804642\n",
      "epoch: 0 | 33632 / 114272 | training loss: 0.22700783610343933\n",
      "epoch: 0 | 33664 / 114272 | training loss: 0.30692991614341736\n",
      "epoch: 0 | 33696 / 114272 | training loss: 0.14737121760845184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 33728 / 114272 | training loss: 0.18807905912399292\n",
      "epoch: 0 | 33760 / 114272 | training loss: 0.17031395435333252\n",
      "epoch: 0 | 33792 / 114272 | training loss: 0.17192506790161133\n",
      "epoch: 0 | 33824 / 114272 | training loss: 0.09873558580875397\n",
      "epoch: 0 | 33856 / 114272 | training loss: 0.15337081253528595\n",
      "epoch: 0 | 33888 / 114272 | training loss: 0.2855170667171478\n",
      "epoch: 0 | 33920 / 114272 | training loss: 0.18365153670310974\n",
      "epoch: 0 | 33952 / 114272 | training loss: 0.1564277708530426\n",
      "epoch: 0 | 33984 / 114272 | training loss: 0.1714698225259781\n",
      "epoch: 0 | 34016 / 114272 | training loss: 0.09804724901914597\n",
      "epoch: 0 | 34048 / 114272 | training loss: 0.157558411359787\n",
      "epoch: 0 | 34080 / 114272 | training loss: 0.09023541957139969\n",
      "epoch: 0 | 34112 / 114272 | training loss: 0.05887511372566223\n",
      "epoch: 0 | 34144 / 114272 | training loss: 0.040492232888936996\n",
      "epoch: 0 | 34176 / 114272 | training loss: 0.13770410418510437\n",
      "epoch: 0 | 34208 / 114272 | training loss: 0.4838773012161255\n",
      "epoch: 0 | 34240 / 114272 | training loss: 0.2837931513786316\n",
      "epoch: 0 | 34272 / 114272 | training loss: 0.18298952281475067\n",
      "epoch: 0 | 34304 / 114272 | training loss: 0.19386346638202667\n",
      "epoch: 0 | 34336 / 114272 | training loss: 0.03302512690424919\n",
      "epoch: 0 | 34368 / 114272 | training loss: 0.2372971773147583\n",
      "epoch: 0 | 34400 / 114272 | training loss: 0.33887314796447754\n",
      "epoch: 0 | 34432 / 114272 | training loss: 0.35843563079833984\n",
      "epoch: 0 | 34464 / 114272 | training loss: 0.12897689640522003\n",
      "epoch: 0 | 34496 / 114272 | training loss: 0.22858040034770966\n",
      "epoch: 0 | 34528 / 114272 | training loss: 0.3002975881099701\n",
      "epoch: 0 | 34560 / 114272 | training loss: 0.11609520763158798\n",
      "epoch: 0 | 34592 / 114272 | training loss: 0.07595822960138321\n",
      "epoch: 0 | 34624 / 114272 | training loss: 0.05652391538023949\n",
      "epoch: 0 | 34656 / 114272 | training loss: 0.24439528584480286\n",
      "epoch: 0 | 34688 / 114272 | training loss: 0.11513953655958176\n",
      "epoch: 0 | 34720 / 114272 | training loss: 0.18120154738426208\n",
      "epoch: 0 | 34752 / 114272 | training loss: 0.06754215806722641\n",
      "epoch: 0 | 34784 / 114272 | training loss: 0.12429945915937424\n",
      "epoch: 0 | 34816 / 114272 | training loss: 0.510608971118927\n",
      "epoch: 0 | 34848 / 114272 | training loss: 0.5153356194496155\n",
      "epoch: 0 | 34880 / 114272 | training loss: 0.27806416153907776\n",
      "epoch: 0 | 34912 / 114272 | training loss: 0.18532733619213104\n",
      "epoch: 0 | 34944 / 114272 | training loss: 0.36011144518852234\n",
      "epoch: 0 | 34976 / 114272 | training loss: 0.2822810113430023\n",
      "epoch: 0 | 35008 / 114272 | training loss: 0.16259023547172546\n",
      "epoch: 0 | 35040 / 114272 | training loss: 0.0922442376613617\n",
      "epoch: 0 | 35072 / 114272 | training loss: 0.3772169053554535\n",
      "epoch: 0 | 35104 / 114272 | training loss: 0.32784661650657654\n",
      "epoch: 0 | 35136 / 114272 | training loss: 0.27460816502571106\n",
      "epoch: 0 | 35168 / 114272 | training loss: 0.26274311542510986\n",
      "epoch: 0 | 35200 / 114272 | training loss: 0.08207777142524719\n",
      "epoch: 0 | 35232 / 114272 | training loss: 0.21529562771320343\n",
      "epoch: 0 | 35264 / 114272 | training loss: 0.21412861347198486\n",
      "epoch: 0 | 35296 / 114272 | training loss: 0.23754072189331055\n",
      "epoch: 0 | 35328 / 114272 | training loss: 0.32415860891342163\n",
      "epoch: 0 | 35360 / 114272 | training loss: 0.15501505136489868\n",
      "epoch: 0 | 35392 / 114272 | training loss: 0.2588728368282318\n",
      "epoch: 0 | 35424 / 114272 | training loss: 0.28087344765663147\n",
      "epoch: 0 | 35456 / 114272 | training loss: 0.20620222389698029\n",
      "epoch: 0 | 35488 / 114272 | training loss: 0.08457444608211517\n",
      "epoch: 0 | 35520 / 114272 | training loss: 0.28013885021209717\n",
      "epoch: 0 | 35552 / 114272 | training loss: 0.25833529233932495\n",
      "epoch: 0 | 35584 / 114272 | training loss: 0.15556174516677856\n",
      "epoch: 0 | 35616 / 114272 | training loss: 0.14558684825897217\n",
      "epoch: 0 | 35648 / 114272 | training loss: 0.17742806673049927\n",
      "epoch: 0 | 35680 / 114272 | training loss: 0.22274434566497803\n",
      "epoch: 0 | 35712 / 114272 | training loss: 0.18468891084194183\n",
      "epoch: 0 | 35744 / 114272 | training loss: 0.0681178942322731\n",
      "epoch: 0 | 35776 / 114272 | training loss: 0.16984151303768158\n",
      "epoch: 0 | 35808 / 114272 | training loss: 0.2616948187351227\n",
      "epoch: 0 | 35840 / 114272 | training loss: 0.19938139617443085\n",
      "epoch: 0 | 35872 / 114272 | training loss: 0.2640972435474396\n",
      "epoch: 0 | 35904 / 114272 | training loss: 0.11160115152597427\n",
      "epoch: 0 | 35936 / 114272 | training loss: 0.22196300327777863\n",
      "epoch: 0 | 35968 / 114272 | training loss: 0.21240390837192535\n",
      "epoch: 0 | 36000 / 114272 | training loss: 0.19252726435661316\n",
      "epoch: 0 | 36032 / 114272 | training loss: 0.1879461109638214\n",
      "epoch: 0 | 36064 / 114272 | training loss: 0.1807088404893875\n",
      "epoch: 0 | 36096 / 114272 | training loss: 0.09852737188339233\n",
      "epoch: 0 | 36128 / 114272 | training loss: 0.05118400603532791\n",
      "epoch: 0 | 36160 / 114272 | training loss: 0.28039073944091797\n",
      "epoch: 0 | 36192 / 114272 | training loss: 0.21811939775943756\n",
      "epoch: 0 | 36224 / 114272 | training loss: 0.08889902383089066\n",
      "epoch: 0 | 36256 / 114272 | training loss: 0.13216671347618103\n",
      "epoch: 0 | 36288 / 114272 | training loss: 0.0882730633020401\n",
      "epoch: 0 | 36320 / 114272 | training loss: 0.04590228572487831\n",
      "epoch: 0 | 36352 / 114272 | training loss: 0.04049301519989967\n",
      "epoch: 0 | 36384 / 114272 | training loss: 0.09535147249698639\n",
      "epoch: 0 | 36416 / 114272 | training loss: 0.23065848648548126\n",
      "epoch: 0 | 36448 / 114272 | training loss: 0.24331627786159515\n",
      "epoch: 0 | 36480 / 114272 | training loss: 0.06722111999988556\n",
      "epoch: 0 | 36512 / 114272 | training loss: 0.13385595381259918\n",
      "epoch: 0 | 36544 / 114272 | training loss: 0.0496939979493618\n",
      "epoch: 0 | 36576 / 114272 | training loss: 0.015831710770726204\n",
      "epoch: 0 | 36608 / 114272 | training loss: 0.16128239035606384\n",
      "epoch: 0 | 36640 / 114272 | training loss: 0.42176109552383423\n",
      "epoch: 0 | 36672 / 114272 | training loss: 0.15439555048942566\n",
      "epoch: 0 | 36704 / 114272 | training loss: 0.16360478103160858\n",
      "epoch: 0 | 36736 / 114272 | training loss: 0.09599793702363968\n",
      "epoch: 0 | 36768 / 114272 | training loss: 0.24045415222644806\n",
      "epoch: 0 | 36800 / 114272 | training loss: 0.03782865032553673\n",
      "epoch: 0 | 36832 / 114272 | training loss: 0.08462205529212952\n",
      "epoch: 0 | 36864 / 114272 | training loss: 0.0679365023970604\n",
      "epoch: 0 | 36896 / 114272 | training loss: 0.12086059898138046\n",
      "epoch: 0 | 36928 / 114272 | training loss: 0.0688825473189354\n",
      "epoch: 0 | 36960 / 114272 | training loss: 0.3297189176082611\n",
      "epoch: 0 | 36992 / 114272 | training loss: 0.0505828820168972\n",
      "epoch: 0 | 37024 / 114272 | training loss: 0.3134607672691345\n",
      "epoch: 0 | 37056 / 114272 | training loss: 0.31560394167900085\n",
      "epoch: 0 | 37088 / 114272 | training loss: 0.14783798158168793\n",
      "epoch: 0 | 37120 / 114272 | training loss: 0.026993896812200546\n",
      "epoch: 0 | 37152 / 114272 | training loss: 0.18005047738552094\n",
      "epoch: 0 | 37184 / 114272 | training loss: 0.23530474305152893\n",
      "epoch: 0 | 37216 / 114272 | training loss: 0.07214338332414627\n",
      "epoch: 0 | 37248 / 114272 | training loss: 0.08016972243785858\n",
      "epoch: 0 | 37280 / 114272 | training loss: 0.3327258825302124\n",
      "epoch: 0 | 37312 / 114272 | training loss: 0.022779881954193115\n",
      "epoch: 0 | 37344 / 114272 | training loss: 0.3929104804992676\n",
      "epoch: 0 | 37376 / 114272 | training loss: 0.17856603860855103\n",
      "epoch: 0 | 37408 / 114272 | training loss: 0.44750356674194336\n",
      "epoch: 0 | 37440 / 114272 | training loss: 0.04638232663273811\n",
      "epoch: 0 | 37472 / 114272 | training loss: 0.154397115111351\n",
      "epoch: 0 | 37504 / 114272 | training loss: 0.2767512798309326\n",
      "epoch: 0 | 37536 / 114272 | training loss: 0.10983888804912567\n",
      "epoch: 0 | 37568 / 114272 | training loss: 0.2527643144130707\n",
      "epoch: 0 | 37600 / 114272 | training loss: 0.09923307597637177\n",
      "epoch: 0 | 37632 / 114272 | training loss: 0.19623082876205444\n",
      "epoch: 0 | 37664 / 114272 | training loss: 0.04636150225996971\n",
      "epoch: 0 | 37696 / 114272 | training loss: 0.0713358223438263\n",
      "epoch: 0 | 37728 / 114272 | training loss: 0.0868719220161438\n",
      "epoch: 0 | 37760 / 114272 | training loss: 0.3045690953731537\n",
      "epoch: 0 | 37792 / 114272 | training loss: 0.33725976943969727\n",
      "epoch: 0 | 37824 / 114272 | training loss: 0.11807055026292801\n",
      "epoch: 0 | 37856 / 114272 | training loss: 0.438044935464859\n",
      "epoch: 0 | 37888 / 114272 | training loss: 0.09841444343328476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 37920 / 114272 | training loss: 0.49862080812454224\n",
      "epoch: 0 | 37952 / 114272 | training loss: 0.04120991379022598\n",
      "epoch: 0 | 37984 / 114272 | training loss: 0.19522689282894135\n",
      "epoch: 0 | 38016 / 114272 | training loss: 0.12552298605442047\n",
      "epoch: 0 | 38048 / 114272 | training loss: 0.050132207572460175\n",
      "epoch: 0 | 38080 / 114272 | training loss: 0.26461219787597656\n",
      "epoch: 0 | 38112 / 114272 | training loss: 0.09787683188915253\n",
      "epoch: 0 | 38144 / 114272 | training loss: 0.10528711974620819\n",
      "epoch: 0 | 38176 / 114272 | training loss: 0.09911704808473587\n",
      "epoch: 0 | 38208 / 114272 | training loss: 0.4321047365665436\n",
      "epoch: 0 | 38240 / 114272 | training loss: 0.24068154394626617\n",
      "epoch: 0 | 38272 / 114272 | training loss: 0.25596341490745544\n",
      "epoch: 0 | 38304 / 114272 | training loss: 0.14566804468631744\n",
      "epoch: 0 | 38336 / 114272 | training loss: 0.1284569799900055\n",
      "epoch: 0 | 38368 / 114272 | training loss: 0.056246355175971985\n",
      "epoch: 0 | 38400 / 114272 | training loss: 0.048265211284160614\n",
      "epoch: 0 | 38432 / 114272 | training loss: 0.16643626987934113\n",
      "epoch: 0 | 38464 / 114272 | training loss: 0.26666900515556335\n",
      "epoch: 0 | 38496 / 114272 | training loss: 0.15017014741897583\n",
      "epoch: 0 | 38528 / 114272 | training loss: 0.4576478600502014\n",
      "epoch: 0 | 38560 / 114272 | training loss: 0.16019213199615479\n",
      "epoch: 0 | 38592 / 114272 | training loss: 0.3264823257923126\n",
      "epoch: 0 | 38624 / 114272 | training loss: 0.1522262692451477\n",
      "epoch: 0 | 38656 / 114272 | training loss: 0.08037108182907104\n",
      "epoch: 0 | 38688 / 114272 | training loss: 0.3554465174674988\n",
      "epoch: 0 | 38720 / 114272 | training loss: 0.08390086889266968\n",
      "epoch: 0 | 38752 / 114272 | training loss: 0.23573194444179535\n",
      "epoch: 0 | 38784 / 114272 | training loss: 0.052094895392656326\n",
      "epoch: 0 | 38816 / 114272 | training loss: 0.21234211325645447\n",
      "epoch: 0 | 38848 / 114272 | training loss: 0.31404516100883484\n",
      "epoch: 0 | 38880 / 114272 | training loss: 0.4566227197647095\n",
      "epoch: 0 | 38912 / 114272 | training loss: 0.3921237885951996\n",
      "epoch: 0 | 38944 / 114272 | training loss: 0.20550288259983063\n",
      "epoch: 0 | 38976 / 114272 | training loss: 0.035430144518613815\n",
      "epoch: 0 | 39008 / 114272 | training loss: 0.0735587328672409\n",
      "epoch: 0 | 39040 / 114272 | training loss: 0.18212302029132843\n",
      "epoch: 0 | 39072 / 114272 | training loss: 0.4198947846889496\n",
      "epoch: 0 | 39104 / 114272 | training loss: 0.12284792959690094\n",
      "epoch: 0 | 39136 / 114272 | training loss: 0.23308174312114716\n",
      "epoch: 0 | 39168 / 114272 | training loss: 0.2570403218269348\n",
      "epoch: 0 | 39200 / 114272 | training loss: 0.45457231998443604\n",
      "epoch: 0 | 39232 / 114272 | training loss: 0.1267116516828537\n",
      "epoch: 0 | 39264 / 114272 | training loss: 0.2663818895816803\n",
      "epoch: 0 | 39296 / 114272 | training loss: 0.2183120846748352\n",
      "epoch: 0 | 39328 / 114272 | training loss: 0.25025296211242676\n",
      "epoch: 0 | 39360 / 114272 | training loss: 0.07587133347988129\n",
      "epoch: 0 | 39392 / 114272 | training loss: 0.1611701399087906\n",
      "epoch: 0 | 39424 / 114272 | training loss: 0.17759834229946136\n",
      "epoch: 0 | 39456 / 114272 | training loss: 0.27983295917510986\n",
      "epoch: 0 | 39488 / 114272 | training loss: 0.43264904618263245\n",
      "epoch: 0 | 39520 / 114272 | training loss: 0.7633572220802307\n",
      "epoch: 0 | 39552 / 114272 | training loss: 0.135171040892601\n",
      "epoch: 0 | 39584 / 114272 | training loss: 0.16232217848300934\n",
      "epoch: 0 | 39616 / 114272 | training loss: 0.04705437645316124\n",
      "epoch: 0 | 39648 / 114272 | training loss: 0.05214390903711319\n",
      "epoch: 0 | 39680 / 114272 | training loss: 0.5681228637695312\n",
      "epoch: 0 | 39712 / 114272 | training loss: 0.08827531337738037\n",
      "epoch: 0 | 39744 / 114272 | training loss: 0.043380118906497955\n",
      "epoch: 0 | 39776 / 114272 | training loss: 0.009361212141811848\n",
      "epoch: 0 | 39808 / 114272 | training loss: 0.09694698452949524\n",
      "epoch: 0 | 39840 / 114272 | training loss: 0.16563281416893005\n",
      "epoch: 0 | 39872 / 114272 | training loss: 0.02596980333328247\n",
      "epoch: 0 | 39904 / 114272 | training loss: 0.08452608436346054\n",
      "epoch: 0 | 39936 / 114272 | training loss: 0.0481313094496727\n",
      "epoch: 0 | 39968 / 114272 | training loss: 0.07574962079524994\n",
      "epoch: 0 | 40000 / 114272 | training loss: 0.24155353009700775\n",
      "epoch: 0 | 40032 / 114272 | training loss: 0.05154946446418762\n",
      "epoch: 0 | 40064 / 114272 | training loss: 0.23019848763942719\n",
      "epoch: 0 | 40096 / 114272 | training loss: 0.0281518641859293\n",
      "epoch: 0 | 40128 / 114272 | training loss: 0.18292082846164703\n",
      "epoch: 0 | 40160 / 114272 | training loss: 0.49108755588531494\n",
      "epoch: 0 | 40192 / 114272 | training loss: 0.17449060082435608\n",
      "epoch: 0 | 40224 / 114272 | training loss: 0.25805729627609253\n",
      "epoch: 0 | 40256 / 114272 | training loss: 0.01753334328532219\n",
      "epoch: 0 | 40288 / 114272 | training loss: 0.22540675103664398\n",
      "epoch: 0 | 40320 / 114272 | training loss: 0.31947794556617737\n",
      "epoch: 0 | 40352 / 114272 | training loss: 0.6475557684898376\n",
      "epoch: 0 | 40384 / 114272 | training loss: 0.04196785017848015\n",
      "epoch: 0 | 40416 / 114272 | training loss: 0.30064138770103455\n",
      "epoch: 0 | 40448 / 114272 | training loss: 0.04565303772687912\n",
      "epoch: 0 | 40480 / 114272 | training loss: 0.24043934047222137\n",
      "epoch: 0 | 40512 / 114272 | training loss: 0.33302950859069824\n",
      "epoch: 0 | 40544 / 114272 | training loss: 0.1043907180428505\n",
      "epoch: 0 | 40576 / 114272 | training loss: 0.08824953436851501\n",
      "epoch: 0 | 40608 / 114272 | training loss: 0.2540086507797241\n",
      "epoch: 0 | 40640 / 114272 | training loss: 0.1762307733297348\n",
      "epoch: 0 | 40672 / 114272 | training loss: 0.315152645111084\n",
      "epoch: 0 | 40704 / 114272 | training loss: 0.2104274332523346\n",
      "epoch: 0 | 40736 / 114272 | training loss: 0.10409281402826309\n",
      "epoch: 0 | 40768 / 114272 | training loss: 0.23027604818344116\n",
      "epoch: 0 | 40800 / 114272 | training loss: 0.05623852461576462\n",
      "epoch: 0 | 40832 / 114272 | training loss: 0.21585433185100555\n",
      "epoch: 0 | 40864 / 114272 | training loss: 0.11414657533168793\n",
      "epoch: 0 | 40896 / 114272 | training loss: 0.24834448099136353\n",
      "epoch: 0 | 40928 / 114272 | training loss: 0.19783253967761993\n",
      "epoch: 0 | 40960 / 114272 | training loss: 0.0758216604590416\n",
      "epoch: 0 | 40992 / 114272 | training loss: 0.12889866530895233\n",
      "epoch: 0 | 41024 / 114272 | training loss: 0.04698662832379341\n",
      "epoch: 0 | 41056 / 114272 | training loss: 0.39920398592948914\n",
      "epoch: 0 | 41088 / 114272 | training loss: 0.4054242670536041\n",
      "epoch: 0 | 41120 / 114272 | training loss: 0.14922712743282318\n",
      "epoch: 0 | 41152 / 114272 | training loss: 0.06615791469812393\n",
      "epoch: 0 | 41184 / 114272 | training loss: 0.39668020606040955\n",
      "epoch: 0 | 41216 / 114272 | training loss: 0.1379155069589615\n",
      "epoch: 0 | 41248 / 114272 | training loss: 0.2971307933330536\n",
      "epoch: 0 | 41280 / 114272 | training loss: 0.07464908808469772\n",
      "epoch: 0 | 41312 / 114272 | training loss: 0.30122944712638855\n",
      "epoch: 0 | 41344 / 114272 | training loss: 0.18863143026828766\n",
      "epoch: 0 | 41376 / 114272 | training loss: 0.01912081614136696\n",
      "epoch: 0 | 41408 / 114272 | training loss: 0.11386264860630035\n",
      "epoch: 0 | 41440 / 114272 | training loss: 0.11670127511024475\n",
      "epoch: 0 | 41472 / 114272 | training loss: 0.022641334682703018\n",
      "epoch: 0 | 41504 / 114272 | training loss: 0.03333601728081703\n",
      "epoch: 0 | 41536 / 114272 | training loss: 0.05027374252676964\n",
      "epoch: 0 | 41568 / 114272 | training loss: 0.3946719765663147\n",
      "epoch: 0 | 41600 / 114272 | training loss: 0.1454784870147705\n",
      "epoch: 0 | 41632 / 114272 | training loss: 0.3689258396625519\n",
      "epoch: 0 | 41664 / 114272 | training loss: 0.12121793627738953\n",
      "epoch: 0 | 41696 / 114272 | training loss: 0.11930588632822037\n",
      "epoch: 0 | 41728 / 114272 | training loss: 0.10779222846031189\n",
      "epoch: 0 | 41760 / 114272 | training loss: 0.2468794882297516\n",
      "epoch: 0 | 41792 / 114272 | training loss: 0.08373083174228668\n",
      "epoch: 0 | 41824 / 114272 | training loss: 0.21707291901111603\n",
      "epoch: 0 | 41856 / 114272 | training loss: 0.05243346467614174\n",
      "epoch: 0 | 41888 / 114272 | training loss: 0.20954900979995728\n",
      "epoch: 0 | 41920 / 114272 | training loss: 0.26853084564208984\n",
      "epoch: 0 | 41952 / 114272 | training loss: 0.03425247594714165\n",
      "epoch: 0 | 41984 / 114272 | training loss: 0.1573639214038849\n",
      "epoch: 0 | 42016 / 114272 | training loss: 0.12540261447429657\n",
      "epoch: 0 | 42048 / 114272 | training loss: 0.16408145427703857\n",
      "epoch: 0 | 42080 / 114272 | training loss: 0.14383208751678467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 42112 / 114272 | training loss: 0.131110280752182\n",
      "epoch: 0 | 42144 / 114272 | training loss: 0.039936505258083344\n",
      "epoch: 0 | 42176 / 114272 | training loss: 0.3867571949958801\n",
      "epoch: 0 | 42208 / 114272 | training loss: 0.03467332199215889\n",
      "epoch: 0 | 42240 / 114272 | training loss: 0.05557345971465111\n",
      "epoch: 0 | 42272 / 114272 | training loss: 0.2239864021539688\n",
      "epoch: 0 | 42304 / 114272 | training loss: 0.03867567330598831\n",
      "epoch: 0 | 42336 / 114272 | training loss: 0.16790558397769928\n",
      "epoch: 0 | 42368 / 114272 | training loss: 0.11945907026529312\n",
      "epoch: 0 | 42400 / 114272 | training loss: 0.11483226716518402\n",
      "epoch: 0 | 42432 / 114272 | training loss: 0.07158050686120987\n",
      "epoch: 0 | 42464 / 114272 | training loss: 0.23133210837841034\n",
      "epoch: 0 | 42496 / 114272 | training loss: 0.10345485806465149\n",
      "epoch: 0 | 42528 / 114272 | training loss: 0.027356473729014397\n",
      "epoch: 0 | 42560 / 114272 | training loss: 0.2035059779882431\n",
      "epoch: 0 | 42592 / 114272 | training loss: 0.2726775109767914\n",
      "epoch: 0 | 42624 / 114272 | training loss: 0.06321769207715988\n",
      "epoch: 0 | 42656 / 114272 | training loss: 0.19462470710277557\n",
      "epoch: 0 | 42688 / 114272 | training loss: 0.33135631680488586\n",
      "epoch: 0 | 42720 / 114272 | training loss: 0.4481041133403778\n",
      "epoch: 0 | 42752 / 114272 | training loss: 0.364534854888916\n",
      "epoch: 0 | 42784 / 114272 | training loss: 0.2077600508928299\n",
      "epoch: 0 | 42816 / 114272 | training loss: 0.3423108458518982\n",
      "epoch: 0 | 42848 / 114272 | training loss: 0.14961639046669006\n",
      "epoch: 0 | 42880 / 114272 | training loss: 0.08603696525096893\n",
      "epoch: 0 | 42912 / 114272 | training loss: 0.3425233066082001\n",
      "epoch: 0 | 42944 / 114272 | training loss: 0.22919483482837677\n",
      "epoch: 0 | 42976 / 114272 | training loss: 0.09521421045064926\n",
      "epoch: 0 | 43008 / 114272 | training loss: 0.06377971172332764\n",
      "epoch: 0 | 43040 / 114272 | training loss: 0.05348751321434975\n",
      "epoch: 0 | 43072 / 114272 | training loss: 0.07869144529104233\n",
      "epoch: 0 | 43104 / 114272 | training loss: 0.30247601866722107\n",
      "epoch: 0 | 43136 / 114272 | training loss: 0.24678710103034973\n",
      "epoch: 0 | 43168 / 114272 | training loss: 0.3792204260826111\n",
      "epoch: 0 | 43200 / 114272 | training loss: 0.07746432721614838\n",
      "epoch: 0 | 43232 / 114272 | training loss: 0.05196824297308922\n",
      "epoch: 0 | 43264 / 114272 | training loss: 0.03296904265880585\n",
      "epoch: 0 | 43296 / 114272 | training loss: 0.44582584500312805\n",
      "epoch: 0 | 43328 / 114272 | training loss: 0.011121915653347969\n",
      "epoch: 0 | 43360 / 114272 | training loss: 0.379484087228775\n",
      "epoch: 0 | 43392 / 114272 | training loss: 0.11555596441030502\n",
      "epoch: 0 | 43424 / 114272 | training loss: 0.16379255056381226\n",
      "epoch: 0 | 43456 / 114272 | training loss: 0.19955089688301086\n",
      "epoch: 0 | 43488 / 114272 | training loss: 0.2906085252761841\n",
      "epoch: 0 | 43520 / 114272 | training loss: 0.11773322522640228\n",
      "epoch: 0 | 43552 / 114272 | training loss: 0.2089468389749527\n",
      "epoch: 0 | 43584 / 114272 | training loss: 0.10624966025352478\n",
      "epoch: 0 | 43616 / 114272 | training loss: 0.04450647905468941\n",
      "epoch: 0 | 43648 / 114272 | training loss: 0.10058073699474335\n",
      "epoch: 0 | 43680 / 114272 | training loss: 0.08792123943567276\n",
      "epoch: 0 | 43712 / 114272 | training loss: 0.23689790070056915\n",
      "epoch: 0 | 43744 / 114272 | training loss: 0.177205890417099\n",
      "epoch: 0 | 43776 / 114272 | training loss: 0.34407880902290344\n",
      "epoch: 0 | 43808 / 114272 | training loss: 0.21283353865146637\n",
      "epoch: 0 | 43840 / 114272 | training loss: 0.12328164279460907\n",
      "epoch: 0 | 43872 / 114272 | training loss: 0.2520110309123993\n",
      "epoch: 0 | 43904 / 114272 | training loss: 0.10659075528383255\n",
      "epoch: 0 | 43936 / 114272 | training loss: 0.20780321955680847\n",
      "epoch: 0 | 43968 / 114272 | training loss: 0.0932575985789299\n",
      "epoch: 0 | 44000 / 114272 | training loss: 0.05735984072089195\n",
      "epoch: 0 | 44032 / 114272 | training loss: 0.15880319476127625\n",
      "epoch: 0 | 44064 / 114272 | training loss: 0.26146700978279114\n",
      "epoch: 0 | 44096 / 114272 | training loss: 0.1313983052968979\n",
      "epoch: 0 | 44128 / 114272 | training loss: 0.25578001141548157\n",
      "epoch: 0 | 44160 / 114272 | training loss: 0.08488729596138\n",
      "epoch: 0 | 44192 / 114272 | training loss: 0.2741048038005829\n",
      "epoch: 0 | 44224 / 114272 | training loss: 0.16269829869270325\n",
      "epoch: 0 | 44256 / 114272 | training loss: 0.0466403067111969\n",
      "epoch: 0 | 44288 / 114272 | training loss: 0.21626432240009308\n",
      "epoch: 0 | 44320 / 114272 | training loss: 0.08091206848621368\n",
      "epoch: 0 | 44352 / 114272 | training loss: 0.2592141330242157\n",
      "epoch: 0 | 44384 / 114272 | training loss: 0.07095430791378021\n",
      "epoch: 0 | 44416 / 114272 | training loss: 0.04419736564159393\n",
      "epoch: 0 | 44448 / 114272 | training loss: 0.3423844873905182\n",
      "epoch: 0 | 44480 / 114272 | training loss: 0.036347877234220505\n",
      "epoch: 0 | 44512 / 114272 | training loss: 0.1951298862695694\n",
      "epoch: 0 | 44544 / 114272 | training loss: 0.17774038016796112\n",
      "epoch: 0 | 44576 / 114272 | training loss: 0.4261417090892792\n",
      "epoch: 0 | 44608 / 114272 | training loss: 0.09572251886129379\n",
      "epoch: 0 | 44640 / 114272 | training loss: 0.5307565331459045\n",
      "epoch: 0 | 44672 / 114272 | training loss: 0.25553420186042786\n",
      "epoch: 0 | 44704 / 114272 | training loss: 0.13053114712238312\n",
      "epoch: 0 | 44736 / 114272 | training loss: 0.19325891137123108\n",
      "epoch: 0 | 44768 / 114272 | training loss: 0.13290564715862274\n",
      "epoch: 0 | 44800 / 114272 | training loss: 0.08628512918949127\n",
      "epoch: 0 | 44832 / 114272 | training loss: 0.07618134468793869\n",
      "epoch: 0 | 44864 / 114272 | training loss: 0.07785388082265854\n",
      "epoch: 0 | 44896 / 114272 | training loss: 0.14303012192249298\n",
      "epoch: 0 | 44928 / 114272 | training loss: 0.02830142341554165\n",
      "epoch: 0 | 44960 / 114272 | training loss: 0.07247898727655411\n",
      "epoch: 0 | 44992 / 114272 | training loss: 0.12966562807559967\n",
      "epoch: 0 | 45024 / 114272 | training loss: 0.11026330292224884\n",
      "epoch: 0 | 45056 / 114272 | training loss: 0.20253989100456238\n",
      "epoch: 0 | 45088 / 114272 | training loss: 0.4618555009365082\n",
      "epoch: 0 | 45120 / 114272 | training loss: 0.18445448577404022\n",
      "epoch: 0 | 45152 / 114272 | training loss: 0.1983310878276825\n",
      "epoch: 0 | 45184 / 114272 | training loss: 0.1244337186217308\n",
      "epoch: 0 | 45216 / 114272 | training loss: 0.19527484476566315\n",
      "epoch: 0 | 45248 / 114272 | training loss: 0.038879863917827606\n",
      "epoch: 0 | 45280 / 114272 | training loss: 0.1313207894563675\n",
      "epoch: 0 | 45312 / 114272 | training loss: 0.3330988585948944\n",
      "epoch: 0 | 45344 / 114272 | training loss: 0.15463930368423462\n",
      "epoch: 0 | 45376 / 114272 | training loss: 0.5016473531723022\n",
      "epoch: 0 | 45408 / 114272 | training loss: 0.25588753819465637\n",
      "epoch: 0 | 45440 / 114272 | training loss: 0.1817074418067932\n",
      "epoch: 0 | 45472 / 114272 | training loss: 0.16194447875022888\n",
      "epoch: 0 | 45504 / 114272 | training loss: 0.3247702419757843\n",
      "epoch: 0 | 45536 / 114272 | training loss: 0.11596477031707764\n",
      "epoch: 0 | 45568 / 114272 | training loss: 0.2278820127248764\n",
      "epoch: 0 | 45600 / 114272 | training loss: 0.15159063041210175\n",
      "epoch: 0 | 45632 / 114272 | training loss: 0.13979890942573547\n",
      "epoch: 0 | 45664 / 114272 | training loss: 0.1804484724998474\n",
      "epoch: 0 | 45696 / 114272 | training loss: 0.11701161414384842\n",
      "epoch: 0 | 45728 / 114272 | training loss: 0.06594743579626083\n",
      "epoch: 0 | 45760 / 114272 | training loss: 0.06982739269733429\n",
      "epoch: 0 | 45792 / 114272 | training loss: 0.12477503716945648\n",
      "epoch: 0 | 45824 / 114272 | training loss: 0.11714966595172882\n",
      "epoch: 0 | 45856 / 114272 | training loss: 0.12954364717006683\n",
      "epoch: 0 | 45888 / 114272 | training loss: 0.14376357197761536\n",
      "epoch: 0 | 45920 / 114272 | training loss: 0.10542632639408112\n",
      "epoch: 0 | 45952 / 114272 | training loss: 0.38955673575401306\n",
      "epoch: 0 | 45984 / 114272 | training loss: 0.27465614676475525\n",
      "epoch: 0 | 46016 / 114272 | training loss: 0.039146214723587036\n",
      "epoch: 0 | 46048 / 114272 | training loss: 0.1035294160246849\n",
      "epoch: 0 | 46080 / 114272 | training loss: 0.14421260356903076\n",
      "epoch: 0 | 46112 / 114272 | training loss: 0.21615654230117798\n",
      "epoch: 0 | 46144 / 114272 | training loss: 0.047223467379808426\n",
      "epoch: 0 | 46176 / 114272 | training loss: 0.13091886043548584\n",
      "epoch: 0 | 46208 / 114272 | training loss: 0.17027638852596283\n",
      "epoch: 0 | 46240 / 114272 | training loss: 0.06734457612037659\n",
      "epoch: 0 | 46272 / 114272 | training loss: 0.12183243036270142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 46304 / 114272 | training loss: 0.29360151290893555\n",
      "epoch: 0 | 46336 / 114272 | training loss: 0.158205047249794\n",
      "epoch: 0 | 46368 / 114272 | training loss: 0.2425844371318817\n",
      "epoch: 0 | 46400 / 114272 | training loss: 0.042432572692632675\n",
      "epoch: 0 | 46432 / 114272 | training loss: 0.08103042840957642\n",
      "epoch: 0 | 46464 / 114272 | training loss: 0.07289879024028778\n",
      "epoch: 0 | 46496 / 114272 | training loss: 0.20419159531593323\n",
      "epoch: 0 | 46528 / 114272 | training loss: 0.24561807513237\n",
      "epoch: 0 | 46560 / 114272 | training loss: 0.12895463407039642\n",
      "epoch: 0 | 46592 / 114272 | training loss: 0.06847468763589859\n",
      "epoch: 0 | 46624 / 114272 | training loss: 0.17101360857486725\n",
      "epoch: 0 | 46656 / 114272 | training loss: 0.23244409263134003\n",
      "epoch: 0 | 46688 / 114272 | training loss: 0.43714869022369385\n",
      "epoch: 0 | 46720 / 114272 | training loss: 0.06671334058046341\n",
      "epoch: 0 | 46752 / 114272 | training loss: 0.4373345971107483\n",
      "epoch: 0 | 46784 / 114272 | training loss: 0.19609715044498444\n",
      "epoch: 0 | 46816 / 114272 | training loss: 0.1878625601530075\n",
      "epoch: 0 | 46848 / 114272 | training loss: 0.022778933867812157\n",
      "epoch: 0 | 46880 / 114272 | training loss: 0.14765813946723938\n",
      "epoch: 0 | 46912 / 114272 | training loss: 0.2332524210214615\n",
      "epoch: 0 | 46944 / 114272 | training loss: 0.26854947209358215\n",
      "epoch: 0 | 46976 / 114272 | training loss: 0.12738238275051117\n",
      "epoch: 0 | 47008 / 114272 | training loss: 0.1236276850104332\n",
      "epoch: 0 | 47040 / 114272 | training loss: 0.24217292666435242\n",
      "epoch: 0 | 47072 / 114272 | training loss: 0.08247347921133041\n",
      "epoch: 0 | 47104 / 114272 | training loss: 0.19074948132038116\n",
      "epoch: 0 | 47136 / 114272 | training loss: 0.040427856147289276\n",
      "epoch: 0 | 47168 / 114272 | training loss: 0.46459436416625977\n",
      "epoch: 0 | 47200 / 114272 | training loss: 0.21155041456222534\n",
      "epoch: 0 | 47232 / 114272 | training loss: 0.18846425414085388\n",
      "epoch: 0 | 47264 / 114272 | training loss: 0.2750396728515625\n",
      "epoch: 0 | 47296 / 114272 | training loss: 0.1492103487253189\n",
      "epoch: 0 | 47328 / 114272 | training loss: 0.12606778740882874\n",
      "epoch: 0 | 47360 / 114272 | training loss: 0.11109364032745361\n",
      "epoch: 0 | 47392 / 114272 | training loss: 0.0326470248401165\n",
      "epoch: 0 | 47424 / 114272 | training loss: 0.15428175032138824\n",
      "epoch: 0 | 47456 / 114272 | training loss: 0.10504071414470673\n",
      "epoch: 0 | 47488 / 114272 | training loss: 0.07683172076940536\n",
      "epoch: 0 | 47520 / 114272 | training loss: 0.2106025218963623\n",
      "epoch: 0 | 47552 / 114272 | training loss: 0.08214046061038971\n",
      "epoch: 0 | 47584 / 114272 | training loss: 0.1807508021593094\n",
      "epoch: 0 | 47616 / 114272 | training loss: 0.03799665719270706\n",
      "epoch: 0 | 47648 / 114272 | training loss: 0.24989919364452362\n",
      "epoch: 0 | 47680 / 114272 | training loss: 0.24608825147151947\n",
      "epoch: 0 | 47712 / 114272 | training loss: 0.15990883111953735\n",
      "epoch: 0 | 47744 / 114272 | training loss: 0.19788789749145508\n",
      "epoch: 0 | 47776 / 114272 | training loss: 0.04443821310997009\n",
      "epoch: 0 | 47808 / 114272 | training loss: 0.21850183606147766\n",
      "epoch: 0 | 47840 / 114272 | training loss: 0.23413942754268646\n",
      "epoch: 0 | 47872 / 114272 | training loss: 0.008902695029973984\n",
      "epoch: 0 | 47904 / 114272 | training loss: 0.2850422263145447\n",
      "epoch: 0 | 47936 / 114272 | training loss: 0.03805865719914436\n",
      "epoch: 0 | 47968 / 114272 | training loss: 0.1364310383796692\n",
      "epoch: 0 | 48000 / 114272 | training loss: 0.22614088654518127\n",
      "epoch: 0 | 48032 / 114272 | training loss: 0.14461879432201385\n",
      "epoch: 0 | 48064 / 114272 | training loss: 0.026124462485313416\n",
      "epoch: 0 | 48096 / 114272 | training loss: 0.1818264275789261\n",
      "epoch: 0 | 48128 / 114272 | training loss: 0.0636000856757164\n",
      "epoch: 0 | 48160 / 114272 | training loss: 0.039766713976860046\n",
      "epoch: 0 | 48192 / 114272 | training loss: 0.3067653477191925\n",
      "epoch: 0 | 48224 / 114272 | training loss: 0.15760265290737152\n",
      "epoch: 0 | 48256 / 114272 | training loss: 0.04885273426771164\n",
      "epoch: 0 | 48288 / 114272 | training loss: 0.12982164323329926\n",
      "epoch: 0 | 48320 / 114272 | training loss: 0.05091487243771553\n",
      "epoch: 0 | 48352 / 114272 | training loss: 0.20651817321777344\n",
      "epoch: 0 | 48384 / 114272 | training loss: 0.272652268409729\n",
      "epoch: 0 | 48416 / 114272 | training loss: 0.11800196021795273\n",
      "epoch: 0 | 48448 / 114272 | training loss: 0.14939728379249573\n",
      "epoch: 0 | 48480 / 114272 | training loss: 0.17530791461467743\n",
      "epoch: 0 | 48512 / 114272 | training loss: 0.05861436948180199\n",
      "epoch: 0 | 48544 / 114272 | training loss: 0.17716339230537415\n",
      "epoch: 0 | 48576 / 114272 | training loss: 0.09079647809267044\n",
      "epoch: 0 | 48608 / 114272 | training loss: 0.11844173073768616\n",
      "epoch: 0 | 48640 / 114272 | training loss: 0.1883956342935562\n",
      "epoch: 0 | 48672 / 114272 | training loss: 0.4300955533981323\n",
      "epoch: 0 | 48704 / 114272 | training loss: 0.2788388729095459\n",
      "epoch: 0 | 48736 / 114272 | training loss: 0.08204348385334015\n",
      "epoch: 0 | 48768 / 114272 | training loss: 0.07493503391742706\n",
      "epoch: 0 | 48800 / 114272 | training loss: 0.22403761744499207\n",
      "epoch: 0 | 48832 / 114272 | training loss: 0.19534893333911896\n",
      "epoch: 0 | 48864 / 114272 | training loss: 0.23976853489875793\n",
      "epoch: 0 | 48896 / 114272 | training loss: 0.12188037484884262\n",
      "epoch: 0 | 48928 / 114272 | training loss: 0.043733589351177216\n",
      "epoch: 0 | 48960 / 114272 | training loss: 0.18550436198711395\n",
      "epoch: 0 | 48992 / 114272 | training loss: 0.0583936907351017\n",
      "epoch: 0 | 49024 / 114272 | training loss: 0.13650618493556976\n",
      "epoch: 0 | 49056 / 114272 | training loss: 0.1589040458202362\n",
      "epoch: 0 | 49088 / 114272 | training loss: 0.1486215889453888\n",
      "epoch: 0 | 49120 / 114272 | training loss: 0.06305132061243057\n",
      "epoch: 0 | 49152 / 114272 | training loss: 0.04184868931770325\n",
      "epoch: 0 | 49184 / 114272 | training loss: 0.041989874094724655\n",
      "epoch: 0 | 49216 / 114272 | training loss: 0.045643601566553116\n",
      "epoch: 0 | 49248 / 114272 | training loss: 0.17991827428340912\n",
      "epoch: 0 | 49280 / 114272 | training loss: 0.14544136822223663\n",
      "epoch: 0 | 49312 / 114272 | training loss: 0.05135395750403404\n",
      "epoch: 0 | 49344 / 114272 | training loss: 0.5390499234199524\n",
      "epoch: 0 | 49376 / 114272 | training loss: 0.3872745931148529\n",
      "epoch: 0 | 49408 / 114272 | training loss: 0.3697042465209961\n",
      "epoch: 0 | 49440 / 114272 | training loss: 0.36908894777297974\n",
      "epoch: 0 | 49472 / 114272 | training loss: 0.02111934684216976\n",
      "epoch: 0 | 49504 / 114272 | training loss: 0.1478872150182724\n",
      "epoch: 0 | 49536 / 114272 | training loss: 0.2231159657239914\n",
      "epoch: 0 | 49568 / 114272 | training loss: 0.2513170838356018\n",
      "epoch: 0 | 49600 / 114272 | training loss: 0.43079674243927\n",
      "epoch: 0 | 49632 / 114272 | training loss: 0.3615070581436157\n",
      "epoch: 0 | 49664 / 114272 | training loss: 0.1234872043132782\n",
      "epoch: 0 | 49696 / 114272 | training loss: 0.010928631760179996\n",
      "epoch: 0 | 49728 / 114272 | training loss: 0.09649036824703217\n",
      "epoch: 0 | 49760 / 114272 | training loss: 0.21054749190807343\n",
      "epoch: 0 | 49792 / 114272 | training loss: 0.28348416090011597\n",
      "epoch: 0 | 49824 / 114272 | training loss: 0.11767387390136719\n",
      "epoch: 0 | 49856 / 114272 | training loss: 0.22846902906894684\n",
      "epoch: 0 | 49888 / 114272 | training loss: 0.029212577268481255\n",
      "epoch: 0 | 49920 / 114272 | training loss: 0.16572928428649902\n",
      "epoch: 0 | 49952 / 114272 | training loss: 0.09407588839530945\n",
      "epoch: 0 | 49984 / 114272 | training loss: 0.45285364985466003\n",
      "epoch: 0 | 50016 / 114272 | training loss: 0.03483591228723526\n",
      "epoch: 0 | 50048 / 114272 | training loss: 0.0874454453587532\n",
      "epoch: 0 | 50080 / 114272 | training loss: 0.05167824774980545\n",
      "epoch: 0 | 50112 / 114272 | training loss: 0.3192937970161438\n",
      "epoch: 0 | 50144 / 114272 | training loss: 0.025477023795247078\n",
      "epoch: 0 | 50176 / 114272 | training loss: 0.37116509675979614\n",
      "epoch: 0 | 50208 / 114272 | training loss: 0.2225031852722168\n",
      "epoch: 0 | 50240 / 114272 | training loss: 0.16031663119792938\n",
      "epoch: 0 | 50272 / 114272 | training loss: 0.19674186408519745\n",
      "epoch: 0 | 50304 / 114272 | training loss: 0.025917593389749527\n",
      "epoch: 0 | 50336 / 114272 | training loss: 0.08920570462942123\n",
      "epoch: 0 | 50368 / 114272 | training loss: 0.44277164340019226\n",
      "epoch: 0 | 50400 / 114272 | training loss: 0.3278130292892456\n",
      "epoch: 0 | 50432 / 114272 | training loss: 0.21390874683856964\n",
      "epoch: 0 | 50464 / 114272 | training loss: 0.1623333841562271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 50496 / 114272 | training loss: 0.08183177560567856\n",
      "epoch: 0 | 50528 / 114272 | training loss: 0.15364325046539307\n",
      "epoch: 0 | 50560 / 114272 | training loss: 0.20937153697013855\n",
      "epoch: 0 | 50592 / 114272 | training loss: 0.11803620308637619\n",
      "epoch: 0 | 50624 / 114272 | training loss: 0.10747815668582916\n",
      "epoch: 0 | 50656 / 114272 | training loss: 0.060002733021974564\n",
      "epoch: 0 | 50688 / 114272 | training loss: 0.21250180900096893\n",
      "epoch: 0 | 50720 / 114272 | training loss: 0.19728697836399078\n",
      "epoch: 0 | 50752 / 114272 | training loss: 0.03670179098844528\n",
      "epoch: 0 | 50784 / 114272 | training loss: 0.12889501452445984\n",
      "epoch: 0 | 50816 / 114272 | training loss: 0.17866559326648712\n",
      "epoch: 0 | 50848 / 114272 | training loss: 0.08470454812049866\n",
      "epoch: 0 | 50880 / 114272 | training loss: 0.22855506837368011\n",
      "epoch: 0 | 50912 / 114272 | training loss: 0.12447924166917801\n",
      "epoch: 0 | 50944 / 114272 | training loss: 0.15254680812358856\n",
      "epoch: 0 | 50976 / 114272 | training loss: 0.13132545351982117\n",
      "epoch: 0 | 51008 / 114272 | training loss: 0.16019895672798157\n",
      "epoch: 0 | 51040 / 114272 | training loss: 0.08804930746555328\n",
      "epoch: 0 | 51072 / 114272 | training loss: 0.23630072176456451\n",
      "epoch: 0 | 51104 / 114272 | training loss: 0.086788110435009\n",
      "epoch: 0 | 51136 / 114272 | training loss: 0.04781646281480789\n",
      "epoch: 0 | 51168 / 114272 | training loss: 0.07547403872013092\n",
      "epoch: 0 | 51200 / 114272 | training loss: 0.363554447889328\n",
      "epoch: 0 | 51232 / 114272 | training loss: 0.035629354417324066\n",
      "epoch: 0 | 51264 / 114272 | training loss: 0.480588436126709\n",
      "epoch: 0 | 51296 / 114272 | training loss: 0.11343817412853241\n",
      "epoch: 0 | 51328 / 114272 | training loss: 0.029864614829421043\n",
      "epoch: 0 | 51360 / 114272 | training loss: 0.04910789802670479\n",
      "epoch: 0 | 51392 / 114272 | training loss: 0.1290702372789383\n",
      "epoch: 0 | 51424 / 114272 | training loss: 0.10004754364490509\n",
      "epoch: 0 | 51456 / 114272 | training loss: 0.09697388857603073\n",
      "epoch: 0 | 51488 / 114272 | training loss: 0.28176721930503845\n",
      "epoch: 0 | 51520 / 114272 | training loss: 0.13696032762527466\n",
      "epoch: 0 | 51552 / 114272 | training loss: 0.20302848517894745\n",
      "epoch: 0 | 51584 / 114272 | training loss: 0.12095928192138672\n",
      "epoch: 0 | 51616 / 114272 | training loss: 0.007603507488965988\n",
      "epoch: 0 | 51648 / 114272 | training loss: 0.020735647529363632\n",
      "epoch: 0 | 51680 / 114272 | training loss: 0.013313264586031437\n",
      "epoch: 0 | 51712 / 114272 | training loss: 0.20252156257629395\n",
      "epoch: 0 | 51744 / 114272 | training loss: 0.4210970401763916\n",
      "epoch: 0 | 51776 / 114272 | training loss: 0.17661409080028534\n",
      "epoch: 0 | 51808 / 114272 | training loss: 0.2681398093700409\n",
      "epoch: 0 | 51840 / 114272 | training loss: 0.1883222907781601\n",
      "epoch: 0 | 51872 / 114272 | training loss: 0.29947835206985474\n",
      "epoch: 0 | 51904 / 114272 | training loss: 0.0988362580537796\n",
      "epoch: 0 | 51936 / 114272 | training loss: 0.019101114943623543\n",
      "epoch: 0 | 51968 / 114272 | training loss: 0.18822525441646576\n",
      "epoch: 0 | 52000 / 114272 | training loss: 0.2337786853313446\n",
      "epoch: 0 | 52032 / 114272 | training loss: 0.09517421573400497\n",
      "epoch: 0 | 52064 / 114272 | training loss: 0.043269362300634384\n",
      "epoch: 0 | 52096 / 114272 | training loss: 0.022934656590223312\n",
      "epoch: 0 | 52128 / 114272 | training loss: 0.05207648128271103\n",
      "epoch: 0 | 52160 / 114272 | training loss: 0.146442249417305\n",
      "epoch: 0 | 52192 / 114272 | training loss: 0.010522640310227871\n",
      "epoch: 0 | 52224 / 114272 | training loss: 0.23427508771419525\n",
      "epoch: 0 | 52256 / 114272 | training loss: 0.13978062570095062\n",
      "epoch: 0 | 52288 / 114272 | training loss: 0.028375960886478424\n",
      "epoch: 0 | 52320 / 114272 | training loss: 0.18602995574474335\n",
      "epoch: 0 | 52352 / 114272 | training loss: 0.12051559239625931\n",
      "epoch: 0 | 52384 / 114272 | training loss: 0.10278796404600143\n",
      "epoch: 0 | 52416 / 114272 | training loss: 0.28835248947143555\n",
      "epoch: 0 | 52448 / 114272 | training loss: 0.2376350462436676\n",
      "epoch: 0 | 52480 / 114272 | training loss: 0.04282331094145775\n",
      "epoch: 0 | 52512 / 114272 | training loss: 0.013962076976895332\n",
      "epoch: 0 | 52544 / 114272 | training loss: 0.04287923127412796\n",
      "epoch: 0 | 52576 / 114272 | training loss: 0.2496815323829651\n",
      "epoch: 0 | 52608 / 114272 | training loss: 0.1235235333442688\n",
      "epoch: 0 | 52640 / 114272 | training loss: 0.2692032754421234\n",
      "epoch: 0 | 52672 / 114272 | training loss: 0.10383960604667664\n",
      "epoch: 0 | 52704 / 114272 | training loss: 0.07095122337341309\n",
      "epoch: 0 | 52736 / 114272 | training loss: 0.3503352999687195\n",
      "epoch: 0 | 52768 / 114272 | training loss: 0.2753770649433136\n",
      "epoch: 0 | 52800 / 114272 | training loss: 0.10201294720172882\n",
      "epoch: 0 | 52832 / 114272 | training loss: 0.043387286365032196\n",
      "epoch: 0 | 52864 / 114272 | training loss: 0.027849724516272545\n",
      "epoch: 0 | 52896 / 114272 | training loss: 0.32957354187965393\n",
      "epoch: 0 | 52928 / 114272 | training loss: 0.2917230725288391\n",
      "epoch: 0 | 52960 / 114272 | training loss: 0.2297678142786026\n",
      "epoch: 0 | 52992 / 114272 | training loss: 0.33822140097618103\n",
      "epoch: 0 | 53024 / 114272 | training loss: 0.19809973239898682\n",
      "epoch: 0 | 53056 / 114272 | training loss: 0.3831600248813629\n",
      "epoch: 0 | 53088 / 114272 | training loss: 0.6344993710517883\n",
      "epoch: 0 | 53120 / 114272 | training loss: 0.282074511051178\n",
      "epoch: 0 | 53152 / 114272 | training loss: 0.06763927638530731\n",
      "epoch: 0 | 53184 / 114272 | training loss: 0.16300365328788757\n",
      "epoch: 0 | 53216 / 114272 | training loss: 0.1003100797533989\n",
      "epoch: 0 | 53248 / 114272 | training loss: 0.10491076111793518\n",
      "epoch: 0 | 53280 / 114272 | training loss: 0.16696329414844513\n",
      "epoch: 0 | 53312 / 114272 | training loss: 0.07105319201946259\n",
      "epoch: 0 | 53344 / 114272 | training loss: 0.4168463349342346\n",
      "epoch: 0 | 53376 / 114272 | training loss: 0.2238243669271469\n",
      "epoch: 0 | 53408 / 114272 | training loss: 0.3115520477294922\n",
      "epoch: 0 | 53440 / 114272 | training loss: 0.09714270383119583\n",
      "epoch: 0 | 53472 / 114272 | training loss: 0.10206760466098785\n",
      "epoch: 0 | 53504 / 114272 | training loss: 0.15954776108264923\n",
      "epoch: 0 | 53536 / 114272 | training loss: 0.057608287781476974\n",
      "epoch: 0 | 53568 / 114272 | training loss: 0.3189692199230194\n",
      "epoch: 0 | 53600 / 114272 | training loss: 0.1284496933221817\n",
      "epoch: 0 | 53632 / 114272 | training loss: 0.15904729068279266\n",
      "epoch: 0 | 53664 / 114272 | training loss: 0.2959820628166199\n",
      "epoch: 0 | 53696 / 114272 | training loss: 0.13365937769412994\n",
      "epoch: 0 | 53728 / 114272 | training loss: 0.09165038168430328\n",
      "epoch: 0 | 53760 / 114272 | training loss: 0.26343047618865967\n",
      "epoch: 0 | 53792 / 114272 | training loss: 0.048393912613391876\n",
      "epoch: 0 | 53824 / 114272 | training loss: 0.1621915102005005\n",
      "epoch: 0 | 53856 / 114272 | training loss: 0.25638654828071594\n",
      "epoch: 0 | 53888 / 114272 | training loss: 0.07091483473777771\n",
      "epoch: 0 | 53920 / 114272 | training loss: 0.04135219752788544\n",
      "epoch: 0 | 53952 / 114272 | training loss: 0.12117744982242584\n",
      "epoch: 0 | 53984 / 114272 | training loss: 0.06084022670984268\n",
      "epoch: 0 | 54016 / 114272 | training loss: 0.15789292752742767\n",
      "epoch: 0 | 54048 / 114272 | training loss: 0.15765151381492615\n",
      "epoch: 0 | 54080 / 114272 | training loss: 0.3103313744068146\n",
      "epoch: 0 | 54112 / 114272 | training loss: 0.012916366569697857\n",
      "epoch: 0 | 54144 / 114272 | training loss: 0.2930835783481598\n",
      "epoch: 0 | 54176 / 114272 | training loss: 0.026751890778541565\n",
      "epoch: 0 | 54208 / 114272 | training loss: 0.09568509459495544\n",
      "epoch: 0 | 54240 / 114272 | training loss: 0.29158154129981995\n",
      "epoch: 0 | 54272 / 114272 | training loss: 0.43346014618873596\n",
      "epoch: 0 | 54304 / 114272 | training loss: 0.17295916378498077\n",
      "epoch: 0 | 54336 / 114272 | training loss: 0.27018338441848755\n",
      "epoch: 0 | 54368 / 114272 | training loss: 0.01254404429346323\n",
      "epoch: 0 | 54400 / 114272 | training loss: 0.18215298652648926\n",
      "epoch: 0 | 54432 / 114272 | training loss: 0.20834554731845856\n",
      "epoch: 0 | 54464 / 114272 | training loss: 0.1919088065624237\n",
      "epoch: 0 | 54496 / 114272 | training loss: 0.05915991961956024\n",
      "epoch: 0 | 54528 / 114272 | training loss: 0.11723922193050385\n",
      "epoch: 0 | 54560 / 114272 | training loss: 0.25129419565200806\n",
      "epoch: 0 | 54592 / 114272 | training loss: 0.1506119817495346\n",
      "epoch: 0 | 54624 / 114272 | training loss: 0.12217231094837189\n",
      "epoch: 0 | 54656 / 114272 | training loss: 0.19363228976726532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 54688 / 114272 | training loss: 0.14083248376846313\n",
      "epoch: 0 | 54720 / 114272 | training loss: 0.3437820374965668\n",
      "epoch: 0 | 54752 / 114272 | training loss: 0.12492237985134125\n",
      "epoch: 0 | 54784 / 114272 | training loss: 0.06165922060608864\n",
      "epoch: 0 | 54816 / 114272 | training loss: 0.2548496425151825\n",
      "epoch: 0 | 54848 / 114272 | training loss: 0.3209418058395386\n",
      "epoch: 0 | 54880 / 114272 | training loss: 0.052280325442552567\n",
      "epoch: 0 | 54912 / 114272 | training loss: 0.12257584184408188\n",
      "epoch: 0 | 54944 / 114272 | training loss: 0.09914392232894897\n",
      "epoch: 0 | 54976 / 114272 | training loss: 0.20842114090919495\n",
      "epoch: 0 | 55008 / 114272 | training loss: 0.09456513822078705\n",
      "epoch: 0 | 55040 / 114272 | training loss: 0.08671540766954422\n",
      "epoch: 0 | 55072 / 114272 | training loss: 0.04301784560084343\n",
      "epoch: 0 | 55104 / 114272 | training loss: 0.03419991210103035\n",
      "epoch: 0 | 55136 / 114272 | training loss: 0.04004867002367973\n",
      "epoch: 0 | 55168 / 114272 | training loss: 0.1566903293132782\n",
      "epoch: 0 | 55200 / 114272 | training loss: 0.15224406123161316\n",
      "epoch: 0 | 55232 / 114272 | training loss: 0.02472701296210289\n",
      "epoch: 0 | 55264 / 114272 | training loss: 0.10271457582712173\n",
      "epoch: 0 | 55296 / 114272 | training loss: 0.1627882868051529\n",
      "epoch: 0 | 55328 / 114272 | training loss: 0.09824510663747787\n",
      "epoch: 0 | 55360 / 114272 | training loss: 0.16520343720912933\n",
      "epoch: 0 | 55392 / 114272 | training loss: 0.03784269466996193\n",
      "epoch: 0 | 55424 / 114272 | training loss: 0.07013288885354996\n",
      "epoch: 0 | 55456 / 114272 | training loss: 0.3517528474330902\n",
      "epoch: 0 | 55488 / 114272 | training loss: 0.23943251371383667\n",
      "epoch: 0 | 55520 / 114272 | training loss: 0.14259524643421173\n",
      "epoch: 0 | 55552 / 114272 | training loss: 0.027011720463633537\n",
      "epoch: 0 | 55584 / 114272 | training loss: 0.19728495180606842\n",
      "epoch: 0 | 55616 / 114272 | training loss: 0.02105785347521305\n",
      "epoch: 0 | 55648 / 114272 | training loss: 0.17173928022384644\n",
      "epoch: 0 | 55680 / 114272 | training loss: 0.25167763233184814\n",
      "epoch: 0 | 55712 / 114272 | training loss: 0.16889688372612\n",
      "epoch: 0 | 55744 / 114272 | training loss: 0.13895581662654877\n",
      "epoch: 0 | 55776 / 114272 | training loss: 0.2417871654033661\n",
      "epoch: 0 | 55808 / 114272 | training loss: 0.05620121955871582\n",
      "epoch: 0 | 55840 / 114272 | training loss: 0.03921376168727875\n",
      "epoch: 0 | 55872 / 114272 | training loss: 0.20211181044578552\n",
      "epoch: 0 | 55904 / 114272 | training loss: 0.09382026642560959\n",
      "epoch: 0 | 55936 / 114272 | training loss: 0.29010432958602905\n",
      "epoch: 0 | 55968 / 114272 | training loss: 0.07730518281459808\n",
      "epoch: 0 | 56000 / 114272 | training loss: 0.08928340673446655\n",
      "epoch: 0 | 56032 / 114272 | training loss: 0.21638768911361694\n",
      "epoch: 0 | 56064 / 114272 | training loss: 0.1668430119752884\n",
      "epoch: 0 | 56096 / 114272 | training loss: 0.25862962007522583\n",
      "epoch: 0 | 56128 / 114272 | training loss: 0.04579417034983635\n",
      "epoch: 0 | 56160 / 114272 | training loss: 0.16606509685516357\n",
      "epoch: 0 | 56192 / 114272 | training loss: 0.06011553853750229\n",
      "epoch: 0 | 56224 / 114272 | training loss: 0.30649465322494507\n",
      "epoch: 0 | 56256 / 114272 | training loss: 0.16698221862316132\n",
      "epoch: 0 | 56288 / 114272 | training loss: 0.2867715358734131\n",
      "epoch: 0 | 56320 / 114272 | training loss: 0.23068146407604218\n",
      "epoch: 0 | 56352 / 114272 | training loss: 0.29095324873924255\n",
      "epoch: 0 | 56384 / 114272 | training loss: 0.22453993558883667\n",
      "epoch: 0 | 56416 / 114272 | training loss: 0.35870110988616943\n",
      "epoch: 0 | 56448 / 114272 | training loss: 0.0730896145105362\n",
      "epoch: 0 | 56480 / 114272 | training loss: 0.261491984128952\n",
      "epoch: 0 | 56512 / 114272 | training loss: 0.08372484892606735\n",
      "epoch: 0 | 56544 / 114272 | training loss: 0.23005244135856628\n",
      "epoch: 0 | 56576 / 114272 | training loss: 0.2857549488544464\n",
      "epoch: 0 | 56608 / 114272 | training loss: 0.05703403055667877\n",
      "epoch: 0 | 56640 / 114272 | training loss: 0.38648906350135803\n",
      "epoch: 0 | 56672 / 114272 | training loss: 0.25105980038642883\n",
      "epoch: 0 | 56704 / 114272 | training loss: 0.24603530764579773\n",
      "epoch: 0 | 56736 / 114272 | training loss: 0.03252219408750534\n",
      "epoch: 0 | 56768 / 114272 | training loss: 0.2942938804626465\n",
      "epoch: 0 | 56800 / 114272 | training loss: 0.13953225314617157\n",
      "epoch: 0 | 56832 / 114272 | training loss: 0.06873652338981628\n",
      "epoch: 0 | 56864 / 114272 | training loss: 0.1253868043422699\n",
      "epoch: 0 | 56896 / 114272 | training loss: 0.40983477234840393\n",
      "epoch: 0 | 56928 / 114272 | training loss: 0.09976077079772949\n",
      "epoch: 0 | 56960 / 114272 | training loss: 0.29378682374954224\n",
      "epoch: 0 | 56992 / 114272 | training loss: 0.22921296954154968\n",
      "epoch: 0 | 57024 / 114272 | training loss: 0.14367426931858063\n",
      "epoch: 0 | 57056 / 114272 | training loss: 0.2095603048801422\n",
      "epoch: 0 | 57088 / 114272 | training loss: 0.08397438377141953\n",
      "epoch: 0 | 57120 / 114272 | training loss: 0.1027287170290947\n",
      "epoch: 0 | 57152 / 114272 | training loss: 0.2346787005662918\n",
      "epoch: 0 | 57184 / 114272 | training loss: 0.1658579707145691\n",
      "epoch: 0 | 57216 / 114272 | training loss: 0.30138224363327026\n",
      "epoch: 0 | 57248 / 114272 | training loss: 0.17995266616344452\n",
      "epoch: 0 | 57280 / 114272 | training loss: 0.28099244832992554\n",
      "epoch: 0 | 57312 / 114272 | training loss: 0.2704005837440491\n",
      "epoch: 0 | 57344 / 114272 | training loss: 0.2091977447271347\n",
      "epoch: 0 | 57376 / 114272 | training loss: 0.16001759469509125\n",
      "epoch: 0 | 57408 / 114272 | training loss: 0.0795234814286232\n",
      "epoch: 0 | 57440 / 114272 | training loss: 0.2540946304798126\n",
      "epoch: 0 | 57472 / 114272 | training loss: 0.2311234027147293\n",
      "epoch: 0 | 57504 / 114272 | training loss: 0.03212818130850792\n",
      "epoch: 0 | 57536 / 114272 | training loss: 0.08170659840106964\n",
      "epoch: 0 | 57568 / 114272 | training loss: 0.23064450919628143\n",
      "epoch: 0 | 57600 / 114272 | training loss: 0.280221164226532\n",
      "epoch: 0 | 57632 / 114272 | training loss: 0.07189281284809113\n",
      "epoch: 0 | 57664 / 114272 | training loss: 0.1874760389328003\n",
      "epoch: 0 | 57696 / 114272 | training loss: 0.0999617725610733\n",
      "epoch: 0 | 57728 / 114272 | training loss: 0.16294410824775696\n",
      "epoch: 0 | 57760 / 114272 | training loss: 0.22423851490020752\n",
      "epoch: 0 | 57792 / 114272 | training loss: 0.07701066881418228\n",
      "epoch: 0 | 57824 / 114272 | training loss: 0.05700615793466568\n",
      "epoch: 0 | 57856 / 114272 | training loss: 0.10843545198440552\n",
      "epoch: 0 | 57888 / 114272 | training loss: 0.08682109415531158\n",
      "epoch: 0 | 57920 / 114272 | training loss: 0.17527130246162415\n",
      "epoch: 0 | 57952 / 114272 | training loss: 0.05864810571074486\n",
      "epoch: 0 | 57984 / 114272 | training loss: 0.09486707299947739\n",
      "epoch: 0 | 58016 / 114272 | training loss: 0.10414189100265503\n",
      "epoch: 0 | 58048 / 114272 | training loss: 0.24263586103916168\n",
      "epoch: 0 | 58080 / 114272 | training loss: 0.07651501893997192\n",
      "epoch: 0 | 58112 / 114272 | training loss: 0.08770009875297546\n",
      "epoch: 0 | 58144 / 114272 | training loss: 0.025084178894758224\n",
      "epoch: 0 | 58176 / 114272 | training loss: 0.1605149209499359\n",
      "epoch: 0 | 58208 / 114272 | training loss: 0.21088182926177979\n",
      "epoch: 0 | 58240 / 114272 | training loss: 0.10521531850099564\n",
      "epoch: 0 | 58272 / 114272 | training loss: 0.046091414988040924\n",
      "epoch: 0 | 58304 / 114272 | training loss: 0.029208125546574593\n",
      "epoch: 0 | 58336 / 114272 | training loss: 0.1330137401819229\n",
      "epoch: 0 | 58368 / 114272 | training loss: 0.33436623215675354\n",
      "epoch: 0 | 58400 / 114272 | training loss: 0.13599435985088348\n",
      "epoch: 0 | 58432 / 114272 | training loss: 0.2058100402355194\n",
      "epoch: 0 | 58464 / 114272 | training loss: 0.07015378773212433\n",
      "epoch: 0 | 58496 / 114272 | training loss: 0.20324282348155975\n",
      "epoch: 0 | 58528 / 114272 | training loss: 0.012513073161244392\n",
      "epoch: 0 | 58560 / 114272 | training loss: 0.3078271150588989\n",
      "epoch: 0 | 58592 / 114272 | training loss: 0.12884649634361267\n",
      "epoch: 0 | 58624 / 114272 | training loss: 0.07664204388856888\n",
      "epoch: 0 | 58656 / 114272 | training loss: 0.5196849703788757\n",
      "epoch: 0 | 58688 / 114272 | training loss: 0.07946132123470306\n",
      "epoch: 0 | 58720 / 114272 | training loss: 0.2834935784339905\n",
      "epoch: 0 | 58752 / 114272 | training loss: 0.014701995998620987\n",
      "epoch: 0 | 58784 / 114272 | training loss: 0.46168187260627747\n",
      "epoch: 0 | 58816 / 114272 | training loss: 0.16620849072933197\n",
      "epoch: 0 | 58848 / 114272 | training loss: 0.038191284984350204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 58880 / 114272 | training loss: 0.2677321434020996\n",
      "epoch: 0 | 58912 / 114272 | training loss: 0.13059918582439423\n",
      "epoch: 0 | 58944 / 114272 | training loss: 0.23689639568328857\n",
      "epoch: 0 | 58976 / 114272 | training loss: 0.3758169114589691\n",
      "epoch: 0 | 59008 / 114272 | training loss: 0.01607748307287693\n",
      "epoch: 0 | 59040 / 114272 | training loss: 0.029170462861657143\n",
      "epoch: 0 | 59072 / 114272 | training loss: 0.3179752826690674\n",
      "epoch: 0 | 59104 / 114272 | training loss: 0.19291605055332184\n",
      "epoch: 0 | 59136 / 114272 | training loss: 0.07133567333221436\n",
      "epoch: 0 | 59168 / 114272 | training loss: 0.0361807644367218\n",
      "epoch: 0 | 59200 / 114272 | training loss: 0.045922622084617615\n",
      "epoch: 0 | 59232 / 114272 | training loss: 0.21327732503414154\n",
      "epoch: 0 | 59264 / 114272 | training loss: 0.07321301102638245\n",
      "epoch: 0 | 59296 / 114272 | training loss: 0.03178173676133156\n",
      "epoch: 0 | 59328 / 114272 | training loss: 0.13523605465888977\n",
      "epoch: 0 | 59360 / 114272 | training loss: 0.1250009685754776\n",
      "epoch: 0 | 59392 / 114272 | training loss: 0.3212609589099884\n",
      "epoch: 0 | 59424 / 114272 | training loss: 0.03307809308171272\n",
      "epoch: 0 | 59456 / 114272 | training loss: 0.07557860761880875\n",
      "epoch: 0 | 59488 / 114272 | training loss: 0.1009790301322937\n",
      "epoch: 0 | 59520 / 114272 | training loss: 0.1938493251800537\n",
      "epoch: 0 | 59552 / 114272 | training loss: 0.10028005391359329\n",
      "epoch: 0 | 59584 / 114272 | training loss: 0.024766381829977036\n",
      "epoch: 0 | 59616 / 114272 | training loss: 0.01361932884901762\n",
      "epoch: 0 | 59648 / 114272 | training loss: 0.25949525833129883\n",
      "epoch: 0 | 59680 / 114272 | training loss: 0.21342548727989197\n",
      "epoch: 0 | 59712 / 114272 | training loss: 0.25100177526474\n",
      "epoch: 0 | 59744 / 114272 | training loss: 0.21484409272670746\n",
      "epoch: 0 | 59776 / 114272 | training loss: 0.1425251066684723\n",
      "epoch: 0 | 59808 / 114272 | training loss: 0.25809863209724426\n",
      "epoch: 0 | 59840 / 114272 | training loss: 0.02406284213066101\n",
      "epoch: 0 | 59872 / 114272 | training loss: 0.1669849455356598\n",
      "epoch: 0 | 59904 / 114272 | training loss: 0.17847973108291626\n",
      "epoch: 0 | 59936 / 114272 | training loss: 0.10993973910808563\n",
      "epoch: 0 | 59968 / 114272 | training loss: 0.008285753428936005\n",
      "epoch: 0 | 60000 / 114272 | training loss: 0.24466325342655182\n",
      "epoch: 0 | 60032 / 114272 | training loss: 0.40804505348205566\n",
      "epoch: 0 | 60064 / 114272 | training loss: 0.6447819471359253\n",
      "epoch: 0 | 60096 / 114272 | training loss: 0.06956114619970322\n",
      "epoch: 0 | 60128 / 114272 | training loss: 0.10649419575929642\n",
      "epoch: 0 | 60160 / 114272 | training loss: 0.02006552927196026\n",
      "epoch: 0 | 60192 / 114272 | training loss: 0.10321734845638275\n",
      "epoch: 0 | 60224 / 114272 | training loss: 0.20947600901126862\n",
      "epoch: 0 | 60256 / 114272 | training loss: 0.1565626859664917\n",
      "epoch: 0 | 60288 / 114272 | training loss: 0.21240855753421783\n",
      "epoch: 0 | 60320 / 114272 | training loss: 0.33451589941978455\n",
      "epoch: 0 | 60352 / 114272 | training loss: 0.10775918513536453\n",
      "epoch: 0 | 60384 / 114272 | training loss: 0.28572404384613037\n",
      "epoch: 0 | 60416 / 114272 | training loss: 0.18192195892333984\n",
      "epoch: 0 | 60448 / 114272 | training loss: 0.09891065210103989\n",
      "epoch: 0 | 60480 / 114272 | training loss: 0.1922171711921692\n",
      "epoch: 0 | 60512 / 114272 | training loss: 0.19987668097019196\n",
      "epoch: 0 | 60544 / 114272 | training loss: 0.03297390788793564\n",
      "epoch: 0 | 60576 / 114272 | training loss: 0.1657186597585678\n",
      "epoch: 0 | 60608 / 114272 | training loss: 0.21366271376609802\n",
      "epoch: 0 | 60640 / 114272 | training loss: 0.18654057383537292\n",
      "epoch: 0 | 60672 / 114272 | training loss: 0.18540701270103455\n",
      "epoch: 0 | 60704 / 114272 | training loss: 0.17026172578334808\n",
      "epoch: 0 | 60736 / 114272 | training loss: 0.07480471581220627\n",
      "epoch: 0 | 60768 / 114272 | training loss: 0.4276083707809448\n",
      "epoch: 0 | 60800 / 114272 | training loss: 0.13090084493160248\n",
      "epoch: 0 | 60832 / 114272 | training loss: 0.07898309081792831\n",
      "epoch: 0 | 60864 / 114272 | training loss: 0.21379418671131134\n",
      "epoch: 0 | 60896 / 114272 | training loss: 0.17992499470710754\n",
      "epoch: 0 | 60928 / 114272 | training loss: 0.18965041637420654\n",
      "epoch: 0 | 60960 / 114272 | training loss: 0.18615923821926117\n",
      "epoch: 0 | 60992 / 114272 | training loss: 0.18229128420352936\n",
      "epoch: 0 | 61024 / 114272 | training loss: 0.1782637983560562\n",
      "epoch: 0 | 61056 / 114272 | training loss: 0.20272542536258698\n",
      "epoch: 0 | 61088 / 114272 | training loss: 0.13039658963680267\n",
      "epoch: 0 | 61120 / 114272 | training loss: 0.2612519860267639\n",
      "epoch: 0 | 61152 / 114272 | training loss: 0.015416909009218216\n",
      "epoch: 0 | 61184 / 114272 | training loss: 0.20818637311458588\n",
      "epoch: 0 | 61216 / 114272 | training loss: 0.21618270874023438\n",
      "epoch: 0 | 61248 / 114272 | training loss: 0.1174938902258873\n",
      "epoch: 0 | 61280 / 114272 | training loss: 0.17135395109653473\n",
      "epoch: 0 | 61312 / 114272 | training loss: 0.2142389863729477\n",
      "epoch: 0 | 61344 / 114272 | training loss: 0.31036263704299927\n",
      "epoch: 0 | 61376 / 114272 | training loss: 0.15887907147407532\n",
      "epoch: 0 | 61408 / 114272 | training loss: 0.19829212129116058\n",
      "epoch: 0 | 61440 / 114272 | training loss: 0.035773955285549164\n",
      "epoch: 0 | 61472 / 114272 | training loss: 0.15152888000011444\n",
      "epoch: 0 | 61504 / 114272 | training loss: 0.04334062710404396\n",
      "epoch: 0 | 61536 / 114272 | training loss: 0.40235376358032227\n",
      "epoch: 0 | 61568 / 114272 | training loss: 0.13778184354305267\n",
      "epoch: 0 | 61600 / 114272 | training loss: 0.16441404819488525\n",
      "epoch: 0 | 61632 / 114272 | training loss: 0.39356866478919983\n",
      "epoch: 0 | 61664 / 114272 | training loss: 0.11880924552679062\n",
      "epoch: 0 | 61696 / 114272 | training loss: 0.057669851928949356\n",
      "epoch: 0 | 61728 / 114272 | training loss: 0.1902759075164795\n",
      "epoch: 0 | 61760 / 114272 | training loss: 0.18868345022201538\n",
      "epoch: 0 | 61792 / 114272 | training loss: 0.3241386115550995\n",
      "epoch: 0 | 61824 / 114272 | training loss: 0.1385907083749771\n",
      "epoch: 0 | 61856 / 114272 | training loss: 0.15401853621006012\n",
      "epoch: 0 | 61888 / 114272 | training loss: 0.058525800704956055\n",
      "epoch: 0 | 61920 / 114272 | training loss: 0.11868906021118164\n",
      "epoch: 0 | 61952 / 114272 | training loss: 0.0758964940905571\n",
      "epoch: 0 | 61984 / 114272 | training loss: 0.5560723543167114\n",
      "epoch: 0 | 62016 / 114272 | training loss: 0.10453895479440689\n",
      "epoch: 0 | 62048 / 114272 | training loss: 0.1814887523651123\n",
      "epoch: 0 | 62080 / 114272 | training loss: 0.10547759383916855\n",
      "epoch: 0 | 62112 / 114272 | training loss: 0.23188337683677673\n",
      "epoch: 0 | 62144 / 114272 | training loss: 0.10810676217079163\n",
      "epoch: 0 | 62176 / 114272 | training loss: 0.06046809256076813\n",
      "epoch: 0 | 62208 / 114272 | training loss: 0.04142235219478607\n",
      "epoch: 0 | 62240 / 114272 | training loss: 0.3059239089488983\n",
      "epoch: 0 | 62272 / 114272 | training loss: 0.08428739756345749\n",
      "epoch: 0 | 62304 / 114272 | training loss: 0.12848404049873352\n",
      "epoch: 0 | 62336 / 114272 | training loss: 0.27993422746658325\n",
      "epoch: 0 | 62368 / 114272 | training loss: 0.37133142352104187\n",
      "epoch: 0 | 62400 / 114272 | training loss: 0.10619237273931503\n",
      "epoch: 0 | 62432 / 114272 | training loss: 0.10536162555217743\n",
      "epoch: 0 | 62464 / 114272 | training loss: 0.08642224967479706\n",
      "epoch: 0 | 62496 / 114272 | training loss: 0.17876335978507996\n",
      "epoch: 0 | 62528 / 114272 | training loss: 0.03362857550382614\n",
      "epoch: 0 | 62560 / 114272 | training loss: 0.07270251959562302\n",
      "epoch: 0 | 62592 / 114272 | training loss: 0.2562705874443054\n",
      "epoch: 0 | 62624 / 114272 | training loss: 0.2589542269706726\n",
      "epoch: 0 | 62656 / 114272 | training loss: 0.12496595829725266\n",
      "epoch: 0 | 62688 / 114272 | training loss: 0.06642884016036987\n",
      "epoch: 0 | 62720 / 114272 | training loss: 0.14108459651470184\n",
      "epoch: 0 | 62752 / 114272 | training loss: 0.16434670984745026\n",
      "epoch: 0 | 62784 / 114272 | training loss: 0.18216168880462646\n",
      "epoch: 0 | 62816 / 114272 | training loss: 0.39312833547592163\n",
      "epoch: 0 | 62848 / 114272 | training loss: 0.0820484310388565\n",
      "epoch: 0 | 62880 / 114272 | training loss: 0.18096821010112762\n",
      "epoch: 0 | 62912 / 114272 | training loss: 0.05787160247564316\n",
      "epoch: 0 | 62944 / 114272 | training loss: 0.26081663370132446\n",
      "epoch: 0 | 62976 / 114272 | training loss: 0.24516116082668304\n",
      "epoch: 0 | 63008 / 114272 | training loss: 0.1074473187327385\n",
      "epoch: 0 | 63040 / 114272 | training loss: 0.13274560868740082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 63072 / 114272 | training loss: 0.29713669419288635\n",
      "epoch: 0 | 63104 / 114272 | training loss: 0.22906789183616638\n",
      "epoch: 0 | 63136 / 114272 | training loss: 0.09487367421388626\n",
      "epoch: 0 | 63168 / 114272 | training loss: 0.12022292613983154\n",
      "epoch: 0 | 63200 / 114272 | training loss: 0.1412220597267151\n",
      "epoch: 0 | 63232 / 114272 | training loss: 0.16193997859954834\n",
      "epoch: 0 | 63264 / 114272 | training loss: 0.25626641511917114\n",
      "epoch: 0 | 63296 / 114272 | training loss: 0.09227227419614792\n",
      "epoch: 0 | 63328 / 114272 | training loss: 0.05513482168316841\n",
      "epoch: 0 | 63360 / 114272 | training loss: 0.24870499968528748\n",
      "epoch: 0 | 63392 / 114272 | training loss: 0.3639829456806183\n",
      "epoch: 0 | 63424 / 114272 | training loss: 0.04765366390347481\n",
      "epoch: 0 | 63456 / 114272 | training loss: 0.19435109198093414\n",
      "epoch: 0 | 63488 / 114272 | training loss: 0.17653152346611023\n",
      "epoch: 0 | 63520 / 114272 | training loss: 0.1877496987581253\n",
      "epoch: 0 | 63552 / 114272 | training loss: 0.023178109899163246\n",
      "epoch: 0 | 63584 / 114272 | training loss: 0.18857282400131226\n",
      "epoch: 0 | 63616 / 114272 | training loss: 0.0478711761534214\n",
      "epoch: 0 | 63648 / 114272 | training loss: 0.06353507190942764\n",
      "epoch: 0 | 63680 / 114272 | training loss: 0.046702511608600616\n",
      "epoch: 0 | 63712 / 114272 | training loss: 0.14503350853919983\n",
      "epoch: 0 | 63744 / 114272 | training loss: 0.06085396558046341\n",
      "epoch: 0 | 63776 / 114272 | training loss: 0.0753733217716217\n",
      "epoch: 0 | 63808 / 114272 | training loss: 0.15036581456661224\n",
      "epoch: 0 | 63840 / 114272 | training loss: 0.04758754372596741\n",
      "epoch: 0 | 63872 / 114272 | training loss: 0.17307916283607483\n",
      "epoch: 0 | 63904 / 114272 | training loss: 0.06378932297229767\n",
      "epoch: 0 | 63936 / 114272 | training loss: 0.04589804261922836\n",
      "epoch: 0 | 63968 / 114272 | training loss: 0.5074328780174255\n",
      "epoch: 0 | 64000 / 114272 | training loss: 0.13663971424102783\n",
      "epoch: 0 | 64032 / 114272 | training loss: 0.015607321634888649\n",
      "epoch: 0 | 64064 / 114272 | training loss: 0.008695541881024837\n",
      "epoch: 0 | 64096 / 114272 | training loss: 0.12254253029823303\n",
      "epoch: 0 | 64128 / 114272 | training loss: 0.1254197657108307\n",
      "epoch: 0 | 64160 / 114272 | training loss: 0.3408721387386322\n",
      "epoch: 0 | 64192 / 114272 | training loss: 0.03448314219713211\n",
      "epoch: 0 | 64224 / 114272 | training loss: 0.007165089715272188\n",
      "epoch: 0 | 64256 / 114272 | training loss: 0.020333275198936462\n",
      "epoch: 0 | 64288 / 114272 | training loss: 0.13679665327072144\n",
      "epoch: 0 | 64320 / 114272 | training loss: 0.20861674845218658\n",
      "epoch: 0 | 64352 / 114272 | training loss: 0.21809172630310059\n",
      "epoch: 0 | 64384 / 114272 | training loss: 0.5460017919540405\n",
      "epoch: 0 | 64416 / 114272 | training loss: 0.3259489834308624\n",
      "epoch: 0 | 64448 / 114272 | training loss: 0.03212079778313637\n",
      "epoch: 0 | 64480 / 114272 | training loss: 0.2063806802034378\n",
      "epoch: 0 | 64512 / 114272 | training loss: 0.26110342144966125\n",
      "epoch: 0 | 64544 / 114272 | training loss: 0.12583644688129425\n",
      "epoch: 0 | 64576 / 114272 | training loss: 0.34627488255500793\n",
      "epoch: 0 | 64608 / 114272 | training loss: 0.32411491870880127\n",
      "epoch: 0 | 64640 / 114272 | training loss: 0.11926417797803879\n",
      "epoch: 0 | 64672 / 114272 | training loss: 0.28795546293258667\n",
      "epoch: 0 | 64704 / 114272 | training loss: 0.15127919614315033\n",
      "epoch: 0 | 64736 / 114272 | training loss: 0.1820273995399475\n",
      "epoch: 0 | 64768 / 114272 | training loss: 0.1100781038403511\n",
      "epoch: 0 | 64800 / 114272 | training loss: 0.10301051288843155\n",
      "epoch: 0 | 64832 / 114272 | training loss: 0.18374931812286377\n",
      "epoch: 0 | 64864 / 114272 | training loss: 0.1798734813928604\n",
      "epoch: 0 | 64896 / 114272 | training loss: 0.2300289422273636\n",
      "epoch: 0 | 64928 / 114272 | training loss: 0.05281021445989609\n",
      "epoch: 0 | 64960 / 114272 | training loss: 0.04036463424563408\n",
      "epoch: 0 | 64992 / 114272 | training loss: 0.31672415137290955\n",
      "epoch: 0 | 65024 / 114272 | training loss: 0.33116909861564636\n",
      "epoch: 0 | 65056 / 114272 | training loss: 0.05087850242853165\n",
      "epoch: 0 | 65088 / 114272 | training loss: 0.3464309275150299\n",
      "epoch: 0 | 65120 / 114272 | training loss: 0.12857595086097717\n",
      "epoch: 0 | 65152 / 114272 | training loss: 0.15747465193271637\n",
      "epoch: 0 | 65184 / 114272 | training loss: 0.0632869154214859\n",
      "epoch: 0 | 65216 / 114272 | training loss: 0.12244707345962524\n",
      "epoch: 0 | 65248 / 114272 | training loss: 0.044533587992191315\n",
      "epoch: 0 | 65280 / 114272 | training loss: 0.074704609811306\n",
      "epoch: 0 | 65312 / 114272 | training loss: 0.06530655920505524\n",
      "epoch: 0 | 65344 / 114272 | training loss: 0.1235552728176117\n",
      "epoch: 0 | 65376 / 114272 | training loss: 0.13396361470222473\n",
      "epoch: 0 | 65408 / 114272 | training loss: 0.09237177670001984\n",
      "epoch: 0 | 65440 / 114272 | training loss: 0.10195042937994003\n",
      "epoch: 0 | 65472 / 114272 | training loss: 0.1369166076183319\n",
      "epoch: 0 | 65504 / 114272 | training loss: 0.26243120431900024\n",
      "epoch: 0 | 65536 / 114272 | training loss: 0.21817006170749664\n",
      "epoch: 0 | 65568 / 114272 | training loss: 0.2918208837509155\n",
      "epoch: 0 | 65600 / 114272 | training loss: 0.15111076831817627\n",
      "epoch: 0 | 65632 / 114272 | training loss: 0.015213904902338982\n",
      "epoch: 0 | 65664 / 114272 | training loss: 0.17397628724575043\n",
      "epoch: 0 | 65696 / 114272 | training loss: 0.02840932458639145\n",
      "epoch: 0 | 65728 / 114272 | training loss: 0.11578685790300369\n",
      "epoch: 0 | 65760 / 114272 | training loss: 0.10516722500324249\n",
      "epoch: 0 | 65792 / 114272 | training loss: 0.18045559525489807\n",
      "epoch: 0 | 65824 / 114272 | training loss: 0.23299281299114227\n",
      "epoch: 0 | 65856 / 114272 | training loss: 0.3450583219528198\n",
      "epoch: 0 | 65888 / 114272 | training loss: 0.1650463491678238\n",
      "epoch: 0 | 65920 / 114272 | training loss: 0.37903130054473877\n",
      "epoch: 0 | 65952 / 114272 | training loss: 0.0992019847035408\n",
      "epoch: 0 | 65984 / 114272 | training loss: 0.042105041444301605\n",
      "epoch: 0 | 66016 / 114272 | training loss: 0.10374510288238525\n",
      "epoch: 0 | 66048 / 114272 | training loss: 0.11362733691930771\n",
      "epoch: 0 | 66080 / 114272 | training loss: 0.13668541610240936\n",
      "epoch: 0 | 66112 / 114272 | training loss: 0.08247508108615875\n",
      "epoch: 0 | 66144 / 114272 | training loss: 0.2893509566783905\n",
      "epoch: 0 | 66176 / 114272 | training loss: 0.13916514813899994\n",
      "epoch: 0 | 66208 / 114272 | training loss: 0.16893437504768372\n",
      "epoch: 0 | 66240 / 114272 | training loss: 0.15417379140853882\n",
      "epoch: 0 | 66272 / 114272 | training loss: 0.1769668310880661\n",
      "epoch: 0 | 66304 / 114272 | training loss: 0.2024965137243271\n",
      "epoch: 0 | 66336 / 114272 | training loss: 0.1562972366809845\n",
      "epoch: 0 | 66368 / 114272 | training loss: 0.30907565355300903\n",
      "epoch: 0 | 66400 / 114272 | training loss: 0.45279914140701294\n",
      "epoch: 0 | 66432 / 114272 | training loss: 0.3317623436450958\n",
      "epoch: 0 | 66464 / 114272 | training loss: 0.3665769398212433\n",
      "epoch: 0 | 66496 / 114272 | training loss: 0.051956310868263245\n",
      "epoch: 0 | 66528 / 114272 | training loss: 0.15002907812595367\n",
      "epoch: 0 | 66560 / 114272 | training loss: 0.25725218653678894\n",
      "epoch: 0 | 66592 / 114272 | training loss: 0.2185065895318985\n",
      "epoch: 0 | 66624 / 114272 | training loss: 0.4196777045726776\n",
      "epoch: 0 | 66656 / 114272 | training loss: 0.1023043766617775\n",
      "epoch: 0 | 66688 / 114272 | training loss: 0.038353074342012405\n",
      "epoch: 0 | 66720 / 114272 | training loss: 0.335202157497406\n",
      "epoch: 0 | 66752 / 114272 | training loss: 0.24664273858070374\n",
      "epoch: 0 | 66784 / 114272 | training loss: 0.202222540974617\n",
      "epoch: 0 | 66816 / 114272 | training loss: 0.2734759449958801\n",
      "epoch: 0 | 66848 / 114272 | training loss: 0.2332487404346466\n",
      "epoch: 0 | 66880 / 114272 | training loss: 0.22515399754047394\n",
      "epoch: 0 | 66912 / 114272 | training loss: 0.49076616764068604\n",
      "epoch: 0 | 66944 / 114272 | training loss: 0.0812857449054718\n",
      "epoch: 0 | 66976 / 114272 | training loss: 0.18624767661094666\n",
      "epoch: 0 | 67008 / 114272 | training loss: 0.08645453304052353\n",
      "epoch: 0 | 67040 / 114272 | training loss: 0.12636426091194153\n",
      "epoch: 0 | 67072 / 114272 | training loss: 0.08531811088323593\n",
      "epoch: 0 | 67104 / 114272 | training loss: 0.08526363968849182\n",
      "epoch: 0 | 67136 / 114272 | training loss: 0.17770057916641235\n",
      "epoch: 0 | 67168 / 114272 | training loss: 0.15016710758209229\n",
      "epoch: 0 | 67200 / 114272 | training loss: 0.21616709232330322\n",
      "epoch: 0 | 67232 / 114272 | training loss: 0.25055089592933655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 67264 / 114272 | training loss: 0.22726888954639435\n",
      "epoch: 0 | 67296 / 114272 | training loss: 0.21889564394950867\n",
      "epoch: 0 | 67328 / 114272 | training loss: 0.03351788595318794\n",
      "epoch: 0 | 67360 / 114272 | training loss: 0.18897758424282074\n",
      "epoch: 0 | 67392 / 114272 | training loss: 0.12394842505455017\n",
      "epoch: 0 | 67424 / 114272 | training loss: 0.1455589234828949\n",
      "epoch: 0 | 67456 / 114272 | training loss: 0.33023378252983093\n",
      "epoch: 0 | 67488 / 114272 | training loss: 0.08982831984758377\n",
      "epoch: 0 | 67520 / 114272 | training loss: 0.032559365034103394\n",
      "epoch: 0 | 67552 / 114272 | training loss: 0.09910781681537628\n",
      "epoch: 0 | 67584 / 114272 | training loss: 0.13647079467773438\n",
      "epoch: 0 | 67616 / 114272 | training loss: 0.14020943641662598\n",
      "epoch: 0 | 67648 / 114272 | training loss: 0.1616402268409729\n",
      "epoch: 0 | 67680 / 114272 | training loss: 0.13356848061084747\n",
      "epoch: 0 | 67712 / 114272 | training loss: 0.4138515591621399\n",
      "epoch: 0 | 67744 / 114272 | training loss: 0.27119794487953186\n",
      "epoch: 0 | 67776 / 114272 | training loss: 0.058859437704086304\n",
      "epoch: 0 | 67808 / 114272 | training loss: 0.08105621486902237\n",
      "epoch: 0 | 67840 / 114272 | training loss: 0.19439087808132172\n",
      "epoch: 0 | 67872 / 114272 | training loss: 0.15767887234687805\n",
      "epoch: 0 | 67904 / 114272 | training loss: 0.07647078484296799\n",
      "epoch: 0 | 67936 / 114272 | training loss: 0.1526770442724228\n",
      "epoch: 0 | 67968 / 114272 | training loss: 0.22861473262310028\n",
      "epoch: 0 | 68000 / 114272 | training loss: 0.028051910921931267\n",
      "epoch: 0 | 68032 / 114272 | training loss: 0.023060832172632217\n",
      "epoch: 0 | 68064 / 114272 | training loss: 0.09681568294763565\n",
      "epoch: 0 | 68096 / 114272 | training loss: 0.09462951868772507\n",
      "epoch: 0 | 68128 / 114272 | training loss: 0.24426770210266113\n",
      "epoch: 0 | 68160 / 114272 | training loss: 0.2804086208343506\n",
      "epoch: 0 | 68192 / 114272 | training loss: 0.06736458092927933\n",
      "epoch: 0 | 68224 / 114272 | training loss: 0.06240889057517052\n",
      "epoch: 0 | 68256 / 114272 | training loss: 0.3484676778316498\n",
      "epoch: 0 | 68288 / 114272 | training loss: 0.11520220339298248\n",
      "epoch: 0 | 68320 / 114272 | training loss: 0.1886981576681137\n",
      "epoch: 0 | 68352 / 114272 | training loss: 0.16141840815544128\n",
      "epoch: 0 | 68384 / 114272 | training loss: 0.059970464557409286\n",
      "epoch: 0 | 68416 / 114272 | training loss: 0.18634198606014252\n",
      "epoch: 0 | 68448 / 114272 | training loss: 0.12321377545595169\n",
      "epoch: 0 | 68480 / 114272 | training loss: 0.052872952073812485\n",
      "epoch: 0 | 68512 / 114272 | training loss: 0.34410560131073\n",
      "epoch: 0 | 68544 / 114272 | training loss: 0.1852131187915802\n",
      "epoch: 0 | 68576 / 114272 | training loss: 0.04785313457250595\n",
      "epoch: 0 | 68608 / 114272 | training loss: 0.0365021787583828\n",
      "epoch: 0 | 68640 / 114272 | training loss: 0.07626135647296906\n",
      "epoch: 0 | 68672 / 114272 | training loss: 0.11690451204776764\n",
      "epoch: 0 | 68704 / 114272 | training loss: 0.14153781533241272\n",
      "epoch: 0 | 68736 / 114272 | training loss: 0.09769253432750702\n",
      "epoch: 0 | 68768 / 114272 | training loss: 0.33130261301994324\n",
      "epoch: 0 | 68800 / 114272 | training loss: 0.021163439378142357\n",
      "epoch: 0 | 68832 / 114272 | training loss: 0.06094662845134735\n",
      "epoch: 0 | 68864 / 114272 | training loss: 0.010810481384396553\n",
      "epoch: 0 | 68896 / 114272 | training loss: 0.11013230681419373\n",
      "epoch: 0 | 68928 / 114272 | training loss: 0.1598394215106964\n",
      "epoch: 0 | 68960 / 114272 | training loss: 0.13443541526794434\n",
      "epoch: 0 | 68992 / 114272 | training loss: 0.14020280539989471\n",
      "epoch: 0 | 69024 / 114272 | training loss: 0.05954906344413757\n",
      "epoch: 0 | 69056 / 114272 | training loss: 0.2877150774002075\n",
      "epoch: 0 | 69088 / 114272 | training loss: 0.027463627979159355\n",
      "epoch: 0 | 69120 / 114272 | training loss: 0.09942706674337387\n",
      "epoch: 0 | 69152 / 114272 | training loss: 0.05250556394457817\n",
      "epoch: 0 | 69184 / 114272 | training loss: 0.19896189868450165\n",
      "epoch: 0 | 69216 / 114272 | training loss: 0.1184568852186203\n",
      "epoch: 0 | 69248 / 114272 | training loss: 0.46109506487846375\n",
      "epoch: 0 | 69280 / 114272 | training loss: 0.13835139572620392\n",
      "epoch: 0 | 69312 / 114272 | training loss: 0.17191988229751587\n",
      "epoch: 0 | 69344 / 114272 | training loss: 0.05521131679415703\n",
      "epoch: 0 | 69376 / 114272 | training loss: 0.1693873256444931\n",
      "epoch: 0 | 69408 / 114272 | training loss: 0.19669751822948456\n",
      "epoch: 0 | 69440 / 114272 | training loss: 0.16840016841888428\n",
      "epoch: 0 | 69472 / 114272 | training loss: 0.15009881556034088\n",
      "epoch: 0 | 69504 / 114272 | training loss: 0.05094439536333084\n",
      "epoch: 0 | 69536 / 114272 | training loss: 0.07801564037799835\n",
      "epoch: 0 | 69568 / 114272 | training loss: 0.06155132129788399\n",
      "epoch: 0 | 69600 / 114272 | training loss: 0.031682223081588745\n",
      "epoch: 0 | 69632 / 114272 | training loss: 0.12369906902313232\n",
      "epoch: 0 | 69664 / 114272 | training loss: 0.04189625009894371\n",
      "epoch: 0 | 69696 / 114272 | training loss: 0.3590245246887207\n",
      "epoch: 0 | 69728 / 114272 | training loss: 0.34610676765441895\n",
      "epoch: 0 | 69760 / 114272 | training loss: 0.14316320419311523\n",
      "epoch: 0 | 69792 / 114272 | training loss: 0.1868661642074585\n",
      "epoch: 0 | 69824 / 114272 | training loss: 0.09661897271871567\n",
      "epoch: 0 | 69856 / 114272 | training loss: 0.1772514134645462\n",
      "epoch: 0 | 69888 / 114272 | training loss: 0.08584118634462357\n",
      "epoch: 0 | 69920 / 114272 | training loss: 0.09519381821155548\n",
      "epoch: 0 | 69952 / 114272 | training loss: 0.1815815269947052\n",
      "epoch: 0 | 69984 / 114272 | training loss: 0.33640962839126587\n",
      "epoch: 0 | 70016 / 114272 | training loss: 0.0888628289103508\n",
      "epoch: 0 | 70048 / 114272 | training loss: 0.09231764823198318\n",
      "epoch: 0 | 70080 / 114272 | training loss: 0.326010137796402\n",
      "epoch: 0 | 70112 / 114272 | training loss: 0.21877548098564148\n",
      "epoch: 0 | 70144 / 114272 | training loss: 0.08034409582614899\n",
      "epoch: 0 | 70176 / 114272 | training loss: 0.022188175469636917\n",
      "epoch: 0 | 70208 / 114272 | training loss: 0.1945548951625824\n",
      "epoch: 0 | 70240 / 114272 | training loss: 0.024004211649298668\n",
      "epoch: 0 | 70272 / 114272 | training loss: 0.06912302225828171\n",
      "epoch: 0 | 70304 / 114272 | training loss: 0.06960658729076385\n",
      "epoch: 0 | 70336 / 114272 | training loss: 0.26923853158950806\n",
      "epoch: 0 | 70368 / 114272 | training loss: 0.11220689117908478\n",
      "epoch: 0 | 70400 / 114272 | training loss: 0.10102790594100952\n",
      "epoch: 0 | 70432 / 114272 | training loss: 0.08394518494606018\n",
      "epoch: 0 | 70464 / 114272 | training loss: 0.10442303121089935\n",
      "epoch: 0 | 70496 / 114272 | training loss: 0.23843570053577423\n",
      "epoch: 0 | 70528 / 114272 | training loss: 0.16388128697872162\n",
      "epoch: 0 | 70560 / 114272 | training loss: 0.09663161635398865\n",
      "epoch: 0 | 70592 / 114272 | training loss: 0.04035661369562149\n",
      "epoch: 0 | 70624 / 114272 | training loss: 0.04915938526391983\n",
      "epoch: 0 | 70656 / 114272 | training loss: 0.02183268591761589\n",
      "epoch: 0 | 70688 / 114272 | training loss: 0.2948853671550751\n",
      "epoch: 0 | 70720 / 114272 | training loss: 0.038617897778749466\n",
      "epoch: 0 | 70752 / 114272 | training loss: 0.09943557530641556\n",
      "epoch: 0 | 70784 / 114272 | training loss: 0.3733375072479248\n",
      "epoch: 0 | 70816 / 114272 | training loss: 0.3627196252346039\n",
      "epoch: 0 | 70848 / 114272 | training loss: 0.034493327140808105\n",
      "epoch: 0 | 70880 / 114272 | training loss: 0.20545965433120728\n",
      "epoch: 0 | 70912 / 114272 | training loss: 0.1404934525489807\n",
      "epoch: 0 | 70944 / 114272 | training loss: 0.17534451186656952\n",
      "epoch: 0 | 70976 / 114272 | training loss: 0.112406887114048\n",
      "epoch: 0 | 71008 / 114272 | training loss: 0.07420596480369568\n",
      "epoch: 0 | 71040 / 114272 | training loss: 0.4156237840652466\n",
      "epoch: 0 | 71072 / 114272 | training loss: 0.05558474734425545\n",
      "epoch: 0 | 71104 / 114272 | training loss: 0.10649723559617996\n",
      "epoch: 0 | 71136 / 114272 | training loss: 0.1602698266506195\n",
      "epoch: 0 | 71168 / 114272 | training loss: 0.07260337471961975\n",
      "epoch: 0 | 71200 / 114272 | training loss: 0.10372628271579742\n",
      "epoch: 0 | 71232 / 114272 | training loss: 0.3371509313583374\n",
      "epoch: 0 | 71264 / 114272 | training loss: 0.37638843059539795\n",
      "epoch: 0 | 71296 / 114272 | training loss: 0.07466879487037659\n",
      "epoch: 0 | 71328 / 114272 | training loss: 0.23285011947155\n",
      "epoch: 0 | 71360 / 114272 | training loss: 0.170640766620636\n",
      "epoch: 0 | 71392 / 114272 | training loss: 0.18486438691616058\n",
      "epoch: 0 | 71424 / 114272 | training loss: 0.09836600720882416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 71456 / 114272 | training loss: 0.29449713230133057\n",
      "epoch: 0 | 71488 / 114272 | training loss: 0.06767300516366959\n",
      "epoch: 0 | 71520 / 114272 | training loss: 0.0785754844546318\n",
      "epoch: 0 | 71552 / 114272 | training loss: 0.2582961320877075\n",
      "epoch: 0 | 71584 / 114272 | training loss: 0.04059615358710289\n",
      "epoch: 0 | 71616 / 114272 | training loss: 0.023785000666975975\n",
      "epoch: 0 | 71648 / 114272 | training loss: 0.04087098315358162\n",
      "epoch: 0 | 71680 / 114272 | training loss: 0.03276699036359787\n",
      "epoch: 0 | 71712 / 114272 | training loss: 0.0065955957397818565\n",
      "epoch: 0 | 71744 / 114272 | training loss: 0.06418278813362122\n",
      "epoch: 0 | 71776 / 114272 | training loss: 0.32946163415908813\n",
      "epoch: 0 | 71808 / 114272 | training loss: 0.2389618307352066\n",
      "epoch: 0 | 71840 / 114272 | training loss: 0.28215157985687256\n",
      "epoch: 0 | 71872 / 114272 | training loss: 0.03868958726525307\n",
      "epoch: 0 | 71904 / 114272 | training loss: 0.04411115124821663\n",
      "epoch: 0 | 71936 / 114272 | training loss: 0.011007924564182758\n",
      "epoch: 0 | 71968 / 114272 | training loss: 0.4724404811859131\n",
      "epoch: 0 | 72000 / 114272 | training loss: 0.3728986084461212\n",
      "epoch: 0 | 72032 / 114272 | training loss: 0.14881911873817444\n",
      "epoch: 0 | 72064 / 114272 | training loss: 0.033014945685863495\n",
      "epoch: 0 | 72096 / 114272 | training loss: 0.21947260200977325\n",
      "epoch: 0 | 72128 / 114272 | training loss: 0.175406351685524\n",
      "epoch: 0 | 72160 / 114272 | training loss: 0.09047719836235046\n",
      "epoch: 0 | 72192 / 114272 | training loss: 0.08620265871286392\n",
      "epoch: 0 | 72224 / 114272 | training loss: 0.18535031378269196\n",
      "epoch: 0 | 72256 / 114272 | training loss: 0.10398594290018082\n",
      "epoch: 0 | 72288 / 114272 | training loss: 0.4381463825702667\n",
      "epoch: 0 | 72320 / 114272 | training loss: 0.35681262612342834\n",
      "epoch: 0 | 72352 / 114272 | training loss: 0.41413354873657227\n",
      "epoch: 0 | 72384 / 114272 | training loss: 0.2787799835205078\n",
      "epoch: 0 | 72416 / 114272 | training loss: 0.2968479096889496\n",
      "epoch: 0 | 72448 / 114272 | training loss: 0.02086377516388893\n",
      "epoch: 0 | 72480 / 114272 | training loss: 0.24559718370437622\n",
      "epoch: 0 | 72512 / 114272 | training loss: 0.10974538326263428\n",
      "epoch: 0 | 72544 / 114272 | training loss: 0.23322391510009766\n",
      "epoch: 0 | 72576 / 114272 | training loss: 0.20405691862106323\n",
      "epoch: 0 | 72608 / 114272 | training loss: 0.08557532727718353\n",
      "epoch: 0 | 72640 / 114272 | training loss: 0.044820044189691544\n",
      "epoch: 0 | 72672 / 114272 | training loss: 0.1050303652882576\n",
      "epoch: 0 | 72704 / 114272 | training loss: 0.1988939344882965\n",
      "epoch: 0 | 72736 / 114272 | training loss: 0.1384854018688202\n",
      "epoch: 0 | 72768 / 114272 | training loss: 0.1711127609014511\n",
      "epoch: 0 | 72800 / 114272 | training loss: 0.0898563340306282\n",
      "epoch: 0 | 72832 / 114272 | training loss: 0.11802873015403748\n",
      "epoch: 0 | 72864 / 114272 | training loss: 0.04924430698156357\n",
      "epoch: 0 | 72896 / 114272 | training loss: 0.06719116121530533\n",
      "epoch: 0 | 72928 / 114272 | training loss: 0.14206330478191376\n",
      "epoch: 0 | 72960 / 114272 | training loss: 0.15832895040512085\n",
      "epoch: 0 | 72992 / 114272 | training loss: 0.190975621342659\n",
      "epoch: 0 | 73024 / 114272 | training loss: 0.10380847007036209\n",
      "epoch: 0 | 73056 / 114272 | training loss: 0.037558116018772125\n",
      "epoch: 0 | 73088 / 114272 | training loss: 0.1507672816514969\n",
      "epoch: 0 | 73120 / 114272 | training loss: 0.06721357256174088\n",
      "epoch: 0 | 73152 / 114272 | training loss: 0.07306063175201416\n",
      "epoch: 0 | 73184 / 114272 | training loss: 0.2031438946723938\n",
      "epoch: 0 | 73216 / 114272 | training loss: 0.2597995400428772\n",
      "epoch: 0 | 73248 / 114272 | training loss: 0.08668365329504013\n",
      "epoch: 0 | 73280 / 114272 | training loss: 0.018238501623272896\n",
      "epoch: 0 | 73312 / 114272 | training loss: 0.02639262191951275\n",
      "epoch: 0 | 73344 / 114272 | training loss: 0.21869312226772308\n",
      "epoch: 0 | 73376 / 114272 | training loss: 0.16178688406944275\n",
      "epoch: 0 | 73408 / 114272 | training loss: 0.24586336314678192\n",
      "epoch: 0 | 73440 / 114272 | training loss: 0.10103784501552582\n",
      "epoch: 0 | 73472 / 114272 | training loss: 0.04956305772066116\n",
      "epoch: 0 | 73504 / 114272 | training loss: 0.19745174050331116\n",
      "epoch: 0 | 73536 / 114272 | training loss: 0.2985484004020691\n",
      "epoch: 0 | 73568 / 114272 | training loss: 0.21199345588684082\n",
      "epoch: 0 | 73600 / 114272 | training loss: 0.17190591990947723\n",
      "epoch: 0 | 73632 / 114272 | training loss: 0.07024113088846207\n",
      "epoch: 0 | 73664 / 114272 | training loss: 0.011809416115283966\n",
      "epoch: 0 | 73696 / 114272 | training loss: 0.3461098372936249\n",
      "epoch: 0 | 73728 / 114272 | training loss: 0.07317199558019638\n",
      "epoch: 0 | 73760 / 114272 | training loss: 0.20996765792369843\n",
      "epoch: 0 | 73792 / 114272 | training loss: 0.25099384784698486\n",
      "epoch: 0 | 73824 / 114272 | training loss: 0.026374561712145805\n",
      "epoch: 0 | 73856 / 114272 | training loss: 0.032270416617393494\n",
      "epoch: 0 | 73888 / 114272 | training loss: 0.04147369787096977\n",
      "epoch: 0 | 73920 / 114272 | training loss: 0.13628126680850983\n",
      "epoch: 0 | 73952 / 114272 | training loss: 0.44566184282302856\n",
      "epoch: 0 | 73984 / 114272 | training loss: 0.11510059982538223\n",
      "epoch: 0 | 74016 / 114272 | training loss: 0.09414131939411163\n",
      "epoch: 0 | 74048 / 114272 | training loss: 0.2453458160161972\n",
      "epoch: 0 | 74080 / 114272 | training loss: 0.11018901318311691\n",
      "epoch: 0 | 74112 / 114272 | training loss: 0.350427508354187\n",
      "epoch: 0 | 74144 / 114272 | training loss: 0.08219538629055023\n",
      "epoch: 0 | 74176 / 114272 | training loss: 0.09955331683158875\n",
      "epoch: 0 | 74208 / 114272 | training loss: 0.033055104315280914\n",
      "epoch: 0 | 74240 / 114272 | training loss: 0.11510594189167023\n",
      "epoch: 0 | 74272 / 114272 | training loss: 0.40612781047821045\n",
      "epoch: 0 | 74304 / 114272 | training loss: 0.015815449878573418\n",
      "epoch: 0 | 74336 / 114272 | training loss: 0.21551431715488434\n",
      "epoch: 0 | 74368 / 114272 | training loss: 0.11721868813037872\n",
      "epoch: 0 | 74400 / 114272 | training loss: 0.26863232254981995\n",
      "epoch: 0 | 74432 / 114272 | training loss: 0.03938706964254379\n",
      "epoch: 0 | 74464 / 114272 | training loss: 0.13469620048999786\n",
      "epoch: 0 | 74496 / 114272 | training loss: 0.08813908696174622\n",
      "epoch: 0 | 74528 / 114272 | training loss: 0.023490656167268753\n",
      "epoch: 0 | 74560 / 114272 | training loss: 0.040400758385658264\n",
      "epoch: 0 | 74592 / 114272 | training loss: 0.15757426619529724\n",
      "epoch: 0 | 74624 / 114272 | training loss: 0.021343054249882698\n",
      "epoch: 0 | 74656 / 114272 | training loss: 0.26192331314086914\n",
      "epoch: 0 | 74688 / 114272 | training loss: 0.04951145499944687\n",
      "epoch: 0 | 74720 / 114272 | training loss: 0.18105445802211761\n",
      "epoch: 0 | 74752 / 114272 | training loss: 0.1905817836523056\n",
      "epoch: 0 | 74784 / 114272 | training loss: 0.08755005896091461\n",
      "epoch: 0 | 74816 / 114272 | training loss: 0.17069078981876373\n",
      "epoch: 0 | 74848 / 114272 | training loss: 0.09258947521448135\n",
      "epoch: 0 | 74880 / 114272 | training loss: 0.08798879384994507\n",
      "epoch: 0 | 74912 / 114272 | training loss: 0.08412571251392365\n",
      "epoch: 0 | 74944 / 114272 | training loss: 0.055020030587911606\n",
      "epoch: 0 | 74976 / 114272 | training loss: 0.11755702644586563\n",
      "epoch: 0 | 75008 / 114272 | training loss: 0.24759551882743835\n",
      "epoch: 0 | 75040 / 114272 | training loss: 0.1814933717250824\n",
      "epoch: 0 | 75072 / 114272 | training loss: 0.20806601643562317\n",
      "epoch: 0 | 75104 / 114272 | training loss: 0.38799768686294556\n",
      "epoch: 0 | 75136 / 114272 | training loss: 0.014859992079436779\n",
      "epoch: 0 | 75168 / 114272 | training loss: 0.1371327042579651\n",
      "epoch: 0 | 75200 / 114272 | training loss: 0.06988181173801422\n",
      "epoch: 0 | 75232 / 114272 | training loss: 0.0716397762298584\n",
      "epoch: 0 | 75264 / 114272 | training loss: 0.2198856920003891\n",
      "epoch: 0 | 75296 / 114272 | training loss: 0.13686101138591766\n",
      "epoch: 0 | 75328 / 114272 | training loss: 0.21461579203605652\n",
      "epoch: 0 | 75360 / 114272 | training loss: 0.2184663861989975\n",
      "epoch: 0 | 75392 / 114272 | training loss: 0.1400761902332306\n",
      "epoch: 0 | 75424 / 114272 | training loss: 0.367295503616333\n",
      "epoch: 0 | 75456 / 114272 | training loss: 0.2324385643005371\n",
      "epoch: 0 | 75488 / 114272 | training loss: 0.15340544283390045\n",
      "epoch: 0 | 75520 / 114272 | training loss: 0.07668483257293701\n",
      "epoch: 0 | 75552 / 114272 | training loss: 0.22288383543491364\n",
      "epoch: 0 | 75584 / 114272 | training loss: 0.022518955171108246\n",
      "epoch: 0 | 75616 / 114272 | training loss: 0.19794495403766632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 75648 / 114272 | training loss: 0.0324985608458519\n",
      "epoch: 0 | 75680 / 114272 | training loss: 0.032959550619125366\n",
      "epoch: 0 | 75712 / 114272 | training loss: 0.23070131242275238\n",
      "epoch: 0 | 75744 / 114272 | training loss: 0.03371112421154976\n",
      "epoch: 0 | 75776 / 114272 | training loss: 0.1289762407541275\n",
      "epoch: 0 | 75808 / 114272 | training loss: 0.20152470469474792\n",
      "epoch: 0 | 75840 / 114272 | training loss: 0.03555513545870781\n",
      "epoch: 0 | 75872 / 114272 | training loss: 0.039070259779691696\n",
      "epoch: 0 | 75904 / 114272 | training loss: 0.07538322359323502\n",
      "epoch: 0 | 75936 / 114272 | training loss: 0.04381801560521126\n",
      "epoch: 0 | 75968 / 114272 | training loss: 0.08062101155519485\n",
      "epoch: 0 | 76000 / 114272 | training loss: 0.2252032607793808\n",
      "epoch: 0 | 76032 / 114272 | training loss: 0.03888833150267601\n",
      "epoch: 0 | 76064 / 114272 | training loss: 0.21243330836296082\n",
      "epoch: 0 | 76096 / 114272 | training loss: 0.2915040850639343\n",
      "epoch: 0 | 76128 / 114272 | training loss: 0.33808445930480957\n",
      "epoch: 0 | 76160 / 114272 | training loss: 0.04402993246912956\n",
      "epoch: 0 | 76192 / 114272 | training loss: 0.2100348323583603\n",
      "epoch: 0 | 76224 / 114272 | training loss: 0.0728049948811531\n",
      "epoch: 0 | 76256 / 114272 | training loss: 0.03367668017745018\n",
      "epoch: 0 | 76288 / 114272 | training loss: 0.07270222157239914\n",
      "epoch: 0 | 76320 / 114272 | training loss: 0.21083617210388184\n",
      "epoch: 0 | 76352 / 114272 | training loss: 0.01777138002216816\n",
      "epoch: 0 | 76384 / 114272 | training loss: 0.19205915927886963\n",
      "epoch: 0 | 76416 / 114272 | training loss: 0.12871265411376953\n",
      "epoch: 0 | 76448 / 114272 | training loss: 0.17214803397655487\n",
      "epoch: 0 | 76480 / 114272 | training loss: 0.19599154591560364\n",
      "epoch: 0 | 76512 / 114272 | training loss: 0.051709625869989395\n",
      "epoch: 0 | 76544 / 114272 | training loss: 0.021004851907491684\n",
      "epoch: 0 | 76576 / 114272 | training loss: 0.04392462968826294\n",
      "epoch: 0 | 76608 / 114272 | training loss: 0.11380434781312943\n",
      "epoch: 0 | 76640 / 114272 | training loss: 0.21210604906082153\n",
      "epoch: 0 | 76672 / 114272 | training loss: 0.04230615869164467\n",
      "epoch: 0 | 76704 / 114272 | training loss: 0.012078591622412205\n",
      "epoch: 0 | 76736 / 114272 | training loss: 0.01587461307644844\n",
      "epoch: 0 | 76768 / 114272 | training loss: 0.019074149429798126\n",
      "epoch: 0 | 76800 / 114272 | training loss: 0.1598941683769226\n",
      "epoch: 0 | 76832 / 114272 | training loss: 0.3180530071258545\n",
      "epoch: 0 | 76864 / 114272 | training loss: 0.1805373579263687\n",
      "epoch: 0 | 76896 / 114272 | training loss: 0.32827162742614746\n",
      "epoch: 0 | 76928 / 114272 | training loss: 0.30698931217193604\n",
      "epoch: 0 | 76960 / 114272 | training loss: 0.18412083387374878\n",
      "epoch: 0 | 76992 / 114272 | training loss: 0.37683355808258057\n",
      "epoch: 0 | 77024 / 114272 | training loss: 0.11961390823125839\n",
      "epoch: 0 | 77056 / 114272 | training loss: 0.05416570603847504\n",
      "epoch: 0 | 77088 / 114272 | training loss: 0.0367402546107769\n",
      "epoch: 0 | 77120 / 114272 | training loss: 0.147505521774292\n",
      "epoch: 0 | 77152 / 114272 | training loss: 0.3100757300853729\n",
      "epoch: 0 | 77184 / 114272 | training loss: 0.07355660945177078\n",
      "epoch: 0 | 77216 / 114272 | training loss: 0.008124057203531265\n",
      "epoch: 0 | 77248 / 114272 | training loss: 0.15724407136440277\n",
      "epoch: 0 | 77280 / 114272 | training loss: 0.13717631995677948\n",
      "epoch: 0 | 77312 / 114272 | training loss: 0.37102675437927246\n",
      "epoch: 0 | 77344 / 114272 | training loss: 0.1964130401611328\n",
      "epoch: 0 | 77376 / 114272 | training loss: 0.1640131026506424\n",
      "epoch: 0 | 77408 / 114272 | training loss: 0.2958986461162567\n",
      "epoch: 0 | 77440 / 114272 | training loss: 0.10702687501907349\n",
      "epoch: 0 | 77472 / 114272 | training loss: 0.1711333692073822\n",
      "epoch: 0 | 77504 / 114272 | training loss: 0.22217659652233124\n",
      "epoch: 0 | 77536 / 114272 | training loss: 0.2471931427717209\n",
      "epoch: 0 | 77568 / 114272 | training loss: 0.13294795155525208\n",
      "epoch: 0 | 77600 / 114272 | training loss: 0.3807164132595062\n",
      "epoch: 0 | 77632 / 114272 | training loss: 0.19659502804279327\n",
      "epoch: 0 | 77664 / 114272 | training loss: 0.292493611574173\n",
      "epoch: 0 | 77696 / 114272 | training loss: 0.0673733577132225\n",
      "epoch: 0 | 77728 / 114272 | training loss: 0.36200815439224243\n",
      "epoch: 0 | 77760 / 114272 | training loss: 0.1513463705778122\n",
      "epoch: 0 | 77792 / 114272 | training loss: 0.022730661556124687\n",
      "epoch: 0 | 77824 / 114272 | training loss: 0.0839889794588089\n",
      "epoch: 0 | 77856 / 114272 | training loss: 0.1597733348608017\n",
      "epoch: 0 | 77888 / 114272 | training loss: 0.18553023040294647\n",
      "epoch: 0 | 77920 / 114272 | training loss: 0.1921887844800949\n",
      "epoch: 0 | 77952 / 114272 | training loss: 0.04856467247009277\n",
      "epoch: 0 | 77984 / 114272 | training loss: 0.1609829068183899\n",
      "epoch: 0 | 78016 / 114272 | training loss: 0.01646614819765091\n",
      "epoch: 0 | 78048 / 114272 | training loss: 0.0987323671579361\n",
      "epoch: 0 | 78080 / 114272 | training loss: 0.17778030037879944\n",
      "epoch: 0 | 78112 / 114272 | training loss: 0.02736532688140869\n",
      "epoch: 0 | 78144 / 114272 | training loss: 0.18154773116111755\n",
      "epoch: 0 | 78176 / 114272 | training loss: 0.053588949143886566\n",
      "epoch: 0 | 78208 / 114272 | training loss: 0.22051548957824707\n",
      "epoch: 0 | 78240 / 114272 | training loss: 0.18036891520023346\n",
      "epoch: 0 | 78272 / 114272 | training loss: 0.10517407953739166\n",
      "epoch: 0 | 78304 / 114272 | training loss: 0.41976436972618103\n",
      "epoch: 0 | 78336 / 114272 | training loss: 0.07238785922527313\n",
      "epoch: 0 | 78368 / 114272 | training loss: 0.20791976153850555\n",
      "epoch: 0 | 78400 / 114272 | training loss: 0.25162795186042786\n",
      "epoch: 0 | 78432 / 114272 | training loss: 0.13867957890033722\n",
      "epoch: 0 | 78464 / 114272 | training loss: 0.12119893729686737\n",
      "epoch: 0 | 78496 / 114272 | training loss: 0.025347938761115074\n",
      "epoch: 0 | 78528 / 114272 | training loss: 0.11534833908081055\n",
      "epoch: 0 | 78560 / 114272 | training loss: 0.09212055802345276\n",
      "epoch: 0 | 78592 / 114272 | training loss: 0.05525717884302139\n",
      "epoch: 0 | 78624 / 114272 | training loss: 0.2945614755153656\n",
      "epoch: 0 | 78656 / 114272 | training loss: 0.16412004828453064\n",
      "epoch: 0 | 78688 / 114272 | training loss: 0.07882460206747055\n",
      "epoch: 0 | 78720 / 114272 | training loss: 0.0221044160425663\n",
      "epoch: 0 | 78752 / 114272 | training loss: 0.24396775662899017\n",
      "epoch: 0 | 78784 / 114272 | training loss: 0.1295098215341568\n",
      "epoch: 0 | 78816 / 114272 | training loss: 0.18312962353229523\n",
      "epoch: 0 | 78848 / 114272 | training loss: 0.3740168511867523\n",
      "epoch: 0 | 78880 / 114272 | training loss: 0.18683645129203796\n",
      "epoch: 0 | 78912 / 114272 | training loss: 0.09654366970062256\n",
      "epoch: 0 | 78944 / 114272 | training loss: 0.16698403656482697\n",
      "epoch: 0 | 78976 / 114272 | training loss: 0.018403416499495506\n",
      "epoch: 0 | 79008 / 114272 | training loss: 0.10065239667892456\n",
      "epoch: 0 | 79040 / 114272 | training loss: 0.12975849211215973\n",
      "epoch: 0 | 79072 / 114272 | training loss: 0.09338926523923874\n",
      "epoch: 0 | 79104 / 114272 | training loss: 0.09693784266710281\n",
      "epoch: 0 | 79136 / 114272 | training loss: 0.14092330634593964\n",
      "epoch: 0 | 79168 / 114272 | training loss: 0.3838497996330261\n",
      "epoch: 0 | 79200 / 114272 | training loss: 0.5846487283706665\n",
      "epoch: 0 | 79232 / 114272 | training loss: 0.5290874242782593\n",
      "epoch: 0 | 79264 / 114272 | training loss: 0.2966799736022949\n",
      "epoch: 0 | 79296 / 114272 | training loss: 0.08795591443777084\n",
      "epoch: 0 | 79328 / 114272 | training loss: 0.09533436596393585\n",
      "epoch: 0 | 79360 / 114272 | training loss: 0.1627950519323349\n",
      "epoch: 0 | 79392 / 114272 | training loss: 0.11963469535112381\n",
      "epoch: 0 | 79424 / 114272 | training loss: 0.144047811627388\n",
      "epoch: 0 | 79456 / 114272 | training loss: 0.07654666900634766\n",
      "epoch: 0 | 79488 / 114272 | training loss: 0.1341051310300827\n",
      "epoch: 0 | 79520 / 114272 | training loss: 0.13234379887580872\n",
      "epoch: 0 | 79552 / 114272 | training loss: 0.020447541028261185\n",
      "epoch: 0 | 79584 / 114272 | training loss: 0.05369352176785469\n",
      "epoch: 0 | 79616 / 114272 | training loss: 0.1563306748867035\n",
      "epoch: 0 | 79648 / 114272 | training loss: 0.21767908334732056\n",
      "epoch: 0 | 79680 / 114272 | training loss: 0.1123838871717453\n",
      "epoch: 0 | 79712 / 114272 | training loss: 0.37181031703948975\n",
      "epoch: 0 | 79744 / 114272 | training loss: 0.32137230038642883\n",
      "epoch: 0 | 79776 / 114272 | training loss: 0.049318891018629074\n",
      "epoch: 0 | 79808 / 114272 | training loss: 0.09207744896411896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 79840 / 114272 | training loss: 0.24386852979660034\n",
      "epoch: 0 | 79872 / 114272 | training loss: 0.0197976715862751\n",
      "epoch: 0 | 79904 / 114272 | training loss: 0.11731568723917007\n",
      "epoch: 0 | 79936 / 114272 | training loss: 0.13585662841796875\n",
      "epoch: 0 | 79968 / 114272 | training loss: 0.022107001394033432\n",
      "epoch: 0 | 80000 / 114272 | training loss: 0.1526937335729599\n",
      "epoch: 0 | 80032 / 114272 | training loss: 0.2395685762166977\n",
      "epoch: 0 | 80064 / 114272 | training loss: 0.044617537409067154\n",
      "epoch: 0 | 80096 / 114272 | training loss: 0.17132847011089325\n",
      "epoch: 0 | 80128 / 114272 | training loss: 0.285503625869751\n",
      "epoch: 0 | 80160 / 114272 | training loss: 0.19851574301719666\n",
      "epoch: 0 | 80192 / 114272 | training loss: 0.07254143059253693\n",
      "epoch: 0 | 80224 / 114272 | training loss: 0.08484715968370438\n",
      "epoch: 0 | 80256 / 114272 | training loss: 0.029804686084389687\n",
      "epoch: 0 | 80288 / 114272 | training loss: 0.16655150055885315\n",
      "epoch: 0 | 80320 / 114272 | training loss: 0.21521325409412384\n",
      "epoch: 0 | 80352 / 114272 | training loss: 0.13199174404144287\n",
      "epoch: 0 | 80384 / 114272 | training loss: 0.12600016593933105\n",
      "epoch: 0 | 80416 / 114272 | training loss: 0.30265435576438904\n",
      "epoch: 0 | 80448 / 114272 | training loss: 0.10131468623876572\n",
      "epoch: 0 | 80480 / 114272 | training loss: 0.23295992612838745\n",
      "epoch: 0 | 80512 / 114272 | training loss: 0.07674078643321991\n",
      "epoch: 0 | 80544 / 114272 | training loss: 0.12435102462768555\n",
      "epoch: 0 | 80576 / 114272 | training loss: 0.2175077497959137\n",
      "epoch: 0 | 80608 / 114272 | training loss: 0.015112027525901794\n",
      "epoch: 0 | 80640 / 114272 | training loss: 0.09166673570871353\n",
      "epoch: 0 | 80672 / 114272 | training loss: 0.26101282238960266\n",
      "epoch: 0 | 80704 / 114272 | training loss: 0.28779417276382446\n",
      "epoch: 0 | 80736 / 114272 | training loss: 0.2054305374622345\n",
      "epoch: 0 | 80768 / 114272 | training loss: 0.1673818975687027\n",
      "epoch: 0 | 80800 / 114272 | training loss: 0.18030667304992676\n",
      "epoch: 0 | 80832 / 114272 | training loss: 0.13398370146751404\n",
      "epoch: 0 | 80864 / 114272 | training loss: 0.13177500665187836\n",
      "epoch: 0 | 80896 / 114272 | training loss: 0.0944426953792572\n",
      "epoch: 0 | 80928 / 114272 | training loss: 0.09683863073587418\n",
      "epoch: 0 | 80960 / 114272 | training loss: 0.11437569558620453\n",
      "epoch: 0 | 80992 / 114272 | training loss: 0.11242418736219406\n",
      "epoch: 0 | 81024 / 114272 | training loss: 0.12028767168521881\n",
      "epoch: 0 | 81056 / 114272 | training loss: 0.0857749879360199\n",
      "epoch: 0 | 81088 / 114272 | training loss: 0.10662970691919327\n",
      "epoch: 0 | 81120 / 114272 | training loss: 0.2457496076822281\n",
      "epoch: 0 | 81152 / 114272 | training loss: 0.39997559785842896\n",
      "epoch: 0 | 81184 / 114272 | training loss: 0.5027456283569336\n",
      "epoch: 0 | 81216 / 114272 | training loss: 0.1216694712638855\n",
      "epoch: 0 | 81248 / 114272 | training loss: 0.08381074666976929\n",
      "epoch: 0 | 81280 / 114272 | training loss: 0.4088948965072632\n",
      "epoch: 0 | 81312 / 114272 | training loss: 0.33552151918411255\n",
      "epoch: 0 | 81344 / 114272 | training loss: 0.3370495140552521\n",
      "epoch: 0 | 81376 / 114272 | training loss: 0.29769501090049744\n",
      "epoch: 0 | 81408 / 114272 | training loss: 0.17383475601673126\n",
      "epoch: 0 | 81440 / 114272 | training loss: 0.23459118604660034\n",
      "epoch: 0 | 81472 / 114272 | training loss: 0.15263910591602325\n",
      "epoch: 0 | 81504 / 114272 | training loss: 0.1463676542043686\n",
      "epoch: 0 | 81536 / 114272 | training loss: 0.226444810628891\n",
      "epoch: 0 | 81568 / 114272 | training loss: 0.0705365389585495\n",
      "epoch: 0 | 81600 / 114272 | training loss: 0.529427707195282\n",
      "epoch: 0 | 81632 / 114272 | training loss: 0.23207032680511475\n",
      "epoch: 0 | 81664 / 114272 | training loss: 0.10928378999233246\n",
      "epoch: 0 | 81696 / 114272 | training loss: 0.08654947578907013\n",
      "epoch: 0 | 81728 / 114272 | training loss: 0.15260814130306244\n",
      "epoch: 0 | 81760 / 114272 | training loss: 0.20361554622650146\n",
      "epoch: 0 | 81792 / 114272 | training loss: 0.05768135190010071\n",
      "epoch: 0 | 81824 / 114272 | training loss: 0.06226339936256409\n",
      "epoch: 0 | 81856 / 114272 | training loss: 0.08032001554965973\n",
      "epoch: 0 | 81888 / 114272 | training loss: 0.1080293282866478\n",
      "epoch: 0 | 81920 / 114272 | training loss: 0.1147688776254654\n",
      "epoch: 0 | 81952 / 114272 | training loss: 0.3014864921569824\n",
      "epoch: 0 | 81984 / 114272 | training loss: 0.12327311187982559\n",
      "epoch: 0 | 82016 / 114272 | training loss: 0.11326678097248077\n",
      "epoch: 0 | 82048 / 114272 | training loss: 0.08306977152824402\n",
      "epoch: 0 | 82080 / 114272 | training loss: 0.07234751433134079\n",
      "epoch: 0 | 82112 / 114272 | training loss: 0.19228702783584595\n",
      "epoch: 0 | 82144 / 114272 | training loss: 0.16005197167396545\n",
      "epoch: 0 | 82176 / 114272 | training loss: 0.0640554204583168\n",
      "epoch: 0 | 82208 / 114272 | training loss: 0.2299601286649704\n",
      "epoch: 0 | 82240 / 114272 | training loss: 0.21871840953826904\n",
      "epoch: 0 | 82272 / 114272 | training loss: 0.250370055437088\n",
      "epoch: 0 | 82304 / 114272 | training loss: 0.49073392152786255\n",
      "epoch: 0 | 82336 / 114272 | training loss: 0.08299628645181656\n",
      "epoch: 0 | 82368 / 114272 | training loss: 0.32768958806991577\n",
      "epoch: 0 | 82400 / 114272 | training loss: 0.2137499749660492\n",
      "epoch: 0 | 82432 / 114272 | training loss: 0.13484744727611542\n",
      "epoch: 0 | 82464 / 114272 | training loss: 0.3510282039642334\n",
      "epoch: 0 | 82496 / 114272 | training loss: 0.23479799926280975\n",
      "epoch: 0 | 82528 / 114272 | training loss: 0.31430932879447937\n",
      "epoch: 0 | 82560 / 114272 | training loss: 0.21273045241832733\n",
      "epoch: 0 | 82592 / 114272 | training loss: 0.0745626762509346\n",
      "epoch: 0 | 82624 / 114272 | training loss: 0.07797783613204956\n",
      "epoch: 0 | 82656 / 114272 | training loss: 0.06463031470775604\n",
      "epoch: 0 | 82688 / 114272 | training loss: 0.03012992814183235\n",
      "epoch: 0 | 82720 / 114272 | training loss: 0.1828891485929489\n",
      "epoch: 0 | 82752 / 114272 | training loss: 0.07259737700223923\n",
      "epoch: 0 | 82784 / 114272 | training loss: 0.08816789835691452\n",
      "epoch: 0 | 82816 / 114272 | training loss: 0.026897789910435677\n",
      "epoch: 0 | 82848 / 114272 | training loss: 0.04634715989232063\n",
      "epoch: 0 | 82880 / 114272 | training loss: 0.337633341550827\n",
      "epoch: 0 | 82912 / 114272 | training loss: 0.14804424345493317\n",
      "epoch: 0 | 82944 / 114272 | training loss: 0.021312959492206573\n",
      "epoch: 0 | 82976 / 114272 | training loss: 0.3229247033596039\n",
      "epoch: 0 | 83008 / 114272 | training loss: 0.044435255229473114\n",
      "epoch: 0 | 83040 / 114272 | training loss: 0.24005815386772156\n",
      "epoch: 0 | 83072 / 114272 | training loss: 0.16476024687290192\n",
      "epoch: 0 | 83104 / 114272 | training loss: 0.21948370337486267\n",
      "epoch: 0 | 83136 / 114272 | training loss: 0.07286500185728073\n",
      "epoch: 0 | 83168 / 114272 | training loss: 0.06124911084771156\n",
      "epoch: 0 | 83200 / 114272 | training loss: 0.314926415681839\n",
      "epoch: 0 | 83232 / 114272 | training loss: 0.03447858244180679\n",
      "epoch: 0 | 83264 / 114272 | training loss: 0.15716849267482758\n",
      "epoch: 0 | 83296 / 114272 | training loss: 0.287023663520813\n",
      "epoch: 0 | 83328 / 114272 | training loss: 0.16439351439476013\n",
      "epoch: 0 | 83360 / 114272 | training loss: 0.01609436422586441\n",
      "epoch: 0 | 83392 / 114272 | training loss: 0.14181742072105408\n",
      "epoch: 0 | 83424 / 114272 | training loss: 0.06930970400571823\n",
      "epoch: 0 | 83456 / 114272 | training loss: 0.275892049074173\n",
      "epoch: 0 | 83488 / 114272 | training loss: 0.21611616015434265\n",
      "epoch: 0 | 83520 / 114272 | training loss: 0.16009953618049622\n",
      "epoch: 0 | 83552 / 114272 | training loss: 0.22371847927570343\n",
      "epoch: 0 | 83584 / 114272 | training loss: 0.10490220040082932\n",
      "epoch: 0 | 83616 / 114272 | training loss: 0.054267074912786484\n",
      "epoch: 0 | 83648 / 114272 | training loss: 0.10035540163516998\n",
      "epoch: 0 | 83680 / 114272 | training loss: 0.2768709361553192\n",
      "epoch: 0 | 83712 / 114272 | training loss: 0.1745358407497406\n",
      "epoch: 0 | 83744 / 114272 | training loss: 0.07373535633087158\n",
      "epoch: 0 | 83776 / 114272 | training loss: 0.030431343242526054\n",
      "epoch: 0 | 83808 / 114272 | training loss: 0.2424858957529068\n",
      "epoch: 0 | 83840 / 114272 | training loss: 0.07733491063117981\n",
      "epoch: 0 | 83872 / 114272 | training loss: 0.2189163863658905\n",
      "epoch: 0 | 83904 / 114272 | training loss: 0.0060801636427640915\n",
      "epoch: 0 | 83936 / 114272 | training loss: 0.21639415621757507\n",
      "epoch: 0 | 83968 / 114272 | training loss: 0.20853164792060852\n",
      "epoch: 0 | 84000 / 114272 | training loss: 0.07922182232141495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 84032 / 114272 | training loss: 0.28505218029022217\n",
      "epoch: 0 | 84064 / 114272 | training loss: 0.10513216257095337\n",
      "epoch: 0 | 84096 / 114272 | training loss: 0.11245741695165634\n",
      "epoch: 0 | 84128 / 114272 | training loss: 0.1625916212797165\n",
      "epoch: 0 | 84160 / 114272 | training loss: 0.07446283847093582\n",
      "epoch: 0 | 84192 / 114272 | training loss: 0.09797213226556778\n",
      "epoch: 0 | 84224 / 114272 | training loss: 0.4307298958301544\n",
      "epoch: 0 | 84256 / 114272 | training loss: 0.18047145009040833\n",
      "epoch: 0 | 84288 / 114272 | training loss: 0.12442561984062195\n",
      "epoch: 0 | 84320 / 114272 | training loss: 0.16054105758666992\n",
      "epoch: 0 | 84352 / 114272 | training loss: 0.07898129522800446\n",
      "epoch: 0 | 84384 / 114272 | training loss: 0.08325499296188354\n",
      "epoch: 0 | 84416 / 114272 | training loss: 0.2013043612241745\n",
      "epoch: 0 | 84448 / 114272 | training loss: 0.21665596961975098\n",
      "epoch: 0 | 84480 / 114272 | training loss: 0.15915891528129578\n",
      "epoch: 0 | 84512 / 114272 | training loss: 0.04244588688015938\n",
      "epoch: 0 | 84544 / 114272 | training loss: 0.2172621786594391\n",
      "epoch: 0 | 84576 / 114272 | training loss: 0.08886103332042694\n",
      "epoch: 0 | 84608 / 114272 | training loss: 0.10896304249763489\n",
      "epoch: 0 | 84640 / 114272 | training loss: 0.11686345934867859\n",
      "epoch: 0 | 84672 / 114272 | training loss: 0.05400872975587845\n",
      "epoch: 0 | 84704 / 114272 | training loss: 0.1382945030927658\n",
      "epoch: 0 | 84736 / 114272 | training loss: 0.1832866370677948\n",
      "epoch: 0 | 84768 / 114272 | training loss: 0.04177521914243698\n",
      "epoch: 0 | 84800 / 114272 | training loss: 0.05722276121377945\n",
      "epoch: 0 | 84832 / 114272 | training loss: 0.30715975165367126\n",
      "epoch: 0 | 84864 / 114272 | training loss: 0.15517306327819824\n",
      "epoch: 0 | 84896 / 114272 | training loss: 0.17392884194850922\n",
      "epoch: 0 | 84928 / 114272 | training loss: 0.04752286896109581\n",
      "epoch: 0 | 84960 / 114272 | training loss: 0.09481484442949295\n",
      "epoch: 0 | 84992 / 114272 | training loss: 0.13045482337474823\n",
      "epoch: 0 | 85024 / 114272 | training loss: 0.08143798261880875\n",
      "epoch: 0 | 85056 / 114272 | training loss: 0.06010633334517479\n",
      "epoch: 0 | 85088 / 114272 | training loss: 0.2126016765832901\n",
      "epoch: 0 | 85120 / 114272 | training loss: 0.08979438990354538\n",
      "epoch: 0 | 85152 / 114272 | training loss: 0.042244039475917816\n",
      "epoch: 0 | 85184 / 114272 | training loss: 0.1773887276649475\n",
      "epoch: 0 | 85216 / 114272 | training loss: 0.11487038433551788\n",
      "epoch: 0 | 85248 / 114272 | training loss: 0.1732306331396103\n",
      "epoch: 0 | 85280 / 114272 | training loss: 0.03214220330119133\n",
      "epoch: 0 | 85312 / 114272 | training loss: 0.25354206562042236\n",
      "epoch: 0 | 85344 / 114272 | training loss: 0.2641200125217438\n",
      "epoch: 0 | 85376 / 114272 | training loss: 0.1055208295583725\n",
      "epoch: 0 | 85408 / 114272 | training loss: 0.10001799464225769\n",
      "epoch: 0 | 85440 / 114272 | training loss: 0.02495700493454933\n",
      "epoch: 0 | 85472 / 114272 | training loss: 0.15532980859279633\n",
      "epoch: 0 | 85504 / 114272 | training loss: 0.046070586889982224\n",
      "epoch: 0 | 85536 / 114272 | training loss: 0.16713330149650574\n",
      "epoch: 0 | 85568 / 114272 | training loss: 0.2059670239686966\n",
      "epoch: 0 | 85600 / 114272 | training loss: 0.3595956563949585\n",
      "epoch: 0 | 85632 / 114272 | training loss: 0.05162164196372032\n",
      "epoch: 0 | 85664 / 114272 | training loss: 0.3215135335922241\n",
      "epoch: 0 | 85696 / 114272 | training loss: 0.12741835415363312\n",
      "epoch: 0 | 85728 / 114272 | training loss: 0.2515125572681427\n",
      "epoch: 0 | 85760 / 114272 | training loss: 0.33296316862106323\n",
      "epoch: 0 | 85792 / 114272 | training loss: 0.007305520586669445\n",
      "epoch: 0 | 85824 / 114272 | training loss: 0.011496096849441528\n",
      "epoch: 0 | 85856 / 114272 | training loss: 0.038043178617954254\n",
      "epoch: 0 | 85888 / 114272 | training loss: 0.2642850875854492\n",
      "epoch: 0 | 85920 / 114272 | training loss: 0.2687464952468872\n",
      "epoch: 0 | 85952 / 114272 | training loss: 0.06675411760807037\n",
      "epoch: 0 | 85984 / 114272 | training loss: 0.09277946501970291\n",
      "epoch: 0 | 86016 / 114272 | training loss: 0.05222108215093613\n",
      "epoch: 0 | 86048 / 114272 | training loss: 0.1508423388004303\n",
      "epoch: 0 | 86080 / 114272 | training loss: 0.07952545583248138\n",
      "epoch: 0 | 86112 / 114272 | training loss: 0.22608160972595215\n",
      "epoch: 0 | 86144 / 114272 | training loss: 0.6114808320999146\n",
      "epoch: 0 | 86176 / 114272 | training loss: 0.01858585886657238\n",
      "epoch: 0 | 86208 / 114272 | training loss: 0.19020096957683563\n",
      "epoch: 0 | 86240 / 114272 | training loss: 0.21441756188869476\n",
      "epoch: 0 | 86272 / 114272 | training loss: 0.01946491375565529\n",
      "epoch: 0 | 86304 / 114272 | training loss: 0.024364186450839043\n",
      "epoch: 0 | 86336 / 114272 | training loss: 0.08039616048336029\n",
      "epoch: 0 | 86368 / 114272 | training loss: 0.08465225994586945\n",
      "epoch: 0 | 86400 / 114272 | training loss: 0.09248964488506317\n",
      "epoch: 0 | 86432 / 114272 | training loss: 0.33878007531166077\n",
      "epoch: 0 | 86464 / 114272 | training loss: 0.04944406449794769\n",
      "epoch: 0 | 86496 / 114272 | training loss: 0.11546307802200317\n",
      "epoch: 0 | 86528 / 114272 | training loss: 0.1506681591272354\n",
      "epoch: 0 | 86560 / 114272 | training loss: 0.10150399059057236\n",
      "epoch: 0 | 86592 / 114272 | training loss: 0.010400011204183102\n",
      "epoch: 0 | 86624 / 114272 | training loss: 0.14624084532260895\n",
      "epoch: 0 | 86656 / 114272 | training loss: 0.4300590753555298\n",
      "epoch: 0 | 86688 / 114272 | training loss: 0.04998675361275673\n",
      "epoch: 0 | 86720 / 114272 | training loss: 0.13012650609016418\n",
      "epoch: 0 | 86752 / 114272 | training loss: 0.41896331310272217\n",
      "epoch: 0 | 86784 / 114272 | training loss: 0.010108890943229198\n",
      "epoch: 0 | 86816 / 114272 | training loss: 0.22463297843933105\n",
      "epoch: 0 | 86848 / 114272 | training loss: 0.21962666511535645\n",
      "epoch: 0 | 86880 / 114272 | training loss: 0.1249481663107872\n",
      "epoch: 0 | 86912 / 114272 | training loss: 0.0323224812746048\n",
      "epoch: 0 | 86944 / 114272 | training loss: 0.3514889180660248\n",
      "epoch: 0 | 86976 / 114272 | training loss: 0.14897118508815765\n",
      "epoch: 0 | 87008 / 114272 | training loss: 0.08700230717658997\n",
      "epoch: 0 | 87040 / 114272 | training loss: 0.209596648812294\n",
      "epoch: 0 | 87072 / 114272 | training loss: 0.02869866043329239\n",
      "epoch: 0 | 87104 / 114272 | training loss: 0.22126521170139313\n",
      "epoch: 0 | 87136 / 114272 | training loss: 0.0979127436876297\n",
      "epoch: 0 | 87168 / 114272 | training loss: 0.02709917351603508\n",
      "epoch: 0 | 87200 / 114272 | training loss: 0.05693570524454117\n",
      "epoch: 0 | 87232 / 114272 | training loss: 0.06391263753175735\n",
      "epoch: 0 | 87264 / 114272 | training loss: 0.14462457597255707\n",
      "epoch: 0 | 87296 / 114272 | training loss: 0.03646203130483627\n",
      "epoch: 0 | 87328 / 114272 | training loss: 0.16008606553077698\n",
      "epoch: 0 | 87360 / 114272 | training loss: 0.26642465591430664\n",
      "epoch: 0 | 87392 / 114272 | training loss: 0.16225050389766693\n",
      "epoch: 0 | 87424 / 114272 | training loss: 0.10594188421964645\n",
      "epoch: 0 | 87456 / 114272 | training loss: 0.16912320256233215\n",
      "epoch: 0 | 87488 / 114272 | training loss: 0.03048221580684185\n",
      "epoch: 0 | 87520 / 114272 | training loss: 0.011732262559235096\n",
      "epoch: 0 | 87552 / 114272 | training loss: 0.08869708329439163\n",
      "epoch: 0 | 87584 / 114272 | training loss: 0.2123131901025772\n",
      "epoch: 0 | 87616 / 114272 | training loss: 0.33119481801986694\n",
      "epoch: 0 | 87648 / 114272 | training loss: 0.45308274030685425\n",
      "epoch: 0 | 87680 / 114272 | training loss: 0.009220810607075691\n",
      "epoch: 0 | 87712 / 114272 | training loss: 0.25264328718185425\n",
      "epoch: 0 | 87744 / 114272 | training loss: 0.014759746380150318\n",
      "epoch: 0 | 87776 / 114272 | training loss: 0.010573840700089931\n",
      "epoch: 0 | 87808 / 114272 | training loss: 0.1995224952697754\n",
      "epoch: 0 | 87840 / 114272 | training loss: 0.048771098256111145\n",
      "epoch: 0 | 87872 / 114272 | training loss: 0.06670065969228745\n",
      "epoch: 0 | 87904 / 114272 | training loss: 0.13274739682674408\n",
      "epoch: 0 | 87936 / 114272 | training loss: 0.09375271201133728\n",
      "epoch: 0 | 87968 / 114272 | training loss: 0.06415833532810211\n",
      "epoch: 0 | 88000 / 114272 | training loss: 0.15659508109092712\n",
      "epoch: 0 | 88032 / 114272 | training loss: 0.020630329847335815\n",
      "epoch: 0 | 88064 / 114272 | training loss: 0.17834070324897766\n",
      "epoch: 0 | 88096 / 114272 | training loss: 0.17090748250484467\n",
      "epoch: 0 | 88128 / 114272 | training loss: 0.06442687660455704\n",
      "epoch: 0 | 88160 / 114272 | training loss: 0.2037758231163025\n",
      "epoch: 0 | 88192 / 114272 | training loss: 0.31984561681747437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 88224 / 114272 | training loss: 0.0660480409860611\n",
      "epoch: 0 | 88256 / 114272 | training loss: 0.12478700280189514\n",
      "epoch: 0 | 88288 / 114272 | training loss: 0.028096221387386322\n",
      "epoch: 0 | 88320 / 114272 | training loss: 0.03191959485411644\n",
      "epoch: 0 | 88352 / 114272 | training loss: 0.10397297888994217\n",
      "epoch: 0 | 88384 / 114272 | training loss: 0.2726746201515198\n",
      "epoch: 0 | 88416 / 114272 | training loss: 0.16375979781150818\n",
      "epoch: 0 | 88448 / 114272 | training loss: 0.29239484667778015\n",
      "epoch: 0 | 88480 / 114272 | training loss: 0.042607177048921585\n",
      "epoch: 0 | 88512 / 114272 | training loss: 0.02745245397090912\n",
      "epoch: 0 | 88544 / 114272 | training loss: 0.026431841775774956\n",
      "epoch: 0 | 88576 / 114272 | training loss: 0.1230425164103508\n",
      "epoch: 0 | 88608 / 114272 | training loss: 0.06350894272327423\n",
      "epoch: 0 | 88640 / 114272 | training loss: 0.18286164104938507\n",
      "epoch: 0 | 88672 / 114272 | training loss: 0.14889825880527496\n",
      "epoch: 0 | 88704 / 114272 | training loss: 0.1418483406305313\n",
      "epoch: 0 | 88736 / 114272 | training loss: 0.08989689499139786\n",
      "epoch: 0 | 88768 / 114272 | training loss: 0.05852688103914261\n",
      "epoch: 0 | 88800 / 114272 | training loss: 0.005287547595798969\n",
      "epoch: 0 | 88832 / 114272 | training loss: 0.11619176715612411\n",
      "epoch: 0 | 88864 / 114272 | training loss: 0.19711905717849731\n",
      "epoch: 0 | 88896 / 114272 | training loss: 0.011662594974040985\n",
      "epoch: 0 | 88928 / 114272 | training loss: 0.05966479703783989\n",
      "epoch: 0 | 88960 / 114272 | training loss: 0.025724167004227638\n",
      "epoch: 0 | 88992 / 114272 | training loss: 0.14863424003124237\n",
      "epoch: 0 | 89024 / 114272 | training loss: 0.025842005386948586\n",
      "epoch: 0 | 89056 / 114272 | training loss: 0.0099120968952775\n",
      "epoch: 0 | 89088 / 114272 | training loss: 0.057694561779499054\n",
      "epoch: 0 | 89120 / 114272 | training loss: 0.05030707269906998\n",
      "epoch: 0 | 89152 / 114272 | training loss: 0.06737479567527771\n",
      "epoch: 0 | 89184 / 114272 | training loss: 0.4090631306171417\n",
      "epoch: 0 | 89216 / 114272 | training loss: 0.1805819571018219\n",
      "epoch: 0 | 89248 / 114272 | training loss: 0.2944662868976593\n",
      "epoch: 0 | 89280 / 114272 | training loss: 0.33641043305397034\n",
      "epoch: 0 | 89312 / 114272 | training loss: 0.33609363436698914\n",
      "epoch: 0 | 89344 / 114272 | training loss: 0.006165807135403156\n",
      "epoch: 0 | 89376 / 114272 | training loss: 0.01201495062559843\n",
      "epoch: 0 | 89408 / 114272 | training loss: 0.06934123486280441\n",
      "epoch: 0 | 89440 / 114272 | training loss: 0.166610449552536\n",
      "epoch: 0 | 89472 / 114272 | training loss: 0.24587306380271912\n",
      "epoch: 0 | 89504 / 114272 | training loss: 0.1302124261856079\n",
      "epoch: 0 | 89536 / 114272 | training loss: 0.050628576427698135\n",
      "epoch: 0 | 89568 / 114272 | training loss: 0.14577677845954895\n",
      "epoch: 0 | 89600 / 114272 | training loss: 0.29505088925361633\n",
      "epoch: 0 | 89632 / 114272 | training loss: 0.0773620754480362\n",
      "epoch: 0 | 89664 / 114272 | training loss: 0.05488390102982521\n",
      "epoch: 0 | 89696 / 114272 | training loss: 0.16087645292282104\n",
      "epoch: 0 | 89728 / 114272 | training loss: 0.20006056129932404\n",
      "epoch: 0 | 89760 / 114272 | training loss: 0.12917070090770721\n",
      "epoch: 0 | 89792 / 114272 | training loss: 0.3036770522594452\n",
      "epoch: 0 | 89824 / 114272 | training loss: 0.17168280482292175\n",
      "epoch: 0 | 89856 / 114272 | training loss: 0.026939742267131805\n",
      "epoch: 0 | 89888 / 114272 | training loss: 0.4743950366973877\n",
      "epoch: 0 | 89920 / 114272 | training loss: 0.07907667756080627\n",
      "epoch: 0 | 89952 / 114272 | training loss: 0.14542339742183685\n",
      "epoch: 0 | 89984 / 114272 | training loss: 0.027924034744501114\n",
      "epoch: 0 | 90016 / 114272 | training loss: 0.035820748656988144\n",
      "epoch: 0 | 90048 / 114272 | training loss: 0.08958014845848083\n",
      "epoch: 0 | 90080 / 114272 | training loss: 0.3506004512310028\n",
      "epoch: 0 | 90112 / 114272 | training loss: 0.181112602353096\n",
      "epoch: 0 | 90144 / 114272 | training loss: 0.04939122870564461\n",
      "epoch: 0 | 90176 / 114272 | training loss: 0.08559776097536087\n",
      "epoch: 0 | 90208 / 114272 | training loss: 0.057083796709775925\n",
      "epoch: 0 | 90240 / 114272 | training loss: 0.04787854477763176\n",
      "epoch: 0 | 90272 / 114272 | training loss: 0.023846741765737534\n",
      "epoch: 0 | 90304 / 114272 | training loss: 0.16204577684402466\n",
      "epoch: 0 | 90336 / 114272 | training loss: 0.04764881730079651\n",
      "epoch: 0 | 90368 / 114272 | training loss: 0.030305545777082443\n",
      "epoch: 0 | 90400 / 114272 | training loss: 0.34764906764030457\n",
      "epoch: 0 | 90432 / 114272 | training loss: 0.07201286405324936\n",
      "epoch: 0 | 90464 / 114272 | training loss: 0.1344660520553589\n",
      "epoch: 0 | 90496 / 114272 | training loss: 0.12024661153554916\n",
      "epoch: 0 | 90528 / 114272 | training loss: 0.11574999988079071\n",
      "epoch: 0 | 90560 / 114272 | training loss: 0.157383531332016\n",
      "epoch: 0 | 90592 / 114272 | training loss: 0.16702371835708618\n",
      "epoch: 0 | 90624 / 114272 | training loss: 0.31121042370796204\n",
      "epoch: 0 | 90656 / 114272 | training loss: 0.23372936248779297\n",
      "epoch: 0 | 90688 / 114272 | training loss: 0.19604504108428955\n",
      "epoch: 0 | 90720 / 114272 | training loss: 0.2751547694206238\n",
      "epoch: 0 | 90752 / 114272 | training loss: 0.17937813699245453\n",
      "epoch: 0 | 90784 / 114272 | training loss: 0.19348499178886414\n",
      "epoch: 0 | 90816 / 114272 | training loss: 0.039151426404714584\n",
      "epoch: 0 | 90848 / 114272 | training loss: 0.1561177670955658\n",
      "epoch: 0 | 90880 / 114272 | training loss: 0.16289973258972168\n",
      "epoch: 0 | 90912 / 114272 | training loss: 0.11144643276929855\n",
      "epoch: 0 | 90944 / 114272 | training loss: 0.07818420231342316\n",
      "epoch: 0 | 90976 / 114272 | training loss: 0.18153174221515656\n",
      "epoch: 0 | 91008 / 114272 | training loss: 0.09193392097949982\n",
      "epoch: 0 | 91040 / 114272 | training loss: 0.18980802595615387\n",
      "epoch: 0 | 91072 / 114272 | training loss: 0.19004052877426147\n",
      "epoch: 0 | 91104 / 114272 | training loss: 0.013148230500519276\n",
      "epoch: 0 | 91136 / 114272 | training loss: 0.1213822215795517\n",
      "epoch: 0 | 91168 / 114272 | training loss: 0.12322637438774109\n",
      "epoch: 0 | 91200 / 114272 | training loss: 0.23416687548160553\n",
      "epoch: 0 | 91232 / 114272 | training loss: 0.06835883110761642\n",
      "epoch: 0 | 91264 / 114272 | training loss: 0.14508184790611267\n",
      "epoch: 0 | 91296 / 114272 | training loss: 0.07387547940015793\n",
      "epoch: 0 | 91328 / 114272 | training loss: 0.2725510597229004\n",
      "epoch: 0 | 91360 / 114272 | training loss: 0.13759803771972656\n",
      "epoch: 0 | 91392 / 114272 | training loss: 0.1820489913225174\n",
      "epoch: 0 | 91424 / 114272 | training loss: 0.3983045518398285\n",
      "epoch: 0 | 91456 / 114272 | training loss: 0.190418541431427\n",
      "epoch: 0 | 91488 / 114272 | training loss: 0.1614845246076584\n",
      "epoch: 0 | 91520 / 114272 | training loss: 0.2636350691318512\n",
      "epoch: 0 | 91552 / 114272 | training loss: 0.09649834781885147\n",
      "epoch: 0 | 91584 / 114272 | training loss: 0.2147245705127716\n",
      "epoch: 0 | 91616 / 114272 | training loss: 0.1723216325044632\n",
      "epoch: 0 | 91648 / 114272 | training loss: 0.2562880218029022\n",
      "epoch: 0 | 91680 / 114272 | training loss: 0.032276734709739685\n",
      "epoch: 0 | 91712 / 114272 | training loss: 0.16534268856048584\n",
      "epoch: 0 | 91744 / 114272 | training loss: 0.3199581503868103\n",
      "epoch: 0 | 91776 / 114272 | training loss: 0.14684709906578064\n",
      "epoch: 0 | 91808 / 114272 | training loss: 0.03407839685678482\n",
      "epoch: 0 | 91840 / 114272 | training loss: 0.16694042086601257\n",
      "epoch: 0 | 91872 / 114272 | training loss: 0.25195714831352234\n",
      "epoch: 0 | 91904 / 114272 | training loss: 0.1772986799478531\n",
      "epoch: 0 | 91936 / 114272 | training loss: 0.1933542639017105\n",
      "epoch: 0 | 91968 / 114272 | training loss: 0.1854766309261322\n",
      "epoch: 0 | 92000 / 114272 | training loss: 0.157276913523674\n",
      "epoch: 0 | 92032 / 114272 | training loss: 0.24372559785842896\n",
      "epoch: 0 | 92064 / 114272 | training loss: 0.17990821599960327\n",
      "epoch: 0 | 92096 / 114272 | training loss: 0.22507920861244202\n",
      "epoch: 0 | 92128 / 114272 | training loss: 0.10227566212415695\n",
      "epoch: 0 | 92160 / 114272 | training loss: 0.19229558110237122\n",
      "epoch: 0 | 92192 / 114272 | training loss: 0.06558994948863983\n",
      "epoch: 0 | 92224 / 114272 | training loss: 0.059972215443849564\n",
      "epoch: 0 | 92256 / 114272 | training loss: 0.05482453107833862\n",
      "epoch: 0 | 92288 / 114272 | training loss: 0.11339130997657776\n",
      "epoch: 0 | 92320 / 114272 | training loss: 0.4093612730503082\n",
      "epoch: 0 | 92352 / 114272 | training loss: 0.1312026083469391\n",
      "epoch: 0 | 92384 / 114272 | training loss: 0.12166569381952286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 92416 / 114272 | training loss: 0.05779188871383667\n",
      "epoch: 0 | 92448 / 114272 | training loss: 0.07417687028646469\n",
      "epoch: 0 | 92480 / 114272 | training loss: 0.2607337236404419\n",
      "epoch: 0 | 92512 / 114272 | training loss: 0.059637293219566345\n",
      "epoch: 0 | 92544 / 114272 | training loss: 0.06270003318786621\n",
      "epoch: 0 | 92576 / 114272 | training loss: 0.40426602959632874\n",
      "epoch: 0 | 92608 / 114272 | training loss: 0.0690624937415123\n",
      "epoch: 0 | 92640 / 114272 | training loss: 0.036872684955596924\n",
      "epoch: 0 | 92672 / 114272 | training loss: 0.0532749742269516\n",
      "epoch: 0 | 92704 / 114272 | training loss: 0.01187248807400465\n",
      "epoch: 0 | 92736 / 114272 | training loss: 0.02548130601644516\n",
      "epoch: 0 | 92768 / 114272 | training loss: 0.40362784266471863\n",
      "epoch: 0 | 92800 / 114272 | training loss: 0.1466086357831955\n",
      "epoch: 0 | 92832 / 114272 | training loss: 0.11422283202409744\n",
      "epoch: 0 | 92864 / 114272 | training loss: 0.07676830887794495\n",
      "epoch: 0 | 92896 / 114272 | training loss: 0.1552913784980774\n",
      "epoch: 0 | 92928 / 114272 | training loss: 0.047986194491386414\n",
      "epoch: 0 | 92960 / 114272 | training loss: 0.049320973455905914\n",
      "epoch: 0 | 92992 / 114272 | training loss: 0.21979708969593048\n",
      "epoch: 0 | 93024 / 114272 | training loss: 0.18629463016986847\n",
      "epoch: 0 | 93056 / 114272 | training loss: 0.21972808241844177\n",
      "epoch: 0 | 93088 / 114272 | training loss: 0.029267819598317146\n",
      "epoch: 0 | 93120 / 114272 | training loss: 0.2513178884983063\n",
      "epoch: 0 | 93152 / 114272 | training loss: 0.044985316693782806\n",
      "epoch: 0 | 93184 / 114272 | training loss: 0.17579156160354614\n",
      "epoch: 0 | 93216 / 114272 | training loss: 0.00984526053071022\n",
      "epoch: 0 | 93248 / 114272 | training loss: 0.11348649114370346\n",
      "epoch: 0 | 93280 / 114272 | training loss: 0.34109124541282654\n",
      "epoch: 0 | 93312 / 114272 | training loss: 0.1594703644514084\n",
      "epoch: 0 | 93344 / 114272 | training loss: 0.10361722111701965\n",
      "epoch: 0 | 93376 / 114272 | training loss: 0.08323349803686142\n",
      "epoch: 0 | 93408 / 114272 | training loss: 0.19134767353534698\n",
      "epoch: 0 | 93440 / 114272 | training loss: 0.22946278750896454\n",
      "epoch: 0 | 93472 / 114272 | training loss: 0.24067926406860352\n",
      "epoch: 0 | 93504 / 114272 | training loss: 0.1934916079044342\n",
      "epoch: 0 | 93536 / 114272 | training loss: 0.41977307200431824\n",
      "epoch: 0 | 93568 / 114272 | training loss: 0.21217943727970123\n",
      "epoch: 0 | 93600 / 114272 | training loss: 0.1687193661928177\n",
      "epoch: 0 | 93632 / 114272 | training loss: 0.039813101291656494\n",
      "epoch: 0 | 93664 / 114272 | training loss: 0.25295591354370117\n",
      "epoch: 0 | 93696 / 114272 | training loss: 0.08101920783519745\n",
      "epoch: 0 | 93728 / 114272 | training loss: 0.15908412635326385\n",
      "epoch: 0 | 93760 / 114272 | training loss: 0.16204288601875305\n",
      "epoch: 0 | 93792 / 114272 | training loss: 0.07252936065196991\n",
      "epoch: 0 | 93824 / 114272 | training loss: 0.05028723552823067\n",
      "epoch: 0 | 93856 / 114272 | training loss: 0.13184794783592224\n",
      "epoch: 0 | 93888 / 114272 | training loss: 0.06803438067436218\n",
      "epoch: 0 | 93920 / 114272 | training loss: 0.08260572701692581\n",
      "epoch: 0 | 93952 / 114272 | training loss: 0.14475899934768677\n",
      "epoch: 0 | 93984 / 114272 | training loss: 0.08054220676422119\n",
      "epoch: 0 | 94016 / 114272 | training loss: 0.46445438265800476\n",
      "epoch: 0 | 94048 / 114272 | training loss: 0.11626673489809036\n",
      "epoch: 0 | 94080 / 114272 | training loss: 0.1568284034729004\n",
      "epoch: 0 | 94112 / 114272 | training loss: 0.1909382939338684\n",
      "epoch: 0 | 94144 / 114272 | training loss: 0.06144721060991287\n",
      "epoch: 0 | 94176 / 114272 | training loss: 0.14127181470394135\n",
      "epoch: 0 | 94208 / 114272 | training loss: 0.0837576687335968\n",
      "epoch: 0 | 94240 / 114272 | training loss: 0.029018357396125793\n",
      "epoch: 0 | 94272 / 114272 | training loss: 0.048947952687740326\n",
      "epoch: 0 | 94304 / 114272 | training loss: 0.28779137134552\n",
      "epoch: 0 | 94336 / 114272 | training loss: 0.0716436430811882\n",
      "epoch: 0 | 94368 / 114272 | training loss: 0.06449002772569656\n",
      "epoch: 0 | 94400 / 114272 | training loss: 0.04801689088344574\n",
      "epoch: 0 | 94432 / 114272 | training loss: 0.0284203439950943\n",
      "epoch: 0 | 94464 / 114272 | training loss: 0.2653629779815674\n",
      "epoch: 0 | 94496 / 114272 | training loss: 0.03984355926513672\n",
      "epoch: 0 | 94528 / 114272 | training loss: 0.15420356392860413\n",
      "epoch: 0 | 94560 / 114272 | training loss: 0.16154365241527557\n",
      "epoch: 0 | 94592 / 114272 | training loss: 0.057749368250370026\n",
      "epoch: 0 | 94624 / 114272 | training loss: 0.09366773068904877\n",
      "epoch: 0 | 94656 / 114272 | training loss: 0.10564285516738892\n",
      "epoch: 0 | 94688 / 114272 | training loss: 0.04567411169409752\n",
      "epoch: 0 | 94720 / 114272 | training loss: 0.2573164403438568\n",
      "epoch: 0 | 94752 / 114272 | training loss: 0.38698527216911316\n",
      "epoch: 0 | 94784 / 114272 | training loss: 0.07268986105918884\n",
      "epoch: 0 | 94816 / 114272 | training loss: 0.053018856793642044\n",
      "epoch: 0 | 94848 / 114272 | training loss: 0.07551189512014389\n",
      "epoch: 0 | 94880 / 114272 | training loss: 0.25037682056427\n",
      "epoch: 0 | 94912 / 114272 | training loss: 0.08485294133424759\n",
      "epoch: 0 | 94944 / 114272 | training loss: 0.13846056163311005\n",
      "epoch: 0 | 94976 / 114272 | training loss: 0.30353543162345886\n",
      "epoch: 0 | 95008 / 114272 | training loss: 0.0949404388666153\n",
      "epoch: 0 | 95040 / 114272 | training loss: 0.2170795351266861\n",
      "epoch: 0 | 95072 / 114272 | training loss: 0.048166532069444656\n",
      "epoch: 0 | 95104 / 114272 | training loss: 0.17333216965198517\n",
      "epoch: 0 | 95136 / 114272 | training loss: 0.26622962951660156\n",
      "epoch: 0 | 95168 / 114272 | training loss: 0.025599796324968338\n",
      "epoch: 0 | 95200 / 114272 | training loss: 0.04163388907909393\n",
      "epoch: 0 | 95232 / 114272 | training loss: 0.2737216651439667\n",
      "epoch: 0 | 95264 / 114272 | training loss: 0.05902302265167236\n",
      "epoch: 0 | 95296 / 114272 | training loss: 0.28545674681663513\n",
      "epoch: 0 | 95328 / 114272 | training loss: 0.03443640470504761\n",
      "epoch: 0 | 95360 / 114272 | training loss: 0.4727647006511688\n",
      "epoch: 0 | 95392 / 114272 | training loss: 0.12439455091953278\n",
      "epoch: 0 | 95424 / 114272 | training loss: 0.2707000970840454\n",
      "epoch: 0 | 95456 / 114272 | training loss: 0.014047943986952305\n",
      "epoch: 0 | 95488 / 114272 | training loss: 0.2862671911716461\n",
      "epoch: 0 | 95520 / 114272 | training loss: 0.15468227863311768\n",
      "epoch: 0 | 95552 / 114272 | training loss: 0.2700115740299225\n",
      "epoch: 0 | 95584 / 114272 | training loss: 0.19480523467063904\n",
      "epoch: 0 | 95616 / 114272 | training loss: 0.07822637259960175\n",
      "epoch: 0 | 95648 / 114272 | training loss: 0.33504000306129456\n",
      "epoch: 0 | 95680 / 114272 | training loss: 0.08633023500442505\n",
      "epoch: 0 | 95712 / 114272 | training loss: 0.3978687524795532\n",
      "epoch: 0 | 95744 / 114272 | training loss: 0.25293806195259094\n",
      "epoch: 0 | 95776 / 114272 | training loss: 0.16341884434223175\n",
      "epoch: 0 | 95808 / 114272 | training loss: 0.3835761845111847\n",
      "epoch: 0 | 95840 / 114272 | training loss: 0.0968126580119133\n",
      "epoch: 0 | 95872 / 114272 | training loss: 0.14504054188728333\n",
      "epoch: 0 | 95904 / 114272 | training loss: 0.020066576078534126\n",
      "epoch: 0 | 95936 / 114272 | training loss: 0.14425525069236755\n",
      "epoch: 0 | 95968 / 114272 | training loss: 0.05545075237751007\n",
      "epoch: 0 | 96000 / 114272 | training loss: 0.15647420287132263\n",
      "epoch: 0 | 96032 / 114272 | training loss: 0.06970279663801193\n",
      "epoch: 0 | 96064 / 114272 | training loss: 0.17458964884281158\n",
      "epoch: 0 | 96096 / 114272 | training loss: 0.055643029510974884\n",
      "epoch: 0 | 96128 / 114272 | training loss: 0.1594921350479126\n",
      "epoch: 0 | 96160 / 114272 | training loss: 0.18757137656211853\n",
      "epoch: 0 | 96192 / 114272 | training loss: 0.06634852290153503\n",
      "epoch: 0 | 96224 / 114272 | training loss: 0.0522032231092453\n",
      "epoch: 0 | 96256 / 114272 | training loss: 0.031172314658761024\n",
      "epoch: 0 | 96288 / 114272 | training loss: 0.31761813163757324\n",
      "epoch: 0 | 96320 / 114272 | training loss: 0.047069791704416275\n",
      "epoch: 0 | 96352 / 114272 | training loss: 0.16365210711956024\n",
      "epoch: 0 | 96384 / 114272 | training loss: 0.11525092273950577\n",
      "epoch: 0 | 96416 / 114272 | training loss: 0.2691522538661957\n",
      "epoch: 0 | 96448 / 114272 | training loss: 0.16443336009979248\n",
      "epoch: 0 | 96480 / 114272 | training loss: 0.3552001714706421\n",
      "epoch: 0 | 96512 / 114272 | training loss: 0.06749548017978668\n",
      "epoch: 0 | 96544 / 114272 | training loss: 0.11882872879505157\n",
      "epoch: 0 | 96576 / 114272 | training loss: 0.15671586990356445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 96608 / 114272 | training loss: 0.012110707350075245\n",
      "epoch: 0 | 96640 / 114272 | training loss: 0.015481805428862572\n",
      "epoch: 0 | 96672 / 114272 | training loss: 0.13184738159179688\n",
      "epoch: 0 | 96704 / 114272 | training loss: 0.11839623004198074\n",
      "epoch: 0 | 96736 / 114272 | training loss: 0.0970141664147377\n",
      "epoch: 0 | 96768 / 114272 | training loss: 0.3662945628166199\n",
      "epoch: 0 | 96800 / 114272 | training loss: 0.019819192588329315\n",
      "epoch: 0 | 96832 / 114272 | training loss: 0.311050683259964\n",
      "epoch: 0 | 96864 / 114272 | training loss: 0.3293427526950836\n",
      "epoch: 0 | 96896 / 114272 | training loss: 0.2571171820163727\n",
      "epoch: 0 | 96928 / 114272 | training loss: 0.3594616949558258\n",
      "epoch: 0 | 96960 / 114272 | training loss: 0.19031420350074768\n",
      "epoch: 0 | 96992 / 114272 | training loss: 0.18934676051139832\n",
      "epoch: 0 | 97024 / 114272 | training loss: 0.1824565976858139\n",
      "epoch: 0 | 97056 / 114272 | training loss: 0.20720316469669342\n",
      "epoch: 0 | 97088 / 114272 | training loss: 0.13775946199893951\n",
      "epoch: 0 | 97120 / 114272 | training loss: 0.2535017430782318\n",
      "epoch: 0 | 97152 / 114272 | training loss: 0.019566519185900688\n",
      "epoch: 0 | 97184 / 114272 | training loss: 0.04621748998761177\n",
      "epoch: 0 | 97216 / 114272 | training loss: 0.11873811483383179\n",
      "epoch: 0 | 97248 / 114272 | training loss: 0.3916682302951813\n",
      "epoch: 0 | 97280 / 114272 | training loss: 0.07484326511621475\n",
      "epoch: 0 | 97312 / 114272 | training loss: 0.2533379793167114\n",
      "epoch: 0 | 97344 / 114272 | training loss: 0.32330694794654846\n",
      "epoch: 0 | 97376 / 114272 | training loss: 0.08434166759252548\n",
      "epoch: 0 | 97408 / 114272 | training loss: 0.04412180185317993\n",
      "epoch: 0 | 97440 / 114272 | training loss: 0.08606477081775665\n",
      "epoch: 0 | 97472 / 114272 | training loss: 0.1550668478012085\n",
      "epoch: 0 | 97504 / 114272 | training loss: 0.07761971652507782\n",
      "epoch: 0 | 97536 / 114272 | training loss: 0.04467809945344925\n",
      "epoch: 0 | 97568 / 114272 | training loss: 0.026955600827932358\n",
      "epoch: 0 | 97600 / 114272 | training loss: 0.11378012597560883\n",
      "epoch: 0 | 97632 / 114272 | training loss: 0.13179996609687805\n",
      "epoch: 0 | 97664 / 114272 | training loss: 0.06928461045026779\n",
      "epoch: 0 | 97696 / 114272 | training loss: 0.5087985992431641\n",
      "epoch: 0 | 97728 / 114272 | training loss: 0.06487326323986053\n",
      "epoch: 0 | 97760 / 114272 | training loss: 0.29041504859924316\n",
      "epoch: 0 | 97792 / 114272 | training loss: 0.024681003764271736\n",
      "epoch: 0 | 97824 / 114272 | training loss: 0.08392331004142761\n",
      "epoch: 0 | 97856 / 114272 | training loss: 0.2443520873785019\n",
      "epoch: 0 | 97888 / 114272 | training loss: 0.0918058454990387\n",
      "epoch: 0 | 97920 / 114272 | training loss: 0.34947893023490906\n",
      "epoch: 0 | 97952 / 114272 | training loss: 0.15471486747264862\n",
      "epoch: 0 | 97984 / 114272 | training loss: 0.15916308760643005\n",
      "epoch: 0 | 98016 / 114272 | training loss: 0.06317157298326492\n",
      "epoch: 0 | 98048 / 114272 | training loss: 0.14484098553657532\n",
      "epoch: 0 | 98080 / 114272 | training loss: 0.031652241945266724\n",
      "epoch: 0 | 98112 / 114272 | training loss: 0.032550618052482605\n",
      "epoch: 0 | 98144 / 114272 | training loss: 0.2332579493522644\n",
      "epoch: 0 | 98176 / 114272 | training loss: 0.03335875645279884\n",
      "epoch: 0 | 98208 / 114272 | training loss: 0.14737507700920105\n",
      "epoch: 0 | 98240 / 114272 | training loss: 0.03236956521868706\n",
      "epoch: 0 | 98272 / 114272 | training loss: 0.09224126487970352\n",
      "epoch: 0 | 98304 / 114272 | training loss: 0.1938522458076477\n",
      "epoch: 0 | 98336 / 114272 | training loss: 0.11637822538614273\n",
      "epoch: 0 | 98368 / 114272 | training loss: 0.21232274174690247\n",
      "epoch: 0 | 98400 / 114272 | training loss: 0.45746761560440063\n",
      "epoch: 0 | 98432 / 114272 | training loss: 0.1329907327890396\n",
      "epoch: 0 | 98464 / 114272 | training loss: 0.10271716862916946\n",
      "epoch: 0 | 98496 / 114272 | training loss: 0.2274070531129837\n",
      "epoch: 0 | 98528 / 114272 | training loss: 0.1564154177904129\n",
      "epoch: 0 | 98560 / 114272 | training loss: 0.13913646340370178\n",
      "epoch: 0 | 98592 / 114272 | training loss: 0.10702622681856155\n",
      "epoch: 0 | 98624 / 114272 | training loss: 0.3360533118247986\n",
      "epoch: 0 | 98656 / 114272 | training loss: 0.2537073493003845\n",
      "epoch: 0 | 98688 / 114272 | training loss: 0.18304039537906647\n",
      "epoch: 0 | 98720 / 114272 | training loss: 0.0706651359796524\n",
      "epoch: 0 | 98752 / 114272 | training loss: 0.3290631175041199\n",
      "epoch: 0 | 98784 / 114272 | training loss: 0.3032565116882324\n",
      "epoch: 0 | 98816 / 114272 | training loss: 0.053760528564453125\n",
      "epoch: 0 | 98848 / 114272 | training loss: 0.07499728351831436\n",
      "epoch: 0 | 98880 / 114272 | training loss: 0.10868623107671738\n",
      "epoch: 0 | 98912 / 114272 | training loss: 0.2630203664302826\n",
      "epoch: 0 | 98944 / 114272 | training loss: 0.03728979453444481\n",
      "epoch: 0 | 98976 / 114272 | training loss: 0.13076651096343994\n",
      "epoch: 0 | 99008 / 114272 | training loss: 0.07913724333047867\n",
      "epoch: 0 | 99040 / 114272 | training loss: 0.23329146206378937\n",
      "epoch: 0 | 99072 / 114272 | training loss: 0.014552345499396324\n",
      "epoch: 0 | 99104 / 114272 | training loss: 0.13818244636058807\n",
      "epoch: 0 | 99136 / 114272 | training loss: 0.052368585020303726\n",
      "epoch: 0 | 99168 / 114272 | training loss: 0.11792915314435959\n",
      "epoch: 0 | 99200 / 114272 | training loss: 0.18272218108177185\n",
      "epoch: 0 | 99232 / 114272 | training loss: 0.018830327317118645\n",
      "epoch: 0 | 99264 / 114272 | training loss: 0.21808554232120514\n",
      "epoch: 0 | 99296 / 114272 | training loss: 0.141927108168602\n",
      "epoch: 0 | 99328 / 114272 | training loss: 0.02787179686129093\n",
      "epoch: 0 | 99360 / 114272 | training loss: 0.15058796107769012\n",
      "epoch: 0 | 99392 / 114272 | training loss: 0.04866505786776543\n",
      "epoch: 0 | 99424 / 114272 | training loss: 0.058535557240247726\n",
      "epoch: 0 | 99456 / 114272 | training loss: 0.22832238674163818\n",
      "epoch: 0 | 99488 / 114272 | training loss: 0.19154129922389984\n",
      "epoch: 0 | 99520 / 114272 | training loss: 0.34693801403045654\n",
      "epoch: 0 | 99552 / 114272 | training loss: 0.3669299781322479\n",
      "epoch: 0 | 99584 / 114272 | training loss: 0.12018194049596786\n",
      "epoch: 0 | 99616 / 114272 | training loss: 0.20021432638168335\n",
      "epoch: 0 | 99648 / 114272 | training loss: 0.08171222358942032\n",
      "epoch: 0 | 99680 / 114272 | training loss: 0.03177407756447792\n",
      "epoch: 0 | 99712 / 114272 | training loss: 0.24007347226142883\n",
      "epoch: 0 | 99744 / 114272 | training loss: 0.3003542125225067\n",
      "epoch: 0 | 99776 / 114272 | training loss: 0.0824631005525589\n",
      "epoch: 0 | 99808 / 114272 | training loss: 0.2557727098464966\n",
      "epoch: 0 | 99840 / 114272 | training loss: 0.12200963497161865\n",
      "epoch: 0 | 99872 / 114272 | training loss: 0.06435836851596832\n",
      "epoch: 0 | 99904 / 114272 | training loss: 0.0574755035340786\n",
      "epoch: 0 | 99936 / 114272 | training loss: 0.16568617522716522\n",
      "epoch: 0 | 99968 / 114272 | training loss: 0.19247861206531525\n",
      "epoch: 0 | 100000 / 114272 | training loss: 0.06573936343193054\n",
      "epoch: 0 | 100032 / 114272 | training loss: 0.02614433504641056\n",
      "epoch: 0 | 100064 / 114272 | training loss: 0.16040295362472534\n",
      "epoch: 0 | 100096 / 114272 | training loss: 0.1359175145626068\n",
      "epoch: 0 | 100128 / 114272 | training loss: 0.23987658321857452\n",
      "epoch: 0 | 100160 / 114272 | training loss: 0.16798973083496094\n",
      "epoch: 0 | 100192 / 114272 | training loss: 0.20678149163722992\n",
      "epoch: 0 | 100224 / 114272 | training loss: 0.16689278185367584\n",
      "epoch: 0 | 100256 / 114272 | training loss: 0.45335549116134644\n",
      "epoch: 0 | 100288 / 114272 | training loss: 0.04108173027634621\n",
      "epoch: 0 | 100320 / 114272 | training loss: 0.13330820202827454\n",
      "epoch: 0 | 100352 / 114272 | training loss: 0.28024420142173767\n",
      "epoch: 0 | 100384 / 114272 | training loss: 0.07319704443216324\n",
      "epoch: 0 | 100416 / 114272 | training loss: 0.1513330638408661\n",
      "epoch: 0 | 100448 / 114272 | training loss: 0.1678156852722168\n",
      "epoch: 0 | 100480 / 114272 | training loss: 0.19351042807102203\n",
      "epoch: 0 | 100512 / 114272 | training loss: 0.12769420444965363\n",
      "epoch: 0 | 100544 / 114272 | training loss: 0.08363531529903412\n",
      "epoch: 0 | 100576 / 114272 | training loss: 0.19157692790031433\n",
      "epoch: 0 | 100608 / 114272 | training loss: 0.1276957392692566\n",
      "epoch: 0 | 100640 / 114272 | training loss: 0.03069743700325489\n",
      "epoch: 0 | 100672 / 114272 | training loss: 0.1208595260977745\n",
      "epoch: 0 | 100704 / 114272 | training loss: 0.29519811272621155\n",
      "epoch: 0 | 100736 / 114272 | training loss: 0.049962546676397324\n",
      "epoch: 0 | 100768 / 114272 | training loss: 0.08495502173900604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 100800 / 114272 | training loss: 0.06127437204122543\n",
      "epoch: 0 | 100832 / 114272 | training loss: 0.17753008008003235\n",
      "epoch: 0 | 100864 / 114272 | training loss: 0.09069269150495529\n",
      "epoch: 0 | 100896 / 114272 | training loss: 0.15342162549495697\n",
      "epoch: 0 | 100928 / 114272 | training loss: 0.04168189316987991\n",
      "epoch: 0 | 100960 / 114272 | training loss: 0.2677314281463623\n",
      "epoch: 0 | 100992 / 114272 | training loss: 0.08354972302913666\n",
      "epoch: 0 | 101024 / 114272 | training loss: 0.08010612428188324\n",
      "epoch: 0 | 101056 / 114272 | training loss: 0.19919803738594055\n",
      "epoch: 0 | 101088 / 114272 | training loss: 0.020038390532135963\n",
      "epoch: 0 | 101120 / 114272 | training loss: 0.2701543867588043\n",
      "epoch: 0 | 101152 / 114272 | training loss: 0.24843424558639526\n",
      "epoch: 0 | 101184 / 114272 | training loss: 0.30198511481285095\n",
      "epoch: 0 | 101216 / 114272 | training loss: 0.05956873297691345\n",
      "epoch: 0 | 101248 / 114272 | training loss: 0.11325691640377045\n",
      "epoch: 0 | 101280 / 114272 | training loss: 0.11021347343921661\n",
      "epoch: 0 | 101312 / 114272 | training loss: 0.22540895640850067\n",
      "epoch: 0 | 101344 / 114272 | training loss: 0.14194661378860474\n",
      "epoch: 0 | 101376 / 114272 | training loss: 0.14833858609199524\n",
      "epoch: 0 | 101408 / 114272 | training loss: 0.04131504148244858\n",
      "epoch: 0 | 101440 / 114272 | training loss: 0.16566480696201324\n",
      "epoch: 0 | 101472 / 114272 | training loss: 0.05101185664534569\n",
      "epoch: 0 | 101504 / 114272 | training loss: 0.2902648150920868\n",
      "epoch: 0 | 101536 / 114272 | training loss: 0.3128916621208191\n",
      "epoch: 0 | 101568 / 114272 | training loss: 0.29949861764907837\n",
      "epoch: 0 | 101600 / 114272 | training loss: 0.021028345450758934\n",
      "epoch: 0 | 101632 / 114272 | training loss: 0.14033162593841553\n",
      "epoch: 0 | 101664 / 114272 | training loss: 0.39099568128585815\n",
      "epoch: 0 | 101696 / 114272 | training loss: 0.30351492762565613\n",
      "epoch: 0 | 101728 / 114272 | training loss: 0.16060307621955872\n",
      "epoch: 0 | 101760 / 114272 | training loss: 0.02096334658563137\n",
      "epoch: 0 | 101792 / 114272 | training loss: 0.13725696504116058\n",
      "epoch: 0 | 101824 / 114272 | training loss: 0.057454224675893784\n",
      "epoch: 0 | 101856 / 114272 | training loss: 0.08382735401391983\n",
      "epoch: 0 | 101888 / 114272 | training loss: 0.04849172383546829\n",
      "epoch: 0 | 101920 / 114272 | training loss: 0.1814344823360443\n",
      "epoch: 0 | 101952 / 114272 | training loss: 0.15553458034992218\n",
      "epoch: 0 | 101984 / 114272 | training loss: 0.05379323288798332\n",
      "epoch: 0 | 102016 / 114272 | training loss: 0.025418341159820557\n",
      "epoch: 0 | 102048 / 114272 | training loss: 0.05514875799417496\n",
      "epoch: 0 | 102080 / 114272 | training loss: 0.27532878518104553\n",
      "epoch: 0 | 102112 / 114272 | training loss: 0.12565962970256805\n",
      "epoch: 0 | 102144 / 114272 | training loss: 0.1685122847557068\n",
      "epoch: 0 | 102176 / 114272 | training loss: 0.09592679888010025\n",
      "epoch: 0 | 102208 / 114272 | training loss: 0.1816866248846054\n",
      "epoch: 0 | 102240 / 114272 | training loss: 0.33965861797332764\n",
      "epoch: 0 | 102272 / 114272 | training loss: 0.20911069214344025\n",
      "epoch: 0 | 102304 / 114272 | training loss: 0.0621231384575367\n",
      "epoch: 0 | 102336 / 114272 | training loss: 0.24199458956718445\n",
      "epoch: 0 | 102368 / 114272 | training loss: 0.07346431910991669\n",
      "epoch: 0 | 102400 / 114272 | training loss: 0.19267165660858154\n",
      "epoch: 0 | 102432 / 114272 | training loss: 0.28807327151298523\n",
      "epoch: 0 | 102464 / 114272 | training loss: 0.18834008276462555\n",
      "epoch: 0 | 102496 / 114272 | training loss: 0.13086149096488953\n",
      "epoch: 0 | 102528 / 114272 | training loss: 0.2392296940088272\n",
      "epoch: 0 | 102560 / 114272 | training loss: 0.10378038883209229\n",
      "epoch: 0 | 102592 / 114272 | training loss: 0.27545857429504395\n",
      "epoch: 0 | 102624 / 114272 | training loss: 0.22670571506023407\n",
      "epoch: 0 | 102656 / 114272 | training loss: 0.20014680922031403\n",
      "epoch: 0 | 102688 / 114272 | training loss: 0.07770057767629623\n",
      "epoch: 0 | 102720 / 114272 | training loss: 0.06265293806791306\n",
      "epoch: 0 | 102752 / 114272 | training loss: 0.5618050694465637\n",
      "epoch: 0 | 102784 / 114272 | training loss: 0.1817004531621933\n",
      "epoch: 0 | 102816 / 114272 | training loss: 0.3327341079711914\n",
      "epoch: 0 | 102848 / 114272 | training loss: 0.08989434689283371\n",
      "epoch: 0 | 102880 / 114272 | training loss: 0.10473368316888809\n",
      "epoch: 0 | 102912 / 114272 | training loss: 0.1622917652130127\n",
      "epoch: 0 | 102944 / 114272 | training loss: 0.1052079051733017\n",
      "epoch: 0 | 102976 / 114272 | training loss: 0.0531623438000679\n",
      "epoch: 0 | 103008 / 114272 | training loss: 0.017327334731817245\n",
      "epoch: 0 | 103040 / 114272 | training loss: 0.5719007849693298\n",
      "epoch: 0 | 103072 / 114272 | training loss: 0.2311871498823166\n",
      "epoch: 0 | 103104 / 114272 | training loss: 0.0722336620092392\n",
      "epoch: 0 | 103136 / 114272 | training loss: 0.039405208081007004\n",
      "epoch: 0 | 103168 / 114272 | training loss: 0.15317736566066742\n",
      "epoch: 0 | 103200 / 114272 | training loss: 0.031668152660131454\n",
      "epoch: 0 | 103232 / 114272 | training loss: 0.14473660290241241\n",
      "epoch: 0 | 103264 / 114272 | training loss: 0.1875510960817337\n",
      "epoch: 0 | 103296 / 114272 | training loss: 0.08861434459686279\n",
      "epoch: 0 | 103328 / 114272 | training loss: 0.15477575361728668\n",
      "epoch: 0 | 103360 / 114272 | training loss: 0.0666959211230278\n",
      "epoch: 0 | 103392 / 114272 | training loss: 0.34317585825920105\n",
      "epoch: 0 | 103424 / 114272 | training loss: 0.14076900482177734\n",
      "epoch: 0 | 103456 / 114272 | training loss: 0.08367349952459335\n",
      "epoch: 0 | 103488 / 114272 | training loss: 0.04065576195716858\n",
      "epoch: 0 | 103520 / 114272 | training loss: 0.0352359265089035\n",
      "epoch: 0 | 103552 / 114272 | training loss: 0.22184894979000092\n",
      "epoch: 0 | 103584 / 114272 | training loss: 0.21595236659049988\n",
      "epoch: 0 | 103616 / 114272 | training loss: 0.3121640086174011\n",
      "epoch: 0 | 103648 / 114272 | training loss: 0.08256296068429947\n",
      "epoch: 0 | 103680 / 114272 | training loss: 0.30922409892082214\n",
      "epoch: 0 | 103712 / 114272 | training loss: 0.10406554490327835\n",
      "epoch: 0 | 103744 / 114272 | training loss: 0.30507537722587585\n",
      "epoch: 0 | 103776 / 114272 | training loss: 0.18018963932991028\n",
      "epoch: 0 | 103808 / 114272 | training loss: 0.2504499852657318\n",
      "epoch: 0 | 103840 / 114272 | training loss: 0.24953782558441162\n",
      "epoch: 0 | 103872 / 114272 | training loss: 0.02535933256149292\n",
      "epoch: 0 | 103904 / 114272 | training loss: 0.34173330664634705\n",
      "epoch: 0 | 103936 / 114272 | training loss: 0.10407775640487671\n",
      "epoch: 0 | 103968 / 114272 | training loss: 0.031800322234630585\n",
      "epoch: 0 | 104000 / 114272 | training loss: 0.21317756175994873\n",
      "epoch: 0 | 104032 / 114272 | training loss: 0.010569097474217415\n",
      "epoch: 0 | 104064 / 114272 | training loss: 0.20567452907562256\n",
      "epoch: 0 | 104096 / 114272 | training loss: 0.11171276122331619\n",
      "epoch: 0 | 104128 / 114272 | training loss: 0.28867650032043457\n",
      "epoch: 0 | 104160 / 114272 | training loss: 0.20208409428596497\n",
      "epoch: 0 | 104192 / 114272 | training loss: 0.12612773478031158\n",
      "epoch: 0 | 104224 / 114272 | training loss: 0.11615944653749466\n",
      "epoch: 0 | 104256 / 114272 | training loss: 0.06896733492612839\n",
      "epoch: 0 | 104288 / 114272 | training loss: 0.2090737372636795\n",
      "epoch: 0 | 104320 / 114272 | training loss: 0.09617601335048676\n",
      "epoch: 0 | 104352 / 114272 | training loss: 0.2433767169713974\n",
      "epoch: 0 | 104384 / 114272 | training loss: 0.1951345056295395\n",
      "epoch: 0 | 104416 / 114272 | training loss: 0.08253517001867294\n",
      "epoch: 0 | 104448 / 114272 | training loss: 0.18308942019939423\n",
      "epoch: 0 | 104480 / 114272 | training loss: 0.17179112136363983\n",
      "epoch: 0 | 104512 / 114272 | training loss: 0.04218577221035957\n",
      "epoch: 0 | 104544 / 114272 | training loss: 0.11221226304769516\n",
      "epoch: 0 | 104576 / 114272 | training loss: 0.32776325941085815\n",
      "epoch: 0 | 104608 / 114272 | training loss: 0.09170413762331009\n",
      "epoch: 0 | 104640 / 114272 | training loss: 0.07566393166780472\n",
      "epoch: 0 | 104672 / 114272 | training loss: 0.1707860827445984\n",
      "epoch: 0 | 104704 / 114272 | training loss: 0.4834077060222626\n",
      "epoch: 0 | 104736 / 114272 | training loss: 0.14286601543426514\n",
      "epoch: 0 | 104768 / 114272 | training loss: 0.02770918607711792\n",
      "epoch: 0 | 104800 / 114272 | training loss: 0.46753770112991333\n",
      "epoch: 0 | 104832 / 114272 | training loss: 0.12643863260746002\n",
      "epoch: 0 | 104864 / 114272 | training loss: 0.03727077692747116\n",
      "epoch: 0 | 104896 / 114272 | training loss: 0.1402248740196228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 104928 / 114272 | training loss: 0.1321038007736206\n",
      "epoch: 0 | 104960 / 114272 | training loss: 0.1345871388912201\n",
      "epoch: 0 | 104992 / 114272 | training loss: 0.1875530183315277\n",
      "epoch: 0 | 105024 / 114272 | training loss: 0.13055430352687836\n",
      "epoch: 0 | 105056 / 114272 | training loss: 0.2651478946208954\n",
      "epoch: 0 | 105088 / 114272 | training loss: 0.04815662279725075\n",
      "epoch: 0 | 105120 / 114272 | training loss: 0.26688075065612793\n",
      "epoch: 0 | 105152 / 114272 | training loss: 0.200486958026886\n",
      "epoch: 0 | 105184 / 114272 | training loss: 0.024510573595762253\n",
      "epoch: 0 | 105216 / 114272 | training loss: 0.0782942846417427\n",
      "epoch: 0 | 105248 / 114272 | training loss: 0.03478636592626572\n",
      "epoch: 0 | 105280 / 114272 | training loss: 0.20294144749641418\n",
      "epoch: 0 | 105312 / 114272 | training loss: 0.022222712635993958\n",
      "epoch: 0 | 105344 / 114272 | training loss: 0.13396382331848145\n",
      "epoch: 0 | 105376 / 114272 | training loss: 0.07398975640535355\n",
      "epoch: 0 | 105408 / 114272 | training loss: 0.22761012613773346\n",
      "epoch: 0 | 105440 / 114272 | training loss: 0.07105951011180878\n",
      "epoch: 0 | 105472 / 114272 | training loss: 0.08113612979650497\n",
      "epoch: 0 | 105504 / 114272 | training loss: 0.07177891582250595\n",
      "epoch: 0 | 105536 / 114272 | training loss: 0.300248384475708\n",
      "epoch: 0 | 105568 / 114272 | training loss: 0.00902883056551218\n",
      "epoch: 0 | 105600 / 114272 | training loss: 0.05628841742873192\n",
      "epoch: 0 | 105632 / 114272 | training loss: 0.010639535263180733\n",
      "epoch: 0 | 105664 / 114272 | training loss: 0.13749811053276062\n",
      "epoch: 0 | 105696 / 114272 | training loss: 0.0283344853669405\n",
      "epoch: 0 | 105728 / 114272 | training loss: 0.016614770516753197\n",
      "epoch: 0 | 105760 / 114272 | training loss: 0.029506614431738853\n",
      "epoch: 0 | 105792 / 114272 | training loss: 0.050076570361852646\n",
      "epoch: 0 | 105824 / 114272 | training loss: 0.006565288174897432\n",
      "epoch: 0 | 105856 / 114272 | training loss: 0.13195915520191193\n",
      "epoch: 0 | 105888 / 114272 | training loss: 0.22257345914840698\n",
      "epoch: 0 | 105920 / 114272 | training loss: 0.02855749987065792\n",
      "epoch: 0 | 105952 / 114272 | training loss: 0.029769577085971832\n",
      "epoch: 0 | 105984 / 114272 | training loss: 0.09086967259645462\n",
      "epoch: 0 | 106016 / 114272 | training loss: 0.03019600175321102\n",
      "epoch: 0 | 106048 / 114272 | training loss: 0.015314583666622639\n",
      "epoch: 0 | 106080 / 114272 | training loss: 0.10822757333517075\n",
      "epoch: 0 | 106112 / 114272 | training loss: 0.2443968802690506\n",
      "epoch: 0 | 106144 / 114272 | training loss: 0.47165268659591675\n",
      "epoch: 0 | 106176 / 114272 | training loss: 0.038261186331510544\n",
      "epoch: 0 | 106208 / 114272 | training loss: 0.006684799212962389\n",
      "epoch: 0 | 106240 / 114272 | training loss: 0.12055657058954239\n",
      "epoch: 0 | 106272 / 114272 | training loss: 0.21891482174396515\n",
      "epoch: 0 | 106304 / 114272 | training loss: 0.10980141162872314\n",
      "epoch: 0 | 106336 / 114272 | training loss: 0.2556711733341217\n",
      "epoch: 0 | 106368 / 114272 | training loss: 0.3285510838031769\n",
      "epoch: 0 | 106400 / 114272 | training loss: 0.2147977203130722\n",
      "epoch: 0 | 106432 / 114272 | training loss: 0.07151542603969574\n",
      "epoch: 0 | 106464 / 114272 | training loss: 0.17053095996379852\n",
      "epoch: 0 | 106496 / 114272 | training loss: 0.2672573924064636\n",
      "epoch: 0 | 106528 / 114272 | training loss: 0.008127751760184765\n",
      "epoch: 0 | 106560 / 114272 | training loss: 0.19780273735523224\n",
      "epoch: 0 | 106592 / 114272 | training loss: 0.056476566940546036\n",
      "epoch: 0 | 106624 / 114272 | training loss: 0.03379557654261589\n",
      "epoch: 0 | 106656 / 114272 | training loss: 0.3465610444545746\n",
      "epoch: 0 | 106688 / 114272 | training loss: 0.0861334353685379\n",
      "epoch: 0 | 106720 / 114272 | training loss: 0.04691316932439804\n",
      "epoch: 0 | 106752 / 114272 | training loss: 0.18792231380939484\n",
      "epoch: 0 | 106784 / 114272 | training loss: 0.18192514777183533\n",
      "epoch: 0 | 106816 / 114272 | training loss: 0.01733533851802349\n",
      "epoch: 0 | 106848 / 114272 | training loss: 0.2723070979118347\n",
      "epoch: 0 | 106880 / 114272 | training loss: 0.23651616275310516\n",
      "epoch: 0 | 106912 / 114272 | training loss: 0.0345270112156868\n",
      "epoch: 0 | 106944 / 114272 | training loss: 0.057330068200826645\n",
      "epoch: 0 | 106976 / 114272 | training loss: 0.2955784201622009\n",
      "epoch: 0 | 107008 / 114272 | training loss: 0.13349324464797974\n",
      "epoch: 0 | 107040 / 114272 | training loss: 0.2234330177307129\n",
      "epoch: 0 | 107072 / 114272 | training loss: 0.2249779999256134\n",
      "epoch: 0 | 107104 / 114272 | training loss: 0.10813427716493607\n",
      "epoch: 0 | 107136 / 114272 | training loss: 0.03905457258224487\n",
      "epoch: 0 | 107168 / 114272 | training loss: 0.14524337649345398\n",
      "epoch: 0 | 107200 / 114272 | training loss: 0.20276029407978058\n",
      "epoch: 0 | 107232 / 114272 | training loss: 0.2167997509241104\n",
      "epoch: 0 | 107264 / 114272 | training loss: 0.02577260695397854\n",
      "epoch: 0 | 107296 / 114272 | training loss: 0.033088043332099915\n",
      "epoch: 0 | 107328 / 114272 | training loss: 0.25047922134399414\n",
      "epoch: 0 | 107360 / 114272 | training loss: 0.08662480115890503\n",
      "epoch: 0 | 107392 / 114272 | training loss: 0.21966132521629333\n",
      "epoch: 0 | 107424 / 114272 | training loss: 0.11279246211051941\n",
      "epoch: 0 | 107456 / 114272 | training loss: 0.1659519374370575\n",
      "epoch: 0 | 107488 / 114272 | training loss: 0.021186988800764084\n",
      "epoch: 0 | 107520 / 114272 | training loss: 0.10392870008945465\n",
      "epoch: 0 | 107552 / 114272 | training loss: 0.05077768489718437\n",
      "epoch: 0 | 107584 / 114272 | training loss: 0.013667119666934013\n",
      "epoch: 0 | 107616 / 114272 | training loss: 0.13197177648544312\n",
      "epoch: 0 | 107648 / 114272 | training loss: 0.11425583064556122\n",
      "epoch: 0 | 107680 / 114272 | training loss: 0.17971403896808624\n",
      "epoch: 0 | 107712 / 114272 | training loss: 0.2896725535392761\n",
      "epoch: 0 | 107744 / 114272 | training loss: 0.2825586497783661\n",
      "epoch: 0 | 107776 / 114272 | training loss: 0.28435832262039185\n",
      "epoch: 0 | 107808 / 114272 | training loss: 0.1785195916891098\n",
      "epoch: 0 | 107840 / 114272 | training loss: 0.04213293641805649\n",
      "epoch: 0 | 107872 / 114272 | training loss: 0.13974529504776\n",
      "epoch: 0 | 107904 / 114272 | training loss: 0.014898031018674374\n",
      "epoch: 0 | 107936 / 114272 | training loss: 0.026799853891134262\n",
      "epoch: 0 | 107968 / 114272 | training loss: 0.059476662427186966\n",
      "epoch: 0 | 108000 / 114272 | training loss: 0.1919250786304474\n",
      "epoch: 0 | 108032 / 114272 | training loss: 0.2982258200645447\n",
      "epoch: 0 | 108064 / 114272 | training loss: 0.08367345482110977\n",
      "epoch: 0 | 108096 / 114272 | training loss: 0.013954004272818565\n",
      "epoch: 0 | 108128 / 114272 | training loss: 0.045629408210515976\n",
      "epoch: 0 | 108160 / 114272 | training loss: 0.3769449293613434\n",
      "epoch: 0 | 108192 / 114272 | training loss: 0.24120895564556122\n",
      "epoch: 0 | 108224 / 114272 | training loss: 0.02234799414873123\n",
      "epoch: 0 | 108256 / 114272 | training loss: 0.07112270593643188\n",
      "epoch: 0 | 108288 / 114272 | training loss: 0.10239013284444809\n",
      "epoch: 0 | 108320 / 114272 | training loss: 0.04382377117872238\n",
      "epoch: 0 | 108352 / 114272 | training loss: 0.4068797528743744\n",
      "epoch: 0 | 108384 / 114272 | training loss: 0.17267869412899017\n",
      "epoch: 0 | 108416 / 114272 | training loss: 0.016895992681384087\n",
      "epoch: 0 | 108448 / 114272 | training loss: 0.2379930317401886\n",
      "epoch: 0 | 108480 / 114272 | training loss: 0.016677845269441605\n",
      "epoch: 0 | 108512 / 114272 | training loss: 0.1558377742767334\n",
      "epoch: 0 | 108544 / 114272 | training loss: 0.014059329405426979\n",
      "epoch: 0 | 108576 / 114272 | training loss: 0.20895783603191376\n",
      "epoch: 0 | 108608 / 114272 | training loss: 0.11999892443418503\n",
      "epoch: 0 | 108640 / 114272 | training loss: 0.1252855509519577\n",
      "epoch: 0 | 108672 / 114272 | training loss: 0.01636791229248047\n",
      "epoch: 0 | 108704 / 114272 | training loss: 0.32666170597076416\n",
      "epoch: 0 | 108736 / 114272 | training loss: 0.15195296704769135\n",
      "epoch: 0 | 108768 / 114272 | training loss: 0.17403511703014374\n",
      "epoch: 0 | 108800 / 114272 | training loss: 0.009464484639465809\n",
      "epoch: 0 | 108832 / 114272 | training loss: 0.29830271005630493\n",
      "epoch: 0 | 108864 / 114272 | training loss: 0.1664051115512848\n",
      "epoch: 0 | 108896 / 114272 | training loss: 0.07080112397670746\n",
      "epoch: 0 | 108928 / 114272 | training loss: 0.2299043834209442\n",
      "epoch: 0 | 108960 / 114272 | training loss: 0.2189723551273346\n",
      "epoch: 0 | 108992 / 114272 | training loss: 0.05754348263144493\n",
      "epoch: 0 | 109024 / 114272 | training loss: 0.11169273406267166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 109056 / 114272 | training loss: 0.01053551398217678\n",
      "epoch: 0 | 109088 / 114272 | training loss: 0.45279937982559204\n",
      "epoch: 0 | 109120 / 114272 | training loss: 0.06987351924180984\n",
      "epoch: 0 | 109152 / 114272 | training loss: 0.17408467829227448\n",
      "epoch: 0 | 109184 / 114272 | training loss: 0.144391268491745\n",
      "epoch: 0 | 109216 / 114272 | training loss: 0.07241357862949371\n",
      "epoch: 0 | 109248 / 114272 | training loss: 0.007782062515616417\n",
      "epoch: 0 | 109280 / 114272 | training loss: 0.014726368710398674\n",
      "epoch: 0 | 109312 / 114272 | training loss: 0.26584774255752563\n",
      "epoch: 0 | 109344 / 114272 | training loss: 0.10425152629613876\n",
      "epoch: 0 | 109376 / 114272 | training loss: 0.0891481339931488\n",
      "epoch: 0 | 109408 / 114272 | training loss: 0.07652924209833145\n",
      "epoch: 0 | 109440 / 114272 | training loss: 0.4512573182582855\n",
      "epoch: 0 | 109472 / 114272 | training loss: 0.1514357626438141\n",
      "epoch: 0 | 109504 / 114272 | training loss: 0.057828329503536224\n",
      "epoch: 0 | 109536 / 114272 | training loss: 0.1283249408006668\n",
      "epoch: 0 | 109568 / 114272 | training loss: 0.1979077160358429\n",
      "epoch: 0 | 109600 / 114272 | training loss: 0.015730202198028564\n",
      "epoch: 0 | 109632 / 114272 | training loss: 0.5049165487289429\n",
      "epoch: 0 | 109664 / 114272 | training loss: 0.31461718678474426\n",
      "epoch: 0 | 109696 / 114272 | training loss: 0.08641430735588074\n",
      "epoch: 0 | 109728 / 114272 | training loss: 0.1630043387413025\n",
      "epoch: 0 | 109760 / 114272 | training loss: 0.14986705780029297\n",
      "epoch: 0 | 109792 / 114272 | training loss: 0.03929058834910393\n",
      "epoch: 0 | 109824 / 114272 | training loss: 0.3676535189151764\n",
      "epoch: 0 | 109856 / 114272 | training loss: 0.037274785339832306\n",
      "epoch: 0 | 109888 / 114272 | training loss: 0.11288661509752274\n",
      "epoch: 0 | 109920 / 114272 | training loss: 0.4080503582954407\n",
      "epoch: 0 | 109952 / 114272 | training loss: 0.07540878653526306\n",
      "epoch: 0 | 109984 / 114272 | training loss: 0.25539660453796387\n",
      "epoch: 0 | 110016 / 114272 | training loss: 0.036059651523828506\n",
      "epoch: 0 | 110048 / 114272 | training loss: 0.2217566967010498\n",
      "epoch: 0 | 110080 / 114272 | training loss: 0.2844008207321167\n",
      "epoch: 0 | 110112 / 114272 | training loss: 0.2037329226732254\n",
      "epoch: 0 | 110144 / 114272 | training loss: 0.11089930683374405\n",
      "epoch: 0 | 110176 / 114272 | training loss: 0.2904753088951111\n",
      "epoch: 0 | 110208 / 114272 | training loss: 0.18057596683502197\n",
      "epoch: 0 | 110240 / 114272 | training loss: 0.01688169501721859\n",
      "epoch: 0 | 110272 / 114272 | training loss: 0.1974899172782898\n",
      "epoch: 0 | 110304 / 114272 | training loss: 0.11232730746269226\n",
      "epoch: 0 | 110336 / 114272 | training loss: 0.17530153691768646\n",
      "epoch: 0 | 110368 / 114272 | training loss: 0.14636513590812683\n",
      "epoch: 0 | 110400 / 114272 | training loss: 0.23332978785037994\n",
      "epoch: 0 | 110432 / 114272 | training loss: 0.2038782835006714\n",
      "epoch: 0 | 110464 / 114272 | training loss: 0.15421342849731445\n",
      "epoch: 0 | 110496 / 114272 | training loss: 0.238823801279068\n",
      "epoch: 0 | 110528 / 114272 | training loss: 0.08576928824186325\n",
      "epoch: 0 | 110560 / 114272 | training loss: 0.24571822583675385\n",
      "epoch: 0 | 110592 / 114272 | training loss: 0.04096150025725365\n",
      "epoch: 0 | 110624 / 114272 | training loss: 0.24995236098766327\n",
      "epoch: 0 | 110656 / 114272 | training loss: 0.09703420102596283\n",
      "epoch: 0 | 110688 / 114272 | training loss: 0.09503138065338135\n",
      "epoch: 0 | 110720 / 114272 | training loss: 0.26104995608329773\n",
      "epoch: 0 | 110752 / 114272 | training loss: 0.08601105213165283\n",
      "epoch: 0 | 110784 / 114272 | training loss: 0.06663058698177338\n",
      "epoch: 0 | 110816 / 114272 | training loss: 0.06143708899617195\n",
      "epoch: 0 | 110848 / 114272 | training loss: 0.24490466713905334\n",
      "epoch: 0 | 110880 / 114272 | training loss: 0.2506983280181885\n",
      "epoch: 0 | 110912 / 114272 | training loss: 0.267082154750824\n",
      "epoch: 0 | 110944 / 114272 | training loss: 0.2840385138988495\n",
      "epoch: 0 | 110976 / 114272 | training loss: 0.3351303040981293\n",
      "epoch: 0 | 111008 / 114272 | training loss: 0.26160749793052673\n",
      "epoch: 0 | 111040 / 114272 | training loss: 0.27879324555397034\n",
      "epoch: 0 | 111072 / 114272 | training loss: 0.1485486626625061\n",
      "epoch: 0 | 111104 / 114272 | training loss: 0.021332673728466034\n",
      "epoch: 0 | 111136 / 114272 | training loss: 0.06576136499643326\n",
      "epoch: 0 | 111168 / 114272 | training loss: 0.08015422523021698\n",
      "epoch: 0 | 111200 / 114272 | training loss: 0.0861721858382225\n",
      "epoch: 0 | 111232 / 114272 | training loss: 0.07470858097076416\n",
      "epoch: 0 | 111264 / 114272 | training loss: 0.11392468959093094\n",
      "epoch: 0 | 111296 / 114272 | training loss: 0.15984615683555603\n",
      "epoch: 0 | 111328 / 114272 | training loss: 0.15530912578105927\n",
      "epoch: 0 | 111360 / 114272 | training loss: 0.10958487540483475\n",
      "epoch: 0 | 111392 / 114272 | training loss: 0.07831761986017227\n",
      "epoch: 0 | 111424 / 114272 | training loss: 0.1733333319425583\n",
      "epoch: 0 | 111456 / 114272 | training loss: 0.1883714348077774\n",
      "epoch: 0 | 111488 / 114272 | training loss: 0.18885555863380432\n",
      "epoch: 0 | 111520 / 114272 | training loss: 0.24999062716960907\n",
      "epoch: 0 | 111552 / 114272 | training loss: 0.057143617421388626\n",
      "epoch: 0 | 111584 / 114272 | training loss: 0.09304123371839523\n",
      "epoch: 0 | 111616 / 114272 | training loss: 0.11659213155508041\n",
      "epoch: 0 | 111648 / 114272 | training loss: 0.29764652252197266\n",
      "epoch: 0 | 111680 / 114272 | training loss: 0.1305057853460312\n",
      "epoch: 0 | 111712 / 114272 | training loss: 0.16367021203041077\n",
      "epoch: 0 | 111744 / 114272 | training loss: 0.18996009230613708\n",
      "epoch: 0 | 111776 / 114272 | training loss: 0.007637436036020517\n",
      "epoch: 0 | 111808 / 114272 | training loss: 0.08827084302902222\n",
      "epoch: 0 | 111840 / 114272 | training loss: 0.036213651299476624\n",
      "epoch: 0 | 111872 / 114272 | training loss: 0.02823612466454506\n",
      "epoch: 0 | 111904 / 114272 | training loss: 0.11069146543741226\n",
      "epoch: 0 | 111936 / 114272 | training loss: 0.25031161308288574\n",
      "epoch: 0 | 111968 / 114272 | training loss: 0.08284047245979309\n",
      "epoch: 0 | 112000 / 114272 | training loss: 0.17099051177501678\n",
      "epoch: 0 | 112032 / 114272 | training loss: 0.1915234923362732\n",
      "epoch: 0 | 112064 / 114272 | training loss: 0.07071898132562637\n",
      "epoch: 0 | 112096 / 114272 | training loss: 0.1348629742860794\n",
      "epoch: 0 | 112128 / 114272 | training loss: 0.07073269784450531\n",
      "epoch: 0 | 112160 / 114272 | training loss: 0.043330688029527664\n",
      "epoch: 0 | 112192 / 114272 | training loss: 0.26377400755882263\n",
      "epoch: 0 | 112224 / 114272 | training loss: 0.2381283938884735\n",
      "epoch: 0 | 112256 / 114272 | training loss: 0.04909205064177513\n",
      "epoch: 0 | 112288 / 114272 | training loss: 0.20139934122562408\n",
      "epoch: 0 | 112320 / 114272 | training loss: 0.40203455090522766\n",
      "epoch: 0 | 112352 / 114272 | training loss: 0.15130271017551422\n",
      "epoch: 0 | 112384 / 114272 | training loss: 0.14779047667980194\n",
      "epoch: 0 | 112416 / 114272 | training loss: 0.04802582785487175\n",
      "epoch: 0 | 112448 / 114272 | training loss: 0.020751049742102623\n",
      "epoch: 0 | 112480 / 114272 | training loss: 0.3838194012641907\n",
      "epoch: 0 | 112512 / 114272 | training loss: 0.3202449679374695\n",
      "epoch: 0 | 112544 / 114272 | training loss: 0.20637916028499603\n",
      "epoch: 0 | 112576 / 114272 | training loss: 0.1092275008559227\n",
      "epoch: 0 | 112608 / 114272 | training loss: 0.27261531352996826\n",
      "epoch: 0 | 112640 / 114272 | training loss: 0.25978773832321167\n",
      "epoch: 0 | 112672 / 114272 | training loss: 0.18004001677036285\n",
      "epoch: 0 | 112704 / 114272 | training loss: 0.14866700768470764\n",
      "epoch: 0 | 112736 / 114272 | training loss: 0.022810988128185272\n",
      "epoch: 0 | 112768 / 114272 | training loss: 0.38577964901924133\n",
      "epoch: 0 | 112800 / 114272 | training loss: 0.01881662756204605\n",
      "epoch: 0 | 112832 / 114272 | training loss: 0.13715118169784546\n",
      "epoch: 0 | 112864 / 114272 | training loss: 0.1932726949453354\n",
      "epoch: 0 | 112896 / 114272 | training loss: 0.30099648237228394\n",
      "epoch: 0 | 112928 / 114272 | training loss: 0.40441635251045227\n",
      "epoch: 0 | 112960 / 114272 | training loss: 0.188152015209198\n",
      "epoch: 0 | 112992 / 114272 | training loss: 0.2535082995891571\n",
      "epoch: 0 | 113024 / 114272 | training loss: 0.23228003084659576\n",
      "epoch: 0 | 113056 / 114272 | training loss: 0.03995378687977791\n",
      "epoch: 0 | 113088 / 114272 | training loss: 0.38419780135154724\n",
      "epoch: 0 | 113120 / 114272 | training loss: 0.11479315906763077\n",
      "epoch: 0 | 113152 / 114272 | training loss: 0.12641188502311707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | 113184 / 114272 | training loss: 0.34806206822395325\n",
      "epoch: 0 | 113216 / 114272 | training loss: 0.09305056929588318\n",
      "epoch: 0 | 113248 / 114272 | training loss: 0.07456352561712265\n",
      "epoch: 0 | 113280 / 114272 | training loss: 0.1278204619884491\n",
      "epoch: 0 | 113312 / 114272 | training loss: 0.1535632461309433\n",
      "epoch: 0 | 113344 / 114272 | training loss: 0.06908245384693146\n",
      "epoch: 0 | 113376 / 114272 | training loss: 0.24564394354820251\n",
      "epoch: 0 | 113408 / 114272 | training loss: 0.07905955612659454\n",
      "epoch: 0 | 113440 / 114272 | training loss: 0.03807804733514786\n",
      "epoch: 0 | 113472 / 114272 | training loss: 0.07419431209564209\n",
      "epoch: 0 | 113504 / 114272 | training loss: 0.0723225325345993\n",
      "epoch: 0 | 113536 / 114272 | training loss: 0.27781522274017334\n",
      "epoch: 0 | 113568 / 114272 | training loss: 0.1302415430545807\n",
      "epoch: 0 | 113600 / 114272 | training loss: 0.10937529057264328\n",
      "epoch: 0 | 113632 / 114272 | training loss: 0.09236788004636765\n",
      "epoch: 0 | 113664 / 114272 | training loss: 0.1321410834789276\n",
      "epoch: 0 | 113696 / 114272 | training loss: 0.029999541118741035\n",
      "epoch: 0 | 113728 / 114272 | training loss: 0.4790975749492645\n",
      "epoch: 0 | 113760 / 114272 | training loss: 0.43021056056022644\n",
      "epoch: 0 | 113792 / 114272 | training loss: 0.11471457034349442\n",
      "epoch: 0 | 113824 / 114272 | training loss: 0.18976597487926483\n",
      "epoch: 0 | 113856 / 114272 | training loss: 0.34578466415405273\n",
      "epoch: 0 | 113888 / 114272 | training loss: 0.1906270682811737\n",
      "epoch: 0 | 113920 / 114272 | training loss: 0.2918682396411896\n",
      "epoch: 0 | 113952 / 114272 | training loss: 0.13117413222789764\n",
      "epoch: 0 | 113984 / 114272 | training loss: 0.17827630043029785\n",
      "epoch: 0 | 114016 / 114272 | training loss: 0.16607461869716644\n",
      "epoch: 0 | 114048 / 114272 | training loss: 0.4265001118183136\n",
      "epoch: 0 | 114080 / 114272 | training loss: 0.11280938982963562\n",
      "epoch: 0 | 114112 / 114272 | training loss: 0.042679399251937866\n",
      "epoch: 0 | 114144 / 114272 | training loss: 0.1598217785358429\n",
      "epoch: 0 | 114176 / 114272 | training loss: 0.30763545632362366\n",
      "epoch: 0 | 114208 / 114272 | training loss: 0.26664817333221436\n",
      "epoch: 0 | 114240 / 114272 | training loss: 0.14318257570266724\n",
      "Training epoch 0 done! Average loss: 0.18240529539893444. Accuracy: 0.9393552226267153\n",
      "Validation epoch 0 done! Average loss: 0.13376423311096844. Accurage: 0.9537192393736018\n",
      "Epoch 2 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 1 | 0 / 114272 | training loss: 0.03283475339412689\n",
      "epoch: 1 | 32 / 114272 | training loss: 0.013421779498457909\n",
      "epoch: 1 | 64 / 114272 | training loss: 0.016155866906046867\n",
      "epoch: 1 | 96 / 114272 | training loss: 0.06231256201863289\n",
      "epoch: 1 | 128 / 114272 | training loss: 0.07321666926145554\n",
      "epoch: 1 | 160 / 114272 | training loss: 0.2081541270017624\n",
      "epoch: 1 | 192 / 114272 | training loss: 0.08618171513080597\n",
      "epoch: 1 | 224 / 114272 | training loss: 0.1276421993970871\n",
      "epoch: 1 | 256 / 114272 | training loss: 0.0855373814702034\n",
      "epoch: 1 | 288 / 114272 | training loss: 0.09700731933116913\n",
      "epoch: 1 | 320 / 114272 | training loss: 0.18691135942935944\n",
      "epoch: 1 | 352 / 114272 | training loss: 0.03470168635249138\n",
      "epoch: 1 | 384 / 114272 | training loss: 0.1069767102599144\n",
      "epoch: 1 | 416 / 114272 | training loss: 0.1472659409046173\n",
      "epoch: 1 | 448 / 114272 | training loss: 0.2348923236131668\n",
      "epoch: 1 | 480 / 114272 | training loss: 0.02985791675746441\n",
      "epoch: 1 | 512 / 114272 | training loss: 0.13007217645645142\n",
      "epoch: 1 | 544 / 114272 | training loss: 0.16689811646938324\n",
      "epoch: 1 | 576 / 114272 | training loss: 0.10166169703006744\n",
      "epoch: 1 | 608 / 114272 | training loss: 0.07574044913053513\n",
      "epoch: 1 | 640 / 114272 | training loss: 0.08307385444641113\n",
      "epoch: 1 | 672 / 114272 | training loss: 0.1558387130498886\n",
      "epoch: 1 | 704 / 114272 | training loss: 0.027006881311535835\n",
      "epoch: 1 | 736 / 114272 | training loss: 0.0863209068775177\n",
      "epoch: 1 | 768 / 114272 | training loss: 0.12714241445064545\n",
      "epoch: 1 | 800 / 114272 | training loss: 0.20617523789405823\n",
      "epoch: 1 | 832 / 114272 | training loss: 0.02848484180867672\n",
      "epoch: 1 | 864 / 114272 | training loss: 0.07026766240596771\n",
      "epoch: 1 | 896 / 114272 | training loss: 0.06990628689527512\n",
      "epoch: 1 | 928 / 114272 | training loss: 0.14555825293064117\n",
      "epoch: 1 | 960 / 114272 | training loss: 0.03074674867093563\n",
      "epoch: 1 | 992 / 114272 | training loss: 0.026649942621588707\n",
      "epoch: 1 | 1024 / 114272 | training loss: 0.023266172036528587\n",
      "epoch: 1 | 1056 / 114272 | training loss: 0.2195880264043808\n",
      "epoch: 1 | 1088 / 114272 | training loss: 0.22675667703151703\n",
      "epoch: 1 | 1120 / 114272 | training loss: 0.009982232004404068\n",
      "epoch: 1 | 1152 / 114272 | training loss: 0.18404319882392883\n",
      "epoch: 1 | 1184 / 114272 | training loss: 0.27379101514816284\n",
      "epoch: 1 | 1216 / 114272 | training loss: 0.10538329184055328\n",
      "epoch: 1 | 1248 / 114272 | training loss: 0.20531925559043884\n",
      "epoch: 1 | 1280 / 114272 | training loss: 0.07980184257030487\n",
      "epoch: 1 | 1312 / 114272 | training loss: 0.04890735074877739\n",
      "epoch: 1 | 1344 / 114272 | training loss: 0.1499847024679184\n",
      "epoch: 1 | 1376 / 114272 | training loss: 0.15356378257274628\n",
      "epoch: 1 | 1408 / 114272 | training loss: 0.15179619193077087\n",
      "epoch: 1 | 1440 / 114272 | training loss: 0.14341263473033905\n",
      "epoch: 1 | 1472 / 114272 | training loss: 0.20770111680030823\n",
      "epoch: 1 | 1504 / 114272 | training loss: 0.21755167841911316\n",
      "epoch: 1 | 1536 / 114272 | training loss: 0.023948723450303078\n",
      "epoch: 1 | 1568 / 114272 | training loss: 0.16557642817497253\n",
      "epoch: 1 | 1600 / 114272 | training loss: 0.009564489126205444\n",
      "epoch: 1 | 1632 / 114272 | training loss: 0.025151927024126053\n",
      "epoch: 1 | 1664 / 114272 | training loss: 0.10155010223388672\n",
      "epoch: 1 | 1696 / 114272 | training loss: 0.2742794454097748\n",
      "epoch: 1 | 1728 / 114272 | training loss: 0.199031263589859\n",
      "epoch: 1 | 1760 / 114272 | training loss: 0.1936895102262497\n",
      "epoch: 1 | 1792 / 114272 | training loss: 0.05518036335706711\n",
      "epoch: 1 | 1824 / 114272 | training loss: 0.1245211809873581\n",
      "epoch: 1 | 1856 / 114272 | training loss: 0.07541109621524811\n",
      "epoch: 1 | 1888 / 114272 | training loss: 0.3515254855155945\n",
      "epoch: 1 | 1920 / 114272 | training loss: 0.02184813655912876\n",
      "epoch: 1 | 1952 / 114272 | training loss: 0.027320148423314095\n",
      "epoch: 1 | 1984 / 114272 | training loss: 0.11488029360771179\n",
      "epoch: 1 | 2016 / 114272 | training loss: 0.03173985704779625\n",
      "epoch: 1 | 2048 / 114272 | training loss: 0.2352709323167801\n",
      "epoch: 1 | 2080 / 114272 | training loss: 0.1596742868423462\n",
      "epoch: 1 | 2112 / 114272 | training loss: 0.09759779274463654\n",
      "epoch: 1 | 2144 / 114272 | training loss: 0.010259176604449749\n",
      "epoch: 1 | 2176 / 114272 | training loss: 0.25329604744911194\n",
      "epoch: 1 | 2208 / 114272 | training loss: 0.11303523927927017\n",
      "epoch: 1 | 2240 / 114272 | training loss: 0.20952291786670685\n",
      "epoch: 1 | 2272 / 114272 | training loss: 0.026698648929595947\n",
      "epoch: 1 | 2304 / 114272 | training loss: 0.08770114928483963\n",
      "epoch: 1 | 2336 / 114272 | training loss: 0.2290027141571045\n",
      "epoch: 1 | 2368 / 114272 | training loss: 0.06476595252752304\n",
      "epoch: 1 | 2400 / 114272 | training loss: 0.15495027601718903\n",
      "epoch: 1 | 2432 / 114272 | training loss: 0.0564335398375988\n",
      "epoch: 1 | 2464 / 114272 | training loss: 0.09776514023542404\n",
      "epoch: 1 | 2496 / 114272 | training loss: 0.04515129327774048\n",
      "epoch: 1 | 2528 / 114272 | training loss: 0.04898878186941147\n",
      "epoch: 1 | 2560 / 114272 | training loss: 0.20045556128025055\n",
      "epoch: 1 | 2592 / 114272 | training loss: 0.05930302292108536\n",
      "epoch: 1 | 2624 / 114272 | training loss: 0.1891394555568695\n",
      "epoch: 1 | 2656 / 114272 | training loss: 0.08536247164011002\n",
      "epoch: 1 | 2688 / 114272 | training loss: 0.48895928263664246\n",
      "epoch: 1 | 2720 / 114272 | training loss: 0.08570483326911926\n",
      "epoch: 1 | 2752 / 114272 | training loss: 0.01508396491408348\n",
      "epoch: 1 | 2784 / 114272 | training loss: 0.14563360810279846\n",
      "epoch: 1 | 2816 / 114272 | training loss: 0.026797376573085785\n",
      "epoch: 1 | 2848 / 114272 | training loss: 0.14607316255569458\n",
      "epoch: 1 | 2880 / 114272 | training loss: 0.07124173641204834\n",
      "epoch: 1 | 2912 / 114272 | training loss: 0.10594524443149567\n",
      "epoch: 1 | 2944 / 114272 | training loss: 0.06586010009050369\n",
      "epoch: 1 | 2976 / 114272 | training loss: 0.05433354899287224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 3008 / 114272 | training loss: 0.01034480519592762\n",
      "epoch: 1 | 3040 / 114272 | training loss: 0.34061285853385925\n",
      "epoch: 1 | 3072 / 114272 | training loss: 0.1834719032049179\n",
      "epoch: 1 | 3104 / 114272 | training loss: 0.17961865663528442\n",
      "epoch: 1 | 3136 / 114272 | training loss: 0.017480269074440002\n",
      "epoch: 1 | 3168 / 114272 | training loss: 0.13460326194763184\n",
      "epoch: 1 | 3200 / 114272 | training loss: 0.015968596562743187\n",
      "epoch: 1 | 3232 / 114272 | training loss: 0.012608258984982967\n",
      "epoch: 1 | 3264 / 114272 | training loss: 0.18702971935272217\n",
      "epoch: 1 | 3296 / 114272 | training loss: 0.02105790562927723\n",
      "epoch: 1 | 3328 / 114272 | training loss: 0.11007644236087799\n",
      "epoch: 1 | 3360 / 114272 | training loss: 0.07806462794542313\n",
      "epoch: 1 | 3392 / 114272 | training loss: 0.13879729807376862\n",
      "epoch: 1 | 3424 / 114272 | training loss: 0.20100639760494232\n",
      "epoch: 1 | 3456 / 114272 | training loss: 0.31242379546165466\n",
      "epoch: 1 | 3488 / 114272 | training loss: 0.016604896634817123\n",
      "epoch: 1 | 3520 / 114272 | training loss: 0.14411869645118713\n",
      "epoch: 1 | 3552 / 114272 | training loss: 0.006618850864470005\n",
      "epoch: 1 | 3584 / 114272 | training loss: 0.013310672715306282\n",
      "epoch: 1 | 3616 / 114272 | training loss: 0.17473334074020386\n",
      "epoch: 1 | 3648 / 114272 | training loss: 0.09353361278772354\n",
      "epoch: 1 | 3680 / 114272 | training loss: 0.1434130221605301\n",
      "epoch: 1 | 3712 / 114272 | training loss: 0.040528230369091034\n",
      "epoch: 1 | 3744 / 114272 | training loss: 0.0837993323802948\n",
      "epoch: 1 | 3776 / 114272 | training loss: 0.10192769765853882\n",
      "epoch: 1 | 3808 / 114272 | training loss: 0.13602298498153687\n",
      "epoch: 1 | 3840 / 114272 | training loss: 0.158741295337677\n",
      "epoch: 1 | 3872 / 114272 | training loss: 0.01274042297154665\n",
      "epoch: 1 | 3904 / 114272 | training loss: 0.23811478912830353\n",
      "epoch: 1 | 3936 / 114272 | training loss: 0.1927744448184967\n",
      "epoch: 1 | 3968 / 114272 | training loss: 0.06923254579305649\n",
      "epoch: 1 | 4000 / 114272 | training loss: 0.284919798374176\n",
      "epoch: 1 | 4032 / 114272 | training loss: 0.127542182803154\n",
      "epoch: 1 | 4064 / 114272 | training loss: 0.09458908438682556\n",
      "epoch: 1 | 4096 / 114272 | training loss: 0.010347194038331509\n",
      "epoch: 1 | 4128 / 114272 | training loss: 0.13088691234588623\n",
      "epoch: 1 | 4160 / 114272 | training loss: 0.08644440025091171\n",
      "epoch: 1 | 4192 / 114272 | training loss: 0.06924320012331009\n",
      "epoch: 1 | 4224 / 114272 | training loss: 0.018198026344180107\n",
      "epoch: 1 | 4256 / 114272 | training loss: 0.041906725615262985\n",
      "epoch: 1 | 4288 / 114272 | training loss: 0.02922501042485237\n",
      "epoch: 1 | 4320 / 114272 | training loss: 0.09066446870565414\n",
      "epoch: 1 | 4352 / 114272 | training loss: 0.053105130791664124\n",
      "epoch: 1 | 4384 / 114272 | training loss: 0.11387855559587479\n",
      "epoch: 1 | 4416 / 114272 | training loss: 0.09440440684556961\n",
      "epoch: 1 | 4448 / 114272 | training loss: 0.143222838640213\n",
      "epoch: 1 | 4480 / 114272 | training loss: 0.16688011586666107\n",
      "epoch: 1 | 4512 / 114272 | training loss: 0.23428234457969666\n",
      "epoch: 1 | 4544 / 114272 | training loss: 0.1853584498167038\n",
      "epoch: 1 | 4576 / 114272 | training loss: 0.17469938099384308\n",
      "epoch: 1 | 4608 / 114272 | training loss: 0.1766592115163803\n",
      "epoch: 1 | 4640 / 114272 | training loss: 0.09220416843891144\n",
      "epoch: 1 | 4672 / 114272 | training loss: 0.13730508089065552\n",
      "epoch: 1 | 4704 / 114272 | training loss: 0.13586243987083435\n",
      "epoch: 1 | 4736 / 114272 | training loss: 0.036211930215358734\n",
      "epoch: 1 | 4768 / 114272 | training loss: 0.05652536824345589\n",
      "epoch: 1 | 4800 / 114272 | training loss: 0.07680787146091461\n",
      "epoch: 1 | 4832 / 114272 | training loss: 0.06293343752622604\n",
      "epoch: 1 | 4864 / 114272 | training loss: 0.2306874543428421\n",
      "epoch: 1 | 4896 / 114272 | training loss: 0.1197054460644722\n",
      "epoch: 1 | 4928 / 114272 | training loss: 0.052763115614652634\n",
      "epoch: 1 | 4960 / 114272 | training loss: 0.06656985729932785\n",
      "epoch: 1 | 4992 / 114272 | training loss: 0.12534722685813904\n",
      "epoch: 1 | 5024 / 114272 | training loss: 0.06914780288934708\n",
      "epoch: 1 | 5056 / 114272 | training loss: 0.1355249285697937\n",
      "epoch: 1 | 5088 / 114272 | training loss: 0.12970034778118134\n",
      "epoch: 1 | 5120 / 114272 | training loss: 0.027601802721619606\n",
      "epoch: 1 | 5152 / 114272 | training loss: 0.2140294313430786\n",
      "epoch: 1 | 5184 / 114272 | training loss: 0.19430223107337952\n",
      "epoch: 1 | 5216 / 114272 | training loss: 0.016467589884996414\n",
      "epoch: 1 | 5248 / 114272 | training loss: 0.07499532401561737\n",
      "epoch: 1 | 5280 / 114272 | training loss: 0.032958224415779114\n",
      "epoch: 1 | 5312 / 114272 | training loss: 0.054438523948192596\n",
      "epoch: 1 | 5344 / 114272 | training loss: 0.17563451826572418\n",
      "epoch: 1 | 5376 / 114272 | training loss: 0.015465624630451202\n",
      "epoch: 1 | 5408 / 114272 | training loss: 0.14972327649593353\n",
      "epoch: 1 | 5440 / 114272 | training loss: 0.5066477060317993\n",
      "epoch: 1 | 5472 / 114272 | training loss: 0.09294148534536362\n",
      "epoch: 1 | 5504 / 114272 | training loss: 0.15025246143341064\n",
      "epoch: 1 | 5536 / 114272 | training loss: 0.1144980788230896\n",
      "epoch: 1 | 5568 / 114272 | training loss: 0.009972869418561459\n",
      "epoch: 1 | 5600 / 114272 | training loss: 0.0736960619688034\n",
      "epoch: 1 | 5632 / 114272 | training loss: 0.03434167057275772\n",
      "epoch: 1 | 5664 / 114272 | training loss: 0.04099949821829796\n",
      "epoch: 1 | 5696 / 114272 | training loss: 0.016391387209296227\n",
      "epoch: 1 | 5728 / 114272 | training loss: 0.013633477501571178\n",
      "epoch: 1 | 5760 / 114272 | training loss: 0.03895024210214615\n",
      "epoch: 1 | 5792 / 114272 | training loss: 0.28305357694625854\n",
      "epoch: 1 | 5824 / 114272 | training loss: 0.06680171191692352\n",
      "epoch: 1 | 5856 / 114272 | training loss: 0.20385590195655823\n",
      "epoch: 1 | 5888 / 114272 | training loss: 0.08057425171136856\n",
      "epoch: 1 | 5920 / 114272 | training loss: 0.054458364844322205\n",
      "epoch: 1 | 5952 / 114272 | training loss: 0.04366889223456383\n",
      "epoch: 1 | 5984 / 114272 | training loss: 0.1087554395198822\n",
      "epoch: 1 | 6016 / 114272 | training loss: 0.04299132153391838\n",
      "epoch: 1 | 6048 / 114272 | training loss: 0.09399723261594772\n",
      "epoch: 1 | 6080 / 114272 | training loss: 0.006408593151718378\n",
      "epoch: 1 | 6112 / 114272 | training loss: 0.02476465329527855\n",
      "epoch: 1 | 6144 / 114272 | training loss: 0.12179657071828842\n",
      "epoch: 1 | 6176 / 114272 | training loss: 0.007909693755209446\n",
      "epoch: 1 | 6208 / 114272 | training loss: 0.08356865495443344\n",
      "epoch: 1 | 6240 / 114272 | training loss: 0.2292105257511139\n",
      "epoch: 1 | 6272 / 114272 | training loss: 0.08507147431373596\n",
      "epoch: 1 | 6304 / 114272 | training loss: 0.2520330548286438\n",
      "epoch: 1 | 6336 / 114272 | training loss: 0.03199922665953636\n",
      "epoch: 1 | 6368 / 114272 | training loss: 0.2409753054380417\n",
      "epoch: 1 | 6400 / 114272 | training loss: 0.005043444689363241\n",
      "epoch: 1 | 6432 / 114272 | training loss: 0.013294163160026073\n",
      "epoch: 1 | 6464 / 114272 | training loss: 0.054002873599529266\n",
      "epoch: 1 | 6496 / 114272 | training loss: 0.0696636438369751\n",
      "epoch: 1 | 6528 / 114272 | training loss: 0.16722673177719116\n",
      "epoch: 1 | 6560 / 114272 | training loss: 0.016373366117477417\n",
      "epoch: 1 | 6592 / 114272 | training loss: 0.13480912148952484\n",
      "epoch: 1 | 6624 / 114272 | training loss: 0.026971517130732536\n",
      "epoch: 1 | 6656 / 114272 | training loss: 0.11270800232887268\n",
      "epoch: 1 | 6688 / 114272 | training loss: 0.010041305795311928\n",
      "epoch: 1 | 6720 / 114272 | training loss: 0.07406338304281235\n",
      "epoch: 1 | 6752 / 114272 | training loss: 0.12549281120300293\n",
      "epoch: 1 | 6784 / 114272 | training loss: 0.027722183614969254\n",
      "epoch: 1 | 6816 / 114272 | training loss: 0.10957110673189163\n",
      "epoch: 1 | 6848 / 114272 | training loss: 0.19958731532096863\n",
      "epoch: 1 | 6880 / 114272 | training loss: 0.02680450864136219\n",
      "epoch: 1 | 6912 / 114272 | training loss: 0.008823555894196033\n",
      "epoch: 1 | 6944 / 114272 | training loss: 0.10051887482404709\n",
      "epoch: 1 | 6976 / 114272 | training loss: 0.019405506551265717\n",
      "epoch: 1 | 7008 / 114272 | training loss: 0.3156246542930603\n",
      "epoch: 1 | 7040 / 114272 | training loss: 0.27520638704299927\n",
      "epoch: 1 | 7072 / 114272 | training loss: 0.009117942303419113\n",
      "epoch: 1 | 7104 / 114272 | training loss: 0.10707514733076096\n",
      "epoch: 1 | 7136 / 114272 | training loss: 0.2728874683380127\n",
      "epoch: 1 | 7168 / 114272 | training loss: 0.0843421220779419\n",
      "epoch: 1 | 7200 / 114272 | training loss: 0.018014393746852875\n",
      "epoch: 1 | 7232 / 114272 | training loss: 0.094676673412323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 7264 / 114272 | training loss: 0.3754611015319824\n",
      "epoch: 1 | 7296 / 114272 | training loss: 0.0369056798517704\n",
      "epoch: 1 | 7328 / 114272 | training loss: 0.2251330316066742\n",
      "epoch: 1 | 7360 / 114272 | training loss: 0.1074279397726059\n",
      "epoch: 1 | 7392 / 114272 | training loss: 0.249886617064476\n",
      "epoch: 1 | 7424 / 114272 | training loss: 0.051241979002952576\n",
      "epoch: 1 | 7456 / 114272 | training loss: 0.0501713901758194\n",
      "epoch: 1 | 7488 / 114272 | training loss: 0.2504437565803528\n",
      "epoch: 1 | 7520 / 114272 | training loss: 0.05376656726002693\n",
      "epoch: 1 | 7552 / 114272 | training loss: 0.16720888018608093\n",
      "epoch: 1 | 7584 / 114272 | training loss: 0.07981041073799133\n",
      "epoch: 1 | 7616 / 114272 | training loss: 0.019762586802244186\n",
      "epoch: 1 | 7648 / 114272 | training loss: 0.002075701951980591\n",
      "epoch: 1 | 7680 / 114272 | training loss: 0.22625017166137695\n",
      "epoch: 1 | 7712 / 114272 | training loss: 0.06220003962516785\n",
      "epoch: 1 | 7744 / 114272 | training loss: 0.015482449904084206\n",
      "epoch: 1 | 7776 / 114272 | training loss: 0.3796265721321106\n",
      "epoch: 1 | 7808 / 114272 | training loss: 0.048603858798742294\n",
      "epoch: 1 | 7840 / 114272 | training loss: 0.1451193243265152\n",
      "epoch: 1 | 7872 / 114272 | training loss: 0.03424209728837013\n",
      "epoch: 1 | 7904 / 114272 | training loss: 0.08474969118833542\n",
      "epoch: 1 | 7936 / 114272 | training loss: 0.2964276373386383\n",
      "epoch: 1 | 7968 / 114272 | training loss: 0.10485130548477173\n",
      "epoch: 1 | 8000 / 114272 | training loss: 0.1711646020412445\n",
      "epoch: 1 | 8032 / 114272 | training loss: 0.2546722888946533\n",
      "epoch: 1 | 8064 / 114272 | training loss: 0.09829845279455185\n",
      "epoch: 1 | 8096 / 114272 | training loss: 0.15078359842300415\n",
      "epoch: 1 | 8128 / 114272 | training loss: 0.11207817494869232\n",
      "epoch: 1 | 8160 / 114272 | training loss: 0.07549940049648285\n",
      "epoch: 1 | 8192 / 114272 | training loss: 0.0256466306746006\n",
      "epoch: 1 | 8224 / 114272 | training loss: 0.02965708076953888\n",
      "epoch: 1 | 8256 / 114272 | training loss: 0.02000122144818306\n",
      "epoch: 1 | 8288 / 114272 | training loss: 0.06507386267185211\n",
      "epoch: 1 | 8320 / 114272 | training loss: 0.032271839678287506\n",
      "epoch: 1 | 8352 / 114272 | training loss: 0.006176498252898455\n",
      "epoch: 1 | 8384 / 114272 | training loss: 0.2369973510503769\n",
      "epoch: 1 | 8416 / 114272 | training loss: 0.025308525189757347\n",
      "epoch: 1 | 8448 / 114272 | training loss: 0.01776118576526642\n",
      "epoch: 1 | 8480 / 114272 | training loss: 0.006015651859343052\n",
      "epoch: 1 | 8512 / 114272 | training loss: 0.27856191992759705\n",
      "epoch: 1 | 8544 / 114272 | training loss: 0.1275242567062378\n",
      "epoch: 1 | 8576 / 114272 | training loss: 0.007385459262877703\n",
      "epoch: 1 | 8608 / 114272 | training loss: 0.012360665947198868\n",
      "epoch: 1 | 8640 / 114272 | training loss: 0.03281235694885254\n",
      "epoch: 1 | 8672 / 114272 | training loss: 0.025815049186348915\n",
      "epoch: 1 | 8704 / 114272 | training loss: 0.04081825539469719\n",
      "epoch: 1 | 8736 / 114272 | training loss: 0.03547310829162598\n",
      "epoch: 1 | 8768 / 114272 | training loss: 0.12338178604841232\n",
      "epoch: 1 | 8800 / 114272 | training loss: 0.2586347162723541\n",
      "epoch: 1 | 8832 / 114272 | training loss: 0.025543345138430595\n",
      "epoch: 1 | 8864 / 114272 | training loss: 0.2363748699426651\n",
      "epoch: 1 | 8896 / 114272 | training loss: 0.046595193445682526\n",
      "epoch: 1 | 8928 / 114272 | training loss: 0.44025707244873047\n",
      "epoch: 1 | 8960 / 114272 | training loss: 0.04041457548737526\n",
      "epoch: 1 | 8992 / 114272 | training loss: 0.008595299907028675\n",
      "epoch: 1 | 9024 / 114272 | training loss: 0.2334514856338501\n",
      "epoch: 1 | 9056 / 114272 | training loss: 0.07277026027441025\n",
      "epoch: 1 | 9088 / 114272 | training loss: 0.1160782128572464\n",
      "epoch: 1 | 9120 / 114272 | training loss: 0.02338835410773754\n",
      "epoch: 1 | 9152 / 114272 | training loss: 0.08486075699329376\n",
      "epoch: 1 | 9184 / 114272 | training loss: 0.15154309570789337\n",
      "epoch: 1 | 9216 / 114272 | training loss: 0.0907844752073288\n",
      "epoch: 1 | 9248 / 114272 | training loss: 0.14696766436100006\n",
      "epoch: 1 | 9280 / 114272 | training loss: 0.031784310936927795\n",
      "epoch: 1 | 9312 / 114272 | training loss: 0.20851755142211914\n",
      "epoch: 1 | 9344 / 114272 | training loss: 0.10688520967960358\n",
      "epoch: 1 | 9376 / 114272 | training loss: 0.014201520942151546\n",
      "epoch: 1 | 9408 / 114272 | training loss: 0.0080268494784832\n",
      "epoch: 1 | 9440 / 114272 | training loss: 0.11444271355867386\n",
      "epoch: 1 | 9472 / 114272 | training loss: 0.1062191054224968\n",
      "epoch: 1 | 9504 / 114272 | training loss: 0.16661059856414795\n",
      "epoch: 1 | 9536 / 114272 | training loss: 0.29389142990112305\n",
      "epoch: 1 | 9568 / 114272 | training loss: 0.08810646086931229\n",
      "epoch: 1 | 9600 / 114272 | training loss: 0.008547166362404823\n",
      "epoch: 1 | 9632 / 114272 | training loss: 0.066911980509758\n",
      "epoch: 1 | 9664 / 114272 | training loss: 0.2656576633453369\n",
      "epoch: 1 | 9696 / 114272 | training loss: 0.27059125900268555\n",
      "epoch: 1 | 9728 / 114272 | training loss: 0.04516100883483887\n",
      "epoch: 1 | 9760 / 114272 | training loss: 0.2145402431488037\n",
      "epoch: 1 | 9792 / 114272 | training loss: 0.03579041734337807\n",
      "epoch: 1 | 9824 / 114272 | training loss: 0.10589637607336044\n",
      "epoch: 1 | 9856 / 114272 | training loss: 0.0705108493566513\n",
      "epoch: 1 | 9888 / 114272 | training loss: 0.2196139097213745\n",
      "epoch: 1 | 9920 / 114272 | training loss: 0.3770545423030853\n",
      "epoch: 1 | 9952 / 114272 | training loss: 0.162174791097641\n",
      "epoch: 1 | 9984 / 114272 | training loss: 0.21334007382392883\n",
      "epoch: 1 | 10016 / 114272 | training loss: 0.09099089354276657\n",
      "epoch: 1 | 10048 / 114272 | training loss: 0.0829056054353714\n",
      "epoch: 1 | 10080 / 114272 | training loss: 0.1833006739616394\n",
      "epoch: 1 | 10112 / 114272 | training loss: 0.15762807428836823\n",
      "epoch: 1 | 10144 / 114272 | training loss: 0.07961712777614594\n",
      "epoch: 1 | 10176 / 114272 | training loss: 0.08633001893758774\n",
      "epoch: 1 | 10208 / 114272 | training loss: 0.037858255207538605\n",
      "epoch: 1 | 10240 / 114272 | training loss: 0.04061959311366081\n",
      "epoch: 1 | 10272 / 114272 | training loss: 0.1886417418718338\n",
      "epoch: 1 | 10304 / 114272 | training loss: 0.1088876873254776\n",
      "epoch: 1 | 10336 / 114272 | training loss: 0.029798345640301704\n",
      "epoch: 1 | 10368 / 114272 | training loss: 0.04600507766008377\n",
      "epoch: 1 | 10400 / 114272 | training loss: 0.3104144334793091\n",
      "epoch: 1 | 10432 / 114272 | training loss: 0.04579739645123482\n",
      "epoch: 1 | 10464 / 114272 | training loss: 0.07462508976459503\n",
      "epoch: 1 | 10496 / 114272 | training loss: 0.14119532704353333\n",
      "epoch: 1 | 10528 / 114272 | training loss: 0.057668037712574005\n",
      "epoch: 1 | 10560 / 114272 | training loss: 0.09200024604797363\n",
      "epoch: 1 | 10592 / 114272 | training loss: 0.12477967888116837\n",
      "epoch: 1 | 10624 / 114272 | training loss: 0.036420788615942\n",
      "epoch: 1 | 10656 / 114272 | training loss: 0.052799783647060394\n",
      "epoch: 1 | 10688 / 114272 | training loss: 0.022777680307626724\n",
      "epoch: 1 | 10720 / 114272 | training loss: 0.015779711306095123\n",
      "epoch: 1 | 10752 / 114272 | training loss: 0.09382640570402145\n",
      "epoch: 1 | 10784 / 114272 | training loss: 0.006331331096589565\n",
      "epoch: 1 | 10816 / 114272 | training loss: 0.02646953985095024\n",
      "epoch: 1 | 10848 / 114272 | training loss: 0.02869969606399536\n",
      "epoch: 1 | 10880 / 114272 | training loss: 0.04482972249388695\n",
      "epoch: 1 | 10912 / 114272 | training loss: 0.011974717490375042\n",
      "epoch: 1 | 10944 / 114272 | training loss: 0.05171789601445198\n",
      "epoch: 1 | 10976 / 114272 | training loss: 0.25530123710632324\n",
      "epoch: 1 | 11008 / 114272 | training loss: 0.027825891971588135\n",
      "epoch: 1 | 11040 / 114272 | training loss: 0.01561038289219141\n",
      "epoch: 1 | 11072 / 114272 | training loss: 0.43364080786705017\n",
      "epoch: 1 | 11104 / 114272 | training loss: 0.010153180919587612\n",
      "epoch: 1 | 11136 / 114272 | training loss: 0.007455634418874979\n",
      "epoch: 1 | 11168 / 114272 | training loss: 0.01741103082895279\n",
      "epoch: 1 | 11200 / 114272 | training loss: 0.3813037872314453\n",
      "epoch: 1 | 11232 / 114272 | training loss: 0.05976572260260582\n",
      "epoch: 1 | 11264 / 114272 | training loss: 0.38655588030815125\n",
      "epoch: 1 | 11296 / 114272 | training loss: 0.14173653721809387\n",
      "epoch: 1 | 11328 / 114272 | training loss: 0.22257663309574127\n",
      "epoch: 1 | 11360 / 114272 | training loss: 0.01891792006790638\n",
      "epoch: 1 | 11392 / 114272 | training loss: 0.020169442519545555\n",
      "epoch: 1 | 11424 / 114272 | training loss: 0.13910409808158875\n",
      "epoch: 1 | 11456 / 114272 | training loss: 0.09869591891765594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 11488 / 114272 | training loss: 0.009107498452067375\n",
      "epoch: 1 | 11520 / 114272 | training loss: 0.07542330026626587\n",
      "epoch: 1 | 11552 / 114272 | training loss: 0.11689013242721558\n",
      "epoch: 1 | 11584 / 114272 | training loss: 0.00639587314799428\n",
      "epoch: 1 | 11616 / 114272 | training loss: 0.09979456663131714\n",
      "epoch: 1 | 11648 / 114272 | training loss: 0.07118343561887741\n",
      "epoch: 1 | 11680 / 114272 | training loss: 0.055921971797943115\n",
      "epoch: 1 | 11712 / 114272 | training loss: 0.010589922778308392\n",
      "epoch: 1 | 11744 / 114272 | training loss: 0.09017714858055115\n",
      "epoch: 1 | 11776 / 114272 | training loss: 0.041411999613046646\n",
      "epoch: 1 | 11808 / 114272 | training loss: 0.17230303585529327\n",
      "epoch: 1 | 11840 / 114272 | training loss: 0.11677801609039307\n",
      "epoch: 1 | 11872 / 114272 | training loss: 0.03241125866770744\n",
      "epoch: 1 | 11904 / 114272 | training loss: 0.04823385179042816\n",
      "epoch: 1 | 11936 / 114272 | training loss: 0.11072345077991486\n",
      "epoch: 1 | 11968 / 114272 | training loss: 0.07491862028837204\n",
      "epoch: 1 | 12000 / 114272 | training loss: 0.0042365179397165775\n",
      "epoch: 1 | 12032 / 114272 | training loss: 0.09466575086116791\n",
      "epoch: 1 | 12064 / 114272 | training loss: 0.004765684250742197\n",
      "epoch: 1 | 12096 / 114272 | training loss: 0.1936071366071701\n",
      "epoch: 1 | 12128 / 114272 | training loss: 0.11058951914310455\n",
      "epoch: 1 | 12160 / 114272 | training loss: 0.052307792007923126\n",
      "epoch: 1 | 12192 / 114272 | training loss: 0.08934462070465088\n",
      "epoch: 1 | 12224 / 114272 | training loss: 0.09635744988918304\n",
      "epoch: 1 | 12256 / 114272 | training loss: 0.2130829244852066\n",
      "epoch: 1 | 12288 / 114272 | training loss: 0.17962868511676788\n",
      "epoch: 1 | 12320 / 114272 | training loss: 0.11973226070404053\n",
      "epoch: 1 | 12352 / 114272 | training loss: 0.15221215784549713\n",
      "epoch: 1 | 12384 / 114272 | training loss: 0.11571007966995239\n",
      "epoch: 1 | 12416 / 114272 | training loss: 0.32851964235305786\n",
      "epoch: 1 | 12448 / 114272 | training loss: 0.04825858771800995\n",
      "epoch: 1 | 12480 / 114272 | training loss: 0.11166450381278992\n",
      "epoch: 1 | 12512 / 114272 | training loss: 0.030316390097141266\n",
      "epoch: 1 | 12544 / 114272 | training loss: 0.0673995241522789\n",
      "epoch: 1 | 12576 / 114272 | training loss: 0.02055366337299347\n",
      "epoch: 1 | 12608 / 114272 | training loss: 0.020350001752376556\n",
      "epoch: 1 | 12640 / 114272 | training loss: 0.030763758346438408\n",
      "epoch: 1 | 12672 / 114272 | training loss: 0.02049386128783226\n",
      "epoch: 1 | 12704 / 114272 | training loss: 0.1506042182445526\n",
      "epoch: 1 | 12736 / 114272 | training loss: 0.08347688615322113\n",
      "epoch: 1 | 12768 / 114272 | training loss: 0.225746288895607\n",
      "epoch: 1 | 12800 / 114272 | training loss: 0.02256246842443943\n",
      "epoch: 1 | 12832 / 114272 | training loss: 0.13605256378650665\n",
      "epoch: 1 | 12864 / 114272 | training loss: 0.12022121250629425\n",
      "epoch: 1 | 12896 / 114272 | training loss: 0.013572152704000473\n",
      "epoch: 1 | 12928 / 114272 | training loss: 0.054558321833610535\n",
      "epoch: 1 | 12960 / 114272 | training loss: 0.12587274610996246\n",
      "epoch: 1 | 12992 / 114272 | training loss: 0.3290691673755646\n",
      "epoch: 1 | 13024 / 114272 | training loss: 0.019213685765862465\n",
      "epoch: 1 | 13056 / 114272 | training loss: 0.057583607733249664\n",
      "epoch: 1 | 13088 / 114272 | training loss: 0.2581350803375244\n",
      "epoch: 1 | 13120 / 114272 | training loss: 0.11324514448642731\n",
      "epoch: 1 | 13152 / 114272 | training loss: 0.032000504434108734\n",
      "epoch: 1 | 13184 / 114272 | training loss: 0.04629150778055191\n",
      "epoch: 1 | 13216 / 114272 | training loss: 0.1280432790517807\n",
      "epoch: 1 | 13248 / 114272 | training loss: 0.20634175837039948\n",
      "epoch: 1 | 13280 / 114272 | training loss: 0.01362795103341341\n",
      "epoch: 1 | 13312 / 114272 | training loss: 0.21024496853351593\n",
      "epoch: 1 | 13344 / 114272 | training loss: 0.09022336453199387\n",
      "epoch: 1 | 13376 / 114272 | training loss: 0.14205212891101837\n",
      "epoch: 1 | 13408 / 114272 | training loss: 0.33084341883659363\n",
      "epoch: 1 | 13440 / 114272 | training loss: 0.006109567359089851\n",
      "epoch: 1 | 13472 / 114272 | training loss: 0.04947767034173012\n",
      "epoch: 1 | 13504 / 114272 | training loss: 0.012182625010609627\n",
      "epoch: 1 | 13536 / 114272 | training loss: 0.08646995574235916\n",
      "epoch: 1 | 13568 / 114272 | training loss: 0.3312821090221405\n",
      "epoch: 1 | 13600 / 114272 | training loss: 0.050037682056427\n",
      "epoch: 1 | 13632 / 114272 | training loss: 0.061052922159433365\n",
      "epoch: 1 | 13664 / 114272 | training loss: 0.11185034364461899\n",
      "epoch: 1 | 13696 / 114272 | training loss: 0.05972428619861603\n",
      "epoch: 1 | 13728 / 114272 | training loss: 0.25747567415237427\n",
      "epoch: 1 | 13760 / 114272 | training loss: 0.11917891353368759\n",
      "epoch: 1 | 13792 / 114272 | training loss: 0.24988585710525513\n",
      "epoch: 1 | 13824 / 114272 | training loss: 0.2830761671066284\n",
      "epoch: 1 | 13856 / 114272 | training loss: 0.026838915422558784\n",
      "epoch: 1 | 13888 / 114272 | training loss: 0.028578460216522217\n",
      "epoch: 1 | 13920 / 114272 | training loss: 0.03541784733533859\n",
      "epoch: 1 | 13952 / 114272 | training loss: 0.13817858695983887\n",
      "epoch: 1 | 13984 / 114272 | training loss: 0.14223210513591766\n",
      "epoch: 1 | 14016 / 114272 | training loss: 0.14226289093494415\n",
      "epoch: 1 | 14048 / 114272 | training loss: 0.10863256454467773\n",
      "epoch: 1 | 14080 / 114272 | training loss: 0.12359297275543213\n",
      "epoch: 1 | 14112 / 114272 | training loss: 0.08656011521816254\n",
      "epoch: 1 | 14144 / 114272 | training loss: 0.29248470067977905\n",
      "epoch: 1 | 14176 / 114272 | training loss: 0.00256716413423419\n",
      "epoch: 1 | 14208 / 114272 | training loss: 0.1844119280576706\n",
      "epoch: 1 | 14240 / 114272 | training loss: 0.2704501748085022\n",
      "epoch: 1 | 14272 / 114272 | training loss: 0.009163865819573402\n",
      "epoch: 1 | 14304 / 114272 | training loss: 0.13770021498203278\n",
      "epoch: 1 | 14336 / 114272 | training loss: 0.10705961287021637\n",
      "epoch: 1 | 14368 / 114272 | training loss: 0.2672227919101715\n",
      "epoch: 1 | 14400 / 114272 | training loss: 0.0073364172130823135\n",
      "epoch: 1 | 14432 / 114272 | training loss: 0.23660928010940552\n",
      "epoch: 1 | 14464 / 114272 | training loss: 0.02693409100174904\n",
      "epoch: 1 | 14496 / 114272 | training loss: 0.025020048022270203\n",
      "epoch: 1 | 14528 / 114272 | training loss: 0.34529903531074524\n",
      "epoch: 1 | 14560 / 114272 | training loss: 0.237781822681427\n",
      "epoch: 1 | 14592 / 114272 | training loss: 0.03274589776992798\n",
      "epoch: 1 | 14624 / 114272 | training loss: 0.08754117041826248\n",
      "epoch: 1 | 14656 / 114272 | training loss: 0.24333958327770233\n",
      "epoch: 1 | 14688 / 114272 | training loss: 0.015926385298371315\n",
      "epoch: 1 | 14720 / 114272 | training loss: 0.017484137788414955\n",
      "epoch: 1 | 14752 / 114272 | training loss: 0.090837262570858\n",
      "epoch: 1 | 14784 / 114272 | training loss: 0.1506234109401703\n",
      "epoch: 1 | 14816 / 114272 | training loss: 0.240627259016037\n",
      "epoch: 1 | 14848 / 114272 | training loss: 0.4074440598487854\n",
      "epoch: 1 | 14880 / 114272 | training loss: 0.3729592263698578\n",
      "epoch: 1 | 14912 / 114272 | training loss: 0.32380953431129456\n",
      "epoch: 1 | 14944 / 114272 | training loss: 0.2776472866535187\n",
      "epoch: 1 | 14976 / 114272 | training loss: 0.13771352171897888\n",
      "epoch: 1 | 15008 / 114272 | training loss: 0.1189948245882988\n",
      "epoch: 1 | 15040 / 114272 | training loss: 0.023386215791106224\n",
      "epoch: 1 | 15072 / 114272 | training loss: 0.15748988091945648\n",
      "epoch: 1 | 15104 / 114272 | training loss: 0.12428303807973862\n",
      "epoch: 1 | 15136 / 114272 | training loss: 0.020600220188498497\n",
      "epoch: 1 | 15168 / 114272 | training loss: 0.03135804831981659\n",
      "epoch: 1 | 15200 / 114272 | training loss: 0.08287457376718521\n",
      "epoch: 1 | 15232 / 114272 | training loss: 0.02937868982553482\n",
      "epoch: 1 | 15264 / 114272 | training loss: 0.047408442944288254\n",
      "epoch: 1 | 15296 / 114272 | training loss: 0.37734177708625793\n",
      "epoch: 1 | 15328 / 114272 | training loss: 0.09681833535432816\n",
      "epoch: 1 | 15360 / 114272 | training loss: 0.21181617677211761\n",
      "epoch: 1 | 15392 / 114272 | training loss: 0.32048314809799194\n",
      "epoch: 1 | 15424 / 114272 | training loss: 0.1072937473654747\n",
      "epoch: 1 | 15456 / 114272 | training loss: 0.15966278314590454\n",
      "epoch: 1 | 15488 / 114272 | training loss: 0.11774235218763351\n",
      "epoch: 1 | 15520 / 114272 | training loss: 0.11328936368227005\n",
      "epoch: 1 | 15552 / 114272 | training loss: 0.045697711408138275\n",
      "epoch: 1 | 15584 / 114272 | training loss: 0.11836908012628555\n",
      "epoch: 1 | 15616 / 114272 | training loss: 0.025667723268270493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 15648 / 114272 | training loss: 0.035771965980529785\n",
      "epoch: 1 | 15680 / 114272 | training loss: 0.23529332876205444\n",
      "epoch: 1 | 15712 / 114272 | training loss: 0.07310401648283005\n",
      "epoch: 1 | 15744 / 114272 | training loss: 0.09904446452856064\n",
      "epoch: 1 | 15776 / 114272 | training loss: 0.04486716166138649\n",
      "epoch: 1 | 15808 / 114272 | training loss: 0.012813687324523926\n",
      "epoch: 1 | 15840 / 114272 | training loss: 0.13556762039661407\n",
      "epoch: 1 | 15872 / 114272 | training loss: 0.009124035947024822\n",
      "epoch: 1 | 15904 / 114272 | training loss: 0.08759485930204391\n",
      "epoch: 1 | 15936 / 114272 | training loss: 0.008316691033542156\n",
      "epoch: 1 | 15968 / 114272 | training loss: 0.07659661769866943\n",
      "epoch: 1 | 16000 / 114272 | training loss: 0.27834150195121765\n",
      "epoch: 1 | 16032 / 114272 | training loss: 0.13490355014801025\n",
      "epoch: 1 | 16064 / 114272 | training loss: 0.36341720819473267\n",
      "epoch: 1 | 16096 / 114272 | training loss: 0.21410466730594635\n",
      "epoch: 1 | 16128 / 114272 | training loss: 0.21411648392677307\n",
      "epoch: 1 | 16160 / 114272 | training loss: 0.012925969436764717\n",
      "epoch: 1 | 16192 / 114272 | training loss: 0.06071251258254051\n",
      "epoch: 1 | 16224 / 114272 | training loss: 0.019331298768520355\n",
      "epoch: 1 | 16256 / 114272 | training loss: 0.08338111639022827\n",
      "epoch: 1 | 16288 / 114272 | training loss: 0.11132624000310898\n",
      "epoch: 1 | 16320 / 114272 | training loss: 0.033378444612026215\n",
      "epoch: 1 | 16352 / 114272 | training loss: 0.11636731773614883\n",
      "epoch: 1 | 16384 / 114272 | training loss: 0.03412893787026405\n",
      "epoch: 1 | 16416 / 114272 | training loss: 0.08889922499656677\n",
      "epoch: 1 | 16448 / 114272 | training loss: 0.0072179995477199554\n",
      "epoch: 1 | 16480 / 114272 | training loss: 0.08707109093666077\n",
      "epoch: 1 | 16512 / 114272 | training loss: 0.12998057901859283\n",
      "epoch: 1 | 16544 / 114272 | training loss: 0.013327229768037796\n",
      "epoch: 1 | 16576 / 114272 | training loss: 0.24441178143024445\n",
      "epoch: 1 | 16608 / 114272 | training loss: 0.0373946912586689\n",
      "epoch: 1 | 16640 / 114272 | training loss: 0.040605101734399796\n",
      "epoch: 1 | 16672 / 114272 | training loss: 0.12454816699028015\n",
      "epoch: 1 | 16704 / 114272 | training loss: 0.11112570762634277\n",
      "epoch: 1 | 16736 / 114272 | training loss: 0.34929975867271423\n",
      "epoch: 1 | 16768 / 114272 | training loss: 0.7089057564735413\n",
      "epoch: 1 | 16800 / 114272 | training loss: 0.061203937977552414\n",
      "epoch: 1 | 16832 / 114272 | training loss: 0.0570061095058918\n",
      "epoch: 1 | 16864 / 114272 | training loss: 0.018463976681232452\n",
      "epoch: 1 | 16896 / 114272 | training loss: 0.08731605112552643\n",
      "epoch: 1 | 16928 / 114272 | training loss: 0.011475817300379276\n",
      "epoch: 1 | 16960 / 114272 | training loss: 0.037269048392772675\n",
      "epoch: 1 | 16992 / 114272 | training loss: 0.07818722724914551\n",
      "epoch: 1 | 17024 / 114272 | training loss: 0.17592525482177734\n",
      "epoch: 1 | 17056 / 114272 | training loss: 0.011501926928758621\n",
      "epoch: 1 | 17088 / 114272 | training loss: 0.06928392499685287\n",
      "epoch: 1 | 17120 / 114272 | training loss: 0.011741560883820057\n",
      "epoch: 1 | 17152 / 114272 | training loss: 0.01661970093846321\n",
      "epoch: 1 | 17184 / 114272 | training loss: 0.22826234996318817\n",
      "epoch: 1 | 17216 / 114272 | training loss: 0.03018893301486969\n",
      "epoch: 1 | 17248 / 114272 | training loss: 0.03907188028097153\n",
      "epoch: 1 | 17280 / 114272 | training loss: 0.00850621610879898\n",
      "epoch: 1 | 17312 / 114272 | training loss: 0.018729383125901222\n",
      "epoch: 1 | 17344 / 114272 | training loss: 0.06103482469916344\n",
      "epoch: 1 | 17376 / 114272 | training loss: 0.06228695064783096\n",
      "epoch: 1 | 17408 / 114272 | training loss: 0.12462126463651657\n",
      "epoch: 1 | 17440 / 114272 | training loss: 0.11275777220726013\n",
      "epoch: 1 | 17472 / 114272 | training loss: 0.019101666286587715\n",
      "epoch: 1 | 17504 / 114272 | training loss: 0.04063521698117256\n",
      "epoch: 1 | 17536 / 114272 | training loss: 0.17333295941352844\n",
      "epoch: 1 | 17568 / 114272 | training loss: 0.0250747911632061\n",
      "epoch: 1 | 17600 / 114272 | training loss: 0.015216292813420296\n",
      "epoch: 1 | 17632 / 114272 | training loss: 0.32084396481513977\n",
      "epoch: 1 | 17664 / 114272 | training loss: 0.24202632904052734\n",
      "epoch: 1 | 17696 / 114272 | training loss: 0.04575555771589279\n",
      "epoch: 1 | 17728 / 114272 | training loss: 0.13339897990226746\n",
      "epoch: 1 | 17760 / 114272 | training loss: 0.014103678055107594\n",
      "epoch: 1 | 17792 / 114272 | training loss: 0.07186497747898102\n",
      "epoch: 1 | 17824 / 114272 | training loss: 0.22292391955852509\n",
      "epoch: 1 | 17856 / 114272 | training loss: 0.011840499937534332\n",
      "epoch: 1 | 17888 / 114272 | training loss: 0.002448855433613062\n",
      "epoch: 1 | 17920 / 114272 | training loss: 0.24408318102359772\n",
      "epoch: 1 | 17952 / 114272 | training loss: 0.6931856870651245\n",
      "epoch: 1 | 17984 / 114272 | training loss: 0.005890191532671452\n",
      "epoch: 1 | 18016 / 114272 | training loss: 0.05660776421427727\n",
      "epoch: 1 | 18048 / 114272 | training loss: 0.24914750456809998\n",
      "epoch: 1 | 18080 / 114272 | training loss: 0.04925607144832611\n",
      "epoch: 1 | 18112 / 114272 | training loss: 0.18635013699531555\n",
      "epoch: 1 | 18144 / 114272 | training loss: 0.006695164833217859\n",
      "epoch: 1 | 18176 / 114272 | training loss: 0.1696242392063141\n",
      "epoch: 1 | 18208 / 114272 | training loss: 0.014712958596646786\n",
      "epoch: 1 | 18240 / 114272 | training loss: 0.07936163246631622\n",
      "epoch: 1 | 18272 / 114272 | training loss: 0.00621916726231575\n",
      "epoch: 1 | 18304 / 114272 | training loss: 0.10613230615854263\n",
      "epoch: 1 | 18336 / 114272 | training loss: 0.002841570880264044\n",
      "epoch: 1 | 18368 / 114272 | training loss: 0.007621401455253363\n",
      "epoch: 1 | 18400 / 114272 | training loss: 0.06188565865159035\n",
      "epoch: 1 | 18432 / 114272 | training loss: 0.21096843481063843\n",
      "epoch: 1 | 18464 / 114272 | training loss: 0.002296149730682373\n",
      "epoch: 1 | 18496 / 114272 | training loss: 0.18460258841514587\n",
      "epoch: 1 | 18528 / 114272 | training loss: 0.2046169936656952\n",
      "epoch: 1 | 18560 / 114272 | training loss: 0.06309183686971664\n",
      "epoch: 1 | 18592 / 114272 | training loss: 0.11893268674612045\n",
      "epoch: 1 | 18624 / 114272 | training loss: 0.12969370186328888\n",
      "epoch: 1 | 18656 / 114272 | training loss: 0.03838803619146347\n",
      "epoch: 1 | 18688 / 114272 | training loss: 0.2765502333641052\n",
      "epoch: 1 | 18720 / 114272 | training loss: 0.03378782048821449\n",
      "epoch: 1 | 18752 / 114272 | training loss: 0.21626496315002441\n",
      "epoch: 1 | 18784 / 114272 | training loss: 0.027407871559262276\n",
      "epoch: 1 | 18816 / 114272 | training loss: 0.06245029717683792\n",
      "epoch: 1 | 18848 / 114272 | training loss: 0.24448536336421967\n",
      "epoch: 1 | 18880 / 114272 | training loss: 0.033951286226511\n",
      "epoch: 1 | 18912 / 114272 | training loss: 0.3024697005748749\n",
      "epoch: 1 | 18944 / 114272 | training loss: 0.03034663386642933\n",
      "epoch: 1 | 18976 / 114272 | training loss: 0.22060605883598328\n",
      "epoch: 1 | 19008 / 114272 | training loss: 0.2210921049118042\n",
      "epoch: 1 | 19040 / 114272 | training loss: 0.01701125130057335\n",
      "epoch: 1 | 19072 / 114272 | training loss: 0.07373696565628052\n",
      "epoch: 1 | 19104 / 114272 | training loss: 0.13338539004325867\n",
      "epoch: 1 | 19136 / 114272 | training loss: 0.012740273959934711\n",
      "epoch: 1 | 19168 / 114272 | training loss: 0.3156682848930359\n",
      "epoch: 1 | 19200 / 114272 | training loss: 0.292613685131073\n",
      "epoch: 1 | 19232 / 114272 | training loss: 0.05075506493449211\n",
      "epoch: 1 | 19264 / 114272 | training loss: 0.02951824478805065\n",
      "epoch: 1 | 19296 / 114272 | training loss: 0.15704548358917236\n",
      "epoch: 1 | 19328 / 114272 | training loss: 0.029396798461675644\n",
      "epoch: 1 | 19360 / 114272 | training loss: 0.040594425052404404\n",
      "epoch: 1 | 19392 / 114272 | training loss: 0.15064561367034912\n",
      "epoch: 1 | 19424 / 114272 | training loss: 0.17638865113258362\n",
      "epoch: 1 | 19456 / 114272 | training loss: 0.11827626824378967\n",
      "epoch: 1 | 19488 / 114272 | training loss: 0.22673344612121582\n",
      "epoch: 1 | 19520 / 114272 | training loss: 0.019480301067233086\n",
      "epoch: 1 | 19552 / 114272 | training loss: 0.024697959423065186\n",
      "epoch: 1 | 19584 / 114272 | training loss: 0.03431291505694389\n",
      "epoch: 1 | 19616 / 114272 | training loss: 0.04711919277906418\n",
      "epoch: 1 | 19648 / 114272 | training loss: 0.083004891872406\n",
      "epoch: 1 | 19680 / 114272 | training loss: 0.05930694565176964\n",
      "epoch: 1 | 19712 / 114272 | training loss: 0.027308261021971703\n",
      "epoch: 1 | 19744 / 114272 | training loss: 0.02482270821928978\n",
      "epoch: 1 | 19776 / 114272 | training loss: 0.27435019612312317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 19808 / 114272 | training loss: 0.21075615286827087\n",
      "epoch: 1 | 19840 / 114272 | training loss: 0.04780340567231178\n",
      "epoch: 1 | 19872 / 114272 | training loss: 0.11284644156694412\n",
      "epoch: 1 | 19904 / 114272 | training loss: 0.1839960366487503\n",
      "epoch: 1 | 19936 / 114272 | training loss: 0.007796538528054953\n",
      "epoch: 1 | 19968 / 114272 | training loss: 0.18729175627231598\n",
      "epoch: 1 | 20000 / 114272 | training loss: 0.08696359395980835\n",
      "epoch: 1 | 20032 / 114272 | training loss: 0.07884855568408966\n",
      "epoch: 1 | 20064 / 114272 | training loss: 0.11663683503866196\n",
      "epoch: 1 | 20096 / 114272 | training loss: 0.21212071180343628\n",
      "epoch: 1 | 20128 / 114272 | training loss: 0.10841242969036102\n",
      "epoch: 1 | 20160 / 114272 | training loss: 0.1653045266866684\n",
      "epoch: 1 | 20192 / 114272 | training loss: 0.0935959666967392\n",
      "epoch: 1 | 20224 / 114272 | training loss: 0.04999513179063797\n",
      "epoch: 1 | 20256 / 114272 | training loss: 0.17062272131443024\n",
      "epoch: 1 | 20288 / 114272 | training loss: 0.04153262451291084\n",
      "epoch: 1 | 20320 / 114272 | training loss: 0.3326718509197235\n",
      "epoch: 1 | 20352 / 114272 | training loss: 0.12016817927360535\n",
      "epoch: 1 | 20384 / 114272 | training loss: 0.11968908458948135\n",
      "epoch: 1 | 20416 / 114272 | training loss: 0.3264797627925873\n",
      "epoch: 1 | 20448 / 114272 | training loss: 0.02880072593688965\n",
      "epoch: 1 | 20480 / 114272 | training loss: 0.006256552413105965\n",
      "epoch: 1 | 20512 / 114272 | training loss: 0.3144267201423645\n",
      "epoch: 1 | 20544 / 114272 | training loss: 0.14914026856422424\n",
      "epoch: 1 | 20576 / 114272 | training loss: 0.01350332796573639\n",
      "epoch: 1 | 20608 / 114272 | training loss: 0.17364133894443512\n",
      "epoch: 1 | 20640 / 114272 | training loss: 0.07572486251592636\n",
      "epoch: 1 | 20672 / 114272 | training loss: 0.24698293209075928\n",
      "epoch: 1 | 20704 / 114272 | training loss: 0.32016435265541077\n",
      "epoch: 1 | 20736 / 114272 | training loss: 0.017576662823557854\n",
      "epoch: 1 | 20768 / 114272 | training loss: 0.019659532234072685\n",
      "epoch: 1 | 20800 / 114272 | training loss: 0.013516411185264587\n",
      "epoch: 1 | 20832 / 114272 | training loss: 0.06902503222227097\n",
      "epoch: 1 | 20864 / 114272 | training loss: 0.02411482483148575\n",
      "epoch: 1 | 20896 / 114272 | training loss: 0.08008718490600586\n",
      "epoch: 1 | 20928 / 114272 | training loss: 0.14569012820720673\n",
      "epoch: 1 | 20960 / 114272 | training loss: 0.09031360596418381\n",
      "epoch: 1 | 20992 / 114272 | training loss: 0.1518222838640213\n",
      "epoch: 1 | 21024 / 114272 | training loss: 0.028186732903122902\n",
      "epoch: 1 | 21056 / 114272 | training loss: 0.041214220225811005\n",
      "epoch: 1 | 21088 / 114272 | training loss: 0.09095082432031631\n",
      "epoch: 1 | 21120 / 114272 | training loss: 0.09735745936632156\n",
      "epoch: 1 | 21152 / 114272 | training loss: 0.014298894442617893\n",
      "epoch: 1 | 21184 / 114272 | training loss: 0.10491836816072464\n",
      "epoch: 1 | 21216 / 114272 | training loss: 0.06339298188686371\n",
      "epoch: 1 | 21248 / 114272 | training loss: 0.05188407003879547\n",
      "epoch: 1 | 21280 / 114272 | training loss: 0.3392573893070221\n",
      "epoch: 1 | 21312 / 114272 | training loss: 0.03381432592868805\n",
      "epoch: 1 | 21344 / 114272 | training loss: 0.005908566527068615\n",
      "epoch: 1 | 21376 / 114272 | training loss: 0.36192798614501953\n",
      "epoch: 1 | 21408 / 114272 | training loss: 0.14032940566539764\n",
      "epoch: 1 | 21440 / 114272 | training loss: 0.0035303893964737654\n",
      "epoch: 1 | 21472 / 114272 | training loss: 0.13279424607753754\n",
      "epoch: 1 | 21504 / 114272 | training loss: 0.055519890040159225\n",
      "epoch: 1 | 21536 / 114272 | training loss: 0.15028950572013855\n",
      "epoch: 1 | 21568 / 114272 | training loss: 0.004873721860349178\n",
      "epoch: 1 | 21600 / 114272 | training loss: 0.20404885709285736\n",
      "epoch: 1 | 21632 / 114272 | training loss: 0.06768035143613815\n",
      "epoch: 1 | 21664 / 114272 | training loss: 0.15889830887317657\n",
      "epoch: 1 | 21696 / 114272 | training loss: 0.010787155479192734\n",
      "epoch: 1 | 21728 / 114272 | training loss: 0.05826835706830025\n",
      "epoch: 1 | 21760 / 114272 | training loss: 0.2693067491054535\n",
      "epoch: 1 | 21792 / 114272 | training loss: 0.327711820602417\n",
      "epoch: 1 | 21824 / 114272 | training loss: 0.2707497775554657\n",
      "epoch: 1 | 21856 / 114272 | training loss: 0.2612651586532593\n",
      "epoch: 1 | 21888 / 114272 | training loss: 0.03469300642609596\n",
      "epoch: 1 | 21920 / 114272 | training loss: 0.005322969984263182\n",
      "epoch: 1 | 21952 / 114272 | training loss: 0.05282537266612053\n",
      "epoch: 1 | 21984 / 114272 | training loss: 0.1865648627281189\n",
      "epoch: 1 | 22016 / 114272 | training loss: 0.12828905880451202\n",
      "epoch: 1 | 22048 / 114272 | training loss: 0.028689680621027946\n",
      "epoch: 1 | 22080 / 114272 | training loss: 0.14426226913928986\n",
      "epoch: 1 | 22112 / 114272 | training loss: 0.04653885215520859\n",
      "epoch: 1 | 22144 / 114272 | training loss: 0.07902107387781143\n",
      "epoch: 1 | 22176 / 114272 | training loss: 0.025600900873541832\n",
      "epoch: 1 | 22208 / 114272 | training loss: 0.3633759617805481\n",
      "epoch: 1 | 22240 / 114272 | training loss: 0.30959394574165344\n",
      "epoch: 1 | 22272 / 114272 | training loss: 0.07028936594724655\n",
      "epoch: 1 | 22304 / 114272 | training loss: 0.024987291544675827\n",
      "epoch: 1 | 22336 / 114272 | training loss: 0.2574951946735382\n",
      "epoch: 1 | 22368 / 114272 | training loss: 0.1614416390657425\n",
      "epoch: 1 | 22400 / 114272 | training loss: 0.08128989487886429\n",
      "epoch: 1 | 22432 / 114272 | training loss: 0.014162985607981682\n",
      "epoch: 1 | 22464 / 114272 | training loss: 0.17965273559093475\n",
      "epoch: 1 | 22496 / 114272 | training loss: 0.09660475701093674\n",
      "epoch: 1 | 22528 / 114272 | training loss: 0.15471698343753815\n",
      "epoch: 1 | 22560 / 114272 | training loss: 0.3706113398075104\n",
      "epoch: 1 | 22592 / 114272 | training loss: 0.0601489320397377\n",
      "epoch: 1 | 22624 / 114272 | training loss: 0.15110835433006287\n",
      "epoch: 1 | 22656 / 114272 | training loss: 0.13344945013523102\n",
      "epoch: 1 | 22688 / 114272 | training loss: 0.04968453571200371\n",
      "epoch: 1 | 22720 / 114272 | training loss: 0.2976546585559845\n",
      "epoch: 1 | 22752 / 114272 | training loss: 0.09226436167955399\n",
      "epoch: 1 | 22784 / 114272 | training loss: 0.014336966909468174\n",
      "epoch: 1 | 22816 / 114272 | training loss: 0.269790917634964\n",
      "epoch: 1 | 22848 / 114272 | training loss: 0.3809448778629303\n",
      "epoch: 1 | 22880 / 114272 | training loss: 0.01890500821173191\n",
      "epoch: 1 | 22912 / 114272 | training loss: 0.07801166921854019\n",
      "epoch: 1 | 22944 / 114272 | training loss: 0.10059349983930588\n",
      "epoch: 1 | 22976 / 114272 | training loss: 0.11892719566822052\n",
      "epoch: 1 | 23008 / 114272 | training loss: 0.09856554120779037\n",
      "epoch: 1 | 23040 / 114272 | training loss: 0.34781697392463684\n",
      "epoch: 1 | 23072 / 114272 | training loss: 0.021565767005085945\n",
      "epoch: 1 | 23104 / 114272 | training loss: 0.04475673660635948\n",
      "epoch: 1 | 23136 / 114272 | training loss: 0.1625763177871704\n",
      "epoch: 1 | 23168 / 114272 | training loss: 0.03665393218398094\n",
      "epoch: 1 | 23200 / 114272 | training loss: 0.017696434631943703\n",
      "epoch: 1 | 23232 / 114272 | training loss: 0.09199320524930954\n",
      "epoch: 1 | 23264 / 114272 | training loss: 0.08613292127847672\n",
      "epoch: 1 | 23296 / 114272 | training loss: 0.23484952747821808\n",
      "epoch: 1 | 23328 / 114272 | training loss: 0.2779943346977234\n",
      "epoch: 1 | 23360 / 114272 | training loss: 0.0652163177728653\n",
      "epoch: 1 | 23392 / 114272 | training loss: 0.08137292414903641\n",
      "epoch: 1 | 23424 / 114272 | training loss: 0.06963761150836945\n",
      "epoch: 1 | 23456 / 114272 | training loss: 0.065864697098732\n",
      "epoch: 1 | 23488 / 114272 | training loss: 0.01284809596836567\n",
      "epoch: 1 | 23520 / 114272 | training loss: 0.04432263225317001\n",
      "epoch: 1 | 23552 / 114272 | training loss: 0.05436798185110092\n",
      "epoch: 1 | 23584 / 114272 | training loss: 0.3215349614620209\n",
      "epoch: 1 | 23616 / 114272 | training loss: 0.005004581529647112\n",
      "epoch: 1 | 23648 / 114272 | training loss: 0.009869747795164585\n",
      "epoch: 1 | 23680 / 114272 | training loss: 0.03377513214945793\n",
      "epoch: 1 | 23712 / 114272 | training loss: 0.07054447382688522\n",
      "epoch: 1 | 23744 / 114272 | training loss: 0.021645287051796913\n",
      "epoch: 1 | 23776 / 114272 | training loss: 0.22608259320259094\n",
      "epoch: 1 | 23808 / 114272 | training loss: 0.03978964313864708\n",
      "epoch: 1 | 23840 / 114272 | training loss: 0.19012390077114105\n",
      "epoch: 1 | 23872 / 114272 | training loss: 0.24529801309108734\n",
      "epoch: 1 | 23904 / 114272 | training loss: 0.06412162631750107\n",
      "epoch: 1 | 23936 / 114272 | training loss: 0.11958948522806168\n",
      "epoch: 1 | 23968 / 114272 | training loss: 0.09733548760414124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 24000 / 114272 | training loss: 0.1064096987247467\n",
      "epoch: 1 | 24032 / 114272 | training loss: 0.2328624129295349\n",
      "epoch: 1 | 24064 / 114272 | training loss: 0.00958568137139082\n",
      "epoch: 1 | 24096 / 114272 | training loss: 0.18037165701389313\n",
      "epoch: 1 | 24128 / 114272 | training loss: 0.18276822566986084\n",
      "epoch: 1 | 24160 / 114272 | training loss: 0.17197629809379578\n",
      "epoch: 1 | 24192 / 114272 | training loss: 0.3849720060825348\n",
      "epoch: 1 | 24224 / 114272 | training loss: 0.012894021347165108\n",
      "epoch: 1 | 24256 / 114272 | training loss: 0.12898215651512146\n",
      "epoch: 1 | 24288 / 114272 | training loss: 0.07061690837144852\n",
      "epoch: 1 | 24320 / 114272 | training loss: 0.09305330365896225\n",
      "epoch: 1 | 24352 / 114272 | training loss: 0.1320493519306183\n",
      "epoch: 1 | 24384 / 114272 | training loss: 0.21242184937000275\n",
      "epoch: 1 | 24416 / 114272 | training loss: 0.13711467385292053\n",
      "epoch: 1 | 24448 / 114272 | training loss: 0.01761036552488804\n",
      "epoch: 1 | 24480 / 114272 | training loss: 0.14610934257507324\n",
      "epoch: 1 | 24512 / 114272 | training loss: 0.07015449553728104\n",
      "epoch: 1 | 24544 / 114272 | training loss: 0.03397141396999359\n",
      "epoch: 1 | 24576 / 114272 | training loss: 0.08543795347213745\n",
      "epoch: 1 | 24608 / 114272 | training loss: 0.03485240787267685\n",
      "epoch: 1 | 24640 / 114272 | training loss: 0.28725603222846985\n",
      "epoch: 1 | 24672 / 114272 | training loss: 0.4942229092121124\n",
      "epoch: 1 | 24704 / 114272 | training loss: 0.20675155520439148\n",
      "epoch: 1 | 24736 / 114272 | training loss: 0.058651719242334366\n",
      "epoch: 1 | 24768 / 114272 | training loss: 0.12526029348373413\n",
      "epoch: 1 | 24800 / 114272 | training loss: 0.0951479822397232\n",
      "epoch: 1 | 24832 / 114272 | training loss: 0.053811121731996536\n",
      "epoch: 1 | 24864 / 114272 | training loss: 0.02108117565512657\n",
      "epoch: 1 | 24896 / 114272 | training loss: 0.31225284934043884\n",
      "epoch: 1 | 24928 / 114272 | training loss: 0.036064375191926956\n",
      "epoch: 1 | 24960 / 114272 | training loss: 0.09937881678342819\n",
      "epoch: 1 | 24992 / 114272 | training loss: 0.11782091856002808\n",
      "epoch: 1 | 25024 / 114272 | training loss: 0.1496974229812622\n",
      "epoch: 1 | 25056 / 114272 | training loss: 0.045544348657131195\n",
      "epoch: 1 | 25088 / 114272 | training loss: 0.09876826405525208\n",
      "epoch: 1 | 25120 / 114272 | training loss: 0.27474522590637207\n",
      "epoch: 1 | 25152 / 114272 | training loss: 0.12794964015483856\n",
      "epoch: 1 | 25184 / 114272 | training loss: 0.004977026488631964\n",
      "epoch: 1 | 25216 / 114272 | training loss: 0.20420478284358978\n",
      "epoch: 1 | 25248 / 114272 | training loss: 0.18202714622020721\n",
      "epoch: 1 | 25280 / 114272 | training loss: 0.02607840485870838\n",
      "epoch: 1 | 25312 / 114272 | training loss: 0.34513628482818604\n",
      "epoch: 1 | 25344 / 114272 | training loss: 0.23000194132328033\n",
      "epoch: 1 | 25376 / 114272 | training loss: 0.15793728828430176\n",
      "epoch: 1 | 25408 / 114272 | training loss: 0.03259526193141937\n",
      "epoch: 1 | 25440 / 114272 | training loss: 0.03680218383669853\n",
      "epoch: 1 | 25472 / 114272 | training loss: 0.0938756987452507\n",
      "epoch: 1 | 25504 / 114272 | training loss: 0.06481964886188507\n",
      "epoch: 1 | 25536 / 114272 | training loss: 0.022697342559695244\n",
      "epoch: 1 | 25568 / 114272 | training loss: 0.032690294086933136\n",
      "epoch: 1 | 25600 / 114272 | training loss: 0.011375533416867256\n",
      "epoch: 1 | 25632 / 114272 | training loss: 0.04145320877432823\n",
      "epoch: 1 | 25664 / 114272 | training loss: 0.03195517510175705\n",
      "epoch: 1 | 25696 / 114272 | training loss: 0.04508936032652855\n",
      "epoch: 1 | 25728 / 114272 | training loss: 0.015547574497759342\n",
      "epoch: 1 | 25760 / 114272 | training loss: 0.23917199671268463\n",
      "epoch: 1 | 25792 / 114272 | training loss: 0.29450392723083496\n",
      "epoch: 1 | 25824 / 114272 | training loss: 0.12660276889801025\n",
      "epoch: 1 | 25856 / 114272 | training loss: 0.1378461718559265\n",
      "epoch: 1 | 25888 / 114272 | training loss: 0.0068314699456095695\n",
      "epoch: 1 | 25920 / 114272 | training loss: 0.16625745594501495\n",
      "epoch: 1 | 25952 / 114272 | training loss: 0.005032649729400873\n",
      "epoch: 1 | 25984 / 114272 | training loss: 0.012870393693447113\n",
      "epoch: 1 | 26016 / 114272 | training loss: 0.07511772215366364\n",
      "epoch: 1 | 26048 / 114272 | training loss: 0.13439087569713593\n",
      "epoch: 1 | 26080 / 114272 | training loss: 0.0039957608096301556\n",
      "epoch: 1 | 26112 / 114272 | training loss: 0.11143964529037476\n",
      "epoch: 1 | 26144 / 114272 | training loss: 0.08757118135690689\n",
      "epoch: 1 | 26176 / 114272 | training loss: 0.11743561178445816\n",
      "epoch: 1 | 26208 / 114272 | training loss: 0.00901686679571867\n",
      "epoch: 1 | 26240 / 114272 | training loss: 0.2530418932437897\n",
      "epoch: 1 | 26272 / 114272 | training loss: 0.007276891265064478\n",
      "epoch: 1 | 26304 / 114272 | training loss: 0.020869998261332512\n",
      "epoch: 1 | 26336 / 114272 | training loss: 0.029910989105701447\n",
      "epoch: 1 | 26368 / 114272 | training loss: 0.1305486112833023\n",
      "epoch: 1 | 26400 / 114272 | training loss: 0.012434183619916439\n",
      "epoch: 1 | 26432 / 114272 | training loss: 0.07187774032354355\n",
      "epoch: 1 | 26464 / 114272 | training loss: 0.3041848838329315\n",
      "epoch: 1 | 26496 / 114272 | training loss: 0.08086713403463364\n",
      "epoch: 1 | 26528 / 114272 | training loss: 0.11097872257232666\n",
      "epoch: 1 | 26560 / 114272 | training loss: 0.344411164522171\n",
      "epoch: 1 | 26592 / 114272 | training loss: 0.2708636224269867\n",
      "epoch: 1 | 26624 / 114272 | training loss: 0.29528552293777466\n",
      "epoch: 1 | 26656 / 114272 | training loss: 0.020122461020946503\n",
      "epoch: 1 | 26688 / 114272 | training loss: 0.14944769442081451\n",
      "epoch: 1 | 26720 / 114272 | training loss: 0.09288496524095535\n",
      "epoch: 1 | 26752 / 114272 | training loss: 0.1710163950920105\n",
      "epoch: 1 | 26784 / 114272 | training loss: 0.09973234683275223\n",
      "epoch: 1 | 26816 / 114272 | training loss: 0.03590145334601402\n",
      "epoch: 1 | 26848 / 114272 | training loss: 0.037513379007577896\n",
      "epoch: 1 | 26880 / 114272 | training loss: 0.014207884669303894\n",
      "epoch: 1 | 26912 / 114272 | training loss: 0.1817062348127365\n",
      "epoch: 1 | 26944 / 114272 | training loss: 0.051668573170900345\n",
      "epoch: 1 | 26976 / 114272 | training loss: 0.06852132827043533\n",
      "epoch: 1 | 27008 / 114272 | training loss: 0.3389233946800232\n",
      "epoch: 1 | 27040 / 114272 | training loss: 0.18011032044887543\n",
      "epoch: 1 | 27072 / 114272 | training loss: 0.1367308795452118\n",
      "epoch: 1 | 27104 / 114272 | training loss: 0.22974893450737\n",
      "epoch: 1 | 27136 / 114272 | training loss: 0.04432890564203262\n",
      "epoch: 1 | 27168 / 114272 | training loss: 0.15717101097106934\n",
      "epoch: 1 | 27200 / 114272 | training loss: 0.20653024315834045\n",
      "epoch: 1 | 27232 / 114272 | training loss: 0.08698195964097977\n",
      "epoch: 1 | 27264 / 114272 | training loss: 0.2450626939535141\n",
      "epoch: 1 | 27296 / 114272 | training loss: 0.14869719743728638\n",
      "epoch: 1 | 27328 / 114272 | training loss: 0.053665641695261\n",
      "epoch: 1 | 27360 / 114272 | training loss: 0.13516773283481598\n",
      "epoch: 1 | 27392 / 114272 | training loss: 0.20394405722618103\n",
      "epoch: 1 | 27424 / 114272 | training loss: 0.02075471542775631\n",
      "epoch: 1 | 27456 / 114272 | training loss: 0.13293133676052094\n",
      "epoch: 1 | 27488 / 114272 | training loss: 0.28442901372909546\n",
      "epoch: 1 | 27520 / 114272 | training loss: 0.2427339404821396\n",
      "epoch: 1 | 27552 / 114272 | training loss: 0.0996822789311409\n",
      "epoch: 1 | 27584 / 114272 | training loss: 0.08884124457836151\n",
      "epoch: 1 | 27616 / 114272 | training loss: 0.06066155806183815\n",
      "epoch: 1 | 27648 / 114272 | training loss: 0.0298455823212862\n",
      "epoch: 1 | 27680 / 114272 | training loss: 0.07165521383285522\n",
      "epoch: 1 | 27712 / 114272 | training loss: 0.02645309641957283\n",
      "epoch: 1 | 27744 / 114272 | training loss: 0.015096998773515224\n",
      "epoch: 1 | 27776 / 114272 | training loss: 0.019501356407999992\n",
      "epoch: 1 | 27808 / 114272 | training loss: 0.015854472294449806\n",
      "epoch: 1 | 27840 / 114272 | training loss: 0.4360859990119934\n",
      "epoch: 1 | 27872 / 114272 | training loss: 0.10322148352861404\n",
      "epoch: 1 | 27904 / 114272 | training loss: 0.07401218265295029\n",
      "epoch: 1 | 27936 / 114272 | training loss: 0.07830044627189636\n",
      "epoch: 1 | 27968 / 114272 | training loss: 0.19704453647136688\n",
      "epoch: 1 | 28000 / 114272 | training loss: 0.012603689916431904\n",
      "epoch: 1 | 28032 / 114272 | training loss: 0.04792143777012825\n",
      "epoch: 1 | 28064 / 114272 | training loss: 0.07816819846630096\n",
      "epoch: 1 | 28096 / 114272 | training loss: 0.23271319270133972\n",
      "epoch: 1 | 28128 / 114272 | training loss: 0.10952050983905792\n",
      "epoch: 1 | 28160 / 114272 | training loss: 0.1338600069284439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 28192 / 114272 | training loss: 0.18329687416553497\n",
      "epoch: 1 | 28224 / 114272 | training loss: 0.20336274802684784\n",
      "epoch: 1 | 28256 / 114272 | training loss: 0.04501502588391304\n",
      "epoch: 1 | 28288 / 114272 | training loss: 0.15958501398563385\n",
      "epoch: 1 | 28320 / 114272 | training loss: 0.0710768923163414\n",
      "epoch: 1 | 28352 / 114272 | training loss: 0.05814456567168236\n",
      "epoch: 1 | 28384 / 114272 | training loss: 0.12709461152553558\n",
      "epoch: 1 | 28416 / 114272 | training loss: 0.046871937811374664\n",
      "epoch: 1 | 28448 / 114272 | training loss: 0.16935206949710846\n",
      "epoch: 1 | 28480 / 114272 | training loss: 0.2734348177909851\n",
      "epoch: 1 | 28512 / 114272 | training loss: 0.07409858703613281\n",
      "epoch: 1 | 28544 / 114272 | training loss: 0.052874330431222916\n",
      "epoch: 1 | 28576 / 114272 | training loss: 0.058228932321071625\n",
      "epoch: 1 | 28608 / 114272 | training loss: 0.07094446569681168\n",
      "epoch: 1 | 28640 / 114272 | training loss: 0.08980697393417358\n",
      "epoch: 1 | 28672 / 114272 | training loss: 0.12996594607830048\n",
      "epoch: 1 | 28704 / 114272 | training loss: 0.10716860741376877\n",
      "epoch: 1 | 28736 / 114272 | training loss: 0.016868164762854576\n",
      "epoch: 1 | 28768 / 114272 | training loss: 0.012558265589177608\n",
      "epoch: 1 | 28800 / 114272 | training loss: 0.050007645040750504\n",
      "epoch: 1 | 28832 / 114272 | training loss: 0.011827264912426472\n",
      "epoch: 1 | 28864 / 114272 | training loss: 0.010312046855688095\n",
      "epoch: 1 | 28896 / 114272 | training loss: 0.18751677870750427\n",
      "epoch: 1 | 28928 / 114272 | training loss: 0.185012549161911\n",
      "epoch: 1 | 28960 / 114272 | training loss: 0.1909305453300476\n",
      "epoch: 1 | 28992 / 114272 | training loss: 0.00405945023521781\n",
      "epoch: 1 | 29024 / 114272 | training loss: 0.16853800415992737\n",
      "epoch: 1 | 29056 / 114272 | training loss: 0.2079169601202011\n",
      "epoch: 1 | 29088 / 114272 | training loss: 0.1805507242679596\n",
      "epoch: 1 | 29120 / 114272 | training loss: 0.00846875924617052\n",
      "epoch: 1 | 29152 / 114272 | training loss: 0.11157815158367157\n",
      "epoch: 1 | 29184 / 114272 | training loss: 0.1993492990732193\n",
      "epoch: 1 | 29216 / 114272 | training loss: 0.16505759954452515\n",
      "epoch: 1 | 29248 / 114272 | training loss: 0.24679969251155853\n",
      "epoch: 1 | 29280 / 114272 | training loss: 0.20170339941978455\n",
      "epoch: 1 | 29312 / 114272 | training loss: 0.11670426279306412\n",
      "epoch: 1 | 29344 / 114272 | training loss: 0.13019424676895142\n",
      "epoch: 1 | 29376 / 114272 | training loss: 0.027887577190995216\n",
      "epoch: 1 | 29408 / 114272 | training loss: 0.0904928594827652\n",
      "epoch: 1 | 29440 / 114272 | training loss: 0.1370508223772049\n",
      "epoch: 1 | 29472 / 114272 | training loss: 0.008950568735599518\n",
      "epoch: 1 | 29504 / 114272 | training loss: 0.14232686161994934\n",
      "epoch: 1 | 29536 / 114272 | training loss: 0.14754879474639893\n",
      "epoch: 1 | 29568 / 114272 | training loss: 0.1336483359336853\n",
      "epoch: 1 | 29600 / 114272 | training loss: 0.07652702182531357\n",
      "epoch: 1 | 29632 / 114272 | training loss: 0.28639641404151917\n",
      "epoch: 1 | 29664 / 114272 | training loss: 0.08016958832740784\n",
      "epoch: 1 | 29696 / 114272 | training loss: 0.02172323688864708\n",
      "epoch: 1 | 29728 / 114272 | training loss: 0.1773180514574051\n",
      "epoch: 1 | 29760 / 114272 | training loss: 0.1100122332572937\n",
      "epoch: 1 | 29792 / 114272 | training loss: 0.10746533423662186\n",
      "epoch: 1 | 29824 / 114272 | training loss: 0.23048757016658783\n",
      "epoch: 1 | 29856 / 114272 | training loss: 0.24689039587974548\n",
      "epoch: 1 | 29888 / 114272 | training loss: 0.07907130569219589\n",
      "epoch: 1 | 29920 / 114272 | training loss: 0.04460397735238075\n",
      "epoch: 1 | 29952 / 114272 | training loss: 0.25067371129989624\n",
      "epoch: 1 | 29984 / 114272 | training loss: 0.10149995237588882\n",
      "epoch: 1 | 30016 / 114272 | training loss: 0.06834785640239716\n",
      "epoch: 1 | 30048 / 114272 | training loss: 0.07686857134103775\n",
      "epoch: 1 | 30080 / 114272 | training loss: 0.029256243258714676\n",
      "epoch: 1 | 30112 / 114272 | training loss: 0.25904011726379395\n",
      "epoch: 1 | 30144 / 114272 | training loss: 0.16422301530838013\n",
      "epoch: 1 | 30176 / 114272 | training loss: 0.03067505918443203\n",
      "epoch: 1 | 30208 / 114272 | training loss: 0.13946841657161713\n",
      "epoch: 1 | 30240 / 114272 | training loss: 0.2577609419822693\n",
      "epoch: 1 | 30272 / 114272 | training loss: 0.10928566008806229\n",
      "epoch: 1 | 30304 / 114272 | training loss: 0.20966094732284546\n",
      "epoch: 1 | 30336 / 114272 | training loss: 0.007580353878438473\n",
      "epoch: 1 | 30368 / 114272 | training loss: 0.05286713317036629\n",
      "epoch: 1 | 30400 / 114272 | training loss: 0.06938547641038895\n",
      "epoch: 1 | 30432 / 114272 | training loss: 0.12147688865661621\n",
      "epoch: 1 | 30464 / 114272 | training loss: 0.07258826494216919\n",
      "epoch: 1 | 30496 / 114272 | training loss: 0.11254632472991943\n",
      "epoch: 1 | 30528 / 114272 | training loss: 0.09096651524305344\n",
      "epoch: 1 | 30560 / 114272 | training loss: 0.11597689241170883\n",
      "epoch: 1 | 30592 / 114272 | training loss: 0.1311982423067093\n",
      "epoch: 1 | 30624 / 114272 | training loss: 0.021365541964769363\n",
      "epoch: 1 | 30656 / 114272 | training loss: 0.19448250532150269\n",
      "epoch: 1 | 30688 / 114272 | training loss: 0.30773448944091797\n",
      "epoch: 1 | 30720 / 114272 | training loss: 0.035272106528282166\n",
      "epoch: 1 | 30752 / 114272 | training loss: 0.21272733807563782\n",
      "epoch: 1 | 30784 / 114272 | training loss: 0.10455074161291122\n",
      "epoch: 1 | 30816 / 114272 | training loss: 0.2944476306438446\n",
      "epoch: 1 | 30848 / 114272 | training loss: 0.12898756563663483\n",
      "epoch: 1 | 30880 / 114272 | training loss: 0.14359335601329803\n",
      "epoch: 1 | 30912 / 114272 | training loss: 0.015564361587166786\n",
      "epoch: 1 | 30944 / 114272 | training loss: 0.2195572853088379\n",
      "epoch: 1 | 30976 / 114272 | training loss: 0.09989163279533386\n",
      "epoch: 1 | 31008 / 114272 | training loss: 0.0273178331553936\n",
      "epoch: 1 | 31040 / 114272 | training loss: 0.20092447102069855\n",
      "epoch: 1 | 31072 / 114272 | training loss: 0.08186344802379608\n",
      "epoch: 1 | 31104 / 114272 | training loss: 0.29961204528808594\n",
      "epoch: 1 | 31136 / 114272 | training loss: 0.11422692239284515\n",
      "epoch: 1 | 31168 / 114272 | training loss: 0.08697868138551712\n",
      "epoch: 1 | 31200 / 114272 | training loss: 0.00462472066283226\n",
      "epoch: 1 | 31232 / 114272 | training loss: 0.042619626969099045\n",
      "epoch: 1 | 31264 / 114272 | training loss: 0.06307900696992874\n",
      "epoch: 1 | 31296 / 114272 | training loss: 0.09591939300298691\n",
      "epoch: 1 | 31328 / 114272 | training loss: 0.2879889905452728\n",
      "epoch: 1 | 31360 / 114272 | training loss: 0.07717113196849823\n",
      "epoch: 1 | 31392 / 114272 | training loss: 0.010024170391261578\n",
      "epoch: 1 | 31424 / 114272 | training loss: 0.04010520130395889\n",
      "epoch: 1 | 31456 / 114272 | training loss: 0.1203162670135498\n",
      "epoch: 1 | 31488 / 114272 | training loss: 0.18803659081459045\n",
      "epoch: 1 | 31520 / 114272 | training loss: 0.13044753670692444\n",
      "epoch: 1 | 31552 / 114272 | training loss: 0.12172114104032516\n",
      "epoch: 1 | 31584 / 114272 | training loss: 0.05006494000554085\n",
      "epoch: 1 | 31616 / 114272 | training loss: 0.199473038315773\n",
      "epoch: 1 | 31648 / 114272 | training loss: 0.3797704875469208\n",
      "epoch: 1 | 31680 / 114272 | training loss: 0.2660394012928009\n",
      "epoch: 1 | 31712 / 114272 | training loss: 0.017144853249192238\n",
      "epoch: 1 | 31744 / 114272 | training loss: 0.07825634628534317\n",
      "epoch: 1 | 31776 / 114272 | training loss: 0.01705395057797432\n",
      "epoch: 1 | 31808 / 114272 | training loss: 0.07626330107450485\n",
      "epoch: 1 | 31840 / 114272 | training loss: 0.11993894726037979\n",
      "epoch: 1 | 31872 / 114272 | training loss: 0.5079523324966431\n",
      "epoch: 1 | 31904 / 114272 | training loss: 0.03334337845444679\n",
      "epoch: 1 | 31936 / 114272 | training loss: 0.02464815229177475\n",
      "epoch: 1 | 31968 / 114272 | training loss: 0.18234582245349884\n",
      "epoch: 1 | 32000 / 114272 | training loss: 0.027109988033771515\n",
      "epoch: 1 | 32032 / 114272 | training loss: 0.224853053689003\n",
      "epoch: 1 | 32064 / 114272 | training loss: 0.018757633864879608\n",
      "epoch: 1 | 32096 / 114272 | training loss: 0.046251844614744186\n",
      "epoch: 1 | 32128 / 114272 | training loss: 0.07535409182310104\n",
      "epoch: 1 | 32160 / 114272 | training loss: 0.136288583278656\n",
      "epoch: 1 | 32192 / 114272 | training loss: 0.05133042484521866\n",
      "epoch: 1 | 32224 / 114272 | training loss: 0.2593953311443329\n",
      "epoch: 1 | 32256 / 114272 | training loss: 0.010349101386964321\n",
      "epoch: 1 | 32288 / 114272 | training loss: 0.1034771054983139\n",
      "epoch: 1 | 32320 / 114272 | training loss: 0.005101555492728949\n",
      "epoch: 1 | 32352 / 114272 | training loss: 0.09136449545621872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 32384 / 114272 | training loss: 0.11379683017730713\n",
      "epoch: 1 | 32416 / 114272 | training loss: 0.011424279771745205\n",
      "epoch: 1 | 32448 / 114272 | training loss: 0.045663438737392426\n",
      "epoch: 1 | 32480 / 114272 | training loss: 0.13796529173851013\n",
      "epoch: 1 | 32512 / 114272 | training loss: 0.10766182094812393\n",
      "epoch: 1 | 32544 / 114272 | training loss: 0.19806554913520813\n",
      "epoch: 1 | 32576 / 114272 | training loss: 0.085065096616745\n",
      "epoch: 1 | 32608 / 114272 | training loss: 0.0826205164194107\n",
      "epoch: 1 | 32640 / 114272 | training loss: 0.17403537034988403\n",
      "epoch: 1 | 32672 / 114272 | training loss: 0.20399509370326996\n",
      "epoch: 1 | 32704 / 114272 | training loss: 0.09387493878602982\n",
      "epoch: 1 | 32736 / 114272 | training loss: 0.2515496611595154\n",
      "epoch: 1 | 32768 / 114272 | training loss: 0.1464860886335373\n",
      "epoch: 1 | 32800 / 114272 | training loss: 0.024946467950940132\n",
      "epoch: 1 | 32832 / 114272 | training loss: 0.16241958737373352\n",
      "epoch: 1 | 32864 / 114272 | training loss: 0.19039161503314972\n",
      "epoch: 1 | 32896 / 114272 | training loss: 0.37211114168167114\n",
      "epoch: 1 | 32928 / 114272 | training loss: 0.23073188960552216\n",
      "epoch: 1 | 32960 / 114272 | training loss: 0.08519851416349411\n",
      "epoch: 1 | 32992 / 114272 | training loss: 0.25651076436042786\n",
      "epoch: 1 | 33024 / 114272 | training loss: 0.04258069023489952\n",
      "epoch: 1 | 33056 / 114272 | training loss: 0.07686438411474228\n",
      "epoch: 1 | 33088 / 114272 | training loss: 0.1741485893726349\n",
      "epoch: 1 | 33120 / 114272 | training loss: 0.18559497594833374\n",
      "epoch: 1 | 33152 / 114272 | training loss: 0.1920374482870102\n",
      "epoch: 1 | 33184 / 114272 | training loss: 0.4021608531475067\n",
      "epoch: 1 | 33216 / 114272 | training loss: 0.2286367565393448\n",
      "epoch: 1 | 33248 / 114272 | training loss: 0.02772924304008484\n",
      "epoch: 1 | 33280 / 114272 | training loss: 0.01868785358965397\n",
      "epoch: 1 | 33312 / 114272 | training loss: 0.046861518174409866\n",
      "epoch: 1 | 33344 / 114272 | training loss: 0.053235601633787155\n",
      "epoch: 1 | 33376 / 114272 | training loss: 0.022754505276679993\n",
      "epoch: 1 | 33408 / 114272 | training loss: 0.15339937806129456\n",
      "epoch: 1 | 33440 / 114272 | training loss: 0.11402952671051025\n",
      "epoch: 1 | 33472 / 114272 | training loss: 0.1172514259815216\n",
      "epoch: 1 | 33504 / 114272 | training loss: 0.2778869569301605\n",
      "epoch: 1 | 33536 / 114272 | training loss: 0.05227525532245636\n",
      "epoch: 1 | 33568 / 114272 | training loss: 0.07030101865530014\n",
      "epoch: 1 | 33600 / 114272 | training loss: 0.08840019255876541\n",
      "epoch: 1 | 33632 / 114272 | training loss: 0.12322664260864258\n",
      "epoch: 1 | 33664 / 114272 | training loss: 0.16904692351818085\n",
      "epoch: 1 | 33696 / 114272 | training loss: 0.21673506498336792\n",
      "epoch: 1 | 33728 / 114272 | training loss: 0.26627683639526367\n",
      "epoch: 1 | 33760 / 114272 | training loss: 0.031441353261470795\n",
      "epoch: 1 | 33792 / 114272 | training loss: 0.18779316544532776\n",
      "epoch: 1 | 33824 / 114272 | training loss: 0.05954974889755249\n",
      "epoch: 1 | 33856 / 114272 | training loss: 0.16502344608306885\n",
      "epoch: 1 | 33888 / 114272 | training loss: 0.024407828226685524\n",
      "epoch: 1 | 33920 / 114272 | training loss: 0.020620396360754967\n",
      "epoch: 1 | 33952 / 114272 | training loss: 0.09621815383434296\n",
      "epoch: 1 | 33984 / 114272 | training loss: 0.041119664907455444\n",
      "epoch: 1 | 34016 / 114272 | training loss: 0.06724340468645096\n",
      "epoch: 1 | 34048 / 114272 | training loss: 0.27502956986427307\n",
      "epoch: 1 | 34080 / 114272 | training loss: 0.08756138384342194\n",
      "epoch: 1 | 34112 / 114272 | training loss: 0.05034821107983589\n",
      "epoch: 1 | 34144 / 114272 | training loss: 0.24944761395454407\n",
      "epoch: 1 | 34176 / 114272 | training loss: 0.1543716937303543\n",
      "epoch: 1 | 34208 / 114272 | training loss: 0.08421927690505981\n",
      "epoch: 1 | 34240 / 114272 | training loss: 0.09008409082889557\n",
      "epoch: 1 | 34272 / 114272 | training loss: 0.05319786071777344\n",
      "epoch: 1 | 34304 / 114272 | training loss: 0.49856847524642944\n",
      "epoch: 1 | 34336 / 114272 | training loss: 0.28570756316185\n",
      "epoch: 1 | 34368 / 114272 | training loss: 0.19336244463920593\n",
      "epoch: 1 | 34400 / 114272 | training loss: 0.10013692080974579\n",
      "epoch: 1 | 34432 / 114272 | training loss: 0.09973814338445663\n",
      "epoch: 1 | 34464 / 114272 | training loss: 0.1981150507926941\n",
      "epoch: 1 | 34496 / 114272 | training loss: 0.09090684354305267\n",
      "epoch: 1 | 34528 / 114272 | training loss: 0.15060387551784515\n",
      "epoch: 1 | 34560 / 114272 | training loss: 0.028636114671826363\n",
      "epoch: 1 | 34592 / 114272 | training loss: 0.03853854537010193\n",
      "epoch: 1 | 34624 / 114272 | training loss: 0.0315689817070961\n",
      "epoch: 1 | 34656 / 114272 | training loss: 0.134331613779068\n",
      "epoch: 1 | 34688 / 114272 | training loss: 0.06256235390901566\n",
      "epoch: 1 | 34720 / 114272 | training loss: 0.024459196254611015\n",
      "epoch: 1 | 34752 / 114272 | training loss: 0.01028094720095396\n",
      "epoch: 1 | 34784 / 114272 | training loss: 0.14697083830833435\n",
      "epoch: 1 | 34816 / 114272 | training loss: 0.12599268555641174\n",
      "epoch: 1 | 34848 / 114272 | training loss: 0.13292807340621948\n",
      "epoch: 1 | 34880 / 114272 | training loss: 0.2998647093772888\n",
      "epoch: 1 | 34912 / 114272 | training loss: 0.10636625438928604\n",
      "epoch: 1 | 34944 / 114272 | training loss: 0.10722106695175171\n",
      "epoch: 1 | 34976 / 114272 | training loss: 0.5052367448806763\n",
      "epoch: 1 | 35008 / 114272 | training loss: 0.0970337763428688\n",
      "epoch: 1 | 35040 / 114272 | training loss: 0.1938319355249405\n",
      "epoch: 1 | 35072 / 114272 | training loss: 0.1922101527452469\n",
      "epoch: 1 | 35104 / 114272 | training loss: 0.03973500803112984\n",
      "epoch: 1 | 35136 / 114272 | training loss: 0.01774742268025875\n",
      "epoch: 1 | 35168 / 114272 | training loss: 0.16559433937072754\n",
      "epoch: 1 | 35200 / 114272 | training loss: 0.1991032361984253\n",
      "epoch: 1 | 35232 / 114272 | training loss: 0.2909155786037445\n",
      "epoch: 1 | 35264 / 114272 | training loss: 0.1058899387717247\n",
      "epoch: 1 | 35296 / 114272 | training loss: 0.11266831308603287\n",
      "epoch: 1 | 35328 / 114272 | training loss: 0.03250252828001976\n",
      "epoch: 1 | 35360 / 114272 | training loss: 0.07640137523412704\n",
      "epoch: 1 | 35392 / 114272 | training loss: 0.23047342896461487\n",
      "epoch: 1 | 35424 / 114272 | training loss: 0.08639754354953766\n",
      "epoch: 1 | 35456 / 114272 | training loss: 0.06843482702970505\n",
      "epoch: 1 | 35488 / 114272 | training loss: 0.03774532303214073\n",
      "epoch: 1 | 35520 / 114272 | training loss: 0.10462801158428192\n",
      "epoch: 1 | 35552 / 114272 | training loss: 0.16738592088222504\n",
      "epoch: 1 | 35584 / 114272 | training loss: 0.06730623543262482\n",
      "epoch: 1 | 35616 / 114272 | training loss: 0.1666334718465805\n",
      "epoch: 1 | 35648 / 114272 | training loss: 0.0706486850976944\n",
      "epoch: 1 | 35680 / 114272 | training loss: 0.12056253105401993\n",
      "epoch: 1 | 35712 / 114272 | training loss: 0.14429031312465668\n",
      "epoch: 1 | 35744 / 114272 | training loss: 0.03936015069484711\n",
      "epoch: 1 | 35776 / 114272 | training loss: 0.08068870007991791\n",
      "epoch: 1 | 35808 / 114272 | training loss: 0.11621402204036713\n",
      "epoch: 1 | 35840 / 114272 | training loss: 0.2607913911342621\n",
      "epoch: 1 | 35872 / 114272 | training loss: 0.056443918496370316\n",
      "epoch: 1 | 35904 / 114272 | training loss: 0.11193815618753433\n",
      "epoch: 1 | 35936 / 114272 | training loss: 0.2627515196800232\n",
      "epoch: 1 | 35968 / 114272 | training loss: 0.3379283547401428\n",
      "epoch: 1 | 36000 / 114272 | training loss: 0.018100522458553314\n",
      "epoch: 1 | 36032 / 114272 | training loss: 0.15572959184646606\n",
      "epoch: 1 | 36064 / 114272 | training loss: 0.07377277314662933\n",
      "epoch: 1 | 36096 / 114272 | training loss: 0.027166465297341347\n",
      "epoch: 1 | 36128 / 114272 | training loss: 0.013830996118485928\n",
      "epoch: 1 | 36160 / 114272 | training loss: 0.15332406759262085\n",
      "epoch: 1 | 36192 / 114272 | training loss: 0.6515061855316162\n",
      "epoch: 1 | 36224 / 114272 | training loss: 0.17441311478614807\n",
      "epoch: 1 | 36256 / 114272 | training loss: 0.16735894978046417\n",
      "epoch: 1 | 36288 / 114272 | training loss: 0.3083779513835907\n",
      "epoch: 1 | 36320 / 114272 | training loss: 0.09392490237951279\n",
      "epoch: 1 | 36352 / 114272 | training loss: 0.125009685754776\n",
      "epoch: 1 | 36384 / 114272 | training loss: 0.29828470945358276\n",
      "epoch: 1 | 36416 / 114272 | training loss: 0.2608076333999634\n",
      "epoch: 1 | 36448 / 114272 | training loss: 0.19801008701324463\n",
      "epoch: 1 | 36480 / 114272 | training loss: 0.09042250365018845\n",
      "epoch: 1 | 36512 / 114272 | training loss: 0.03172651678323746\n",
      "epoch: 1 | 36544 / 114272 | training loss: 0.12108714133501053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 36576 / 114272 | training loss: 0.10369046032428741\n",
      "epoch: 1 | 36608 / 114272 | training loss: 0.027568146586418152\n",
      "epoch: 1 | 36640 / 114272 | training loss: 0.06403066962957382\n",
      "epoch: 1 | 36672 / 114272 | training loss: 0.036675408482551575\n",
      "epoch: 1 | 36704 / 114272 | training loss: 0.047422993928194046\n",
      "epoch: 1 | 36736 / 114272 | training loss: 0.019322451204061508\n",
      "epoch: 1 | 36768 / 114272 | training loss: 0.08935215324163437\n",
      "epoch: 1 | 36800 / 114272 | training loss: 0.021038569509983063\n",
      "epoch: 1 | 36832 / 114272 | training loss: 0.04708541929721832\n",
      "epoch: 1 | 36864 / 114272 | training loss: 0.010540375486016273\n",
      "epoch: 1 | 36896 / 114272 | training loss: 0.11039980500936508\n",
      "epoch: 1 | 36928 / 114272 | training loss: 0.3145383298397064\n",
      "epoch: 1 | 36960 / 114272 | training loss: 0.0056334189139306545\n",
      "epoch: 1 | 36992 / 114272 | training loss: 0.04262019321322441\n",
      "epoch: 1 | 37024 / 114272 | training loss: 0.10896331816911697\n",
      "epoch: 1 | 37056 / 114272 | training loss: 0.07715348899364471\n",
      "epoch: 1 | 37088 / 114272 | training loss: 0.23690305650234222\n",
      "epoch: 1 | 37120 / 114272 | training loss: 0.06232288479804993\n",
      "epoch: 1 | 37152 / 114272 | training loss: 0.10415632277727127\n",
      "epoch: 1 | 37184 / 114272 | training loss: 0.09193258732557297\n",
      "epoch: 1 | 37216 / 114272 | training loss: 0.2450966089963913\n",
      "epoch: 1 | 37248 / 114272 | training loss: 0.09280039370059967\n",
      "epoch: 1 | 37280 / 114272 | training loss: 0.08406790345907211\n",
      "epoch: 1 | 37312 / 114272 | training loss: 0.050446152687072754\n",
      "epoch: 1 | 37344 / 114272 | training loss: 0.35009679198265076\n",
      "epoch: 1 | 37376 / 114272 | training loss: 0.017056386917829514\n",
      "epoch: 1 | 37408 / 114272 | training loss: 0.23092308640480042\n",
      "epoch: 1 | 37440 / 114272 | training loss: 0.013021324761211872\n",
      "epoch: 1 | 37472 / 114272 | training loss: 0.048776354640722275\n",
      "epoch: 1 | 37504 / 114272 | training loss: 0.24271981418132782\n",
      "epoch: 1 | 37536 / 114272 | training loss: 0.0747336819767952\n",
      "epoch: 1 | 37568 / 114272 | training loss: 0.20235712826251984\n",
      "epoch: 1 | 37600 / 114272 | training loss: 0.17087796330451965\n",
      "epoch: 1 | 37632 / 114272 | training loss: 0.08661580085754395\n",
      "epoch: 1 | 37664 / 114272 | training loss: 0.04119859263300896\n",
      "epoch: 1 | 37696 / 114272 | training loss: 0.18940696120262146\n",
      "epoch: 1 | 37728 / 114272 | training loss: 0.20835304260253906\n",
      "epoch: 1 | 37760 / 114272 | training loss: 0.11255469918251038\n",
      "epoch: 1 | 37792 / 114272 | training loss: 0.01997985690832138\n",
      "epoch: 1 | 37824 / 114272 | training loss: 0.013367922976613045\n",
      "epoch: 1 | 37856 / 114272 | training loss: 0.18184620141983032\n",
      "epoch: 1 | 37888 / 114272 | training loss: 0.21294616162776947\n",
      "epoch: 1 | 37920 / 114272 | training loss: 0.005292934365570545\n",
      "epoch: 1 | 37952 / 114272 | training loss: 0.17824022471904755\n",
      "epoch: 1 | 37984 / 114272 | training loss: 0.17713747918605804\n",
      "epoch: 1 | 38016 / 114272 | training loss: 0.1781429946422577\n",
      "epoch: 1 | 38048 / 114272 | training loss: 0.08299043774604797\n",
      "epoch: 1 | 38080 / 114272 | training loss: 0.022815294563770294\n",
      "epoch: 1 | 38112 / 114272 | training loss: 0.01541740633547306\n",
      "epoch: 1 | 38144 / 114272 | training loss: 0.023362506181001663\n",
      "epoch: 1 | 38176 / 114272 | training loss: 0.01977584697306156\n",
      "epoch: 1 | 38208 / 114272 | training loss: 0.036552805453538895\n",
      "epoch: 1 | 38240 / 114272 | training loss: 0.00848213117569685\n",
      "epoch: 1 | 38272 / 114272 | training loss: 0.012008036486804485\n",
      "epoch: 1 | 38304 / 114272 | training loss: 0.20018832385540009\n",
      "epoch: 1 | 38336 / 114272 | training loss: 0.09942660480737686\n",
      "epoch: 1 | 38368 / 114272 | training loss: 0.009720065630972385\n",
      "epoch: 1 | 38400 / 114272 | training loss: 0.07176096737384796\n",
      "epoch: 1 | 38432 / 114272 | training loss: 0.021170558407902718\n",
      "epoch: 1 | 38464 / 114272 | training loss: 0.4227886497974396\n",
      "epoch: 1 | 38496 / 114272 | training loss: 0.39041343331336975\n",
      "epoch: 1 | 38528 / 114272 | training loss: 0.17730391025543213\n",
      "epoch: 1 | 38560 / 114272 | training loss: 0.13044802844524384\n",
      "epoch: 1 | 38592 / 114272 | training loss: 0.20835097134113312\n",
      "epoch: 1 | 38624 / 114272 | training loss: 0.17547160387039185\n",
      "epoch: 1 | 38656 / 114272 | training loss: 0.11013979464769363\n",
      "epoch: 1 | 38688 / 114272 | training loss: 0.14537928998470306\n",
      "epoch: 1 | 38720 / 114272 | training loss: 0.3256593346595764\n",
      "epoch: 1 | 38752 / 114272 | training loss: 0.015462346374988556\n",
      "epoch: 1 | 38784 / 114272 | training loss: 0.06445103138685226\n",
      "epoch: 1 | 38816 / 114272 | training loss: 0.03868239000439644\n",
      "epoch: 1 | 38848 / 114272 | training loss: 0.0210975743830204\n",
      "epoch: 1 | 38880 / 114272 | training loss: 0.019071053713560104\n",
      "epoch: 1 | 38912 / 114272 | training loss: 0.19118036329746246\n",
      "epoch: 1 | 38944 / 114272 | training loss: 0.06444378942251205\n",
      "epoch: 1 | 38976 / 114272 | training loss: 0.05271069332957268\n",
      "epoch: 1 | 39008 / 114272 | training loss: 0.13467562198638916\n",
      "epoch: 1 | 39040 / 114272 | training loss: 0.40676596760749817\n",
      "epoch: 1 | 39072 / 114272 | training loss: 0.14189070463180542\n",
      "epoch: 1 | 39104 / 114272 | training loss: 0.0655905231833458\n",
      "epoch: 1 | 39136 / 114272 | training loss: 0.1730329543352127\n",
      "epoch: 1 | 39168 / 114272 | training loss: 0.04734237864613533\n",
      "epoch: 1 | 39200 / 114272 | training loss: 0.24135179817676544\n",
      "epoch: 1 | 39232 / 114272 | training loss: 0.15036509931087494\n",
      "epoch: 1 | 39264 / 114272 | training loss: 0.054394420236349106\n",
      "epoch: 1 | 39296 / 114272 | training loss: 0.07248350232839584\n",
      "epoch: 1 | 39328 / 114272 | training loss: 0.13105438649654388\n",
      "epoch: 1 | 39360 / 114272 | training loss: 0.049871861934661865\n",
      "epoch: 1 | 39392 / 114272 | training loss: 0.15292561054229736\n",
      "epoch: 1 | 39424 / 114272 | training loss: 0.01493382453918457\n",
      "epoch: 1 | 39456 / 114272 | training loss: 0.11222556233406067\n",
      "epoch: 1 | 39488 / 114272 | training loss: 0.0365847572684288\n",
      "epoch: 1 | 39520 / 114272 | training loss: 0.006336270831525326\n",
      "epoch: 1 | 39552 / 114272 | training loss: 0.02075802907347679\n",
      "epoch: 1 | 39584 / 114272 | training loss: 0.024583345279097557\n",
      "epoch: 1 | 39616 / 114272 | training loss: 0.1466042548418045\n",
      "epoch: 1 | 39648 / 114272 | training loss: 0.007052597589790821\n",
      "epoch: 1 | 39680 / 114272 | training loss: 0.04946272820234299\n",
      "epoch: 1 | 39712 / 114272 | training loss: 0.15866924822330475\n",
      "epoch: 1 | 39744 / 114272 | training loss: 0.04561397805809975\n",
      "epoch: 1 | 39776 / 114272 | training loss: 0.20691725611686707\n",
      "epoch: 1 | 39808 / 114272 | training loss: 0.4965267479419708\n",
      "epoch: 1 | 39840 / 114272 | training loss: 0.0199431125074625\n",
      "epoch: 1 | 39872 / 114272 | training loss: 0.14039380848407745\n",
      "epoch: 1 | 39904 / 114272 | training loss: 0.42568981647491455\n",
      "epoch: 1 | 39936 / 114272 | training loss: 0.2795172929763794\n",
      "epoch: 1 | 39968 / 114272 | training loss: 0.033279191702604294\n",
      "epoch: 1 | 40000 / 114272 | training loss: 0.05073169618844986\n",
      "epoch: 1 | 40032 / 114272 | training loss: 0.01964566484093666\n",
      "epoch: 1 | 40064 / 114272 | training loss: 0.019109468907117844\n",
      "epoch: 1 | 40096 / 114272 | training loss: 0.071442149579525\n",
      "epoch: 1 | 40128 / 114272 | training loss: 0.12010795623064041\n",
      "epoch: 1 | 40160 / 114272 | training loss: 0.007317694835364819\n",
      "epoch: 1 | 40192 / 114272 | training loss: 0.009787417016923428\n",
      "epoch: 1 | 40224 / 114272 | training loss: 0.20075209438800812\n",
      "epoch: 1 | 40256 / 114272 | training loss: 0.48718324303627014\n",
      "epoch: 1 | 40288 / 114272 | training loss: 0.0073172664269804955\n",
      "epoch: 1 | 40320 / 114272 | training loss: 0.06626863032579422\n",
      "epoch: 1 | 40352 / 114272 | training loss: 0.04969866946339607\n",
      "epoch: 1 | 40384 / 114272 | training loss: 0.03600818291306496\n",
      "epoch: 1 | 40416 / 114272 | training loss: 0.1150154396891594\n",
      "epoch: 1 | 40448 / 114272 | training loss: 0.08458299189805984\n",
      "epoch: 1 | 40480 / 114272 | training loss: 0.04803304001688957\n",
      "epoch: 1 | 40512 / 114272 | training loss: 0.01755569502711296\n",
      "epoch: 1 | 40544 / 114272 | training loss: 0.006629730574786663\n",
      "epoch: 1 | 40576 / 114272 | training loss: 0.15260927379131317\n",
      "epoch: 1 | 40608 / 114272 | training loss: 0.01507981214672327\n",
      "epoch: 1 | 40640 / 114272 | training loss: 0.18462379276752472\n",
      "epoch: 1 | 40672 / 114272 | training loss: 0.07975378632545471\n",
      "epoch: 1 | 40704 / 114272 | training loss: 0.09205647557973862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 40736 / 114272 | training loss: 0.5947095155715942\n",
      "epoch: 1 | 40768 / 114272 | training loss: 0.21410024166107178\n",
      "epoch: 1 | 40800 / 114272 | training loss: 0.01505285780876875\n",
      "epoch: 1 | 40832 / 114272 | training loss: 0.7444748282432556\n",
      "epoch: 1 | 40864 / 114272 | training loss: 0.04373907297849655\n",
      "epoch: 1 | 40896 / 114272 | training loss: 0.14355763792991638\n",
      "epoch: 1 | 40928 / 114272 | training loss: 0.032447315752506256\n",
      "epoch: 1 | 40960 / 114272 | training loss: 0.09679970890283585\n",
      "epoch: 1 | 40992 / 114272 | training loss: 0.21924874186515808\n",
      "epoch: 1 | 41024 / 114272 | training loss: 0.14904889464378357\n",
      "epoch: 1 | 41056 / 114272 | training loss: 0.016137752681970596\n",
      "epoch: 1 | 41088 / 114272 | training loss: 0.35119351744651794\n",
      "epoch: 1 | 41120 / 114272 | training loss: 0.14625202119350433\n",
      "epoch: 1 | 41152 / 114272 | training loss: 0.2264859974384308\n",
      "epoch: 1 | 41184 / 114272 | training loss: 0.23933596909046173\n",
      "epoch: 1 | 41216 / 114272 | training loss: 0.23498034477233887\n",
      "epoch: 1 | 41248 / 114272 | training loss: 0.06993178278207779\n",
      "epoch: 1 | 41280 / 114272 | training loss: 0.4957237243652344\n",
      "epoch: 1 | 41312 / 114272 | training loss: 0.25949710607528687\n",
      "epoch: 1 | 41344 / 114272 | training loss: 0.08531909435987473\n",
      "epoch: 1 | 41376 / 114272 | training loss: 0.011417334899306297\n",
      "epoch: 1 | 41408 / 114272 | training loss: 0.038295432925224304\n",
      "epoch: 1 | 41440 / 114272 | training loss: 0.035581208765506744\n",
      "epoch: 1 | 41472 / 114272 | training loss: 0.04313376173377037\n",
      "epoch: 1 | 41504 / 114272 | training loss: 0.0628361776471138\n",
      "epoch: 1 | 41536 / 114272 | training loss: 0.042897678911685944\n",
      "epoch: 1 | 41568 / 114272 | training loss: 0.02963792346417904\n",
      "epoch: 1 | 41600 / 114272 | training loss: 0.12848375737667084\n",
      "epoch: 1 | 41632 / 114272 | training loss: 0.03342607989907265\n",
      "epoch: 1 | 41664 / 114272 | training loss: 0.11600865423679352\n",
      "epoch: 1 | 41696 / 114272 | training loss: 0.1052018329501152\n",
      "epoch: 1 | 41728 / 114272 | training loss: 0.04322240501642227\n",
      "epoch: 1 | 41760 / 114272 | training loss: 0.12268785387277603\n",
      "epoch: 1 | 41792 / 114272 | training loss: 0.22133736312389374\n",
      "epoch: 1 | 41824 / 114272 | training loss: 0.01237110048532486\n",
      "epoch: 1 | 41856 / 114272 | training loss: 0.3510530889034271\n",
      "epoch: 1 | 41888 / 114272 | training loss: 0.06934699416160583\n",
      "epoch: 1 | 41920 / 114272 | training loss: 0.028123820200562477\n",
      "epoch: 1 | 41952 / 114272 | training loss: 0.016656704246997833\n",
      "epoch: 1 | 41984 / 114272 | training loss: 0.017784912139177322\n",
      "epoch: 1 | 42016 / 114272 | training loss: 0.0933198630809784\n",
      "epoch: 1 | 42048 / 114272 | training loss: 0.018371708691120148\n",
      "epoch: 1 | 42080 / 114272 | training loss: 0.11992929875850677\n",
      "epoch: 1 | 42112 / 114272 | training loss: 0.14861975610256195\n",
      "epoch: 1 | 42144 / 114272 | training loss: 0.33671092987060547\n",
      "epoch: 1 | 42176 / 114272 | training loss: 0.0057340203784406185\n",
      "epoch: 1 | 42208 / 114272 | training loss: 0.13638101518154144\n",
      "epoch: 1 | 42240 / 114272 | training loss: 0.2511206567287445\n",
      "epoch: 1 | 42272 / 114272 | training loss: 0.008034029975533485\n",
      "epoch: 1 | 42304 / 114272 | training loss: 0.16614478826522827\n",
      "epoch: 1 | 42336 / 114272 | training loss: 0.2622058391571045\n",
      "epoch: 1 | 42368 / 114272 | training loss: 0.01626555062830448\n",
      "epoch: 1 | 42400 / 114272 | training loss: 0.21312151849269867\n",
      "epoch: 1 | 42432 / 114272 | training loss: 0.5349241495132446\n",
      "epoch: 1 | 42464 / 114272 | training loss: 0.013617197051644325\n",
      "epoch: 1 | 42496 / 114272 | training loss: 0.010215140879154205\n",
      "epoch: 1 | 42528 / 114272 | training loss: 0.3708830177783966\n",
      "epoch: 1 | 42560 / 114272 | training loss: 0.0931507796049118\n",
      "epoch: 1 | 42592 / 114272 | training loss: 0.38495978713035583\n",
      "epoch: 1 | 42624 / 114272 | training loss: 0.20850102603435516\n",
      "epoch: 1 | 42656 / 114272 | training loss: 0.32997873425483704\n",
      "epoch: 1 | 42688 / 114272 | training loss: 0.16908499598503113\n",
      "epoch: 1 | 42720 / 114272 | training loss: 0.14422489702701569\n",
      "epoch: 1 | 42752 / 114272 | training loss: 0.1935838758945465\n",
      "epoch: 1 | 42784 / 114272 | training loss: 0.028987541794776917\n",
      "epoch: 1 | 42816 / 114272 | training loss: 0.026871580630540848\n",
      "epoch: 1 | 42848 / 114272 | training loss: 0.09262917935848236\n",
      "epoch: 1 | 42880 / 114272 | training loss: 0.08860716223716736\n",
      "epoch: 1 | 42912 / 114272 | training loss: 0.041225410997867584\n",
      "epoch: 1 | 42944 / 114272 | training loss: 0.26955175399780273\n",
      "epoch: 1 | 42976 / 114272 | training loss: 0.04024825990200043\n",
      "epoch: 1 | 43008 / 114272 | training loss: 0.17363831400871277\n",
      "epoch: 1 | 43040 / 114272 | training loss: 0.12337054312229156\n",
      "epoch: 1 | 43072 / 114272 | training loss: 0.025748275220394135\n",
      "epoch: 1 | 43104 / 114272 | training loss: 0.16093827784061432\n",
      "epoch: 1 | 43136 / 114272 | training loss: 0.013502173125743866\n",
      "epoch: 1 | 43168 / 114272 | training loss: 0.024954771623015404\n",
      "epoch: 1 | 43200 / 114272 | training loss: 0.03666791319847107\n",
      "epoch: 1 | 43232 / 114272 | training loss: 0.0773639902472496\n",
      "epoch: 1 | 43264 / 114272 | training loss: 0.012679465115070343\n",
      "epoch: 1 | 43296 / 114272 | training loss: 0.08749190717935562\n",
      "epoch: 1 | 43328 / 114272 | training loss: 0.01890382543206215\n",
      "epoch: 1 | 43360 / 114272 | training loss: 0.19278930127620697\n",
      "epoch: 1 | 43392 / 114272 | training loss: 0.1855594515800476\n",
      "epoch: 1 | 43424 / 114272 | training loss: 0.068381167948246\n",
      "epoch: 1 | 43456 / 114272 | training loss: 0.06752955168485641\n",
      "epoch: 1 | 43488 / 114272 | training loss: 0.03703123703598976\n",
      "epoch: 1 | 43520 / 114272 | training loss: 0.05138292908668518\n",
      "epoch: 1 | 43552 / 114272 | training loss: 0.03762797638773918\n",
      "epoch: 1 | 43584 / 114272 | training loss: 0.009521367028355598\n",
      "epoch: 1 | 43616 / 114272 | training loss: 0.07925057411193848\n",
      "epoch: 1 | 43648 / 114272 | training loss: 0.06117033213376999\n",
      "epoch: 1 | 43680 / 114272 | training loss: 0.048496101051568985\n",
      "epoch: 1 | 43712 / 114272 | training loss: 0.03406504541635513\n",
      "epoch: 1 | 43744 / 114272 | training loss: 0.001819402677938342\n",
      "epoch: 1 | 43776 / 114272 | training loss: 0.15761767327785492\n",
      "epoch: 1 | 43808 / 114272 | training loss: 0.15367957949638367\n",
      "epoch: 1 | 43840 / 114272 | training loss: 0.19030800461769104\n",
      "epoch: 1 | 43872 / 114272 | training loss: 0.03935135900974274\n",
      "epoch: 1 | 43904 / 114272 | training loss: 0.016659080982208252\n",
      "epoch: 1 | 43936 / 114272 | training loss: 0.24974288046360016\n",
      "epoch: 1 | 43968 / 114272 | training loss: 0.007967335171997547\n",
      "epoch: 1 | 44000 / 114272 | training loss: 0.32191696763038635\n",
      "epoch: 1 | 44032 / 114272 | training loss: 0.3253932297229767\n",
      "epoch: 1 | 44064 / 114272 | training loss: 0.24481426179409027\n",
      "epoch: 1 | 44096 / 114272 | training loss: 0.2995179295539856\n",
      "epoch: 1 | 44128 / 114272 | training loss: 0.13780389726161957\n",
      "epoch: 1 | 44160 / 114272 | training loss: 0.08918417245149612\n",
      "epoch: 1 | 44192 / 114272 | training loss: 0.1034548208117485\n",
      "epoch: 1 | 44224 / 114272 | training loss: 0.04694577306509018\n",
      "epoch: 1 | 44256 / 114272 | training loss: 0.24875295162200928\n",
      "epoch: 1 | 44288 / 114272 | training loss: 0.1850394457578659\n",
      "epoch: 1 | 44320 / 114272 | training loss: 0.008184579201042652\n",
      "epoch: 1 | 44352 / 114272 | training loss: 0.20123188197612762\n",
      "epoch: 1 | 44384 / 114272 | training loss: 0.2090962529182434\n",
      "epoch: 1 | 44416 / 114272 | training loss: 0.31936946511268616\n",
      "epoch: 1 | 44448 / 114272 | training loss: 0.06543020159006119\n",
      "epoch: 1 | 44480 / 114272 | training loss: 0.009966572746634483\n",
      "epoch: 1 | 44512 / 114272 | training loss: 0.015127977356314659\n",
      "epoch: 1 | 44544 / 114272 | training loss: 0.013595309108495712\n",
      "epoch: 1 | 44576 / 114272 | training loss: 0.011806686408817768\n",
      "epoch: 1 | 44608 / 114272 | training loss: 0.020096948370337486\n",
      "epoch: 1 | 44640 / 114272 | training loss: 0.09383484721183777\n",
      "epoch: 1 | 44672 / 114272 | training loss: 0.21864643692970276\n",
      "epoch: 1 | 44704 / 114272 | training loss: 0.029000384733080864\n",
      "epoch: 1 | 44736 / 114272 | training loss: 0.17265993356704712\n",
      "epoch: 1 | 44768 / 114272 | training loss: 0.028162280097603798\n",
      "epoch: 1 | 44800 / 114272 | training loss: 0.06503435969352722\n",
      "epoch: 1 | 44832 / 114272 | training loss: 0.015397513285279274\n",
      "epoch: 1 | 44864 / 114272 | training loss: 0.2654626667499542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 44896 / 114272 | training loss: 0.05084418132901192\n",
      "epoch: 1 | 44928 / 114272 | training loss: 0.08734631538391113\n",
      "epoch: 1 | 44960 / 114272 | training loss: 0.13888905942440033\n",
      "epoch: 1 | 44992 / 114272 | training loss: 0.03343351185321808\n",
      "epoch: 1 | 45024 / 114272 | training loss: 0.1802123636007309\n",
      "epoch: 1 | 45056 / 114272 | training loss: 0.23780351877212524\n",
      "epoch: 1 | 45088 / 114272 | training loss: 0.05922449752688408\n",
      "epoch: 1 | 45120 / 114272 | training loss: 0.022166524082422256\n",
      "epoch: 1 | 45152 / 114272 | training loss: 0.35179856419563293\n",
      "epoch: 1 | 45184 / 114272 | training loss: 0.1634211540222168\n",
      "epoch: 1 | 45216 / 114272 | training loss: 0.282048761844635\n",
      "epoch: 1 | 45248 / 114272 | training loss: 0.08403166383504868\n",
      "epoch: 1 | 45280 / 114272 | training loss: 0.10801655799150467\n",
      "epoch: 1 | 45312 / 114272 | training loss: 0.011331159621477127\n",
      "epoch: 1 | 45344 / 114272 | training loss: 0.11228819191455841\n",
      "epoch: 1 | 45376 / 114272 | training loss: 0.33682358264923096\n",
      "epoch: 1 | 45408 / 114272 | training loss: 0.20121993124485016\n",
      "epoch: 1 | 45440 / 114272 | training loss: 0.21605180203914642\n",
      "epoch: 1 | 45472 / 114272 | training loss: 0.002537959720939398\n",
      "epoch: 1 | 45504 / 114272 | training loss: 0.10842923074960709\n",
      "epoch: 1 | 45536 / 114272 | training loss: 0.027802571654319763\n",
      "epoch: 1 | 45568 / 114272 | training loss: 0.035178154706954956\n",
      "epoch: 1 | 45600 / 114272 | training loss: 0.006752452813088894\n",
      "epoch: 1 | 45632 / 114272 | training loss: 0.013179931789636612\n",
      "epoch: 1 | 45664 / 114272 | training loss: 0.21402399241924286\n",
      "epoch: 1 | 45696 / 114272 | training loss: 0.028602013364434242\n",
      "epoch: 1 | 45728 / 114272 | training loss: 0.16600537300109863\n",
      "epoch: 1 | 45760 / 114272 | training loss: 0.01790691539645195\n",
      "epoch: 1 | 45792 / 114272 | training loss: 0.00877210684120655\n",
      "epoch: 1 | 45824 / 114272 | training loss: 0.10000316798686981\n",
      "epoch: 1 | 45856 / 114272 | training loss: 0.17495548725128174\n",
      "epoch: 1 | 45888 / 114272 | training loss: 0.21515025198459625\n",
      "epoch: 1 | 45920 / 114272 | training loss: 0.11222993582487106\n",
      "epoch: 1 | 45952 / 114272 | training loss: 0.30078771710395813\n",
      "epoch: 1 | 45984 / 114272 | training loss: 0.06666029244661331\n",
      "epoch: 1 | 46016 / 114272 | training loss: 0.01191770751029253\n",
      "epoch: 1 | 46048 / 114272 | training loss: 0.24435926973819733\n",
      "epoch: 1 | 46080 / 114272 | training loss: 0.0475124716758728\n",
      "epoch: 1 | 46112 / 114272 | training loss: 0.31262290477752686\n",
      "epoch: 1 | 46144 / 114272 | training loss: 0.07318726927042007\n",
      "epoch: 1 | 46176 / 114272 | training loss: 0.017381643876433372\n",
      "epoch: 1 | 46208 / 114272 | training loss: 0.08774328976869583\n",
      "epoch: 1 | 46240 / 114272 | training loss: 0.043426088988780975\n",
      "epoch: 1 | 46272 / 114272 | training loss: 0.011823468841612339\n",
      "epoch: 1 | 46304 / 114272 | training loss: 0.009815271943807602\n",
      "epoch: 1 | 46336 / 114272 | training loss: 0.11359404027462006\n",
      "epoch: 1 | 46368 / 114272 | training loss: 0.16003847122192383\n",
      "epoch: 1 | 46400 / 114272 | training loss: 0.028874164447188377\n",
      "epoch: 1 | 46432 / 114272 | training loss: 0.040970329195261\n",
      "epoch: 1 | 46464 / 114272 | training loss: 0.0906003937125206\n",
      "epoch: 1 | 46496 / 114272 | training loss: 0.06702832877635956\n",
      "epoch: 1 | 46528 / 114272 | training loss: 0.0256449393928051\n",
      "epoch: 1 | 46560 / 114272 | training loss: 0.3010192811489105\n",
      "epoch: 1 | 46592 / 114272 | training loss: 0.008165697567164898\n",
      "epoch: 1 | 46624 / 114272 | training loss: 0.006696606986224651\n",
      "epoch: 1 | 46656 / 114272 | training loss: 0.11057458072900772\n",
      "epoch: 1 | 46688 / 114272 | training loss: 0.3185928165912628\n",
      "epoch: 1 | 46720 / 114272 | training loss: 0.08781521767377853\n",
      "epoch: 1 | 46752 / 114272 | training loss: 0.0192140843719244\n",
      "epoch: 1 | 46784 / 114272 | training loss: 0.14516866207122803\n",
      "epoch: 1 | 46816 / 114272 | training loss: 0.06391560286283493\n",
      "epoch: 1 | 46848 / 114272 | training loss: 0.02496577985584736\n",
      "epoch: 1 | 46880 / 114272 | training loss: 0.007576053496450186\n",
      "epoch: 1 | 46912 / 114272 | training loss: 0.13274234533309937\n",
      "epoch: 1 | 46944 / 114272 | training loss: 0.0757957398891449\n",
      "epoch: 1 | 46976 / 114272 | training loss: 0.08556631952524185\n",
      "epoch: 1 | 47008 / 114272 | training loss: 0.20389854907989502\n",
      "epoch: 1 | 47040 / 114272 | training loss: 0.15622735023498535\n",
      "epoch: 1 | 47072 / 114272 | training loss: 0.1419350653886795\n",
      "epoch: 1 | 47104 / 114272 | training loss: 0.1416703164577484\n",
      "epoch: 1 | 47136 / 114272 | training loss: 0.33654704689979553\n",
      "epoch: 1 | 47168 / 114272 | training loss: 0.3329768478870392\n",
      "epoch: 1 | 47200 / 114272 | training loss: 0.15014834702014923\n",
      "epoch: 1 | 47232 / 114272 | training loss: 0.023446472361683846\n",
      "epoch: 1 | 47264 / 114272 | training loss: 0.3188473582267761\n",
      "epoch: 1 | 47296 / 114272 | training loss: 0.06891444325447083\n",
      "epoch: 1 | 47328 / 114272 | training loss: 0.26746439933776855\n",
      "epoch: 1 | 47360 / 114272 | training loss: 0.1504359096288681\n",
      "epoch: 1 | 47392 / 114272 | training loss: 0.05129322409629822\n",
      "epoch: 1 | 47424 / 114272 | training loss: 0.04847925156354904\n",
      "epoch: 1 | 47456 / 114272 | training loss: 0.03859486058354378\n",
      "epoch: 1 | 47488 / 114272 | training loss: 0.03872324153780937\n",
      "epoch: 1 | 47520 / 114272 | training loss: 0.24117067456245422\n",
      "epoch: 1 | 47552 / 114272 | training loss: 0.03386198729276657\n",
      "epoch: 1 | 47584 / 114272 | training loss: 0.07738174498081207\n",
      "epoch: 1 | 47616 / 114272 | training loss: 0.13452938199043274\n",
      "epoch: 1 | 47648 / 114272 | training loss: 0.012375835329294205\n",
      "epoch: 1 | 47680 / 114272 | training loss: 0.1133531928062439\n",
      "epoch: 1 | 47712 / 114272 | training loss: 0.07536155730485916\n",
      "epoch: 1 | 47744 / 114272 | training loss: 0.09521903842687607\n",
      "epoch: 1 | 47776 / 114272 | training loss: 0.022690171375870705\n",
      "epoch: 1 | 47808 / 114272 | training loss: 0.09973879903554916\n",
      "epoch: 1 | 47840 / 114272 | training loss: 0.17592564225196838\n",
      "epoch: 1 | 47872 / 114272 | training loss: 0.08687685430049896\n",
      "epoch: 1 | 47904 / 114272 | training loss: 0.006324777379631996\n",
      "epoch: 1 | 47936 / 114272 | training loss: 0.008950081653892994\n",
      "epoch: 1 | 47968 / 114272 | training loss: 0.1215134710073471\n",
      "epoch: 1 | 48000 / 114272 | training loss: 0.011936243623495102\n",
      "epoch: 1 | 48032 / 114272 | training loss: 0.11586343497037888\n",
      "epoch: 1 | 48064 / 114272 | training loss: 0.009520108811557293\n",
      "epoch: 1 | 48096 / 114272 | training loss: 0.20869393646717072\n",
      "epoch: 1 | 48128 / 114272 | training loss: 0.011583350598812103\n",
      "epoch: 1 | 48160 / 114272 | training loss: 0.14745528995990753\n",
      "epoch: 1 | 48192 / 114272 | training loss: 0.08218348026275635\n",
      "epoch: 1 | 48224 / 114272 | training loss: 0.23650985956192017\n",
      "epoch: 1 | 48256 / 114272 | training loss: 0.10801860690116882\n",
      "epoch: 1 | 48288 / 114272 | training loss: 0.008711544796824455\n",
      "epoch: 1 | 48320 / 114272 | training loss: 0.03500639274716377\n",
      "epoch: 1 | 48352 / 114272 | training loss: 0.2528323233127594\n",
      "epoch: 1 | 48384 / 114272 | training loss: 0.01268060877919197\n",
      "epoch: 1 | 48416 / 114272 | training loss: 0.005983056966215372\n",
      "epoch: 1 | 48448 / 114272 | training loss: 0.010479582473635674\n",
      "epoch: 1 | 48480 / 114272 | training loss: 0.09908951073884964\n",
      "epoch: 1 | 48512 / 114272 | training loss: 0.022513682022690773\n",
      "epoch: 1 | 48544 / 114272 | training loss: 0.018389549106359482\n",
      "epoch: 1 | 48576 / 114272 | training loss: 0.156041219830513\n",
      "epoch: 1 | 48608 / 114272 | training loss: 0.041891638189554214\n",
      "epoch: 1 | 48640 / 114272 | training loss: 0.006627647206187248\n",
      "epoch: 1 | 48672 / 114272 | training loss: 0.341549813747406\n",
      "epoch: 1 | 48704 / 114272 | training loss: 0.1364365518093109\n",
      "epoch: 1 | 48736 / 114272 | training loss: 0.1431797295808792\n",
      "epoch: 1 | 48768 / 114272 | training loss: 0.003676158608868718\n",
      "epoch: 1 | 48800 / 114272 | training loss: 0.09922552853822708\n",
      "epoch: 1 | 48832 / 114272 | training loss: 0.6991352438926697\n",
      "epoch: 1 | 48864 / 114272 | training loss: 0.004174607805907726\n",
      "epoch: 1 | 48896 / 114272 | training loss: 0.08831612765789032\n",
      "epoch: 1 | 48928 / 114272 | training loss: 0.1487271636724472\n",
      "epoch: 1 | 48960 / 114272 | training loss: 0.36045747995376587\n",
      "epoch: 1 | 48992 / 114272 | training loss: 0.28612464666366577\n",
      "epoch: 1 | 49024 / 114272 | training loss: 0.1728450208902359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 49056 / 114272 | training loss: 0.11518502235412598\n",
      "epoch: 1 | 49088 / 114272 | training loss: 0.008147263899445534\n",
      "epoch: 1 | 49120 / 114272 | training loss: 0.14062006771564484\n",
      "epoch: 1 | 49152 / 114272 | training loss: 0.04740152880549431\n",
      "epoch: 1 | 49184 / 114272 | training loss: 0.2394733875989914\n",
      "epoch: 1 | 49216 / 114272 | training loss: 0.10996383428573608\n",
      "epoch: 1 | 49248 / 114272 | training loss: 0.07460974156856537\n",
      "epoch: 1 | 49280 / 114272 | training loss: 0.17290261387825012\n",
      "epoch: 1 | 49312 / 114272 | training loss: 0.09903769195079803\n",
      "epoch: 1 | 49344 / 114272 | training loss: 0.012201896868646145\n",
      "epoch: 1 | 49376 / 114272 | training loss: 0.034977223724126816\n",
      "epoch: 1 | 49408 / 114272 | training loss: 0.2727213501930237\n",
      "epoch: 1 | 49440 / 114272 | training loss: 0.34336844086647034\n",
      "epoch: 1 | 49472 / 114272 | training loss: 0.0809493437409401\n",
      "epoch: 1 | 49504 / 114272 | training loss: 0.11065725982189178\n",
      "epoch: 1 | 49536 / 114272 | training loss: 0.026907596737146378\n",
      "epoch: 1 | 49568 / 114272 | training loss: 0.19053390622138977\n",
      "epoch: 1 | 49600 / 114272 | training loss: 0.3073960840702057\n",
      "epoch: 1 | 49632 / 114272 | training loss: 0.02981492318212986\n",
      "epoch: 1 | 49664 / 114272 | training loss: 0.018215015530586243\n",
      "epoch: 1 | 49696 / 114272 | training loss: 0.09932909905910492\n",
      "epoch: 1 | 49728 / 114272 | training loss: 0.09428727626800537\n",
      "epoch: 1 | 49760 / 114272 | training loss: 0.11838647723197937\n",
      "epoch: 1 | 49792 / 114272 | training loss: 0.08473221212625504\n",
      "epoch: 1 | 49824 / 114272 | training loss: 0.06619482487440109\n",
      "epoch: 1 | 49856 / 114272 | training loss: 0.08580418676137924\n",
      "epoch: 1 | 49888 / 114272 | training loss: 0.10147400945425034\n",
      "epoch: 1 | 49920 / 114272 | training loss: 0.4493991732597351\n",
      "epoch: 1 | 49952 / 114272 | training loss: 0.06603634357452393\n",
      "epoch: 1 | 49984 / 114272 | training loss: 0.008721091784536839\n",
      "epoch: 1 | 50016 / 114272 | training loss: 0.0744771882891655\n",
      "epoch: 1 | 50048 / 114272 | training loss: 0.009796155616641045\n",
      "epoch: 1 | 50080 / 114272 | training loss: 0.2634654641151428\n",
      "epoch: 1 | 50112 / 114272 | training loss: 0.04018166661262512\n",
      "epoch: 1 | 50144 / 114272 | training loss: 0.22005558013916016\n",
      "epoch: 1 | 50176 / 114272 | training loss: 0.021245192736387253\n",
      "epoch: 1 | 50208 / 114272 | training loss: 0.09480581432580948\n",
      "epoch: 1 | 50240 / 114272 | training loss: 0.0712990015745163\n",
      "epoch: 1 | 50272 / 114272 | training loss: 0.03802824392914772\n",
      "epoch: 1 | 50304 / 114272 | training loss: 0.020594628527760506\n",
      "epoch: 1 | 50336 / 114272 | training loss: 0.25596854090690613\n",
      "epoch: 1 | 50368 / 114272 | training loss: 0.01860148087143898\n",
      "epoch: 1 | 50400 / 114272 | training loss: 0.033379681408405304\n",
      "epoch: 1 | 50432 / 114272 | training loss: 0.01976538635790348\n",
      "epoch: 1 | 50464 / 114272 | training loss: 0.1949033886194229\n",
      "epoch: 1 | 50496 / 114272 | training loss: 0.11902692168951035\n",
      "epoch: 1 | 50528 / 114272 | training loss: 0.09403826296329498\n",
      "epoch: 1 | 50560 / 114272 | training loss: 0.11780277639627457\n",
      "epoch: 1 | 50592 / 114272 | training loss: 0.052919644862413406\n",
      "epoch: 1 | 50624 / 114272 | training loss: 0.07262632995843887\n",
      "epoch: 1 | 50656 / 114272 | training loss: 0.006335091311484575\n",
      "epoch: 1 | 50688 / 114272 | training loss: 0.012745307758450508\n",
      "epoch: 1 | 50720 / 114272 | training loss: 0.16718657314777374\n",
      "epoch: 1 | 50752 / 114272 | training loss: 0.009809339419007301\n",
      "epoch: 1 | 50784 / 114272 | training loss: 0.22791287302970886\n",
      "epoch: 1 | 50816 / 114272 | training loss: 0.04493281990289688\n",
      "epoch: 1 | 50848 / 114272 | training loss: 0.09050748497247696\n",
      "epoch: 1 | 50880 / 114272 | training loss: 0.03500884026288986\n",
      "epoch: 1 | 50912 / 114272 | training loss: 0.1499076932668686\n",
      "epoch: 1 | 50944 / 114272 | training loss: 0.019225358963012695\n",
      "epoch: 1 | 50976 / 114272 | training loss: 0.12133144587278366\n",
      "epoch: 1 | 51008 / 114272 | training loss: 0.03532978519797325\n",
      "epoch: 1 | 51040 / 114272 | training loss: 0.19389232993125916\n",
      "epoch: 1 | 51072 / 114272 | training loss: 0.1788463294506073\n",
      "epoch: 1 | 51104 / 114272 | training loss: 0.05078911781311035\n",
      "epoch: 1 | 51136 / 114272 | training loss: 0.014198653399944305\n",
      "epoch: 1 | 51168 / 114272 | training loss: 0.10131311416625977\n",
      "epoch: 1 | 51200 / 114272 | training loss: 0.04090181365609169\n",
      "epoch: 1 | 51232 / 114272 | training loss: 0.050102125853300095\n",
      "epoch: 1 | 51264 / 114272 | training loss: 0.0182393379509449\n",
      "epoch: 1 | 51296 / 114272 | training loss: 0.20999661087989807\n",
      "epoch: 1 | 51328 / 114272 | training loss: 0.2762684226036072\n",
      "epoch: 1 | 51360 / 114272 | training loss: 0.16138771176338196\n",
      "epoch: 1 | 51392 / 114272 | training loss: 0.015505262650549412\n",
      "epoch: 1 | 51424 / 114272 | training loss: 0.2496974915266037\n",
      "epoch: 1 | 51456 / 114272 | training loss: 0.13505348563194275\n",
      "epoch: 1 | 51488 / 114272 | training loss: 0.08187229931354523\n",
      "epoch: 1 | 51520 / 114272 | training loss: 0.35250023007392883\n",
      "epoch: 1 | 51552 / 114272 | training loss: 0.022950660437345505\n",
      "epoch: 1 | 51584 / 114272 | training loss: 0.36948105692863464\n",
      "epoch: 1 | 51616 / 114272 | training loss: 0.21849532425403595\n",
      "epoch: 1 | 51648 / 114272 | training loss: 0.23874975740909576\n",
      "epoch: 1 | 51680 / 114272 | training loss: 0.007677205838263035\n",
      "epoch: 1 | 51712 / 114272 | training loss: 0.1239364892244339\n",
      "epoch: 1 | 51744 / 114272 | training loss: 0.021531840786337852\n",
      "epoch: 1 | 51776 / 114272 | training loss: 0.04439563304185867\n",
      "epoch: 1 | 51808 / 114272 | training loss: 0.16732977330684662\n",
      "epoch: 1 | 51840 / 114272 | training loss: 0.01664290577173233\n",
      "epoch: 1 | 51872 / 114272 | training loss: 0.0429929718375206\n",
      "epoch: 1 | 51904 / 114272 | training loss: 0.03603077307343483\n",
      "epoch: 1 | 51936 / 114272 | training loss: 0.3539153039455414\n",
      "epoch: 1 | 51968 / 114272 | training loss: 0.049467213451862335\n",
      "epoch: 1 | 52000 / 114272 | training loss: 0.05534013360738754\n",
      "epoch: 1 | 52032 / 114272 | training loss: 0.13109339773654938\n",
      "epoch: 1 | 52064 / 114272 | training loss: 0.16110579669475555\n",
      "epoch: 1 | 52096 / 114272 | training loss: 0.00873689353466034\n",
      "epoch: 1 | 52128 / 114272 | training loss: 0.12206138670444489\n",
      "epoch: 1 | 52160 / 114272 | training loss: 0.025949064642190933\n",
      "epoch: 1 | 52192 / 114272 | training loss: 0.020298179239034653\n",
      "epoch: 1 | 52224 / 114272 | training loss: 0.03411011025309563\n",
      "epoch: 1 | 52256 / 114272 | training loss: 0.13863344490528107\n",
      "epoch: 1 | 52288 / 114272 | training loss: 0.013964233919978142\n",
      "epoch: 1 | 52320 / 114272 | training loss: 0.009178229607641697\n",
      "epoch: 1 | 52352 / 114272 | training loss: 0.09592760354280472\n",
      "epoch: 1 | 52384 / 114272 | training loss: 0.014237324707210064\n",
      "epoch: 1 | 52416 / 114272 | training loss: 0.04484626650810242\n",
      "epoch: 1 | 52448 / 114272 | training loss: 0.08655954152345657\n",
      "epoch: 1 | 52480 / 114272 | training loss: 0.00950570497661829\n",
      "epoch: 1 | 52512 / 114272 | training loss: 0.09221512824296951\n",
      "epoch: 1 | 52544 / 114272 | training loss: 0.19900096952915192\n",
      "epoch: 1 | 52576 / 114272 | training loss: 0.10772570967674255\n",
      "epoch: 1 | 52608 / 114272 | training loss: 0.07976970076560974\n",
      "epoch: 1 | 52640 / 114272 | training loss: 0.009241190738976002\n",
      "epoch: 1 | 52672 / 114272 | training loss: 0.017489859834313393\n",
      "epoch: 1 | 52704 / 114272 | training loss: 0.08337921649217606\n",
      "epoch: 1 | 52736 / 114272 | training loss: 0.00750330276787281\n",
      "epoch: 1 | 52768 / 114272 | training loss: 0.1696835607290268\n",
      "epoch: 1 | 52800 / 114272 | training loss: 0.0517384335398674\n",
      "epoch: 1 | 52832 / 114272 | training loss: 0.010521444492042065\n",
      "epoch: 1 | 52864 / 114272 | training loss: 0.039425645023584366\n",
      "epoch: 1 | 52896 / 114272 | training loss: 0.07235445827245712\n",
      "epoch: 1 | 52928 / 114272 | training loss: 0.056431036442518234\n",
      "epoch: 1 | 52960 / 114272 | training loss: 0.06059040501713753\n",
      "epoch: 1 | 52992 / 114272 | training loss: 0.15900419652462006\n",
      "epoch: 1 | 53024 / 114272 | training loss: 0.0055754040367901325\n",
      "epoch: 1 | 53056 / 114272 | training loss: 0.06394250690937042\n",
      "epoch: 1 | 53088 / 114272 | training loss: 0.15774135291576385\n",
      "epoch: 1 | 53120 / 114272 | training loss: 0.041344791650772095\n",
      "epoch: 1 | 53152 / 114272 | training loss: 0.04520951583981514\n",
      "epoch: 1 | 53184 / 114272 | training loss: 0.0038665542379021645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 53216 / 114272 | training loss: 0.06862130761146545\n",
      "epoch: 1 | 53248 / 114272 | training loss: 0.03989182785153389\n",
      "epoch: 1 | 53280 / 114272 | training loss: 0.010010025463998318\n",
      "epoch: 1 | 53312 / 114272 | training loss: 0.19572092592716217\n",
      "epoch: 1 | 53344 / 114272 | training loss: 0.008679129183292389\n",
      "epoch: 1 | 53376 / 114272 | training loss: 0.14824005961418152\n",
      "epoch: 1 | 53408 / 114272 | training loss: 0.06219155341386795\n",
      "epoch: 1 | 53440 / 114272 | training loss: 0.18994200229644775\n",
      "epoch: 1 | 53472 / 114272 | training loss: 0.24543428421020508\n",
      "epoch: 1 | 53504 / 114272 | training loss: 0.00941911619156599\n",
      "epoch: 1 | 53536 / 114272 | training loss: 0.05974668636918068\n",
      "epoch: 1 | 53568 / 114272 | training loss: 0.14608871936798096\n",
      "epoch: 1 | 53600 / 114272 | training loss: 0.02063264511525631\n",
      "epoch: 1 | 53632 / 114272 | training loss: 0.15878073871135712\n",
      "epoch: 1 | 53664 / 114272 | training loss: 0.27943965792655945\n",
      "epoch: 1 | 53696 / 114272 | training loss: 0.021907610818743706\n",
      "epoch: 1 | 53728 / 114272 | training loss: 0.2730223536491394\n",
      "epoch: 1 | 53760 / 114272 | training loss: 0.3543674945831299\n",
      "epoch: 1 | 53792 / 114272 | training loss: 0.1999494433403015\n",
      "epoch: 1 | 53824 / 114272 | training loss: 0.04514612257480621\n",
      "epoch: 1 | 53856 / 114272 | training loss: 0.03031344898045063\n",
      "epoch: 1 | 53888 / 114272 | training loss: 0.2397819310426712\n",
      "epoch: 1 | 53920 / 114272 | training loss: 0.16472111642360687\n",
      "epoch: 1 | 53952 / 114272 | training loss: 0.0820845291018486\n",
      "epoch: 1 | 53984 / 114272 | training loss: 0.23110324144363403\n",
      "epoch: 1 | 54016 / 114272 | training loss: 0.16605161130428314\n",
      "epoch: 1 | 54048 / 114272 | training loss: 0.04985899105668068\n",
      "epoch: 1 | 54080 / 114272 | training loss: 0.007756916340440512\n",
      "epoch: 1 | 54112 / 114272 | training loss: 0.027127094566822052\n",
      "epoch: 1 | 54144 / 114272 | training loss: 0.12323600798845291\n",
      "epoch: 1 | 54176 / 114272 | training loss: 0.01679803431034088\n",
      "epoch: 1 | 54208 / 114272 | training loss: 0.034059446305036545\n",
      "epoch: 1 | 54240 / 114272 | training loss: 0.04571903124451637\n",
      "epoch: 1 | 54272 / 114272 | training loss: 0.03350299224257469\n",
      "epoch: 1 | 54304 / 114272 | training loss: 0.09409558027982712\n",
      "epoch: 1 | 54336 / 114272 | training loss: 0.08166924118995667\n",
      "epoch: 1 | 54368 / 114272 | training loss: 0.259456992149353\n",
      "epoch: 1 | 54400 / 114272 | training loss: 0.11204756051301956\n",
      "epoch: 1 | 54432 / 114272 | training loss: 0.06518364697694778\n",
      "epoch: 1 | 54464 / 114272 | training loss: 0.06909718364477158\n",
      "epoch: 1 | 54496 / 114272 | training loss: 0.02401784062385559\n",
      "epoch: 1 | 54528 / 114272 | training loss: 0.26150092482566833\n",
      "epoch: 1 | 54560 / 114272 | training loss: 0.017055485397577286\n",
      "epoch: 1 | 54592 / 114272 | training loss: 0.03568057343363762\n",
      "epoch: 1 | 54624 / 114272 | training loss: 0.08272805064916611\n",
      "epoch: 1 | 54656 / 114272 | training loss: 0.006172915454953909\n",
      "epoch: 1 | 54688 / 114272 | training loss: 0.17164090275764465\n",
      "epoch: 1 | 54720 / 114272 | training loss: 0.1176876649260521\n",
      "epoch: 1 | 54752 / 114272 | training loss: 0.30060330033302307\n",
      "epoch: 1 | 54784 / 114272 | training loss: 0.1236635074019432\n",
      "epoch: 1 | 54816 / 114272 | training loss: 0.04567762464284897\n",
      "epoch: 1 | 54848 / 114272 | training loss: 0.06694810837507248\n",
      "epoch: 1 | 54880 / 114272 | training loss: 0.2685162425041199\n",
      "epoch: 1 | 54912 / 114272 | training loss: 0.3080940842628479\n",
      "epoch: 1 | 54944 / 114272 | training loss: 0.3250005841255188\n",
      "epoch: 1 | 54976 / 114272 | training loss: 0.09307081252336502\n",
      "epoch: 1 | 55008 / 114272 | training loss: 0.12244117259979248\n",
      "epoch: 1 | 55040 / 114272 | training loss: 0.041806068271398544\n",
      "epoch: 1 | 55072 / 114272 | training loss: 0.2541308104991913\n",
      "epoch: 1 | 55104 / 114272 | training loss: 0.3706083297729492\n",
      "epoch: 1 | 55136 / 114272 | training loss: 0.06192592531442642\n",
      "epoch: 1 | 55168 / 114272 | training loss: 0.03701169788837433\n",
      "epoch: 1 | 55200 / 114272 | training loss: 0.04407639801502228\n",
      "epoch: 1 | 55232 / 114272 | training loss: 0.010745885781943798\n",
      "epoch: 1 | 55264 / 114272 | training loss: 0.20132151246070862\n",
      "epoch: 1 | 55296 / 114272 | training loss: 0.30654579401016235\n",
      "epoch: 1 | 55328 / 114272 | training loss: 0.0076337577775120735\n",
      "epoch: 1 | 55360 / 114272 | training loss: 0.14656737446784973\n",
      "epoch: 1 | 55392 / 114272 | training loss: 0.07572685927152634\n",
      "epoch: 1 | 55424 / 114272 | training loss: 0.011044932529330254\n",
      "epoch: 1 | 55456 / 114272 | training loss: 0.21512751281261444\n",
      "epoch: 1 | 55488 / 114272 | training loss: 0.21866193413734436\n",
      "epoch: 1 | 55520 / 114272 | training loss: 0.04983803629875183\n",
      "epoch: 1 | 55552 / 114272 | training loss: 0.2666667103767395\n",
      "epoch: 1 | 55584 / 114272 | training loss: 0.021490488201379776\n",
      "epoch: 1 | 55616 / 114272 | training loss: 0.21054784953594208\n",
      "epoch: 1 | 55648 / 114272 | training loss: 0.22522523999214172\n",
      "epoch: 1 | 55680 / 114272 | training loss: 0.19681209325790405\n",
      "epoch: 1 | 55712 / 114272 | training loss: 0.051136232912540436\n",
      "epoch: 1 | 55744 / 114272 | training loss: 0.16704486310482025\n",
      "epoch: 1 | 55776 / 114272 | training loss: 0.20726095139980316\n",
      "epoch: 1 | 55808 / 114272 | training loss: 0.04884553700685501\n",
      "epoch: 1 | 55840 / 114272 | training loss: 0.12633222341537476\n",
      "epoch: 1 | 55872 / 114272 | training loss: 0.08090551942586899\n",
      "epoch: 1 | 55904 / 114272 | training loss: 0.034452639520168304\n",
      "epoch: 1 | 55936 / 114272 | training loss: 0.22695007920265198\n",
      "epoch: 1 | 55968 / 114272 | training loss: 0.06268825381994247\n",
      "epoch: 1 | 56000 / 114272 | training loss: 0.03670747950673103\n",
      "epoch: 1 | 56032 / 114272 | training loss: 0.03241701051592827\n",
      "epoch: 1 | 56064 / 114272 | training loss: 0.006454022601246834\n",
      "epoch: 1 | 56096 / 114272 | training loss: 0.15777994692325592\n",
      "epoch: 1 | 56128 / 114272 | training loss: 0.050815023481845856\n",
      "epoch: 1 | 56160 / 114272 | training loss: 0.1275138556957245\n",
      "epoch: 1 | 56192 / 114272 | training loss: 0.2349683940410614\n",
      "epoch: 1 | 56224 / 114272 | training loss: 0.21216687560081482\n",
      "epoch: 1 | 56256 / 114272 | training loss: 0.050711579620838165\n",
      "epoch: 1 | 56288 / 114272 | training loss: 0.032556142657995224\n",
      "epoch: 1 | 56320 / 114272 | training loss: 0.03485773131251335\n",
      "epoch: 1 | 56352 / 114272 | training loss: 0.15686389803886414\n",
      "epoch: 1 | 56384 / 114272 | training loss: 0.03404517471790314\n",
      "epoch: 1 | 56416 / 114272 | training loss: 0.34769535064697266\n",
      "epoch: 1 | 56448 / 114272 | training loss: 0.007351349573582411\n",
      "epoch: 1 | 56480 / 114272 | training loss: 0.07055529206991196\n",
      "epoch: 1 | 56512 / 114272 | training loss: 0.05742361396551132\n",
      "epoch: 1 | 56544 / 114272 | training loss: 0.03438841551542282\n",
      "epoch: 1 | 56576 / 114272 | training loss: 0.07353168725967407\n",
      "epoch: 1 | 56608 / 114272 | training loss: 0.015260379761457443\n",
      "epoch: 1 | 56640 / 114272 | training loss: 0.06798423081636429\n",
      "epoch: 1 | 56672 / 114272 | training loss: 0.05551692470908165\n",
      "epoch: 1 | 56704 / 114272 | training loss: 0.6035705208778381\n",
      "epoch: 1 | 56736 / 114272 | training loss: 0.04774119332432747\n",
      "epoch: 1 | 56768 / 114272 | training loss: 0.1752096265554428\n",
      "epoch: 1 | 56800 / 114272 | training loss: 0.05250081419944763\n",
      "epoch: 1 | 56832 / 114272 | training loss: 0.1750074028968811\n",
      "epoch: 1 | 56864 / 114272 | training loss: 0.07229598611593246\n",
      "epoch: 1 | 56896 / 114272 | training loss: 0.05674760788679123\n",
      "epoch: 1 | 56928 / 114272 | training loss: 0.056490276008844376\n",
      "epoch: 1 | 56960 / 114272 | training loss: 0.19706062972545624\n",
      "epoch: 1 | 56992 / 114272 | training loss: 0.00827590562403202\n",
      "epoch: 1 | 57024 / 114272 | training loss: 0.3456457853317261\n",
      "epoch: 1 | 57056 / 114272 | training loss: 0.291157990694046\n",
      "epoch: 1 | 57088 / 114272 | training loss: 0.08176983892917633\n",
      "epoch: 1 | 57120 / 114272 | training loss: 0.035203173756599426\n",
      "epoch: 1 | 57152 / 114272 | training loss: 0.011967433616518974\n",
      "epoch: 1 | 57184 / 114272 | training loss: 0.02846224419772625\n",
      "epoch: 1 | 57216 / 114272 | training loss: 0.05291270837187767\n",
      "epoch: 1 | 57248 / 114272 | training loss: 0.012533187866210938\n",
      "epoch: 1 | 57280 / 114272 | training loss: 0.10941829532384872\n",
      "epoch: 1 | 57312 / 114272 | training loss: 0.14310383796691895\n",
      "epoch: 1 | 57344 / 114272 | training loss: 0.0880594253540039\n",
      "epoch: 1 | 57376 / 114272 | training loss: 0.1130371242761612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 57408 / 114272 | training loss: 0.039614152163267136\n",
      "epoch: 1 | 57440 / 114272 | training loss: 0.03547050431370735\n",
      "epoch: 1 | 57472 / 114272 | training loss: 0.10515060275793076\n",
      "epoch: 1 | 57504 / 114272 | training loss: 0.024060899391770363\n",
      "epoch: 1 | 57536 / 114272 | training loss: 0.24733981490135193\n",
      "epoch: 1 | 57568 / 114272 | training loss: 0.37391310930252075\n",
      "epoch: 1 | 57600 / 114272 | training loss: 0.013930010609328747\n",
      "epoch: 1 | 57632 / 114272 | training loss: 0.019737698137760162\n",
      "epoch: 1 | 57664 / 114272 | training loss: 0.1901155412197113\n",
      "epoch: 1 | 57696 / 114272 | training loss: 0.06223294511437416\n",
      "epoch: 1 | 57728 / 114272 | training loss: 0.030268244445323944\n",
      "epoch: 1 | 57760 / 114272 | training loss: 0.22851672768592834\n",
      "epoch: 1 | 57792 / 114272 | training loss: 0.020221030339598656\n",
      "epoch: 1 | 57824 / 114272 | training loss: 0.2265695482492447\n",
      "epoch: 1 | 57856 / 114272 | training loss: 0.03052251785993576\n",
      "epoch: 1 | 57888 / 114272 | training loss: 0.025527192279696465\n",
      "epoch: 1 | 57920 / 114272 | training loss: 0.1834138184785843\n",
      "epoch: 1 | 57952 / 114272 | training loss: 0.12927791476249695\n",
      "epoch: 1 | 57984 / 114272 | training loss: 0.17437773942947388\n",
      "epoch: 1 | 58016 / 114272 | training loss: 0.0967012345790863\n",
      "epoch: 1 | 58048 / 114272 | training loss: 0.10761714726686478\n",
      "epoch: 1 | 58080 / 114272 | training loss: 0.06444289535284042\n",
      "epoch: 1 | 58112 / 114272 | training loss: 0.022390246391296387\n",
      "epoch: 1 | 58144 / 114272 | training loss: 0.06621924787759781\n",
      "epoch: 1 | 58176 / 114272 | training loss: 0.01094746496528387\n",
      "epoch: 1 | 58208 / 114272 | training loss: 0.0857309028506279\n",
      "epoch: 1 | 58240 / 114272 | training loss: 0.04911871999502182\n",
      "epoch: 1 | 58272 / 114272 | training loss: 0.07100534439086914\n",
      "epoch: 1 | 58304 / 114272 | training loss: 0.023281173780560493\n",
      "epoch: 1 | 58336 / 114272 | training loss: 0.24149152636528015\n",
      "epoch: 1 | 58368 / 114272 | training loss: 0.011667036451399326\n",
      "epoch: 1 | 58400 / 114272 | training loss: 0.015886671841144562\n",
      "epoch: 1 | 58432 / 114272 | training loss: 0.17789267003536224\n",
      "epoch: 1 | 58464 / 114272 | training loss: 0.012142138555645943\n",
      "epoch: 1 | 58496 / 114272 | training loss: 0.15674327313899994\n",
      "epoch: 1 | 58528 / 114272 | training loss: 0.1903325766324997\n",
      "epoch: 1 | 58560 / 114272 | training loss: 0.09903235733509064\n",
      "epoch: 1 | 58592 / 114272 | training loss: 0.02694648690521717\n",
      "epoch: 1 | 58624 / 114272 | training loss: 0.0028229407034814358\n",
      "epoch: 1 | 58656 / 114272 | training loss: 0.0860370397567749\n",
      "epoch: 1 | 58688 / 114272 | training loss: 0.12900249660015106\n",
      "epoch: 1 | 58720 / 114272 | training loss: 0.05056648328900337\n",
      "epoch: 1 | 58752 / 114272 | training loss: 0.014419108629226685\n",
      "epoch: 1 | 58784 / 114272 | training loss: 0.14552004635334015\n",
      "epoch: 1 | 58816 / 114272 | training loss: 0.1062416285276413\n",
      "epoch: 1 | 58848 / 114272 | training loss: 0.15179277956485748\n",
      "epoch: 1 | 58880 / 114272 | training loss: 0.2266218364238739\n",
      "epoch: 1 | 58912 / 114272 | training loss: 0.04317774623632431\n",
      "epoch: 1 | 58944 / 114272 | training loss: 0.3228546679019928\n",
      "epoch: 1 | 58976 / 114272 | training loss: 0.01179945096373558\n",
      "epoch: 1 | 59008 / 114272 | training loss: 0.13037824630737305\n",
      "epoch: 1 | 59040 / 114272 | training loss: 0.17327003180980682\n",
      "epoch: 1 | 59072 / 114272 | training loss: 0.017788685858249664\n",
      "epoch: 1 | 59104 / 114272 | training loss: 0.034246448427438736\n",
      "epoch: 1 | 59136 / 114272 | training loss: 0.007923262193799019\n",
      "epoch: 1 | 59168 / 114272 | training loss: 0.09637396782636642\n",
      "epoch: 1 | 59200 / 114272 | training loss: 0.05985848978161812\n",
      "epoch: 1 | 59232 / 114272 | training loss: 0.20270521938800812\n",
      "epoch: 1 | 59264 / 114272 | training loss: 0.018098659813404083\n",
      "epoch: 1 | 59296 / 114272 | training loss: 0.11585605889558792\n",
      "epoch: 1 | 59328 / 114272 | training loss: 0.10661081969738007\n",
      "epoch: 1 | 59360 / 114272 | training loss: 0.10955332219600677\n",
      "epoch: 1 | 59392 / 114272 | training loss: 0.17046737670898438\n",
      "epoch: 1 | 59424 / 114272 | training loss: 0.11215686053037643\n",
      "epoch: 1 | 59456 / 114272 | training loss: 0.1970084309577942\n",
      "epoch: 1 | 59488 / 114272 | training loss: 0.24768581986427307\n",
      "epoch: 1 | 59520 / 114272 | training loss: 0.11682697385549545\n",
      "epoch: 1 | 59552 / 114272 | training loss: 0.012330234050750732\n",
      "epoch: 1 | 59584 / 114272 | training loss: 0.19688375294208527\n",
      "epoch: 1 | 59616 / 114272 | training loss: 0.10626407712697983\n",
      "epoch: 1 | 59648 / 114272 | training loss: 0.08392275869846344\n",
      "epoch: 1 | 59680 / 114272 | training loss: 0.06578250229358673\n",
      "epoch: 1 | 59712 / 114272 | training loss: 0.07683275640010834\n",
      "epoch: 1 | 59744 / 114272 | training loss: 0.16297638416290283\n",
      "epoch: 1 | 59776 / 114272 | training loss: 0.04844747111201286\n",
      "epoch: 1 | 59808 / 114272 | training loss: 0.08484075218439102\n",
      "epoch: 1 | 59840 / 114272 | training loss: 0.1548503041267395\n",
      "epoch: 1 | 59872 / 114272 | training loss: 0.16007912158966064\n",
      "epoch: 1 | 59904 / 114272 | training loss: 0.215406134724617\n",
      "epoch: 1 | 59936 / 114272 | training loss: 0.27057531476020813\n",
      "epoch: 1 | 59968 / 114272 | training loss: 0.046779412776231766\n",
      "epoch: 1 | 60000 / 114272 | training loss: 0.11304377019405365\n",
      "epoch: 1 | 60032 / 114272 | training loss: 0.3107220530509949\n",
      "epoch: 1 | 60064 / 114272 | training loss: 0.03575268015265465\n",
      "epoch: 1 | 60096 / 114272 | training loss: 0.10196588933467865\n",
      "epoch: 1 | 60128 / 114272 | training loss: 0.08441095054149628\n",
      "epoch: 1 | 60160 / 114272 | training loss: 0.05557597428560257\n",
      "epoch: 1 | 60192 / 114272 | training loss: 0.1547892987728119\n",
      "epoch: 1 | 60224 / 114272 | training loss: 0.1783401072025299\n",
      "epoch: 1 | 60256 / 114272 | training loss: 0.11109045147895813\n",
      "epoch: 1 | 60288 / 114272 | training loss: 0.0816107913851738\n",
      "epoch: 1 | 60320 / 114272 | training loss: 0.07355315238237381\n",
      "epoch: 1 | 60352 / 114272 | training loss: 0.03193303942680359\n",
      "epoch: 1 | 60384 / 114272 | training loss: 0.18614649772644043\n",
      "epoch: 1 | 60416 / 114272 | training loss: 0.06050662323832512\n",
      "epoch: 1 | 60448 / 114272 | training loss: 0.3592626750469208\n",
      "epoch: 1 | 60480 / 114272 | training loss: 0.04932637885212898\n",
      "epoch: 1 | 60512 / 114272 | training loss: 0.2415131777524948\n",
      "epoch: 1 | 60544 / 114272 | training loss: 0.17249912023544312\n",
      "epoch: 1 | 60576 / 114272 | training loss: 0.2808641195297241\n",
      "epoch: 1 | 60608 / 114272 | training loss: 0.18739956617355347\n",
      "epoch: 1 | 60640 / 114272 | training loss: 0.1451585739850998\n",
      "epoch: 1 | 60672 / 114272 | training loss: 0.19426874816417694\n",
      "epoch: 1 | 60704 / 114272 | training loss: 0.018382547423243523\n",
      "epoch: 1 | 60736 / 114272 | training loss: 0.1583632230758667\n",
      "epoch: 1 | 60768 / 114272 | training loss: 0.12797395884990692\n",
      "epoch: 1 | 60800 / 114272 | training loss: 0.11704744398593903\n",
      "epoch: 1 | 60832 / 114272 | training loss: 0.04062850773334503\n",
      "epoch: 1 | 60864 / 114272 | training loss: 0.09999553114175797\n",
      "epoch: 1 | 60896 / 114272 | training loss: 0.014940384775400162\n",
      "epoch: 1 | 60928 / 114272 | training loss: 0.254090815782547\n",
      "epoch: 1 | 60960 / 114272 | training loss: 0.20627769827842712\n",
      "epoch: 1 | 60992 / 114272 | training loss: 0.04120706394314766\n",
      "epoch: 1 | 61024 / 114272 | training loss: 0.17643724381923676\n",
      "epoch: 1 | 61056 / 114272 | training loss: 0.25550293922424316\n",
      "epoch: 1 | 61088 / 114272 | training loss: 0.10331856459379196\n",
      "epoch: 1 | 61120 / 114272 | training loss: 0.07582896947860718\n",
      "epoch: 1 | 61152 / 114272 | training loss: 0.09469199180603027\n",
      "epoch: 1 | 61184 / 114272 | training loss: 0.017246505245566368\n",
      "epoch: 1 | 61216 / 114272 | training loss: 0.0670393630862236\n",
      "epoch: 1 | 61248 / 114272 | training loss: 0.0337543785572052\n",
      "epoch: 1 | 61280 / 114272 | training loss: 0.4640006721019745\n",
      "epoch: 1 | 61312 / 114272 | training loss: 0.13894779980182648\n",
      "epoch: 1 | 61344 / 114272 | training loss: 0.10758393257856369\n",
      "epoch: 1 | 61376 / 114272 | training loss: 0.017621930688619614\n",
      "epoch: 1 | 61408 / 114272 | training loss: 0.015735508874058723\n",
      "epoch: 1 | 61440 / 114272 | training loss: 0.19513137638568878\n",
      "epoch: 1 | 61472 / 114272 | training loss: 0.02237887680530548\n",
      "epoch: 1 | 61504 / 114272 | training loss: 0.11688772588968277\n",
      "epoch: 1 | 61536 / 114272 | training loss: 0.11085160076618195\n",
      "epoch: 1 | 61568 / 114272 | training loss: 0.20748990774154663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 61600 / 114272 | training loss: 0.04425328969955444\n",
      "epoch: 1 | 61632 / 114272 | training loss: 0.18476954102516174\n",
      "epoch: 1 | 61664 / 114272 | training loss: 0.19269761443138123\n",
      "epoch: 1 | 61696 / 114272 | training loss: 0.06668321788311005\n",
      "epoch: 1 | 61728 / 114272 | training loss: 0.05289721488952637\n",
      "epoch: 1 | 61760 / 114272 | training loss: 0.3532409071922302\n",
      "epoch: 1 | 61792 / 114272 | training loss: 0.3523064851760864\n",
      "epoch: 1 | 61824 / 114272 | training loss: 0.018014252185821533\n",
      "epoch: 1 | 61856 / 114272 | training loss: 0.08438350260257721\n",
      "epoch: 1 | 61888 / 114272 | training loss: 0.04827624931931496\n",
      "epoch: 1 | 61920 / 114272 | training loss: 0.2007189244031906\n",
      "epoch: 1 | 61952 / 114272 | training loss: 0.22980158030986786\n",
      "epoch: 1 | 61984 / 114272 | training loss: 0.02595801092684269\n",
      "epoch: 1 | 62016 / 114272 | training loss: 0.004583070054650307\n",
      "epoch: 1 | 62048 / 114272 | training loss: 0.17670609056949615\n",
      "epoch: 1 | 62080 / 114272 | training loss: 0.10951033234596252\n",
      "epoch: 1 | 62112 / 114272 | training loss: 0.04070379585027695\n",
      "epoch: 1 | 62144 / 114272 | training loss: 0.23617219924926758\n",
      "epoch: 1 | 62176 / 114272 | training loss: 0.2308720201253891\n",
      "epoch: 1 | 62208 / 114272 | training loss: 0.11082131415605545\n",
      "epoch: 1 | 62240 / 114272 | training loss: 0.37169212102890015\n",
      "epoch: 1 | 62272 / 114272 | training loss: 0.3933883607387543\n",
      "epoch: 1 | 62304 / 114272 | training loss: 0.06536006182432175\n",
      "epoch: 1 | 62336 / 114272 | training loss: 0.16926945745944977\n",
      "epoch: 1 | 62368 / 114272 | training loss: 0.024436749517917633\n",
      "epoch: 1 | 62400 / 114272 | training loss: 0.0942392572760582\n",
      "epoch: 1 | 62432 / 114272 | training loss: 0.03647357225418091\n",
      "epoch: 1 | 62464 / 114272 | training loss: 0.03469480201601982\n",
      "epoch: 1 | 62496 / 114272 | training loss: 0.25847768783569336\n",
      "epoch: 1 | 62528 / 114272 | training loss: 0.18125541508197784\n",
      "epoch: 1 | 62560 / 114272 | training loss: 0.11295793950557709\n",
      "epoch: 1 | 62592 / 114272 | training loss: 0.03586206212639809\n",
      "epoch: 1 | 62624 / 114272 | training loss: 0.27286046743392944\n",
      "epoch: 1 | 62656 / 114272 | training loss: 0.0038687470369040966\n",
      "epoch: 1 | 62688 / 114272 | training loss: 0.3244788944721222\n",
      "epoch: 1 | 62720 / 114272 | training loss: 0.0062806447967886925\n",
      "epoch: 1 | 62752 / 114272 | training loss: 0.0239860862493515\n",
      "epoch: 1 | 62784 / 114272 | training loss: 0.06166082248091698\n",
      "epoch: 1 | 62816 / 114272 | training loss: 0.01834031566977501\n",
      "epoch: 1 | 62848 / 114272 | training loss: 0.015780720859766006\n",
      "epoch: 1 | 62880 / 114272 | training loss: 0.09944353252649307\n",
      "epoch: 1 | 62912 / 114272 | training loss: 0.13349798321723938\n",
      "epoch: 1 | 62944 / 114272 | training loss: 0.392935574054718\n",
      "epoch: 1 | 62976 / 114272 | training loss: 0.05001308023929596\n",
      "epoch: 1 | 63008 / 114272 | training loss: 0.09540040791034698\n",
      "epoch: 1 | 63040 / 114272 | training loss: 0.15789970755577087\n",
      "epoch: 1 | 63072 / 114272 | training loss: 0.038455117493867874\n",
      "epoch: 1 | 63104 / 114272 | training loss: 0.055547457188367844\n",
      "epoch: 1 | 63136 / 114272 | training loss: 0.012599659152328968\n",
      "epoch: 1 | 63168 / 114272 | training loss: 0.01805480197072029\n",
      "epoch: 1 | 63200 / 114272 | training loss: 0.12142051756381989\n",
      "epoch: 1 | 63232 / 114272 | training loss: 0.08437230437994003\n",
      "epoch: 1 | 63264 / 114272 | training loss: 0.09085080027580261\n",
      "epoch: 1 | 63296 / 114272 | training loss: 0.29297029972076416\n",
      "epoch: 1 | 63328 / 114272 | training loss: 0.11901845037937164\n",
      "epoch: 1 | 63360 / 114272 | training loss: 0.11267951130867004\n",
      "epoch: 1 | 63392 / 114272 | training loss: 0.09020108729600906\n",
      "epoch: 1 | 63424 / 114272 | training loss: 0.04736080765724182\n",
      "epoch: 1 | 63456 / 114272 | training loss: 0.16254465281963348\n",
      "epoch: 1 | 63488 / 114272 | training loss: 0.018185323104262352\n",
      "epoch: 1 | 63520 / 114272 | training loss: 0.02924785017967224\n",
      "epoch: 1 | 63552 / 114272 | training loss: 0.06210843846201897\n",
      "epoch: 1 | 63584 / 114272 | training loss: 0.3188220262527466\n",
      "epoch: 1 | 63616 / 114272 | training loss: 0.06606055796146393\n",
      "epoch: 1 | 63648 / 114272 | training loss: 0.058290425688028336\n",
      "epoch: 1 | 63680 / 114272 | training loss: 0.012377367354929447\n",
      "epoch: 1 | 63712 / 114272 | training loss: 0.2623245418071747\n",
      "epoch: 1 | 63744 / 114272 | training loss: 0.05932390317320824\n",
      "epoch: 1 | 63776 / 114272 | training loss: 0.07019326835870743\n",
      "epoch: 1 | 63808 / 114272 | training loss: 0.03354789316654205\n",
      "epoch: 1 | 63840 / 114272 | training loss: 0.2637675702571869\n",
      "epoch: 1 | 63872 / 114272 | training loss: 0.00959812756627798\n",
      "epoch: 1 | 63904 / 114272 | training loss: 0.21293078362941742\n",
      "epoch: 1 | 63936 / 114272 | training loss: 0.14269807934761047\n",
      "epoch: 1 | 63968 / 114272 | training loss: 0.24512524902820587\n",
      "epoch: 1 | 64000 / 114272 | training loss: 0.02873968705534935\n",
      "epoch: 1 | 64032 / 114272 | training loss: 0.009767717681825161\n",
      "epoch: 1 | 64064 / 114272 | training loss: 0.16479192674160004\n",
      "epoch: 1 | 64096 / 114272 | training loss: 0.19666863977909088\n",
      "epoch: 1 | 64128 / 114272 | training loss: 0.08640206605195999\n",
      "epoch: 1 | 64160 / 114272 | training loss: 0.04380781948566437\n",
      "epoch: 1 | 64192 / 114272 | training loss: 0.15106260776519775\n",
      "epoch: 1 | 64224 / 114272 | training loss: 0.03256213665008545\n",
      "epoch: 1 | 64256 / 114272 | training loss: 0.007797867059707642\n",
      "epoch: 1 | 64288 / 114272 | training loss: 0.01795775070786476\n",
      "epoch: 1 | 64320 / 114272 | training loss: 0.06583663076162338\n",
      "epoch: 1 | 64352 / 114272 | training loss: 0.47243109345436096\n",
      "epoch: 1 | 64384 / 114272 | training loss: 0.03786361590027809\n",
      "epoch: 1 | 64416 / 114272 | training loss: 0.016388876363635063\n",
      "epoch: 1 | 64448 / 114272 | training loss: 0.10317069292068481\n",
      "epoch: 1 | 64480 / 114272 | training loss: 0.008310813456773758\n",
      "epoch: 1 | 64512 / 114272 | training loss: 0.09433497488498688\n",
      "epoch: 1 | 64544 / 114272 | training loss: 0.2189408242702484\n",
      "epoch: 1 | 64576 / 114272 | training loss: 0.04123597592115402\n",
      "epoch: 1 | 64608 / 114272 | training loss: 0.1779046356678009\n",
      "epoch: 1 | 64640 / 114272 | training loss: 0.5872407555580139\n",
      "epoch: 1 | 64672 / 114272 | training loss: 0.009931960143148899\n",
      "epoch: 1 | 64704 / 114272 | training loss: 0.031308118253946304\n",
      "epoch: 1 | 64736 / 114272 | training loss: 0.2101183533668518\n",
      "epoch: 1 | 64768 / 114272 | training loss: 0.08788887411355972\n",
      "epoch: 1 | 64800 / 114272 | training loss: 0.16692619025707245\n",
      "epoch: 1 | 64832 / 114272 | training loss: 0.05326684191823006\n",
      "epoch: 1 | 64864 / 114272 | training loss: 0.1727517545223236\n",
      "epoch: 1 | 64896 / 114272 | training loss: 0.2089073359966278\n",
      "epoch: 1 | 64928 / 114272 | training loss: 0.05667956545948982\n",
      "epoch: 1 | 64960 / 114272 | training loss: 0.0476820208132267\n",
      "epoch: 1 | 64992 / 114272 | training loss: 0.11727757006883621\n",
      "epoch: 1 | 65024 / 114272 | training loss: 0.0940200462937355\n",
      "epoch: 1 | 65056 / 114272 | training loss: 0.10756777971982956\n",
      "epoch: 1 | 65088 / 114272 | training loss: 0.20375509560108185\n",
      "epoch: 1 | 65120 / 114272 | training loss: 0.1861017495393753\n",
      "epoch: 1 | 65152 / 114272 | training loss: 0.14250926673412323\n",
      "epoch: 1 | 65184 / 114272 | training loss: 0.04431656002998352\n",
      "epoch: 1 | 65216 / 114272 | training loss: 0.11398015916347504\n",
      "epoch: 1 | 65248 / 114272 | training loss: 0.012272468768060207\n",
      "epoch: 1 | 65280 / 114272 | training loss: 0.008316518738865852\n",
      "epoch: 1 | 65312 / 114272 | training loss: 0.16585002839565277\n",
      "epoch: 1 | 65344 / 114272 | training loss: 0.05441723391413689\n",
      "epoch: 1 | 65376 / 114272 | training loss: 0.22958818078041077\n",
      "epoch: 1 | 65408 / 114272 | training loss: 0.17370763421058655\n",
      "epoch: 1 | 65440 / 114272 | training loss: 0.14308807253837585\n",
      "epoch: 1 | 65472 / 114272 | training loss: 0.03534602373838425\n",
      "epoch: 1 | 65504 / 114272 | training loss: 0.1349812150001526\n",
      "epoch: 1 | 65536 / 114272 | training loss: 0.17385154962539673\n",
      "epoch: 1 | 65568 / 114272 | training loss: 0.03704423829913139\n",
      "epoch: 1 | 65600 / 114272 | training loss: 0.18705590069293976\n",
      "epoch: 1 | 65632 / 114272 | training loss: 0.023073187097907066\n",
      "epoch: 1 | 65664 / 114272 | training loss: 0.12998399138450623\n",
      "epoch: 1 | 65696 / 114272 | training loss: 0.10399943590164185\n",
      "epoch: 1 | 65728 / 114272 | training loss: 0.1107306256890297\n",
      "epoch: 1 | 65760 / 114272 | training loss: 0.34779250621795654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 65792 / 114272 | training loss: 0.010880565270781517\n",
      "epoch: 1 | 65824 / 114272 | training loss: 0.032922834157943726\n",
      "epoch: 1 | 65856 / 114272 | training loss: 0.21978406608104706\n",
      "epoch: 1 | 65888 / 114272 | training loss: 0.014501859433948994\n",
      "epoch: 1 | 65920 / 114272 | training loss: 0.046864189207553864\n",
      "epoch: 1 | 65952 / 114272 | training loss: 0.08484045416116714\n",
      "epoch: 1 | 65984 / 114272 | training loss: 0.3740149736404419\n",
      "epoch: 1 | 66016 / 114272 | training loss: 0.08458178490400314\n",
      "epoch: 1 | 66048 / 114272 | training loss: 0.10988058149814606\n",
      "epoch: 1 | 66080 / 114272 | training loss: 0.013519207946956158\n",
      "epoch: 1 | 66112 / 114272 | training loss: 0.19517582654953003\n",
      "epoch: 1 | 66144 / 114272 | training loss: 0.4352811574935913\n",
      "epoch: 1 | 66176 / 114272 | training loss: 0.018393533304333687\n",
      "epoch: 1 | 66208 / 114272 | training loss: 0.24020396173000336\n",
      "epoch: 1 | 66240 / 114272 | training loss: 0.023018693551421165\n",
      "epoch: 1 | 66272 / 114272 | training loss: 0.007132077123969793\n",
      "epoch: 1 | 66304 / 114272 | training loss: 0.1462465077638626\n",
      "epoch: 1 | 66336 / 114272 | training loss: 0.1564948409795761\n",
      "epoch: 1 | 66368 / 114272 | training loss: 0.011313685216009617\n",
      "epoch: 1 | 66400 / 114272 | training loss: 0.07355435937643051\n",
      "epoch: 1 | 66432 / 114272 | training loss: 0.07302695512771606\n",
      "epoch: 1 | 66464 / 114272 | training loss: 0.24862287938594818\n",
      "epoch: 1 | 66496 / 114272 | training loss: 0.021043643355369568\n",
      "epoch: 1 | 66528 / 114272 | training loss: 0.1142808198928833\n",
      "epoch: 1 | 66560 / 114272 | training loss: 0.32197535037994385\n",
      "epoch: 1 | 66592 / 114272 | training loss: 0.247805655002594\n",
      "epoch: 1 | 66624 / 114272 | training loss: 0.07138945907354355\n",
      "epoch: 1 | 66656 / 114272 | training loss: 0.016949212178587914\n",
      "epoch: 1 | 66688 / 114272 | training loss: 0.2760038375854492\n",
      "epoch: 1 | 66720 / 114272 | training loss: 0.13855665922164917\n",
      "epoch: 1 | 66752 / 114272 | training loss: 0.007246540393680334\n",
      "epoch: 1 | 66784 / 114272 | training loss: 0.12991833686828613\n",
      "epoch: 1 | 66816 / 114272 | training loss: 0.13144029676914215\n",
      "epoch: 1 | 66848 / 114272 | training loss: 0.05619895085692406\n",
      "epoch: 1 | 66880 / 114272 | training loss: 0.20794492959976196\n",
      "epoch: 1 | 66912 / 114272 | training loss: 0.23471057415008545\n",
      "epoch: 1 | 66944 / 114272 | training loss: 0.1255933940410614\n",
      "epoch: 1 | 66976 / 114272 | training loss: 0.26109036803245544\n",
      "epoch: 1 | 67008 / 114272 | training loss: 0.09277242422103882\n",
      "epoch: 1 | 67040 / 114272 | training loss: 0.0783035159111023\n",
      "epoch: 1 | 67072 / 114272 | training loss: 0.04272755980491638\n",
      "epoch: 1 | 67104 / 114272 | training loss: 0.06783587485551834\n",
      "epoch: 1 | 67136 / 114272 | training loss: 0.06934098154306412\n",
      "epoch: 1 | 67168 / 114272 | training loss: 0.21661891043186188\n",
      "epoch: 1 | 67200 / 114272 | training loss: 0.2035418301820755\n",
      "epoch: 1 | 67232 / 114272 | training loss: 0.11786064505577087\n",
      "epoch: 1 | 67264 / 114272 | training loss: 0.03596660494804382\n",
      "epoch: 1 | 67296 / 114272 | training loss: 0.043273285031318665\n",
      "epoch: 1 | 67328 / 114272 | training loss: 0.11736826598644257\n",
      "epoch: 1 | 67360 / 114272 | training loss: 0.07064579427242279\n",
      "epoch: 1 | 67392 / 114272 | training loss: 0.10549551993608475\n",
      "epoch: 1 | 67424 / 114272 | training loss: 0.03923932835459709\n",
      "epoch: 1 | 67456 / 114272 | training loss: 0.07096585631370544\n",
      "epoch: 1 | 67488 / 114272 | training loss: 0.0870579406619072\n",
      "epoch: 1 | 67520 / 114272 | training loss: 0.022116150707006454\n",
      "epoch: 1 | 67552 / 114272 | training loss: 0.08070822060108185\n",
      "epoch: 1 | 67584 / 114272 | training loss: 0.017883937805891037\n",
      "epoch: 1 | 67616 / 114272 | training loss: 0.06919796019792557\n",
      "epoch: 1 | 67648 / 114272 | training loss: 0.007185875903815031\n",
      "epoch: 1 | 67680 / 114272 | training loss: 0.18512119352817535\n",
      "epoch: 1 | 67712 / 114272 | training loss: 0.17538538575172424\n",
      "epoch: 1 | 67744 / 114272 | training loss: 0.16453997790813446\n",
      "epoch: 1 | 67776 / 114272 | training loss: 0.08458766341209412\n",
      "epoch: 1 | 67808 / 114272 | training loss: 0.172538623213768\n",
      "epoch: 1 | 67840 / 114272 | training loss: 0.08269968628883362\n",
      "epoch: 1 | 67872 / 114272 | training loss: 0.08688536286354065\n",
      "epoch: 1 | 67904 / 114272 | training loss: 0.10113637894392014\n",
      "epoch: 1 | 67936 / 114272 | training loss: 0.04252668470144272\n",
      "epoch: 1 | 67968 / 114272 | training loss: 0.2691970467567444\n",
      "epoch: 1 | 68000 / 114272 | training loss: 0.13846728205680847\n",
      "epoch: 1 | 68032 / 114272 | training loss: 0.3240877687931061\n",
      "epoch: 1 | 68064 / 114272 | training loss: 0.43614277243614197\n",
      "epoch: 1 | 68096 / 114272 | training loss: 0.029329761862754822\n",
      "epoch: 1 | 68128 / 114272 | training loss: 0.07912946492433548\n",
      "epoch: 1 | 68160 / 114272 | training loss: 0.06353258341550827\n",
      "epoch: 1 | 68192 / 114272 | training loss: 0.024581938982009888\n",
      "epoch: 1 | 68224 / 114272 | training loss: 0.08581113815307617\n",
      "epoch: 1 | 68256 / 114272 | training loss: 0.26248306035995483\n",
      "epoch: 1 | 68288 / 114272 | training loss: 0.023615222424268723\n",
      "epoch: 1 | 68320 / 114272 | training loss: 0.2942066788673401\n",
      "epoch: 1 | 68352 / 114272 | training loss: 0.03681834787130356\n",
      "epoch: 1 | 68384 / 114272 | training loss: 0.08573886007070541\n",
      "epoch: 1 | 68416 / 114272 | training loss: 0.14707306027412415\n",
      "epoch: 1 | 68448 / 114272 | training loss: 0.3442486822605133\n",
      "epoch: 1 | 68480 / 114272 | training loss: 0.02618379145860672\n",
      "epoch: 1 | 68512 / 114272 | training loss: 0.227559432387352\n",
      "epoch: 1 | 68544 / 114272 | training loss: 0.014153632335364819\n",
      "epoch: 1 | 68576 / 114272 | training loss: 0.05970801040530205\n",
      "epoch: 1 | 68608 / 114272 | training loss: 0.015282465144991875\n",
      "epoch: 1 | 68640 / 114272 | training loss: 0.13966841995716095\n",
      "epoch: 1 | 68672 / 114272 | training loss: 0.03286803513765335\n",
      "epoch: 1 | 68704 / 114272 | training loss: 0.07166829705238342\n",
      "epoch: 1 | 68736 / 114272 | training loss: 0.012305091135203838\n",
      "epoch: 1 | 68768 / 114272 | training loss: 0.019027238711714745\n",
      "epoch: 1 | 68800 / 114272 | training loss: 0.27241429686546326\n",
      "epoch: 1 | 68832 / 114272 | training loss: 0.1571802794933319\n",
      "epoch: 1 | 68864 / 114272 | training loss: 0.019643379375338554\n",
      "epoch: 1 | 68896 / 114272 | training loss: 0.21087433397769928\n",
      "epoch: 1 | 68928 / 114272 | training loss: 0.0031823625322431326\n",
      "epoch: 1 | 68960 / 114272 | training loss: 0.08871866017580032\n",
      "epoch: 1 | 68992 / 114272 | training loss: 0.04666129872202873\n",
      "epoch: 1 | 69024 / 114272 | training loss: 0.010447787120938301\n",
      "epoch: 1 | 69056 / 114272 | training loss: 0.2688053250312805\n",
      "epoch: 1 | 69088 / 114272 | training loss: 0.08977358043193817\n",
      "epoch: 1 | 69120 / 114272 | training loss: 0.002225085161626339\n",
      "epoch: 1 | 69152 / 114272 | training loss: 0.011439249850809574\n",
      "epoch: 1 | 69184 / 114272 | training loss: 0.06592518836259842\n",
      "epoch: 1 | 69216 / 114272 | training loss: 0.03843133524060249\n",
      "epoch: 1 | 69248 / 114272 | training loss: 0.17109590768814087\n",
      "epoch: 1 | 69280 / 114272 | training loss: 0.05832401290535927\n",
      "epoch: 1 | 69312 / 114272 | training loss: 0.20410612225532532\n",
      "epoch: 1 | 69344 / 114272 | training loss: 0.14973081648349762\n",
      "epoch: 1 | 69376 / 114272 | training loss: 0.1811579167842865\n",
      "epoch: 1 | 69408 / 114272 | training loss: 0.10121284425258636\n",
      "epoch: 1 | 69440 / 114272 | training loss: 0.06544453650712967\n",
      "epoch: 1 | 69472 / 114272 | training loss: 0.2935355603694916\n",
      "epoch: 1 | 69504 / 114272 | training loss: 0.017819786444306374\n",
      "epoch: 1 | 69536 / 114272 | training loss: 0.02504255436360836\n",
      "epoch: 1 | 69568 / 114272 | training loss: 0.0252175722271204\n",
      "epoch: 1 | 69600 / 114272 | training loss: 0.025077542290091515\n",
      "epoch: 1 | 69632 / 114272 | training loss: 0.06549093872308731\n",
      "epoch: 1 | 69664 / 114272 | training loss: 0.17757628858089447\n",
      "epoch: 1 | 69696 / 114272 | training loss: 0.02133243903517723\n",
      "epoch: 1 | 69728 / 114272 | training loss: 0.29443758726119995\n",
      "epoch: 1 | 69760 / 114272 | training loss: 0.16294731199741364\n",
      "epoch: 1 | 69792 / 114272 | training loss: 0.006705267354846001\n",
      "epoch: 1 | 69824 / 114272 | training loss: 0.07708029448986053\n",
      "epoch: 1 | 69856 / 114272 | training loss: 0.08101809024810791\n",
      "epoch: 1 | 69888 / 114272 | training loss: 0.10588110983371735\n",
      "epoch: 1 | 69920 / 114272 | training loss: 0.030421214178204536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 69952 / 114272 | training loss: 0.03567367419600487\n",
      "epoch: 1 | 69984 / 114272 | training loss: 0.021876905113458633\n",
      "epoch: 1 | 70016 / 114272 | training loss: 0.23524653911590576\n",
      "epoch: 1 | 70048 / 114272 | training loss: 0.13490897417068481\n",
      "epoch: 1 | 70080 / 114272 | training loss: 0.011623525060713291\n",
      "epoch: 1 | 70112 / 114272 | training loss: 0.029840774834156036\n",
      "epoch: 1 | 70144 / 114272 | training loss: 0.0988311618566513\n",
      "epoch: 1 | 70176 / 114272 | training loss: 0.06749705970287323\n",
      "epoch: 1 | 70208 / 114272 | training loss: 0.30471253395080566\n",
      "epoch: 1 | 70240 / 114272 | training loss: 0.14318060874938965\n",
      "epoch: 1 | 70272 / 114272 | training loss: 0.01039513386785984\n",
      "epoch: 1 | 70304 / 114272 | training loss: 0.012574859894812107\n",
      "epoch: 1 | 70336 / 114272 | training loss: 0.05061560124158859\n",
      "epoch: 1 | 70368 / 114272 | training loss: 0.009311498142778873\n",
      "epoch: 1 | 70400 / 114272 | training loss: 0.08258583396673203\n",
      "epoch: 1 | 70432 / 114272 | training loss: 0.012638691812753677\n",
      "epoch: 1 | 70464 / 114272 | training loss: 0.021273108199238777\n",
      "epoch: 1 | 70496 / 114272 | training loss: 0.009751943871378899\n",
      "epoch: 1 | 70528 / 114272 | training loss: 0.17149968445301056\n",
      "epoch: 1 | 70560 / 114272 | training loss: 0.0689714327454567\n",
      "epoch: 1 | 70592 / 114272 | training loss: 0.07529377192258835\n",
      "epoch: 1 | 70624 / 114272 | training loss: 0.07895521819591522\n",
      "epoch: 1 | 70656 / 114272 | training loss: 0.08511380106210709\n",
      "epoch: 1 | 70688 / 114272 | training loss: 0.15275727212429047\n",
      "epoch: 1 | 70720 / 114272 | training loss: 0.10354584455490112\n",
      "epoch: 1 | 70752 / 114272 | training loss: 0.29891568422317505\n",
      "epoch: 1 | 70784 / 114272 | training loss: 0.02715810388326645\n",
      "epoch: 1 | 70816 / 114272 | training loss: 0.12034308165311813\n",
      "epoch: 1 | 70848 / 114272 | training loss: 0.017514830455183983\n",
      "epoch: 1 | 70880 / 114272 | training loss: 0.013582006096839905\n",
      "epoch: 1 | 70912 / 114272 | training loss: 0.006638293154537678\n",
      "epoch: 1 | 70944 / 114272 | training loss: 0.25364238023757935\n",
      "epoch: 1 | 70976 / 114272 | training loss: 0.20010994374752045\n",
      "epoch: 1 | 71008 / 114272 | training loss: 0.18382994830608368\n",
      "epoch: 1 | 71040 / 114272 | training loss: 0.25143492221832275\n",
      "epoch: 1 | 71072 / 114272 | training loss: 0.17073595523834229\n",
      "epoch: 1 | 71104 / 114272 | training loss: 0.026631740853190422\n",
      "epoch: 1 | 71136 / 114272 | training loss: 0.0894845649600029\n",
      "epoch: 1 | 71168 / 114272 | training loss: 0.3677171468734741\n",
      "epoch: 1 | 71200 / 114272 | training loss: 0.22542740404605865\n",
      "epoch: 1 | 71232 / 114272 | training loss: 0.158760666847229\n",
      "epoch: 1 | 71264 / 114272 | training loss: 0.08984819799661636\n",
      "epoch: 1 | 71296 / 114272 | training loss: 0.25096043944358826\n",
      "epoch: 1 | 71328 / 114272 | training loss: 0.04059179872274399\n",
      "epoch: 1 | 71360 / 114272 | training loss: 0.12750308215618134\n",
      "epoch: 1 | 71392 / 114272 | training loss: 0.013455653563141823\n",
      "epoch: 1 | 71424 / 114272 | training loss: 0.010727818123996258\n",
      "epoch: 1 | 71456 / 114272 | training loss: 0.08986999839544296\n",
      "epoch: 1 | 71488 / 114272 | training loss: 0.0269775353372097\n",
      "epoch: 1 | 71520 / 114272 | training loss: 0.22486943006515503\n",
      "epoch: 1 | 71552 / 114272 | training loss: 0.027010450139641762\n",
      "epoch: 1 | 71584 / 114272 | training loss: 0.18782934546470642\n",
      "epoch: 1 | 71616 / 114272 | training loss: 0.09299110621213913\n",
      "epoch: 1 | 71648 / 114272 | training loss: 0.29817020893096924\n",
      "epoch: 1 | 71680 / 114272 | training loss: 0.020922351628541946\n",
      "epoch: 1 | 71712 / 114272 | training loss: 0.03181876987218857\n",
      "epoch: 1 | 71744 / 114272 | training loss: 0.08177797496318817\n",
      "epoch: 1 | 71776 / 114272 | training loss: 0.19127433001995087\n",
      "epoch: 1 | 71808 / 114272 | training loss: 0.08380313217639923\n",
      "epoch: 1 | 71840 / 114272 | training loss: 0.2400907576084137\n",
      "epoch: 1 | 71872 / 114272 | training loss: 0.02823433093726635\n",
      "epoch: 1 | 71904 / 114272 | training loss: 0.015141000039875507\n",
      "epoch: 1 | 71936 / 114272 | training loss: 0.09391109645366669\n",
      "epoch: 1 | 71968 / 114272 | training loss: 0.10356081277132034\n",
      "epoch: 1 | 72000 / 114272 | training loss: 0.3361927270889282\n",
      "epoch: 1 | 72032 / 114272 | training loss: 0.16851040720939636\n",
      "epoch: 1 | 72064 / 114272 | training loss: 0.4069662392139435\n",
      "epoch: 1 | 72096 / 114272 | training loss: 0.08696168661117554\n",
      "epoch: 1 | 72128 / 114272 | training loss: 0.04708413779735565\n",
      "epoch: 1 | 72160 / 114272 | training loss: 0.060209598392248154\n",
      "epoch: 1 | 72192 / 114272 | training loss: 0.00589776411652565\n",
      "epoch: 1 | 72224 / 114272 | training loss: 0.10646847635507584\n",
      "epoch: 1 | 72256 / 114272 | training loss: 0.13517501950263977\n",
      "epoch: 1 | 72288 / 114272 | training loss: 0.21859529614448547\n",
      "epoch: 1 | 72320 / 114272 | training loss: 0.017691507935523987\n",
      "epoch: 1 | 72352 / 114272 | training loss: 0.17974507808685303\n",
      "epoch: 1 | 72384 / 114272 | training loss: 0.008650443516671658\n",
      "epoch: 1 | 72416 / 114272 | training loss: 0.09022015333175659\n",
      "epoch: 1 | 72448 / 114272 | training loss: 0.01251314114779234\n",
      "epoch: 1 | 72480 / 114272 | training loss: 0.15329015254974365\n",
      "epoch: 1 | 72512 / 114272 | training loss: 0.11032846570014954\n",
      "epoch: 1 | 72544 / 114272 | training loss: 0.13243713974952698\n",
      "epoch: 1 | 72576 / 114272 | training loss: 0.03754659742116928\n",
      "epoch: 1 | 72608 / 114272 | training loss: 0.015256178565323353\n",
      "epoch: 1 | 72640 / 114272 | training loss: 0.2820199429988861\n",
      "epoch: 1 | 72672 / 114272 | training loss: 0.05621526390314102\n",
      "epoch: 1 | 72704 / 114272 | training loss: 0.04953726381063461\n",
      "epoch: 1 | 72736 / 114272 | training loss: 0.13283312320709229\n",
      "epoch: 1 | 72768 / 114272 | training loss: 0.06426063179969788\n",
      "epoch: 1 | 72800 / 114272 | training loss: 0.023660974577069283\n",
      "epoch: 1 | 72832 / 114272 | training loss: 0.3834165334701538\n",
      "epoch: 1 | 72864 / 114272 | training loss: 0.09039389342069626\n",
      "epoch: 1 | 72896 / 114272 | training loss: 0.031175613403320312\n",
      "epoch: 1 | 72928 / 114272 | training loss: 0.015151825733482838\n",
      "epoch: 1 | 72960 / 114272 | training loss: 0.29220542311668396\n",
      "epoch: 1 | 72992 / 114272 | training loss: 0.017958100885152817\n",
      "epoch: 1 | 73024 / 114272 | training loss: 0.05510054901242256\n",
      "epoch: 1 | 73056 / 114272 | training loss: 0.32594749331474304\n",
      "epoch: 1 | 73088 / 114272 | training loss: 0.17231804132461548\n",
      "epoch: 1 | 73120 / 114272 | training loss: 0.2139810174703598\n",
      "epoch: 1 | 73152 / 114272 | training loss: 0.026479506865143776\n",
      "epoch: 1 | 73184 / 114272 | training loss: 0.11193898320198059\n",
      "epoch: 1 | 73216 / 114272 | training loss: 0.18644624948501587\n",
      "epoch: 1 | 73248 / 114272 | training loss: 0.21024149656295776\n",
      "epoch: 1 | 73280 / 114272 | training loss: 0.20360596477985382\n",
      "epoch: 1 | 73312 / 114272 | training loss: 0.02338968962430954\n",
      "epoch: 1 | 73344 / 114272 | training loss: 0.20109598338603973\n",
      "epoch: 1 | 73376 / 114272 | training loss: 0.07598903775215149\n",
      "epoch: 1 | 73408 / 114272 | training loss: 0.2038416862487793\n",
      "epoch: 1 | 73440 / 114272 | training loss: 0.10358396172523499\n",
      "epoch: 1 | 73472 / 114272 | training loss: 0.11576176434755325\n",
      "epoch: 1 | 73504 / 114272 | training loss: 0.09184051305055618\n",
      "epoch: 1 | 73536 / 114272 | training loss: 0.023046083748340607\n",
      "epoch: 1 | 73568 / 114272 | training loss: 0.1661657989025116\n",
      "epoch: 1 | 73600 / 114272 | training loss: 0.08859281241893768\n",
      "epoch: 1 | 73632 / 114272 | training loss: 0.3568931519985199\n",
      "epoch: 1 | 73664 / 114272 | training loss: 0.0067034089006483555\n",
      "epoch: 1 | 73696 / 114272 | training loss: 0.019355500116944313\n",
      "epoch: 1 | 73728 / 114272 | training loss: 0.024398811161518097\n",
      "epoch: 1 | 73760 / 114272 | training loss: 0.08359784632921219\n",
      "epoch: 1 | 73792 / 114272 | training loss: 0.045596763491630554\n",
      "epoch: 1 | 73824 / 114272 | training loss: 0.07073792070150375\n",
      "epoch: 1 | 73856 / 114272 | training loss: 0.014160390943288803\n",
      "epoch: 1 | 73888 / 114272 | training loss: 0.09209559112787247\n",
      "epoch: 1 | 73920 / 114272 | training loss: 0.4712063670158386\n",
      "epoch: 1 | 73952 / 114272 | training loss: 0.026686077937483788\n",
      "epoch: 1 | 73984 / 114272 | training loss: 0.03645886853337288\n",
      "epoch: 1 | 74016 / 114272 | training loss: 0.3452528119087219\n",
      "epoch: 1 | 74048 / 114272 | training loss: 0.05569370463490486\n",
      "epoch: 1 | 74080 / 114272 | training loss: 0.10458339005708694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 74112 / 114272 | training loss: 0.09591430425643921\n",
      "epoch: 1 | 74144 / 114272 | training loss: 0.36451277136802673\n",
      "epoch: 1 | 74176 / 114272 | training loss: 0.29818832874298096\n",
      "epoch: 1 | 74208 / 114272 | training loss: 0.10693693906068802\n",
      "epoch: 1 | 74240 / 114272 | training loss: 0.29141902923583984\n",
      "epoch: 1 | 74272 / 114272 | training loss: 0.0416112095117569\n",
      "epoch: 1 | 74304 / 114272 | training loss: 0.006904202979058027\n",
      "epoch: 1 | 74336 / 114272 | training loss: 0.16011227667331696\n",
      "epoch: 1 | 74368 / 114272 | training loss: 0.013515584170818329\n",
      "epoch: 1 | 74400 / 114272 | training loss: 0.08459560573101044\n",
      "epoch: 1 | 74432 / 114272 | training loss: 0.2102658897638321\n",
      "epoch: 1 | 74464 / 114272 | training loss: 0.243601456284523\n",
      "epoch: 1 | 74496 / 114272 | training loss: 0.01260626781731844\n",
      "epoch: 1 | 74528 / 114272 | training loss: 0.13195039331912994\n",
      "epoch: 1 | 74560 / 114272 | training loss: 0.0930437445640564\n",
      "epoch: 1 | 74592 / 114272 | training loss: 0.3315984010696411\n",
      "epoch: 1 | 74624 / 114272 | training loss: 0.06819555163383484\n",
      "epoch: 1 | 74656 / 114272 | training loss: 0.08012156188488007\n",
      "epoch: 1 | 74688 / 114272 | training loss: 0.12507317960262299\n",
      "epoch: 1 | 74720 / 114272 | training loss: 0.056737322360277176\n",
      "epoch: 1 | 74752 / 114272 | training loss: 0.044367920607328415\n",
      "epoch: 1 | 74784 / 114272 | training loss: 0.017207052558660507\n",
      "epoch: 1 | 74816 / 114272 | training loss: 0.1871039867401123\n",
      "epoch: 1 | 74848 / 114272 | training loss: 0.12377507239580154\n",
      "epoch: 1 | 74880 / 114272 | training loss: 0.09154919534921646\n",
      "epoch: 1 | 74912 / 114272 | training loss: 0.07807491719722748\n",
      "epoch: 1 | 74944 / 114272 | training loss: 0.016012366861104965\n",
      "epoch: 1 | 74976 / 114272 | training loss: 0.06468518078327179\n",
      "epoch: 1 | 75008 / 114272 | training loss: 0.1851113736629486\n",
      "epoch: 1 | 75040 / 114272 | training loss: 0.013224195688962936\n",
      "epoch: 1 | 75072 / 114272 | training loss: 0.10699313879013062\n",
      "epoch: 1 | 75104 / 114272 | training loss: 0.07619745284318924\n",
      "epoch: 1 | 75136 / 114272 | training loss: 0.1939626783132553\n",
      "epoch: 1 | 75168 / 114272 | training loss: 0.08599529415369034\n",
      "epoch: 1 | 75200 / 114272 | training loss: 0.03564412519335747\n",
      "epoch: 1 | 75232 / 114272 | training loss: 0.08717196434736252\n",
      "epoch: 1 | 75264 / 114272 | training loss: 0.15697629749774933\n",
      "epoch: 1 | 75296 / 114272 | training loss: 0.34295734763145447\n",
      "epoch: 1 | 75328 / 114272 | training loss: 0.13943026959896088\n",
      "epoch: 1 | 75360 / 114272 | training loss: 0.029139366000890732\n",
      "epoch: 1 | 75392 / 114272 | training loss: 0.08789322525262833\n",
      "epoch: 1 | 75424 / 114272 | training loss: 0.3119419515132904\n",
      "epoch: 1 | 75456 / 114272 | training loss: 0.19859695434570312\n",
      "epoch: 1 | 75488 / 114272 | training loss: 0.16023407876491547\n",
      "epoch: 1 | 75520 / 114272 | training loss: 0.19890795648097992\n",
      "epoch: 1 | 75552 / 114272 | training loss: 0.015066915191709995\n",
      "epoch: 1 | 75584 / 114272 | training loss: 0.02554982341825962\n",
      "epoch: 1 | 75616 / 114272 | training loss: 0.33367273211479187\n",
      "epoch: 1 | 75648 / 114272 | training loss: 0.018061010167002678\n",
      "epoch: 1 | 75680 / 114272 | training loss: 0.18747800588607788\n",
      "epoch: 1 | 75712 / 114272 | training loss: 0.011584918946027756\n",
      "epoch: 1 | 75744 / 114272 | training loss: 0.0415123887360096\n",
      "epoch: 1 | 75776 / 114272 | training loss: 0.008613375946879387\n",
      "epoch: 1 | 75808 / 114272 | training loss: 0.019325781613588333\n",
      "epoch: 1 | 75840 / 114272 | training loss: 0.1301824301481247\n",
      "epoch: 1 | 75872 / 114272 | training loss: 0.19140353798866272\n",
      "epoch: 1 | 75904 / 114272 | training loss: 0.006367230322211981\n",
      "epoch: 1 | 75936 / 114272 | training loss: 0.05329481139779091\n",
      "epoch: 1 | 75968 / 114272 | training loss: 0.06738552451133728\n",
      "epoch: 1 | 76000 / 114272 | training loss: 0.22515521943569183\n",
      "epoch: 1 | 76032 / 114272 | training loss: 0.15443196892738342\n",
      "epoch: 1 | 76064 / 114272 | training loss: 0.1754189133644104\n",
      "epoch: 1 | 76096 / 114272 | training loss: 0.3202809989452362\n",
      "epoch: 1 | 76128 / 114272 | training loss: 0.3714996874332428\n",
      "epoch: 1 | 76160 / 114272 | training loss: 0.009514905512332916\n",
      "epoch: 1 | 76192 / 114272 | training loss: 0.1599682867527008\n",
      "epoch: 1 | 76224 / 114272 | training loss: 0.00435295607894659\n",
      "epoch: 1 | 76256 / 114272 | training loss: 0.2536661624908447\n",
      "epoch: 1 | 76288 / 114272 | training loss: 0.17959846556186676\n",
      "epoch: 1 | 76320 / 114272 | training loss: 0.004339728970080614\n",
      "epoch: 1 | 76352 / 114272 | training loss: 0.03789133206009865\n",
      "epoch: 1 | 76384 / 114272 | training loss: 0.05402282997965813\n",
      "epoch: 1 | 76416 / 114272 | training loss: 0.08115982264280319\n",
      "epoch: 1 | 76448 / 114272 | training loss: 0.18919970095157623\n",
      "epoch: 1 | 76480 / 114272 | training loss: 0.20256833732128143\n",
      "epoch: 1 | 76512 / 114272 | training loss: 0.05245479568839073\n",
      "epoch: 1 | 76544 / 114272 | training loss: 0.03228958696126938\n",
      "epoch: 1 | 76576 / 114272 | training loss: 0.039084311574697495\n",
      "epoch: 1 | 76608 / 114272 | training loss: 0.02726166695356369\n",
      "epoch: 1 | 76640 / 114272 | training loss: 0.08101215958595276\n",
      "epoch: 1 | 76672 / 114272 | training loss: 0.1632450968027115\n",
      "epoch: 1 | 76704 / 114272 | training loss: 0.06981373578310013\n",
      "epoch: 1 | 76736 / 114272 | training loss: 0.05079355835914612\n",
      "epoch: 1 | 76768 / 114272 | training loss: 0.07594064623117447\n",
      "epoch: 1 | 76800 / 114272 | training loss: 0.01271765399724245\n",
      "epoch: 1 | 76832 / 114272 | training loss: 0.21034711599349976\n",
      "epoch: 1 | 76864 / 114272 | training loss: 0.045833196491003036\n",
      "epoch: 1 | 76896 / 114272 | training loss: 0.19736547768115997\n",
      "epoch: 1 | 76928 / 114272 | training loss: 0.14591938257217407\n",
      "epoch: 1 | 76960 / 114272 | training loss: 0.15387094020843506\n",
      "epoch: 1 | 76992 / 114272 | training loss: 0.1841842085123062\n",
      "epoch: 1 | 77024 / 114272 | training loss: 0.10612773150205612\n",
      "epoch: 1 | 77056 / 114272 | training loss: 0.1368810087442398\n",
      "epoch: 1 | 77088 / 114272 | training loss: 0.008100011385977268\n",
      "epoch: 1 | 77120 / 114272 | training loss: 0.012990628369152546\n",
      "epoch: 1 | 77152 / 114272 | training loss: 0.0966159924864769\n",
      "epoch: 1 | 77184 / 114272 | training loss: 0.08429547399282455\n",
      "epoch: 1 | 77216 / 114272 | training loss: 0.22225697338581085\n",
      "epoch: 1 | 77248 / 114272 | training loss: 0.01755773089826107\n",
      "epoch: 1 | 77280 / 114272 | training loss: 0.1013953685760498\n",
      "epoch: 1 | 77312 / 114272 | training loss: 0.18574020266532898\n",
      "epoch: 1 | 77344 / 114272 | training loss: 0.023024102672934532\n",
      "epoch: 1 | 77376 / 114272 | training loss: 0.18950079381465912\n",
      "epoch: 1 | 77408 / 114272 | training loss: 0.03564552217721939\n",
      "epoch: 1 | 77440 / 114272 | training loss: 0.012647935189306736\n",
      "epoch: 1 | 77472 / 114272 | training loss: 0.007198314648121595\n",
      "epoch: 1 | 77504 / 114272 | training loss: 0.08104398846626282\n",
      "epoch: 1 | 77536 / 114272 | training loss: 0.3598039150238037\n",
      "epoch: 1 | 77568 / 114272 | training loss: 0.025746596977114677\n",
      "epoch: 1 | 77600 / 114272 | training loss: 0.012085683643817902\n",
      "epoch: 1 | 77632 / 114272 | training loss: 0.019480610266327858\n",
      "epoch: 1 | 77664 / 114272 | training loss: 0.007378385867923498\n",
      "epoch: 1 | 77696 / 114272 | training loss: 0.04183847829699516\n",
      "epoch: 1 | 77728 / 114272 | training loss: 0.10841688513755798\n",
      "epoch: 1 | 77760 / 114272 | training loss: 0.020198220387101173\n",
      "epoch: 1 | 77792 / 114272 | training loss: 0.004383836407214403\n",
      "epoch: 1 | 77824 / 114272 | training loss: 0.0053245509043335915\n",
      "epoch: 1 | 77856 / 114272 | training loss: 0.003255431540310383\n",
      "epoch: 1 | 77888 / 114272 | training loss: 0.03465721011161804\n",
      "epoch: 1 | 77920 / 114272 | training loss: 0.23045918345451355\n",
      "epoch: 1 | 77952 / 114272 | training loss: 0.01493720430880785\n",
      "epoch: 1 | 77984 / 114272 | training loss: 0.08508290350437164\n",
      "epoch: 1 | 78016 / 114272 | training loss: 0.3396432399749756\n",
      "epoch: 1 | 78048 / 114272 | training loss: 0.024335810914635658\n",
      "epoch: 1 | 78080 / 114272 | training loss: 0.01428704708814621\n",
      "epoch: 1 | 78112 / 114272 | training loss: 0.32640430331230164\n",
      "epoch: 1 | 78144 / 114272 | training loss: 0.17995062470436096\n",
      "epoch: 1 | 78176 / 114272 | training loss: 0.6844210624694824\n",
      "epoch: 1 | 78208 / 114272 | training loss: 0.013548160903155804\n",
      "epoch: 1 | 78240 / 114272 | training loss: 0.019001852720975876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 78272 / 114272 | training loss: 0.3837432861328125\n",
      "epoch: 1 | 78304 / 114272 | training loss: 0.3218289613723755\n",
      "epoch: 1 | 78336 / 114272 | training loss: 0.05385111644864082\n",
      "epoch: 1 | 78368 / 114272 | training loss: 0.011207520961761475\n",
      "epoch: 1 | 78400 / 114272 | training loss: 0.006614715792238712\n",
      "epoch: 1 | 78432 / 114272 | training loss: 0.10559149831533432\n",
      "epoch: 1 | 78464 / 114272 | training loss: 0.23937393724918365\n",
      "epoch: 1 | 78496 / 114272 | training loss: 0.029073653742671013\n",
      "epoch: 1 | 78528 / 114272 | training loss: 0.09816315025091171\n",
      "epoch: 1 | 78560 / 114272 | training loss: 0.12579764425754547\n",
      "epoch: 1 | 78592 / 114272 | training loss: 0.1016947478055954\n",
      "epoch: 1 | 78624 / 114272 | training loss: 0.16008232533931732\n",
      "epoch: 1 | 78656 / 114272 | training loss: 0.0191044844686985\n",
      "epoch: 1 | 78688 / 114272 | training loss: 0.19320809841156006\n",
      "epoch: 1 | 78720 / 114272 | training loss: 0.11354715377092361\n",
      "epoch: 1 | 78752 / 114272 | training loss: 0.02109433151781559\n",
      "epoch: 1 | 78784 / 114272 | training loss: 0.11830605566501617\n",
      "epoch: 1 | 78816 / 114272 | training loss: 0.19128546118736267\n",
      "epoch: 1 | 78848 / 114272 | training loss: 0.21389427781105042\n",
      "epoch: 1 | 78880 / 114272 | training loss: 0.07241915911436081\n",
      "epoch: 1 | 78912 / 114272 | training loss: 0.21905910968780518\n",
      "epoch: 1 | 78944 / 114272 | training loss: 0.16759173572063446\n",
      "epoch: 1 | 78976 / 114272 | training loss: 0.010803710669279099\n",
      "epoch: 1 | 79008 / 114272 | training loss: 0.03008764050900936\n",
      "epoch: 1 | 79040 / 114272 | training loss: 0.0679062157869339\n",
      "epoch: 1 | 79072 / 114272 | training loss: 0.023337770253419876\n",
      "epoch: 1 | 79104 / 114272 | training loss: 0.13315260410308838\n",
      "epoch: 1 | 79136 / 114272 | training loss: 0.06874997913837433\n",
      "epoch: 1 | 79168 / 114272 | training loss: 0.0667821615934372\n",
      "epoch: 1 | 79200 / 114272 | training loss: 0.15752238035202026\n",
      "epoch: 1 | 79232 / 114272 | training loss: 0.25950387120246887\n",
      "epoch: 1 | 79264 / 114272 | training loss: 0.031011849641799927\n",
      "epoch: 1 | 79296 / 114272 | training loss: 0.08676878362894058\n",
      "epoch: 1 | 79328 / 114272 | training loss: 0.2789548337459564\n",
      "epoch: 1 | 79360 / 114272 | training loss: 0.144892156124115\n",
      "epoch: 1 | 79392 / 114272 | training loss: 0.07742113620042801\n",
      "epoch: 1 | 79424 / 114272 | training loss: 0.10469871759414673\n",
      "epoch: 1 | 79456 / 114272 | training loss: 0.23832659423351288\n",
      "epoch: 1 | 79488 / 114272 | training loss: 0.20827648043632507\n",
      "epoch: 1 | 79520 / 114272 | training loss: 0.11842844635248184\n",
      "epoch: 1 | 79552 / 114272 | training loss: 0.3290124833583832\n",
      "epoch: 1 | 79584 / 114272 | training loss: 0.10977375507354736\n",
      "epoch: 1 | 79616 / 114272 | training loss: 0.06917297840118408\n",
      "epoch: 1 | 79648 / 114272 | training loss: 0.024627596139907837\n",
      "epoch: 1 | 79680 / 114272 | training loss: 0.07577799260616302\n",
      "epoch: 1 | 79712 / 114272 | training loss: 0.21520859003067017\n",
      "epoch: 1 | 79744 / 114272 | training loss: 0.11112595349550247\n",
      "epoch: 1 | 79776 / 114272 | training loss: 0.10848918557167053\n",
      "epoch: 1 | 79808 / 114272 | training loss: 0.1315186470746994\n",
      "epoch: 1 | 79840 / 114272 | training loss: 0.04692693427205086\n",
      "epoch: 1 | 79872 / 114272 | training loss: 0.25399094820022583\n",
      "epoch: 1 | 79904 / 114272 | training loss: 0.022283487021923065\n",
      "epoch: 1 | 79936 / 114272 | training loss: 0.12345170229673386\n",
      "epoch: 1 | 79968 / 114272 | training loss: 0.11463113129138947\n",
      "epoch: 1 | 80000 / 114272 | training loss: 0.14413146674633026\n",
      "epoch: 1 | 80032 / 114272 | training loss: 0.012546214275062084\n",
      "epoch: 1 | 80064 / 114272 | training loss: 0.11540652811527252\n",
      "epoch: 1 | 80096 / 114272 | training loss: 0.12757976353168488\n",
      "epoch: 1 | 80128 / 114272 | training loss: 0.18700926005840302\n",
      "epoch: 1 | 80160 / 114272 | training loss: 0.11998145282268524\n",
      "epoch: 1 | 80192 / 114272 | training loss: 0.3190844655036926\n",
      "epoch: 1 | 80224 / 114272 | training loss: 0.10738424211740494\n",
      "epoch: 1 | 80256 / 114272 | training loss: 0.13948845863342285\n",
      "epoch: 1 | 80288 / 114272 | training loss: 0.019802235066890717\n",
      "epoch: 1 | 80320 / 114272 | training loss: 0.09476611763238907\n",
      "epoch: 1 | 80352 / 114272 | training loss: 0.08138081431388855\n",
      "epoch: 1 | 80384 / 114272 | training loss: 0.1113564670085907\n",
      "epoch: 1 | 80416 / 114272 | training loss: 0.09946513921022415\n",
      "epoch: 1 | 80448 / 114272 | training loss: 0.01967708021402359\n",
      "epoch: 1 | 80480 / 114272 | training loss: 0.0255085751414299\n",
      "epoch: 1 | 80512 / 114272 | training loss: 0.026512090116739273\n",
      "epoch: 1 | 80544 / 114272 | training loss: 0.2915681004524231\n",
      "epoch: 1 | 80576 / 114272 | training loss: 0.20553934574127197\n",
      "epoch: 1 | 80608 / 114272 | training loss: 0.1147955134510994\n",
      "epoch: 1 | 80640 / 114272 | training loss: 0.019325368106365204\n",
      "epoch: 1 | 80672 / 114272 | training loss: 0.02229936234652996\n",
      "epoch: 1 | 80704 / 114272 | training loss: 0.1390855610370636\n",
      "epoch: 1 | 80736 / 114272 | training loss: 0.08571379631757736\n",
      "epoch: 1 | 80768 / 114272 | training loss: 0.050883449614048004\n",
      "epoch: 1 | 80800 / 114272 | training loss: 0.03020274080336094\n",
      "epoch: 1 | 80832 / 114272 | training loss: 0.01657048985362053\n",
      "epoch: 1 | 80864 / 114272 | training loss: 0.014938060194253922\n",
      "epoch: 1 | 80896 / 114272 | training loss: 0.13390691578388214\n",
      "epoch: 1 | 80928 / 114272 | training loss: 0.0427517294883728\n",
      "epoch: 1 | 80960 / 114272 | training loss: 0.14145112037658691\n",
      "epoch: 1 | 80992 / 114272 | training loss: 0.06375272572040558\n",
      "epoch: 1 | 81024 / 114272 | training loss: 0.046207185834646225\n",
      "epoch: 1 | 81056 / 114272 | training loss: 0.00792340375483036\n",
      "epoch: 1 | 81088 / 114272 | training loss: 0.09669146686792374\n",
      "epoch: 1 | 81120 / 114272 | training loss: 0.07769832015037537\n",
      "epoch: 1 | 81152 / 114272 | training loss: 0.08566801995038986\n",
      "epoch: 1 | 81184 / 114272 | training loss: 0.08028651773929596\n",
      "epoch: 1 | 81216 / 114272 | training loss: 0.1757950484752655\n",
      "epoch: 1 | 81248 / 114272 | training loss: 0.01265027653425932\n",
      "epoch: 1 | 81280 / 114272 | training loss: 0.11469320952892303\n",
      "epoch: 1 | 81312 / 114272 | training loss: 0.055909980088472366\n",
      "epoch: 1 | 81344 / 114272 | training loss: 0.007984810508787632\n",
      "epoch: 1 | 81376 / 114272 | training loss: 0.2887361943721771\n",
      "epoch: 1 | 81408 / 114272 | training loss: 0.15620547533035278\n",
      "epoch: 1 | 81440 / 114272 | training loss: 0.19285818934440613\n",
      "epoch: 1 | 81472 / 114272 | training loss: 0.06916514784097672\n",
      "epoch: 1 | 81504 / 114272 | training loss: 0.1478046029806137\n",
      "epoch: 1 | 81536 / 114272 | training loss: 0.029415270313620567\n",
      "epoch: 1 | 81568 / 114272 | training loss: 0.2552212178707123\n",
      "epoch: 1 | 81600 / 114272 | training loss: 0.27770477533340454\n",
      "epoch: 1 | 81632 / 114272 | training loss: 0.009603869169950485\n",
      "epoch: 1 | 81664 / 114272 | training loss: 0.0338633768260479\n",
      "epoch: 1 | 81696 / 114272 | training loss: 0.08930842578411102\n",
      "epoch: 1 | 81728 / 114272 | training loss: 0.015352614223957062\n",
      "epoch: 1 | 81760 / 114272 | training loss: 0.04732365906238556\n",
      "epoch: 1 | 81792 / 114272 | training loss: 0.014283456839621067\n",
      "epoch: 1 | 81824 / 114272 | training loss: 0.28177183866500854\n",
      "epoch: 1 | 81856 / 114272 | training loss: 0.002730319742113352\n",
      "epoch: 1 | 81888 / 114272 | training loss: 0.06360921263694763\n",
      "epoch: 1 | 81920 / 114272 | training loss: 0.04283815249800682\n",
      "epoch: 1 | 81952 / 114272 | training loss: 0.07080889493227005\n",
      "epoch: 1 | 81984 / 114272 | training loss: 0.08970091491937637\n",
      "epoch: 1 | 82016 / 114272 | training loss: 0.02755337581038475\n",
      "epoch: 1 | 82048 / 114272 | training loss: 0.18104234337806702\n",
      "epoch: 1 | 82080 / 114272 | training loss: 0.5337653756141663\n",
      "epoch: 1 | 82112 / 114272 | training loss: 0.015080530196428299\n",
      "epoch: 1 | 82144 / 114272 | training loss: 0.14624620974063873\n",
      "epoch: 1 | 82176 / 114272 | training loss: 0.2086922973394394\n",
      "epoch: 1 | 82208 / 114272 | training loss: 0.14401337504386902\n",
      "epoch: 1 | 82240 / 114272 | training loss: 0.255622535943985\n",
      "epoch: 1 | 82272 / 114272 | training loss: 0.023204021155834198\n",
      "epoch: 1 | 82304 / 114272 | training loss: 0.07371000200510025\n",
      "epoch: 1 | 82336 / 114272 | training loss: 0.07934847474098206\n",
      "epoch: 1 | 82368 / 114272 | training loss: 0.00537272309884429\n",
      "epoch: 1 | 82400 / 114272 | training loss: 0.14073920249938965\n",
      "epoch: 1 | 82432 / 114272 | training loss: 0.023288728669285774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 82464 / 114272 | training loss: 0.19193105399608612\n",
      "epoch: 1 | 82496 / 114272 | training loss: 0.2933160960674286\n",
      "epoch: 1 | 82528 / 114272 | training loss: 0.02129775658249855\n",
      "epoch: 1 | 82560 / 114272 | training loss: 0.029661674052476883\n",
      "epoch: 1 | 82592 / 114272 | training loss: 0.08572763949632645\n",
      "epoch: 1 | 82624 / 114272 | training loss: 0.05131068825721741\n",
      "epoch: 1 | 82656 / 114272 | training loss: 0.18834471702575684\n",
      "epoch: 1 | 82688 / 114272 | training loss: 0.13338682055473328\n",
      "epoch: 1 | 82720 / 114272 | training loss: 0.06696814298629761\n",
      "epoch: 1 | 82752 / 114272 | training loss: 0.013587921857833862\n",
      "epoch: 1 | 82784 / 114272 | training loss: 0.05480574071407318\n",
      "epoch: 1 | 82816 / 114272 | training loss: 0.06577764451503754\n",
      "epoch: 1 | 82848 / 114272 | training loss: 0.06899650394916534\n",
      "epoch: 1 | 82880 / 114272 | training loss: 0.19175074994564056\n",
      "epoch: 1 | 82912 / 114272 | training loss: 0.21973595023155212\n",
      "epoch: 1 | 82944 / 114272 | training loss: 0.007177410647273064\n",
      "epoch: 1 | 82976 / 114272 | training loss: 0.12029437720775604\n",
      "epoch: 1 | 83008 / 114272 | training loss: 0.030419286340475082\n",
      "epoch: 1 | 83040 / 114272 | training loss: 0.01785249635577202\n",
      "epoch: 1 | 83072 / 114272 | training loss: 0.28734952211380005\n",
      "epoch: 1 | 83104 / 114272 | training loss: 0.09627144038677216\n",
      "epoch: 1 | 83136 / 114272 | training loss: 0.11209683120250702\n",
      "epoch: 1 | 83168 / 114272 | training loss: 0.110243059694767\n",
      "epoch: 1 | 83200 / 114272 | training loss: 0.011797451414167881\n",
      "epoch: 1 | 83232 / 114272 | training loss: 0.3001968562602997\n",
      "epoch: 1 | 83264 / 114272 | training loss: 0.1256103217601776\n",
      "epoch: 1 | 83296 / 114272 | training loss: 0.2336038053035736\n",
      "epoch: 1 | 83328 / 114272 | training loss: 0.015373734757304192\n",
      "epoch: 1 | 83360 / 114272 | training loss: 0.11188624054193497\n",
      "epoch: 1 | 83392 / 114272 | training loss: 0.14135430753231049\n",
      "epoch: 1 | 83424 / 114272 | training loss: 0.050412751734256744\n",
      "epoch: 1 | 83456 / 114272 | training loss: 0.13637301325798035\n",
      "epoch: 1 | 83488 / 114272 | training loss: 0.06059088185429573\n",
      "epoch: 1 | 83520 / 114272 | training loss: 0.04498055577278137\n",
      "epoch: 1 | 83552 / 114272 | training loss: 0.09572691470384598\n",
      "epoch: 1 | 83584 / 114272 | training loss: 0.15758349001407623\n",
      "epoch: 1 | 83616 / 114272 | training loss: 0.05479389801621437\n",
      "epoch: 1 | 83648 / 114272 | training loss: 0.13018770515918732\n",
      "epoch: 1 | 83680 / 114272 | training loss: 0.13500665128231049\n",
      "epoch: 1 | 83712 / 114272 | training loss: 0.32059112191200256\n",
      "epoch: 1 | 83744 / 114272 | training loss: 0.16150398552417755\n",
      "epoch: 1 | 83776 / 114272 | training loss: 0.2780226767063141\n",
      "epoch: 1 | 83808 / 114272 | training loss: 0.0920116975903511\n",
      "epoch: 1 | 83840 / 114272 | training loss: 0.009171143174171448\n",
      "epoch: 1 | 83872 / 114272 | training loss: 0.047895800322294235\n",
      "epoch: 1 | 83904 / 114272 | training loss: 0.1519273966550827\n",
      "epoch: 1 | 83936 / 114272 | training loss: 0.14580035209655762\n",
      "epoch: 1 | 83968 / 114272 | training loss: 0.3193289637565613\n",
      "epoch: 1 | 84000 / 114272 | training loss: 0.029400121420621872\n",
      "epoch: 1 | 84032 / 114272 | training loss: 0.16378824412822723\n",
      "epoch: 1 | 84064 / 114272 | training loss: 0.03200665861368179\n",
      "epoch: 1 | 84096 / 114272 | training loss: 0.028320642188191414\n",
      "epoch: 1 | 84128 / 114272 | training loss: 0.01925997994840145\n",
      "epoch: 1 | 84160 / 114272 | training loss: 0.068997323513031\n",
      "epoch: 1 | 84192 / 114272 | training loss: 0.02782163769006729\n",
      "epoch: 1 | 84224 / 114272 | training loss: 0.024889318272471428\n",
      "epoch: 1 | 84256 / 114272 | training loss: 0.6747145652770996\n",
      "epoch: 1 | 84288 / 114272 | training loss: 0.01317485049366951\n",
      "epoch: 1 | 84320 / 114272 | training loss: 0.21110130846500397\n",
      "epoch: 1 | 84352 / 114272 | training loss: 0.047777771949768066\n",
      "epoch: 1 | 84384 / 114272 | training loss: 0.187903493642807\n",
      "epoch: 1 | 84416 / 114272 | training loss: 0.08672460913658142\n",
      "epoch: 1 | 84448 / 114272 | training loss: 0.4419019818305969\n",
      "epoch: 1 | 84480 / 114272 | training loss: 0.025700373575091362\n",
      "epoch: 1 | 84512 / 114272 | training loss: 0.14107228815555573\n",
      "epoch: 1 | 84544 / 114272 | training loss: 0.230482280254364\n",
      "epoch: 1 | 84576 / 114272 | training loss: 0.015580793842673302\n",
      "epoch: 1 | 84608 / 114272 | training loss: 0.16025452315807343\n",
      "epoch: 1 | 84640 / 114272 | training loss: 0.0173250250518322\n",
      "epoch: 1 | 84672 / 114272 | training loss: 0.059459153562784195\n",
      "epoch: 1 | 84704 / 114272 | training loss: 0.38168689608573914\n",
      "epoch: 1 | 84736 / 114272 | training loss: 0.011340922676026821\n",
      "epoch: 1 | 84768 / 114272 | training loss: 0.008912313729524612\n",
      "epoch: 1 | 84800 / 114272 | training loss: 0.04942525923252106\n",
      "epoch: 1 | 84832 / 114272 | training loss: 0.17095617949962616\n",
      "epoch: 1 | 84864 / 114272 | training loss: 0.07799776643514633\n",
      "epoch: 1 | 84896 / 114272 | training loss: 0.013727989047765732\n",
      "epoch: 1 | 84928 / 114272 | training loss: 0.004563536494970322\n",
      "epoch: 1 | 84960 / 114272 | training loss: 0.007738991174846888\n",
      "epoch: 1 | 84992 / 114272 | training loss: 0.014363966882228851\n",
      "epoch: 1 | 85024 / 114272 | training loss: 0.22201001644134521\n",
      "epoch: 1 | 85056 / 114272 | training loss: 0.16900140047073364\n",
      "epoch: 1 | 85088 / 114272 | training loss: 0.055821824818849564\n",
      "epoch: 1 | 85120 / 114272 | training loss: 0.36823657155036926\n",
      "epoch: 1 | 85152 / 114272 | training loss: 0.19083058834075928\n",
      "epoch: 1 | 85184 / 114272 | training loss: 0.02143816463649273\n",
      "epoch: 1 | 85216 / 114272 | training loss: 0.11549381166696548\n",
      "epoch: 1 | 85248 / 114272 | training loss: 0.1341196596622467\n",
      "epoch: 1 | 85280 / 114272 | training loss: 0.02033519744873047\n",
      "epoch: 1 | 85312 / 114272 | training loss: 0.08810285478830338\n",
      "epoch: 1 | 85344 / 114272 | training loss: 0.004409663379192352\n",
      "epoch: 1 | 85376 / 114272 | training loss: 0.08076974749565125\n",
      "epoch: 1 | 85408 / 114272 | training loss: 0.010246065445244312\n",
      "epoch: 1 | 85440 / 114272 | training loss: 0.1678638607263565\n",
      "epoch: 1 | 85472 / 114272 | training loss: 0.1766394078731537\n",
      "epoch: 1 | 85504 / 114272 | training loss: 0.24260735511779785\n",
      "epoch: 1 | 85536 / 114272 | training loss: 0.13246601819992065\n",
      "epoch: 1 | 85568 / 114272 | training loss: 0.09460125118494034\n",
      "epoch: 1 | 85600 / 114272 | training loss: 0.04744971543550491\n",
      "epoch: 1 | 85632 / 114272 | training loss: 0.1583806425333023\n",
      "epoch: 1 | 85664 / 114272 | training loss: 0.02336112968623638\n",
      "epoch: 1 | 85696 / 114272 | training loss: 0.011473147198557854\n",
      "epoch: 1 | 85728 / 114272 | training loss: 0.24015533924102783\n",
      "epoch: 1 | 85760 / 114272 | training loss: 0.14898698031902313\n",
      "epoch: 1 | 85792 / 114272 | training loss: 0.11016426980495453\n",
      "epoch: 1 | 85824 / 114272 | training loss: 0.04832695797085762\n",
      "epoch: 1 | 85856 / 114272 | training loss: 0.11873140931129456\n",
      "epoch: 1 | 85888 / 114272 | training loss: 0.1719556748867035\n",
      "epoch: 1 | 85920 / 114272 | training loss: 0.22093935310840607\n",
      "epoch: 1 | 85952 / 114272 | training loss: 0.02939392253756523\n",
      "epoch: 1 | 85984 / 114272 | training loss: 0.01900995522737503\n",
      "epoch: 1 | 86016 / 114272 | training loss: 0.044654481112957\n",
      "epoch: 1 | 86048 / 114272 | training loss: 0.03567294776439667\n",
      "epoch: 1 | 86080 / 114272 | training loss: 0.03900276497006416\n",
      "epoch: 1 | 86112 / 114272 | training loss: 0.04098794609308243\n",
      "epoch: 1 | 86144 / 114272 | training loss: 0.1623833328485489\n",
      "epoch: 1 | 86176 / 114272 | training loss: 0.16548936069011688\n",
      "epoch: 1 | 86208 / 114272 | training loss: 0.16263186931610107\n",
      "epoch: 1 | 86240 / 114272 | training loss: 0.3928510844707489\n",
      "epoch: 1 | 86272 / 114272 | training loss: 0.036280859261751175\n",
      "epoch: 1 | 86304 / 114272 | training loss: 0.12796062231063843\n",
      "epoch: 1 | 86336 / 114272 | training loss: 0.2737598121166229\n",
      "epoch: 1 | 86368 / 114272 | training loss: 0.09018918126821518\n",
      "epoch: 1 | 86400 / 114272 | training loss: 0.04493040218949318\n",
      "epoch: 1 | 86432 / 114272 | training loss: 0.18128852546215057\n",
      "epoch: 1 | 86464 / 114272 | training loss: 0.21122711896896362\n",
      "epoch: 1 | 86496 / 114272 | training loss: 0.040203504264354706\n",
      "epoch: 1 | 86528 / 114272 | training loss: 0.021318314597010612\n",
      "epoch: 1 | 86560 / 114272 | training loss: 0.14486104249954224\n",
      "epoch: 1 | 86592 / 114272 | training loss: 0.08885294944047928\n",
      "epoch: 1 | 86624 / 114272 | training loss: 0.19599609076976776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 86656 / 114272 | training loss: 0.04854341968894005\n",
      "epoch: 1 | 86688 / 114272 | training loss: 0.07058735936880112\n",
      "epoch: 1 | 86720 / 114272 | training loss: 0.08215780556201935\n",
      "epoch: 1 | 86752 / 114272 | training loss: 0.049072109162807465\n",
      "epoch: 1 | 86784 / 114272 | training loss: 0.018963832408189774\n",
      "epoch: 1 | 86816 / 114272 | training loss: 0.006840321235358715\n",
      "epoch: 1 | 86848 / 114272 | training loss: 0.058707430958747864\n",
      "epoch: 1 | 86880 / 114272 | training loss: 0.2103893756866455\n",
      "epoch: 1 | 86912 / 114272 | training loss: 0.015008868649601936\n",
      "epoch: 1 | 86944 / 114272 | training loss: 0.1822519749403\n",
      "epoch: 1 | 86976 / 114272 | training loss: 0.28394970297813416\n",
      "epoch: 1 | 87008 / 114272 | training loss: 0.23969176411628723\n",
      "epoch: 1 | 87040 / 114272 | training loss: 0.21602791547775269\n",
      "epoch: 1 | 87072 / 114272 | training loss: 0.14444054663181305\n",
      "epoch: 1 | 87104 / 114272 | training loss: 0.10887986421585083\n",
      "epoch: 1 | 87136 / 114272 | training loss: 0.31523841619491577\n",
      "epoch: 1 | 87168 / 114272 | training loss: 0.04572106525301933\n",
      "epoch: 1 | 87200 / 114272 | training loss: 0.09489882737398148\n",
      "epoch: 1 | 87232 / 114272 | training loss: 0.09527822583913803\n",
      "epoch: 1 | 87264 / 114272 | training loss: 0.12983708083629608\n",
      "epoch: 1 | 87296 / 114272 | training loss: 0.013206405565142632\n",
      "epoch: 1 | 87328 / 114272 | training loss: 0.04773064702749252\n",
      "epoch: 1 | 87360 / 114272 | training loss: 0.02494431473314762\n",
      "epoch: 1 | 87392 / 114272 | training loss: 0.022789163514971733\n",
      "epoch: 1 | 87424 / 114272 | training loss: 0.042529936879873276\n",
      "epoch: 1 | 87456 / 114272 | training loss: 0.05631185695528984\n",
      "epoch: 1 | 87488 / 114272 | training loss: 0.30549633502960205\n",
      "epoch: 1 | 87520 / 114272 | training loss: 0.24492041766643524\n",
      "epoch: 1 | 87552 / 114272 | training loss: 0.15476663410663605\n",
      "epoch: 1 | 87584 / 114272 | training loss: 0.13303759694099426\n",
      "epoch: 1 | 87616 / 114272 | training loss: 0.08963310718536377\n",
      "epoch: 1 | 87648 / 114272 | training loss: 0.024711454287171364\n",
      "epoch: 1 | 87680 / 114272 | training loss: 0.054700497537851334\n",
      "epoch: 1 | 87712 / 114272 | training loss: 0.0222349651157856\n",
      "epoch: 1 | 87744 / 114272 | training loss: 0.07606259733438492\n",
      "epoch: 1 | 87776 / 114272 | training loss: 0.09594207257032394\n",
      "epoch: 1 | 87808 / 114272 | training loss: 0.20443095266819\n",
      "epoch: 1 | 87840 / 114272 | training loss: 0.06609199941158295\n",
      "epoch: 1 | 87872 / 114272 | training loss: 0.1491982340812683\n",
      "epoch: 1 | 87904 / 114272 | training loss: 0.1438245177268982\n",
      "epoch: 1 | 87936 / 114272 | training loss: 0.1266922652721405\n",
      "epoch: 1 | 87968 / 114272 | training loss: 0.09054461866617203\n",
      "epoch: 1 | 88000 / 114272 | training loss: 0.013324874453246593\n",
      "epoch: 1 | 88032 / 114272 | training loss: 0.11429733783006668\n",
      "epoch: 1 | 88064 / 114272 | training loss: 0.05352342128753662\n",
      "epoch: 1 | 88096 / 114272 | training loss: 0.2768256962299347\n",
      "epoch: 1 | 88128 / 114272 | training loss: 0.018071582540869713\n",
      "epoch: 1 | 88160 / 114272 | training loss: 0.06674043834209442\n",
      "epoch: 1 | 88192 / 114272 | training loss: 0.05642956122756004\n",
      "epoch: 1 | 88224 / 114272 | training loss: 0.039653558284044266\n",
      "epoch: 1 | 88256 / 114272 | training loss: 0.06058162823319435\n",
      "epoch: 1 | 88288 / 114272 | training loss: 0.021124720573425293\n",
      "epoch: 1 | 88320 / 114272 | training loss: 0.14783945679664612\n",
      "epoch: 1 | 88352 / 114272 | training loss: 0.09811027348041534\n",
      "epoch: 1 | 88384 / 114272 | training loss: 0.05155569687485695\n",
      "epoch: 1 | 88416 / 114272 | training loss: 0.3761831820011139\n",
      "epoch: 1 | 88448 / 114272 | training loss: 0.28929901123046875\n",
      "epoch: 1 | 88480 / 114272 | training loss: 0.12519875168800354\n",
      "epoch: 1 | 88512 / 114272 | training loss: 0.0763714388012886\n",
      "epoch: 1 | 88544 / 114272 | training loss: 0.02464032545685768\n",
      "epoch: 1 | 88576 / 114272 | training loss: 0.10183331370353699\n",
      "epoch: 1 | 88608 / 114272 | training loss: 0.17592188715934753\n",
      "epoch: 1 | 88640 / 114272 | training loss: 0.04798261821269989\n",
      "epoch: 1 | 88672 / 114272 | training loss: 0.21740417182445526\n",
      "epoch: 1 | 88704 / 114272 | training loss: 0.10795120149850845\n",
      "epoch: 1 | 88736 / 114272 | training loss: 0.291146457195282\n",
      "epoch: 1 | 88768 / 114272 | training loss: 0.03328128159046173\n",
      "epoch: 1 | 88800 / 114272 | training loss: 0.2909074127674103\n",
      "epoch: 1 | 88832 / 114272 | training loss: 0.16050393879413605\n",
      "epoch: 1 | 88864 / 114272 | training loss: 0.021175188943743706\n",
      "epoch: 1 | 88896 / 114272 | training loss: 0.20517177879810333\n",
      "epoch: 1 | 88928 / 114272 | training loss: 0.017645807936787605\n",
      "epoch: 1 | 88960 / 114272 | training loss: 0.14152021706104279\n",
      "epoch: 1 | 88992 / 114272 | training loss: 0.02527899295091629\n",
      "epoch: 1 | 89024 / 114272 | training loss: 0.16768981516361237\n",
      "epoch: 1 | 89056 / 114272 | training loss: 0.092999666929245\n",
      "epoch: 1 | 89088 / 114272 | training loss: 0.23092228174209595\n",
      "epoch: 1 | 89120 / 114272 | training loss: 0.24010105431079865\n",
      "epoch: 1 | 89152 / 114272 | training loss: 0.08963969349861145\n",
      "epoch: 1 | 89184 / 114272 | training loss: 0.06488173454999924\n",
      "epoch: 1 | 89216 / 114272 | training loss: 0.007981112226843834\n",
      "epoch: 1 | 89248 / 114272 | training loss: 0.15547068417072296\n",
      "epoch: 1 | 89280 / 114272 | training loss: 0.025522105395793915\n",
      "epoch: 1 | 89312 / 114272 | training loss: 0.09111657738685608\n",
      "epoch: 1 | 89344 / 114272 | training loss: 0.1053069531917572\n",
      "epoch: 1 | 89376 / 114272 | training loss: 0.13249292969703674\n",
      "epoch: 1 | 89408 / 114272 | training loss: 0.043866995722055435\n",
      "epoch: 1 | 89440 / 114272 | training loss: 0.06989748775959015\n",
      "epoch: 1 | 89472 / 114272 | training loss: 0.03897543624043465\n",
      "epoch: 1 | 89504 / 114272 | training loss: 0.21668750047683716\n",
      "epoch: 1 | 89536 / 114272 | training loss: 0.02893718332052231\n",
      "epoch: 1 | 89568 / 114272 | training loss: 0.051869992166757584\n",
      "epoch: 1 | 89600 / 114272 | training loss: 0.148725226521492\n",
      "epoch: 1 | 89632 / 114272 | training loss: 0.13946573436260223\n",
      "epoch: 1 | 89664 / 114272 | training loss: 0.04649254307150841\n",
      "epoch: 1 | 89696 / 114272 | training loss: 0.09066647291183472\n",
      "epoch: 1 | 89728 / 114272 | training loss: 0.04851039499044418\n",
      "epoch: 1 | 89760 / 114272 | training loss: 0.0846194252371788\n",
      "epoch: 1 | 89792 / 114272 | training loss: 0.011083267629146576\n",
      "epoch: 1 | 89824 / 114272 | training loss: 0.016844365745782852\n",
      "epoch: 1 | 89856 / 114272 | training loss: 0.0658608227968216\n",
      "epoch: 1 | 89888 / 114272 | training loss: 0.01694008708000183\n",
      "epoch: 1 | 89920 / 114272 | training loss: 0.29438599944114685\n",
      "epoch: 1 | 89952 / 114272 | training loss: 0.05092175304889679\n",
      "epoch: 1 | 89984 / 114272 | training loss: 0.0502336286008358\n",
      "epoch: 1 | 90016 / 114272 | training loss: 0.3737126290798187\n",
      "epoch: 1 | 90048 / 114272 | training loss: 0.09083636105060577\n",
      "epoch: 1 | 90080 / 114272 | training loss: 0.011093724519014359\n",
      "epoch: 1 | 90112 / 114272 | training loss: 0.03378067910671234\n",
      "epoch: 1 | 90144 / 114272 | training loss: 0.01241714134812355\n",
      "epoch: 1 | 90176 / 114272 | training loss: 0.043557047843933105\n",
      "epoch: 1 | 90208 / 114272 | training loss: 0.004445595666766167\n",
      "epoch: 1 | 90240 / 114272 | training loss: 0.007237228564918041\n",
      "epoch: 1 | 90272 / 114272 | training loss: 0.01796782575547695\n",
      "epoch: 1 | 90304 / 114272 | training loss: 0.20512370765209198\n",
      "epoch: 1 | 90336 / 114272 | training loss: 0.05720015987753868\n",
      "epoch: 1 | 90368 / 114272 | training loss: 0.013764448463916779\n",
      "epoch: 1 | 90400 / 114272 | training loss: 0.11365729570388794\n",
      "epoch: 1 | 90432 / 114272 | training loss: 0.06919325888156891\n",
      "epoch: 1 | 90464 / 114272 | training loss: 0.12062754482030869\n",
      "epoch: 1 | 90496 / 114272 | training loss: 0.07730066031217575\n",
      "epoch: 1 | 90528 / 114272 | training loss: 0.014810304157435894\n",
      "epoch: 1 | 90560 / 114272 | training loss: 0.009504584595561028\n",
      "epoch: 1 | 90592 / 114272 | training loss: 0.24136455357074738\n",
      "epoch: 1 | 90624 / 114272 | training loss: 0.004073285032063723\n",
      "epoch: 1 | 90656 / 114272 | training loss: 0.10807883739471436\n",
      "epoch: 1 | 90688 / 114272 | training loss: 0.16328729689121246\n",
      "epoch: 1 | 90720 / 114272 | training loss: 0.234223410487175\n",
      "epoch: 1 | 90752 / 114272 | training loss: 0.219379723072052\n",
      "epoch: 1 | 90784 / 114272 | training loss: 0.14602315425872803\n",
      "epoch: 1 | 90816 / 114272 | training loss: 0.3058003783226013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 90848 / 114272 | training loss: 0.10037065297365189\n",
      "epoch: 1 | 90880 / 114272 | training loss: 0.21075662970542908\n",
      "epoch: 1 | 90912 / 114272 | training loss: 0.06310499459505081\n",
      "epoch: 1 | 90944 / 114272 | training loss: 0.03871593996882439\n",
      "epoch: 1 | 90976 / 114272 | training loss: 0.02066757343709469\n",
      "epoch: 1 | 91008 / 114272 | training loss: 0.21108891069889069\n",
      "epoch: 1 | 91040 / 114272 | training loss: 0.23062175512313843\n",
      "epoch: 1 | 91072 / 114272 | training loss: 0.05923193693161011\n",
      "epoch: 1 | 91104 / 114272 | training loss: 0.0508890300989151\n",
      "epoch: 1 | 91136 / 114272 | training loss: 0.10895026475191116\n",
      "epoch: 1 | 91168 / 114272 | training loss: 0.012258726172149181\n",
      "epoch: 1 | 91200 / 114272 | training loss: 0.2086440771818161\n",
      "epoch: 1 | 91232 / 114272 | training loss: 0.07022365182638168\n",
      "epoch: 1 | 91264 / 114272 | training loss: 0.04066235572099686\n",
      "epoch: 1 | 91296 / 114272 | training loss: 0.3020336329936981\n",
      "epoch: 1 | 91328 / 114272 | training loss: 0.019567253068089485\n",
      "epoch: 1 | 91360 / 114272 | training loss: 0.02495455928146839\n",
      "epoch: 1 | 91392 / 114272 | training loss: 0.04747837781906128\n",
      "epoch: 1 | 91424 / 114272 | training loss: 0.06194005534052849\n",
      "epoch: 1 | 91456 / 114272 | training loss: 0.08010780811309814\n",
      "epoch: 1 | 91488 / 114272 | training loss: 0.06021547690033913\n",
      "epoch: 1 | 91520 / 114272 | training loss: 0.04170394688844681\n",
      "epoch: 1 | 91552 / 114272 | training loss: 0.009318538941442966\n",
      "epoch: 1 | 91584 / 114272 | training loss: 0.05950548127293587\n",
      "epoch: 1 | 91616 / 114272 | training loss: 0.20686286687850952\n",
      "epoch: 1 | 91648 / 114272 | training loss: 0.017817433923482895\n",
      "epoch: 1 | 91680 / 114272 | training loss: 0.07120898365974426\n",
      "epoch: 1 | 91712 / 114272 | training loss: 0.21440613269805908\n",
      "epoch: 1 | 91744 / 114272 | training loss: 0.3216913044452667\n",
      "epoch: 1 | 91776 / 114272 | training loss: 0.23856212198734283\n",
      "epoch: 1 | 91808 / 114272 | training loss: 0.26163366436958313\n",
      "epoch: 1 | 91840 / 114272 | training loss: 0.10443873703479767\n",
      "epoch: 1 | 91872 / 114272 | training loss: 0.0064752665348351\n",
      "epoch: 1 | 91904 / 114272 | training loss: 0.011973626911640167\n",
      "epoch: 1 | 91936 / 114272 | training loss: 0.317486047744751\n",
      "epoch: 1 | 91968 / 114272 | training loss: 0.029819445684552193\n",
      "epoch: 1 | 92000 / 114272 | training loss: 0.10169044137001038\n",
      "epoch: 1 | 92032 / 114272 | training loss: 0.05150742456316948\n",
      "epoch: 1 | 92064 / 114272 | training loss: 0.007676193490624428\n",
      "epoch: 1 | 92096 / 114272 | training loss: 0.02708575315773487\n",
      "epoch: 1 | 92128 / 114272 | training loss: 0.10581839829683304\n",
      "epoch: 1 | 92160 / 114272 | training loss: 0.0749359279870987\n",
      "epoch: 1 | 92192 / 114272 | training loss: 0.28702181577682495\n",
      "epoch: 1 | 92224 / 114272 | training loss: 0.014133183285593987\n",
      "epoch: 1 | 92256 / 114272 | training loss: 0.0685381069779396\n",
      "epoch: 1 | 92288 / 114272 | training loss: 0.02901620604097843\n",
      "epoch: 1 | 92320 / 114272 | training loss: 0.03735492751002312\n",
      "epoch: 1 | 92352 / 114272 | training loss: 0.019266650080680847\n",
      "epoch: 1 | 92384 / 114272 | training loss: 0.007442099507898092\n",
      "epoch: 1 | 92416 / 114272 | training loss: 0.005311070941388607\n",
      "epoch: 1 | 92448 / 114272 | training loss: 0.08850880712270737\n",
      "epoch: 1 | 92480 / 114272 | training loss: 0.10200084000825882\n",
      "epoch: 1 | 92512 / 114272 | training loss: 0.23699238896369934\n",
      "epoch: 1 | 92544 / 114272 | training loss: 0.22481127083301544\n",
      "epoch: 1 | 92576 / 114272 | training loss: 0.16489054262638092\n",
      "epoch: 1 | 92608 / 114272 | training loss: 0.16160233318805695\n",
      "epoch: 1 | 92640 / 114272 | training loss: 0.37611907720565796\n",
      "epoch: 1 | 92672 / 114272 | training loss: 0.06234898790717125\n",
      "epoch: 1 | 92704 / 114272 | training loss: 0.007810355629771948\n",
      "epoch: 1 | 92736 / 114272 | training loss: 0.10781747847795486\n",
      "epoch: 1 | 92768 / 114272 | training loss: 0.21752534806728363\n",
      "epoch: 1 | 92800 / 114272 | training loss: 0.044826749712228775\n",
      "epoch: 1 | 92832 / 114272 | training loss: 0.5242868661880493\n",
      "epoch: 1 | 92864 / 114272 | training loss: 0.13119550049304962\n",
      "epoch: 1 | 92896 / 114272 | training loss: 0.01701229251921177\n",
      "epoch: 1 | 92928 / 114272 | training loss: 0.20353606343269348\n",
      "epoch: 1 | 92960 / 114272 | training loss: 0.0523524209856987\n",
      "epoch: 1 | 92992 / 114272 | training loss: 0.2470765858888626\n",
      "epoch: 1 | 93024 / 114272 | training loss: 0.0829339325428009\n",
      "epoch: 1 | 93056 / 114272 | training loss: 0.12226696312427521\n",
      "epoch: 1 | 93088 / 114272 | training loss: 0.061340201646089554\n",
      "epoch: 1 | 93120 / 114272 | training loss: 0.045939669013023376\n",
      "epoch: 1 | 93152 / 114272 | training loss: 0.13501513004302979\n",
      "epoch: 1 | 93184 / 114272 | training loss: 0.07630234956741333\n",
      "epoch: 1 | 93216 / 114272 | training loss: 0.18507066369056702\n",
      "epoch: 1 | 93248 / 114272 | training loss: 0.007745024282485247\n",
      "epoch: 1 | 93280 / 114272 | training loss: 0.18591968715190887\n",
      "epoch: 1 | 93312 / 114272 | training loss: 0.05832383409142494\n",
      "epoch: 1 | 93344 / 114272 | training loss: 0.48470407724380493\n",
      "epoch: 1 | 93376 / 114272 | training loss: 0.07172074913978577\n",
      "epoch: 1 | 93408 / 114272 | training loss: 0.01220659539103508\n",
      "epoch: 1 | 93440 / 114272 | training loss: 0.02238813228905201\n",
      "epoch: 1 | 93472 / 114272 | training loss: 0.09313205629587173\n",
      "epoch: 1 | 93504 / 114272 | training loss: 0.043753620237112045\n",
      "epoch: 1 | 93536 / 114272 | training loss: 0.070121631026268\n",
      "epoch: 1 | 93568 / 114272 | training loss: 0.10109131038188934\n",
      "epoch: 1 | 93600 / 114272 | training loss: 0.1831497699022293\n",
      "epoch: 1 | 93632 / 114272 | training loss: 0.1390460580587387\n",
      "epoch: 1 | 93664 / 114272 | training loss: 0.07055368274450302\n",
      "epoch: 1 | 93696 / 114272 | training loss: 0.10477647185325623\n",
      "epoch: 1 | 93728 / 114272 | training loss: 0.060299720615148544\n",
      "epoch: 1 | 93760 / 114272 | training loss: 0.5056621432304382\n",
      "epoch: 1 | 93792 / 114272 | training loss: 0.38297122716903687\n",
      "epoch: 1 | 93824 / 114272 | training loss: 0.037386998534202576\n",
      "epoch: 1 | 93856 / 114272 | training loss: 0.030743028968572617\n",
      "epoch: 1 | 93888 / 114272 | training loss: 0.0465284138917923\n",
      "epoch: 1 | 93920 / 114272 | training loss: 0.21215970814228058\n",
      "epoch: 1 | 93952 / 114272 | training loss: 0.013117693364620209\n",
      "epoch: 1 | 93984 / 114272 | training loss: 0.04516904801130295\n",
      "epoch: 1 | 94016 / 114272 | training loss: 0.06535568833351135\n",
      "epoch: 1 | 94048 / 114272 | training loss: 0.16436266899108887\n",
      "epoch: 1 | 94080 / 114272 | training loss: 0.07599090039730072\n",
      "epoch: 1 | 94112 / 114272 | training loss: 0.037783440202474594\n",
      "epoch: 1 | 94144 / 114272 | training loss: 0.12996892631053925\n",
      "epoch: 1 | 94176 / 114272 | training loss: 0.05147767439484596\n",
      "epoch: 1 | 94208 / 114272 | training loss: 0.016483375802636147\n",
      "epoch: 1 | 94240 / 114272 | training loss: 0.028193030506372452\n",
      "epoch: 1 | 94272 / 114272 | training loss: 0.24322739243507385\n",
      "epoch: 1 | 94304 / 114272 | training loss: 0.11709537357091904\n",
      "epoch: 1 | 94336 / 114272 | training loss: 0.023801421746611595\n",
      "epoch: 1 | 94368 / 114272 | training loss: 0.07086828351020813\n",
      "epoch: 1 | 94400 / 114272 | training loss: 0.3095408082008362\n",
      "epoch: 1 | 94432 / 114272 | training loss: 0.005614207126200199\n",
      "epoch: 1 | 94464 / 114272 | training loss: 0.02526785060763359\n",
      "epoch: 1 | 94496 / 114272 | training loss: 0.06910669058561325\n",
      "epoch: 1 | 94528 / 114272 | training loss: 0.04871520400047302\n",
      "epoch: 1 | 94560 / 114272 | training loss: 0.02529291622340679\n",
      "epoch: 1 | 94592 / 114272 | training loss: 0.1919941008090973\n",
      "epoch: 1 | 94624 / 114272 | training loss: 0.11280177533626556\n",
      "epoch: 1 | 94656 / 114272 | training loss: 0.08986267447471619\n",
      "epoch: 1 | 94688 / 114272 | training loss: 0.039511214941740036\n",
      "epoch: 1 | 94720 / 114272 | training loss: 0.07142268866300583\n",
      "epoch: 1 | 94752 / 114272 | training loss: 0.01834351196885109\n",
      "epoch: 1 | 94784 / 114272 | training loss: 0.0031240133102983236\n",
      "epoch: 1 | 94816 / 114272 | training loss: 0.27114543318748474\n",
      "epoch: 1 | 94848 / 114272 | training loss: 0.0415947288274765\n",
      "epoch: 1 | 94880 / 114272 | training loss: 0.15122582018375397\n",
      "epoch: 1 | 94912 / 114272 | training loss: 0.04385939612984657\n",
      "epoch: 1 | 94944 / 114272 | training loss: 0.20077615976333618\n",
      "epoch: 1 | 94976 / 114272 | training loss: 0.26274073123931885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 95008 / 114272 | training loss: 0.05312895402312279\n",
      "epoch: 1 | 95040 / 114272 | training loss: 0.02891567349433899\n",
      "epoch: 1 | 95072 / 114272 | training loss: 0.11539167165756226\n",
      "epoch: 1 | 95104 / 114272 | training loss: 0.00544558372348547\n",
      "epoch: 1 | 95136 / 114272 | training loss: 0.11888080835342407\n",
      "epoch: 1 | 95168 / 114272 | training loss: 0.011234857141971588\n",
      "epoch: 1 | 95200 / 114272 | training loss: 0.15222404897212982\n",
      "epoch: 1 | 95232 / 114272 | training loss: 0.1523749977350235\n",
      "epoch: 1 | 95264 / 114272 | training loss: 0.05589644983410835\n",
      "epoch: 1 | 95296 / 114272 | training loss: 0.0941649079322815\n",
      "epoch: 1 | 95328 / 114272 | training loss: 0.04834449291229248\n",
      "epoch: 1 | 95360 / 114272 | training loss: 0.0675029531121254\n",
      "epoch: 1 | 95392 / 114272 | training loss: 0.09070805460214615\n",
      "epoch: 1 | 95424 / 114272 | training loss: 0.009499209932982922\n",
      "epoch: 1 | 95456 / 114272 | training loss: 0.11271626502275467\n",
      "epoch: 1 | 95488 / 114272 | training loss: 0.07084055244922638\n",
      "epoch: 1 | 95520 / 114272 | training loss: 0.24678859114646912\n",
      "epoch: 1 | 95552 / 114272 | training loss: 0.08043456822633743\n",
      "epoch: 1 | 95584 / 114272 | training loss: 0.06699969619512558\n",
      "epoch: 1 | 95616 / 114272 | training loss: 0.03625713288784027\n",
      "epoch: 1 | 95648 / 114272 | training loss: 0.21193110942840576\n",
      "epoch: 1 | 95680 / 114272 | training loss: 0.27247750759124756\n",
      "epoch: 1 | 95712 / 114272 | training loss: 0.059265147894620895\n",
      "epoch: 1 | 95744 / 114272 | training loss: 0.043232060968875885\n",
      "epoch: 1 | 95776 / 114272 | training loss: 0.4963705837726593\n",
      "epoch: 1 | 95808 / 114272 | training loss: 0.03356429561972618\n",
      "epoch: 1 | 95840 / 114272 | training loss: 0.07159479707479477\n",
      "epoch: 1 | 95872 / 114272 | training loss: 0.02874740958213806\n",
      "epoch: 1 | 95904 / 114272 | training loss: 0.25318023562431335\n",
      "epoch: 1 | 95936 / 114272 | training loss: 0.06512320786714554\n",
      "epoch: 1 | 95968 / 114272 | training loss: 0.04475027322769165\n",
      "epoch: 1 | 96000 / 114272 | training loss: 0.06371338665485382\n",
      "epoch: 1 | 96032 / 114272 | training loss: 0.15725074708461761\n",
      "epoch: 1 | 96064 / 114272 | training loss: 0.015718549489974976\n",
      "epoch: 1 | 96096 / 114272 | training loss: 0.03019949235022068\n",
      "epoch: 1 | 96128 / 114272 | training loss: 0.02969510294497013\n",
      "epoch: 1 | 96160 / 114272 | training loss: 0.09178361296653748\n",
      "epoch: 1 | 96192 / 114272 | training loss: 0.13396228849887848\n",
      "epoch: 1 | 96224 / 114272 | training loss: 0.016127347946166992\n",
      "epoch: 1 | 96256 / 114272 | training loss: 0.23581117391586304\n",
      "epoch: 1 | 96288 / 114272 | training loss: 0.011701601557433605\n",
      "epoch: 1 | 96320 / 114272 | training loss: 0.43776172399520874\n",
      "epoch: 1 | 96352 / 114272 | training loss: 0.0029217940755188465\n",
      "epoch: 1 | 96384 / 114272 | training loss: 0.10071539133787155\n",
      "epoch: 1 | 96416 / 114272 | training loss: 0.018923072144389153\n",
      "epoch: 1 | 96448 / 114272 | training loss: 0.2706834077835083\n",
      "epoch: 1 | 96480 / 114272 | training loss: 0.024240056052803993\n",
      "epoch: 1 | 96512 / 114272 | training loss: 0.028135554865002632\n",
      "epoch: 1 | 96544 / 114272 | training loss: 0.2162417620420456\n",
      "epoch: 1 | 96576 / 114272 | training loss: 0.22786161303520203\n",
      "epoch: 1 | 96608 / 114272 | training loss: 0.18974994122982025\n",
      "epoch: 1 | 96640 / 114272 | training loss: 0.3239811062812805\n",
      "epoch: 1 | 96672 / 114272 | training loss: 0.1325346678495407\n",
      "epoch: 1 | 96704 / 114272 | training loss: 0.2617490887641907\n",
      "epoch: 1 | 96736 / 114272 | training loss: 0.01911187171936035\n",
      "epoch: 1 | 96768 / 114272 | training loss: 0.02586803399026394\n",
      "epoch: 1 | 96800 / 114272 | training loss: 0.1466229259967804\n",
      "epoch: 1 | 96832 / 114272 | training loss: 0.25247934460639954\n",
      "epoch: 1 | 96864 / 114272 | training loss: 0.010576670058071613\n",
      "epoch: 1 | 96896 / 114272 | training loss: 0.04488403722643852\n",
      "epoch: 1 | 96928 / 114272 | training loss: 0.23534689843654633\n",
      "epoch: 1 | 96960 / 114272 | training loss: 0.33987146615982056\n",
      "epoch: 1 | 96992 / 114272 | training loss: 0.2065412402153015\n",
      "epoch: 1 | 97024 / 114272 | training loss: 0.2336522340774536\n",
      "epoch: 1 | 97056 / 114272 | training loss: 0.07722935825586319\n",
      "epoch: 1 | 97088 / 114272 | training loss: 0.16654905676841736\n",
      "epoch: 1 | 97120 / 114272 | training loss: 0.047272615134716034\n",
      "epoch: 1 | 97152 / 114272 | training loss: 0.12832553684711456\n",
      "epoch: 1 | 97184 / 114272 | training loss: 0.16471250355243683\n",
      "epoch: 1 | 97216 / 114272 | training loss: 0.18811412155628204\n",
      "epoch: 1 | 97248 / 114272 | training loss: 0.008509168401360512\n",
      "epoch: 1 | 97280 / 114272 | training loss: 0.19358131289482117\n",
      "epoch: 1 | 97312 / 114272 | training loss: 0.08481369912624359\n",
      "epoch: 1 | 97344 / 114272 | training loss: 0.2521320581436157\n",
      "epoch: 1 | 97376 / 114272 | training loss: 0.12987828254699707\n",
      "epoch: 1 | 97408 / 114272 | training loss: 0.2723337411880493\n",
      "epoch: 1 | 97440 / 114272 | training loss: 0.3466776907444\n",
      "epoch: 1 | 97472 / 114272 | training loss: 0.3299829959869385\n",
      "epoch: 1 | 97504 / 114272 | training loss: 0.07879837602376938\n",
      "epoch: 1 | 97536 / 114272 | training loss: 0.06621623784303665\n",
      "epoch: 1 | 97568 / 114272 | training loss: 0.18734076619148254\n",
      "epoch: 1 | 97600 / 114272 | training loss: 0.3220638036727905\n",
      "epoch: 1 | 97632 / 114272 | training loss: 0.04048314690589905\n",
      "epoch: 1 | 97664 / 114272 | training loss: 0.14191269874572754\n",
      "epoch: 1 | 97696 / 114272 | training loss: 0.02698267623782158\n",
      "epoch: 1 | 97728 / 114272 | training loss: 0.26678064465522766\n",
      "epoch: 1 | 97760 / 114272 | training loss: 0.04106076434254646\n",
      "epoch: 1 | 97792 / 114272 | training loss: 0.1043902188539505\n",
      "epoch: 1 | 97824 / 114272 | training loss: 0.01743583381175995\n",
      "epoch: 1 | 97856 / 114272 | training loss: 0.055924031883478165\n",
      "epoch: 1 | 97888 / 114272 | training loss: 0.01375214196741581\n",
      "epoch: 1 | 97920 / 114272 | training loss: 0.08562812954187393\n",
      "epoch: 1 | 97952 / 114272 | training loss: 0.23505181074142456\n",
      "epoch: 1 | 97984 / 114272 | training loss: 0.1810416281223297\n",
      "epoch: 1 | 98016 / 114272 | training loss: 0.10188642144203186\n",
      "epoch: 1 | 98048 / 114272 | training loss: 0.01854362152516842\n",
      "epoch: 1 | 98080 / 114272 | training loss: 0.011803491041064262\n",
      "epoch: 1 | 98112 / 114272 | training loss: 0.023310992866754532\n",
      "epoch: 1 | 98144 / 114272 | training loss: 0.18367256224155426\n",
      "epoch: 1 | 98176 / 114272 | training loss: 0.1594454050064087\n",
      "epoch: 1 | 98208 / 114272 | training loss: 0.02432738058269024\n",
      "epoch: 1 | 98240 / 114272 | training loss: 0.3080371618270874\n",
      "epoch: 1 | 98272 / 114272 | training loss: 0.033609405159950256\n",
      "epoch: 1 | 98304 / 114272 | training loss: 0.2313317209482193\n",
      "epoch: 1 | 98336 / 114272 | training loss: 0.15427136421203613\n",
      "epoch: 1 | 98368 / 114272 | training loss: 0.012381739914417267\n",
      "epoch: 1 | 98400 / 114272 | training loss: 0.018880346789956093\n",
      "epoch: 1 | 98432 / 114272 | training loss: 0.23647135496139526\n",
      "epoch: 1 | 98464 / 114272 | training loss: 0.12399778515100479\n",
      "epoch: 1 | 98496 / 114272 | training loss: 0.19179697334766388\n",
      "epoch: 1 | 98528 / 114272 | training loss: 0.021224088966846466\n",
      "epoch: 1 | 98560 / 114272 | training loss: 0.07659615576267242\n",
      "epoch: 1 | 98592 / 114272 | training loss: 0.03966161981225014\n",
      "epoch: 1 | 98624 / 114272 | training loss: 0.1976613700389862\n",
      "epoch: 1 | 98656 / 114272 | training loss: 0.2350386530160904\n",
      "epoch: 1 | 98688 / 114272 | training loss: 0.02186778001487255\n",
      "epoch: 1 | 98720 / 114272 | training loss: 0.10679025202989578\n",
      "epoch: 1 | 98752 / 114272 | training loss: 0.3619637191295624\n",
      "epoch: 1 | 98784 / 114272 | training loss: 0.3040630519390106\n",
      "epoch: 1 | 98816 / 114272 | training loss: 0.009407836012542248\n",
      "epoch: 1 | 98848 / 114272 | training loss: 0.017069190740585327\n",
      "epoch: 1 | 98880 / 114272 | training loss: 0.04907714203000069\n",
      "epoch: 1 | 98912 / 114272 | training loss: 0.23382705450057983\n",
      "epoch: 1 | 98944 / 114272 | training loss: 0.026048311963677406\n",
      "epoch: 1 | 98976 / 114272 | training loss: 0.08904492110013962\n",
      "epoch: 1 | 99008 / 114272 | training loss: 0.04096916317939758\n",
      "epoch: 1 | 99040 / 114272 | training loss: 0.1710929274559021\n",
      "epoch: 1 | 99072 / 114272 | training loss: 0.5571128129959106\n",
      "epoch: 1 | 99104 / 114272 | training loss: 0.011821094900369644\n",
      "epoch: 1 | 99136 / 114272 | training loss: 0.06329035758972168\n",
      "epoch: 1 | 99168 / 114272 | training loss: 0.15481282770633698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 99200 / 114272 | training loss: 0.06768165528774261\n",
      "epoch: 1 | 99232 / 114272 | training loss: 0.06554820388555527\n",
      "epoch: 1 | 99264 / 114272 | training loss: 0.31577029824256897\n",
      "epoch: 1 | 99296 / 114272 | training loss: 0.30984988808631897\n",
      "epoch: 1 | 99328 / 114272 | training loss: 0.08902455866336823\n",
      "epoch: 1 | 99360 / 114272 | training loss: 0.11410421133041382\n",
      "epoch: 1 | 99392 / 114272 | training loss: 0.11501684039831161\n",
      "epoch: 1 | 99424 / 114272 | training loss: 0.2011885643005371\n",
      "epoch: 1 | 99456 / 114272 | training loss: 0.3345327377319336\n",
      "epoch: 1 | 99488 / 114272 | training loss: 0.027087947353720665\n",
      "epoch: 1 | 99520 / 114272 | training loss: 0.0103323208168149\n",
      "epoch: 1 | 99552 / 114272 | training loss: 0.0513652078807354\n",
      "epoch: 1 | 99584 / 114272 | training loss: 0.03023705817759037\n",
      "epoch: 1 | 99616 / 114272 | training loss: 0.34606531262397766\n",
      "epoch: 1 | 99648 / 114272 | training loss: 0.024276308715343475\n",
      "epoch: 1 | 99680 / 114272 | training loss: 0.19699819386005402\n",
      "epoch: 1 | 99712 / 114272 | training loss: 0.03488469868898392\n",
      "epoch: 1 | 99744 / 114272 | training loss: 0.14198285341262817\n",
      "epoch: 1 | 99776 / 114272 | training loss: 0.022461164742708206\n",
      "epoch: 1 | 99808 / 114272 | training loss: 0.020842205733060837\n",
      "epoch: 1 | 99840 / 114272 | training loss: 0.11282394826412201\n",
      "epoch: 1 | 99872 / 114272 | training loss: 0.2304769903421402\n",
      "epoch: 1 | 99904 / 114272 | training loss: 0.09807662665843964\n",
      "epoch: 1 | 99936 / 114272 | training loss: 0.08614481985569\n",
      "epoch: 1 | 99968 / 114272 | training loss: 0.08962062001228333\n",
      "epoch: 1 | 100000 / 114272 | training loss: 0.2473202496767044\n",
      "epoch: 1 | 100032 / 114272 | training loss: 0.27755844593048096\n",
      "epoch: 1 | 100064 / 114272 | training loss: 0.011674884706735611\n",
      "epoch: 1 | 100096 / 114272 | training loss: 0.1590333729982376\n",
      "epoch: 1 | 100128 / 114272 | training loss: 0.16400881111621857\n",
      "epoch: 1 | 100160 / 114272 | training loss: 0.15482716262340546\n",
      "epoch: 1 | 100192 / 114272 | training loss: 0.02455594390630722\n",
      "epoch: 1 | 100224 / 114272 | training loss: 0.009395518340170383\n",
      "epoch: 1 | 100256 / 114272 | training loss: 0.22892001271247864\n",
      "epoch: 1 | 100288 / 114272 | training loss: 0.35184669494628906\n",
      "epoch: 1 | 100320 / 114272 | training loss: 0.026354854926466942\n",
      "epoch: 1 | 100352 / 114272 | training loss: 0.09713879227638245\n",
      "epoch: 1 | 100384 / 114272 | training loss: 0.35963356494903564\n",
      "epoch: 1 | 100416 / 114272 | training loss: 0.009080720134079456\n",
      "epoch: 1 | 100448 / 114272 | training loss: 0.3778586685657501\n",
      "epoch: 1 | 100480 / 114272 | training loss: 0.2767305076122284\n",
      "epoch: 1 | 100512 / 114272 | training loss: 0.18552744388580322\n",
      "epoch: 1 | 100544 / 114272 | training loss: 0.18843555450439453\n",
      "epoch: 1 | 100576 / 114272 | training loss: 0.01380730327218771\n",
      "epoch: 1 | 100608 / 114272 | training loss: 0.14883512258529663\n",
      "epoch: 1 | 100640 / 114272 | training loss: 0.05925757810473442\n",
      "epoch: 1 | 100672 / 114272 | training loss: 0.19652780890464783\n",
      "epoch: 1 | 100704 / 114272 | training loss: 0.029466157779097557\n",
      "epoch: 1 | 100736 / 114272 | training loss: 0.2446581870317459\n",
      "epoch: 1 | 100768 / 114272 | training loss: 0.026051202788949013\n",
      "epoch: 1 | 100800 / 114272 | training loss: 0.17963625490665436\n",
      "epoch: 1 | 100832 / 114272 | training loss: 0.015371720306575298\n",
      "epoch: 1 | 100864 / 114272 | training loss: 0.08289337158203125\n",
      "epoch: 1 | 100896 / 114272 | training loss: 0.20659798383712769\n",
      "epoch: 1 | 100928 / 114272 | training loss: 0.4046531915664673\n",
      "epoch: 1 | 100960 / 114272 | training loss: 0.14736317098140717\n",
      "epoch: 1 | 100992 / 114272 | training loss: 0.1576278805732727\n",
      "epoch: 1 | 101024 / 114272 | training loss: 0.16203522682189941\n",
      "epoch: 1 | 101056 / 114272 | training loss: 0.22327661514282227\n",
      "epoch: 1 | 101088 / 114272 | training loss: 0.2313861846923828\n",
      "epoch: 1 | 101120 / 114272 | training loss: 0.10961733013391495\n",
      "epoch: 1 | 101152 / 114272 | training loss: 0.06429944932460785\n",
      "epoch: 1 | 101184 / 114272 | training loss: 0.053230419754981995\n",
      "epoch: 1 | 101216 / 114272 | training loss: 0.025176046416163445\n",
      "epoch: 1 | 101248 / 114272 | training loss: 0.06295140087604523\n",
      "epoch: 1 | 101280 / 114272 | training loss: 0.0785437598824501\n",
      "epoch: 1 | 101312 / 114272 | training loss: 0.1480230838060379\n",
      "epoch: 1 | 101344 / 114272 | training loss: 0.33829495310783386\n",
      "epoch: 1 | 101376 / 114272 | training loss: 0.11075283586978912\n",
      "epoch: 1 | 101408 / 114272 | training loss: 0.16746820509433746\n",
      "epoch: 1 | 101440 / 114272 | training loss: 0.06376055628061295\n",
      "epoch: 1 | 101472 / 114272 | training loss: 0.05347888544201851\n",
      "epoch: 1 | 101504 / 114272 | training loss: 0.1474609375\n",
      "epoch: 1 | 101536 / 114272 | training loss: 0.010929301381111145\n",
      "epoch: 1 | 101568 / 114272 | training loss: 0.2185472697019577\n",
      "epoch: 1 | 101600 / 114272 | training loss: 0.08613904565572739\n",
      "epoch: 1 | 101632 / 114272 | training loss: 0.0739036574959755\n",
      "epoch: 1 | 101664 / 114272 | training loss: 0.08140365034341812\n",
      "epoch: 1 | 101696 / 114272 | training loss: 0.08098411560058594\n",
      "epoch: 1 | 101728 / 114272 | training loss: 0.05938084423542023\n",
      "epoch: 1 | 101760 / 114272 | training loss: 0.06258135288953781\n",
      "epoch: 1 | 101792 / 114272 | training loss: 0.1262306421995163\n",
      "epoch: 1 | 101824 / 114272 | training loss: 0.20351266860961914\n",
      "epoch: 1 | 101856 / 114272 | training loss: 0.15412351489067078\n",
      "epoch: 1 | 101888 / 114272 | training loss: 0.06946155428886414\n",
      "epoch: 1 | 101920 / 114272 | training loss: 0.20016305148601532\n",
      "epoch: 1 | 101952 / 114272 | training loss: 0.008652395568788052\n",
      "epoch: 1 | 101984 / 114272 | training loss: 0.03749571368098259\n",
      "epoch: 1 | 102016 / 114272 | training loss: 0.11118187755346298\n",
      "epoch: 1 | 102048 / 114272 | training loss: 0.01575627364218235\n",
      "epoch: 1 | 102080 / 114272 | training loss: 0.0008029547752812505\n",
      "epoch: 1 | 102112 / 114272 | training loss: 0.0629509910941124\n",
      "epoch: 1 | 102144 / 114272 | training loss: 0.10024404525756836\n",
      "epoch: 1 | 102176 / 114272 | training loss: 0.17867639660835266\n",
      "epoch: 1 | 102208 / 114272 | training loss: 0.36271560192108154\n",
      "epoch: 1 | 102240 / 114272 | training loss: 0.04433978721499443\n",
      "epoch: 1 | 102272 / 114272 | training loss: 0.08273643255233765\n",
      "epoch: 1 | 102304 / 114272 | training loss: 0.2015327662229538\n",
      "epoch: 1 | 102336 / 114272 | training loss: 0.26540234684944153\n",
      "epoch: 1 | 102368 / 114272 | training loss: 0.030629867687821388\n",
      "epoch: 1 | 102400 / 114272 | training loss: 0.04272393137216568\n",
      "epoch: 1 | 102432 / 114272 | training loss: 0.44025611877441406\n",
      "epoch: 1 | 102464 / 114272 | training loss: 0.05139491334557533\n",
      "epoch: 1 | 102496 / 114272 | training loss: 0.02688593417406082\n",
      "epoch: 1 | 102528 / 114272 | training loss: 0.07931878417730331\n",
      "epoch: 1 | 102560 / 114272 | training loss: 0.051540493965148926\n",
      "epoch: 1 | 102592 / 114272 | training loss: 0.11858823895454407\n",
      "epoch: 1 | 102624 / 114272 | training loss: 0.030997294932603836\n",
      "epoch: 1 | 102656 / 114272 | training loss: 0.19053488969802856\n",
      "epoch: 1 | 102688 / 114272 | training loss: 0.09213966131210327\n",
      "epoch: 1 | 102720 / 114272 | training loss: 0.0662640705704689\n",
      "epoch: 1 | 102752 / 114272 | training loss: 0.01277462299913168\n",
      "epoch: 1 | 102784 / 114272 | training loss: 0.22341127693653107\n",
      "epoch: 1 | 102816 / 114272 | training loss: 0.07130362093448639\n",
      "epoch: 1 | 102848 / 114272 | training loss: 0.032844096422195435\n",
      "epoch: 1 | 102880 / 114272 | training loss: 0.1040642037987709\n",
      "epoch: 1 | 102912 / 114272 | training loss: 0.20886045694351196\n",
      "epoch: 1 | 102944 / 114272 | training loss: 0.008095054887235165\n",
      "epoch: 1 | 102976 / 114272 | training loss: 0.06122380495071411\n",
      "epoch: 1 | 103008 / 114272 | training loss: 0.18662618100643158\n",
      "epoch: 1 | 103040 / 114272 | training loss: 0.08898595720529556\n",
      "epoch: 1 | 103072 / 114272 | training loss: 0.1203204095363617\n",
      "epoch: 1 | 103104 / 114272 | training loss: 0.14149391651153564\n",
      "epoch: 1 | 103136 / 114272 | training loss: 0.02731163613498211\n",
      "epoch: 1 | 103168 / 114272 | training loss: 0.07235854864120483\n",
      "epoch: 1 | 103200 / 114272 | training loss: 0.07321865856647491\n",
      "epoch: 1 | 103232 / 114272 | training loss: 0.26775527000427246\n",
      "epoch: 1 | 103264 / 114272 | training loss: 0.012804265134036541\n",
      "epoch: 1 | 103296 / 114272 | training loss: 0.36408427357673645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 103328 / 114272 | training loss: 0.22701771557331085\n",
      "epoch: 1 | 103360 / 114272 | training loss: 0.46395039558410645\n",
      "epoch: 1 | 103392 / 114272 | training loss: 0.051766447722911835\n",
      "epoch: 1 | 103424 / 114272 | training loss: 0.24423643946647644\n",
      "epoch: 1 | 103456 / 114272 | training loss: 0.11634255200624466\n",
      "epoch: 1 | 103488 / 114272 | training loss: 0.012602496892213821\n",
      "epoch: 1 | 103520 / 114272 | training loss: 0.10887779295444489\n",
      "epoch: 1 | 103552 / 114272 | training loss: 0.17162133753299713\n",
      "epoch: 1 | 103584 / 114272 | training loss: 0.1428215354681015\n",
      "epoch: 1 | 103616 / 114272 | training loss: 0.07828102260828018\n",
      "epoch: 1 | 103648 / 114272 | training loss: 0.007112445309758186\n",
      "epoch: 1 | 103680 / 114272 | training loss: 0.2229541838169098\n",
      "epoch: 1 | 103712 / 114272 | training loss: 0.1610042303800583\n",
      "epoch: 1 | 103744 / 114272 | training loss: 0.07742650806903839\n",
      "epoch: 1 | 103776 / 114272 | training loss: 0.20814235508441925\n",
      "epoch: 1 | 103808 / 114272 | training loss: 0.014053497463464737\n",
      "epoch: 1 | 103840 / 114272 | training loss: 0.1623900830745697\n",
      "epoch: 1 | 103872 / 114272 | training loss: 0.033809103071689606\n",
      "epoch: 1 | 103904 / 114272 | training loss: 0.03692350164055824\n",
      "epoch: 1 | 103936 / 114272 | training loss: 0.0847790390253067\n",
      "epoch: 1 | 103968 / 114272 | training loss: 0.4776742458343506\n",
      "epoch: 1 | 104000 / 114272 | training loss: 0.0353095643222332\n",
      "epoch: 1 | 104032 / 114272 | training loss: 0.11936173588037491\n",
      "epoch: 1 | 104064 / 114272 | training loss: 0.09779320657253265\n",
      "epoch: 1 | 104096 / 114272 | training loss: 0.2571246027946472\n",
      "epoch: 1 | 104128 / 114272 | training loss: 0.038551539182662964\n",
      "epoch: 1 | 104160 / 114272 | training loss: 0.0329417921602726\n",
      "epoch: 1 | 104192 / 114272 | training loss: 0.00949808955192566\n",
      "epoch: 1 | 104224 / 114272 | training loss: 0.1779782772064209\n",
      "epoch: 1 | 104256 / 114272 | training loss: 0.13963255286216736\n",
      "epoch: 1 | 104288 / 114272 | training loss: 0.009422757662832737\n",
      "epoch: 1 | 104320 / 114272 | training loss: 0.21341794729232788\n",
      "epoch: 1 | 104352 / 114272 | training loss: 0.30723312497138977\n",
      "epoch: 1 | 104384 / 114272 | training loss: 0.14765942096710205\n",
      "epoch: 1 | 104416 / 114272 | training loss: 0.07814816385507584\n",
      "epoch: 1 | 104448 / 114272 | training loss: 0.04865571856498718\n",
      "epoch: 1 | 104480 / 114272 | training loss: 0.09456584602594376\n",
      "epoch: 1 | 104512 / 114272 | training loss: 0.2866140305995941\n",
      "epoch: 1 | 104544 / 114272 | training loss: 0.08740907907485962\n",
      "epoch: 1 | 104576 / 114272 | training loss: 0.033571451902389526\n",
      "epoch: 1 | 104608 / 114272 | training loss: 0.028582299128174782\n",
      "epoch: 1 | 104640 / 114272 | training loss: 0.42414674162864685\n",
      "epoch: 1 | 104672 / 114272 | training loss: 0.13219450414180756\n",
      "epoch: 1 | 104704 / 114272 | training loss: 0.21892336010932922\n",
      "epoch: 1 | 104736 / 114272 | training loss: 0.19010920822620392\n",
      "epoch: 1 | 104768 / 114272 | training loss: 0.10733461380004883\n",
      "epoch: 1 | 104800 / 114272 | training loss: 0.35942643880844116\n",
      "epoch: 1 | 104832 / 114272 | training loss: 0.16237594187259674\n",
      "epoch: 1 | 104864 / 114272 | training loss: 0.020620526745915413\n",
      "epoch: 1 | 104896 / 114272 | training loss: 0.21824313700199127\n",
      "epoch: 1 | 104928 / 114272 | training loss: 0.04023556783795357\n",
      "epoch: 1 | 104960 / 114272 | training loss: 0.03591368719935417\n",
      "epoch: 1 | 104992 / 114272 | training loss: 0.05112900584936142\n",
      "epoch: 1 | 105024 / 114272 | training loss: 0.10728996992111206\n",
      "epoch: 1 | 105056 / 114272 | training loss: 0.026097696274518967\n",
      "epoch: 1 | 105088 / 114272 | training loss: 0.026662131771445274\n",
      "epoch: 1 | 105120 / 114272 | training loss: 0.2804788053035736\n",
      "epoch: 1 | 105152 / 114272 | training loss: 0.07219648361206055\n",
      "epoch: 1 | 105184 / 114272 | training loss: 0.14469753205776215\n",
      "epoch: 1 | 105216 / 114272 | training loss: 0.02621915563941002\n",
      "epoch: 1 | 105248 / 114272 | training loss: 0.13717584311962128\n",
      "epoch: 1 | 105280 / 114272 | training loss: 0.05269009992480278\n",
      "epoch: 1 | 105312 / 114272 | training loss: 0.1268409937620163\n",
      "epoch: 1 | 105344 / 114272 | training loss: 0.246886745095253\n",
      "epoch: 1 | 105376 / 114272 | training loss: 0.2796527147293091\n",
      "epoch: 1 | 105408 / 114272 | training loss: 0.01111964974552393\n",
      "epoch: 1 | 105440 / 114272 | training loss: 0.009413755498826504\n",
      "epoch: 1 | 105472 / 114272 | training loss: 0.011493357829749584\n",
      "epoch: 1 | 105504 / 114272 | training loss: 0.253747820854187\n",
      "epoch: 1 | 105536 / 114272 | training loss: 0.3227229118347168\n",
      "epoch: 1 | 105568 / 114272 | training loss: 0.1394139528274536\n",
      "epoch: 1 | 105600 / 114272 | training loss: 0.2217165231704712\n",
      "epoch: 1 | 105632 / 114272 | training loss: 0.11305145174264908\n",
      "epoch: 1 | 105664 / 114272 | training loss: 0.1990920752286911\n",
      "epoch: 1 | 105696 / 114272 | training loss: 0.1870715171098709\n",
      "epoch: 1 | 105728 / 114272 | training loss: 0.15337654948234558\n",
      "epoch: 1 | 105760 / 114272 | training loss: 0.17850765585899353\n",
      "epoch: 1 | 105792 / 114272 | training loss: 0.10338419675827026\n",
      "epoch: 1 | 105824 / 114272 | training loss: 0.1341710090637207\n",
      "epoch: 1 | 105856 / 114272 | training loss: 0.011146070435643196\n",
      "epoch: 1 | 105888 / 114272 | training loss: 0.04043668881058693\n",
      "epoch: 1 | 105920 / 114272 | training loss: 0.010458322241902351\n",
      "epoch: 1 | 105952 / 114272 | training loss: 0.02452467568218708\n",
      "epoch: 1 | 105984 / 114272 | training loss: 0.055555980652570724\n",
      "epoch: 1 | 106016 / 114272 | training loss: 0.26356980204582214\n",
      "epoch: 1 | 106048 / 114272 | training loss: 0.08464453369379044\n",
      "epoch: 1 | 106080 / 114272 | training loss: 0.09719602763652802\n",
      "epoch: 1 | 106112 / 114272 | training loss: 0.2131439745426178\n",
      "epoch: 1 | 106144 / 114272 | training loss: 0.16597265005111694\n",
      "epoch: 1 | 106176 / 114272 | training loss: 0.1297527402639389\n",
      "epoch: 1 | 106208 / 114272 | training loss: 0.023137029260396957\n",
      "epoch: 1 | 106240 / 114272 | training loss: 0.121125727891922\n",
      "epoch: 1 | 106272 / 114272 | training loss: 0.27482980489730835\n",
      "epoch: 1 | 106304 / 114272 | training loss: 0.13625849783420563\n",
      "epoch: 1 | 106336 / 114272 | training loss: 0.05139119550585747\n",
      "epoch: 1 | 106368 / 114272 | training loss: 0.03329920396208763\n",
      "epoch: 1 | 106400 / 114272 | training loss: 0.029200857505202293\n",
      "epoch: 1 | 106432 / 114272 | training loss: 0.02251097559928894\n",
      "epoch: 1 | 106464 / 114272 | training loss: 0.10972374677658081\n",
      "epoch: 1 | 106496 / 114272 | training loss: 0.06445159763097763\n",
      "epoch: 1 | 106528 / 114272 | training loss: 0.20190338790416718\n",
      "epoch: 1 | 106560 / 114272 | training loss: 0.05111744999885559\n",
      "epoch: 1 | 106592 / 114272 | training loss: 0.08612032234668732\n",
      "epoch: 1 | 106624 / 114272 | training loss: 0.24569353461265564\n",
      "epoch: 1 | 106656 / 114272 | training loss: 0.14408475160598755\n",
      "epoch: 1 | 106688 / 114272 | training loss: 0.06489735096693039\n",
      "epoch: 1 | 106720 / 114272 | training loss: 0.06246684119105339\n",
      "epoch: 1 | 106752 / 114272 | training loss: 0.019128987565636635\n",
      "epoch: 1 | 106784 / 114272 | training loss: 0.2013910710811615\n",
      "epoch: 1 | 106816 / 114272 | training loss: 0.2180795520544052\n",
      "epoch: 1 | 106848 / 114272 | training loss: 0.006004644557833672\n",
      "epoch: 1 | 106880 / 114272 | training loss: 0.10721759498119354\n",
      "epoch: 1 | 106912 / 114272 | training loss: 0.15781119465827942\n",
      "epoch: 1 | 106944 / 114272 | training loss: 0.0205184668302536\n",
      "epoch: 1 | 106976 / 114272 | training loss: 0.09334978461265564\n",
      "epoch: 1 | 107008 / 114272 | training loss: 0.07386083900928497\n",
      "epoch: 1 | 107040 / 114272 | training loss: 0.13230469822883606\n",
      "epoch: 1 | 107072 / 114272 | training loss: 0.13659648597240448\n",
      "epoch: 1 | 107104 / 114272 | training loss: 0.20699350535869598\n",
      "epoch: 1 | 107136 / 114272 | training loss: 0.02672533690929413\n",
      "epoch: 1 | 107168 / 114272 | training loss: 0.174499049782753\n",
      "epoch: 1 | 107200 / 114272 | training loss: 0.12811756134033203\n",
      "epoch: 1 | 107232 / 114272 | training loss: 0.04738743603229523\n",
      "epoch: 1 | 107264 / 114272 | training loss: 0.009760502725839615\n",
      "epoch: 1 | 107296 / 114272 | training loss: 0.2438289225101471\n",
      "epoch: 1 | 107328 / 114272 | training loss: 0.027354951947927475\n",
      "epoch: 1 | 107360 / 114272 | training loss: 0.08982153236865997\n",
      "epoch: 1 | 107392 / 114272 | training loss: 0.08062168955802917\n",
      "epoch: 1 | 107424 / 114272 | training loss: 0.21275919675827026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 107456 / 114272 | training loss: 0.16024717688560486\n",
      "epoch: 1 | 107488 / 114272 | training loss: 0.01917058788239956\n",
      "epoch: 1 | 107520 / 114272 | training loss: 0.09355680644512177\n",
      "epoch: 1 | 107552 / 114272 | training loss: 0.011423797346651554\n",
      "epoch: 1 | 107584 / 114272 | training loss: 0.2975907027721405\n",
      "epoch: 1 | 107616 / 114272 | training loss: 0.2126515656709671\n",
      "epoch: 1 | 107648 / 114272 | training loss: 0.3538604974746704\n",
      "epoch: 1 | 107680 / 114272 | training loss: 0.0381467230618\n",
      "epoch: 1 | 107712 / 114272 | training loss: 0.11271531879901886\n",
      "epoch: 1 | 107744 / 114272 | training loss: 0.03120531141757965\n",
      "epoch: 1 | 107776 / 114272 | training loss: 0.08196955174207687\n",
      "epoch: 1 | 107808 / 114272 | training loss: 0.028640812262892723\n",
      "epoch: 1 | 107840 / 114272 | training loss: 0.28523552417755127\n",
      "epoch: 1 | 107872 / 114272 | training loss: 0.05106218904256821\n",
      "epoch: 1 | 107904 / 114272 | training loss: 0.4283360242843628\n",
      "epoch: 1 | 107936 / 114272 | training loss: 0.2581113874912262\n",
      "epoch: 1 | 107968 / 114272 | training loss: 0.020352549850940704\n",
      "epoch: 1 | 108000 / 114272 | training loss: 0.21008262038230896\n",
      "epoch: 1 | 108032 / 114272 | training loss: 0.14276602864265442\n",
      "epoch: 1 | 108064 / 114272 | training loss: 0.09812906384468079\n",
      "epoch: 1 | 108096 / 114272 | training loss: 0.23089314997196198\n",
      "epoch: 1 | 108128 / 114272 | training loss: 0.026009902358055115\n",
      "epoch: 1 | 108160 / 114272 | training loss: 0.022953439503908157\n",
      "epoch: 1 | 108192 / 114272 | training loss: 0.17381581664085388\n",
      "epoch: 1 | 108224 / 114272 | training loss: 0.14484943449497223\n",
      "epoch: 1 | 108256 / 114272 | training loss: 0.007458447013050318\n",
      "epoch: 1 | 108288 / 114272 | training loss: 0.054081205278635025\n",
      "epoch: 1 | 108320 / 114272 | training loss: 0.020837707445025444\n",
      "epoch: 1 | 108352 / 114272 | training loss: 0.14026305079460144\n",
      "epoch: 1 | 108384 / 114272 | training loss: 0.08738305419683456\n",
      "epoch: 1 | 108416 / 114272 | training loss: 0.09961356222629547\n",
      "epoch: 1 | 108448 / 114272 | training loss: 0.0855659693479538\n",
      "epoch: 1 | 108480 / 114272 | training loss: 0.019451864063739777\n",
      "epoch: 1 | 108512 / 114272 | training loss: 0.019258832558989525\n",
      "epoch: 1 | 108544 / 114272 | training loss: 0.07203705608844757\n",
      "epoch: 1 | 108576 / 114272 | training loss: 0.04079914465546608\n",
      "epoch: 1 | 108608 / 114272 | training loss: 0.012903239578008652\n",
      "epoch: 1 | 108640 / 114272 | training loss: 0.10254202783107758\n",
      "epoch: 1 | 108672 / 114272 | training loss: 0.22217664122581482\n",
      "epoch: 1 | 108704 / 114272 | training loss: 0.012628216296434402\n",
      "epoch: 1 | 108736 / 114272 | training loss: 0.1469488888978958\n",
      "epoch: 1 | 108768 / 114272 | training loss: 0.05083048343658447\n",
      "epoch: 1 | 108800 / 114272 | training loss: 0.03486534208059311\n",
      "epoch: 1 | 108832 / 114272 | training loss: 0.06949785351753235\n",
      "epoch: 1 | 108864 / 114272 | training loss: 0.008566740900278091\n",
      "epoch: 1 | 108896 / 114272 | training loss: 0.22990699112415314\n",
      "epoch: 1 | 108928 / 114272 | training loss: 0.26062342524528503\n",
      "epoch: 1 | 108960 / 114272 | training loss: 0.2981380820274353\n",
      "epoch: 1 | 108992 / 114272 | training loss: 0.09204407036304474\n",
      "epoch: 1 | 109024 / 114272 | training loss: 0.17885179817676544\n",
      "epoch: 1 | 109056 / 114272 | training loss: 0.061449967324733734\n",
      "epoch: 1 | 109088 / 114272 | training loss: 0.43772032856941223\n",
      "epoch: 1 | 109120 / 114272 | training loss: 0.13796505331993103\n",
      "epoch: 1 | 109152 / 114272 | training loss: 0.05960487201809883\n",
      "epoch: 1 | 109184 / 114272 | training loss: 0.0324215330183506\n",
      "epoch: 1 | 109216 / 114272 | training loss: 0.07911096513271332\n",
      "epoch: 1 | 109248 / 114272 | training loss: 0.21495389938354492\n",
      "epoch: 1 | 109280 / 114272 | training loss: 0.1478155106306076\n",
      "epoch: 1 | 109312 / 114272 | training loss: 0.06643971055746078\n",
      "epoch: 1 | 109344 / 114272 | training loss: 0.11301419138908386\n",
      "epoch: 1 | 109376 / 114272 | training loss: 0.0434882678091526\n",
      "epoch: 1 | 109408 / 114272 | training loss: 0.01103069819509983\n",
      "epoch: 1 | 109440 / 114272 | training loss: 0.026296108961105347\n",
      "epoch: 1 | 109472 / 114272 | training loss: 0.030210405588150024\n",
      "epoch: 1 | 109504 / 114272 | training loss: 0.0929289236664772\n",
      "epoch: 1 | 109536 / 114272 | training loss: 0.02401943877339363\n",
      "epoch: 1 | 109568 / 114272 | training loss: 0.17745329439640045\n",
      "epoch: 1 | 109600 / 114272 | training loss: 0.08408797532320023\n",
      "epoch: 1 | 109632 / 114272 | training loss: 0.11335410177707672\n",
      "epoch: 1 | 109664 / 114272 | training loss: 0.14237263798713684\n",
      "epoch: 1 | 109696 / 114272 | training loss: 0.2300940901041031\n",
      "epoch: 1 | 109728 / 114272 | training loss: 0.003962878603488207\n",
      "epoch: 1 | 109760 / 114272 | training loss: 0.037474822252988815\n",
      "epoch: 1 | 109792 / 114272 | training loss: 0.014828896149992943\n",
      "epoch: 1 | 109824 / 114272 | training loss: 0.09141697734594345\n",
      "epoch: 1 | 109856 / 114272 | training loss: 0.09507555514574051\n",
      "epoch: 1 | 109888 / 114272 | training loss: 0.4299660921096802\n",
      "epoch: 1 | 109920 / 114272 | training loss: 0.07745020091533661\n",
      "epoch: 1 | 109952 / 114272 | training loss: 0.4111920893192291\n",
      "epoch: 1 | 109984 / 114272 | training loss: 0.19808965921401978\n",
      "epoch: 1 | 110016 / 114272 | training loss: 0.016094136983156204\n",
      "epoch: 1 | 110048 / 114272 | training loss: 0.08838904649019241\n",
      "epoch: 1 | 110080 / 114272 | training loss: 0.07495229691267014\n",
      "epoch: 1 | 110112 / 114272 | training loss: 0.07158952951431274\n",
      "epoch: 1 | 110144 / 114272 | training loss: 0.08165789395570755\n",
      "epoch: 1 | 110176 / 114272 | training loss: 0.09364058077335358\n",
      "epoch: 1 | 110208 / 114272 | training loss: 0.01108347438275814\n",
      "epoch: 1 | 110240 / 114272 | training loss: 0.0500430166721344\n",
      "epoch: 1 | 110272 / 114272 | training loss: 0.15301787853240967\n",
      "epoch: 1 | 110304 / 114272 | training loss: 0.10290639102458954\n",
      "epoch: 1 | 110336 / 114272 | training loss: 0.1879119873046875\n",
      "epoch: 1 | 110368 / 114272 | training loss: 0.06208735704421997\n",
      "epoch: 1 | 110400 / 114272 | training loss: 0.33509960770606995\n",
      "epoch: 1 | 110432 / 114272 | training loss: 0.13365907967090607\n",
      "epoch: 1 | 110464 / 114272 | training loss: 0.17067328095436096\n",
      "epoch: 1 | 110496 / 114272 | training loss: 0.05706445127725601\n",
      "epoch: 1 | 110528 / 114272 | training loss: 0.2075381577014923\n",
      "epoch: 1 | 110560 / 114272 | training loss: 0.02770397998392582\n",
      "epoch: 1 | 110592 / 114272 | training loss: 0.09996435791254044\n",
      "epoch: 1 | 110624 / 114272 | training loss: 0.3081323504447937\n",
      "epoch: 1 | 110656 / 114272 | training loss: 0.03406214341521263\n",
      "epoch: 1 | 110688 / 114272 | training loss: 0.004913900047540665\n",
      "epoch: 1 | 110720 / 114272 | training loss: 0.01498773880302906\n",
      "epoch: 1 | 110752 / 114272 | training loss: 0.11707592755556107\n",
      "epoch: 1 | 110784 / 114272 | training loss: 0.020551711320877075\n",
      "epoch: 1 | 110816 / 114272 | training loss: 0.1459886133670807\n",
      "epoch: 1 | 110848 / 114272 | training loss: 0.0266415998339653\n",
      "epoch: 1 | 110880 / 114272 | training loss: 0.014542809687554836\n",
      "epoch: 1 | 110912 / 114272 | training loss: 0.19757650792598724\n",
      "epoch: 1 | 110944 / 114272 | training loss: 0.05430765077471733\n",
      "epoch: 1 | 110976 / 114272 | training loss: 0.225189670920372\n",
      "epoch: 1 | 111008 / 114272 | training loss: 0.007191715762019157\n",
      "epoch: 1 | 111040 / 114272 | training loss: 0.22982260584831238\n",
      "epoch: 1 | 111072 / 114272 | training loss: 0.42523279786109924\n",
      "epoch: 1 | 111104 / 114272 | training loss: 0.07959062606096268\n",
      "epoch: 1 | 111136 / 114272 | training loss: 0.14170926809310913\n",
      "epoch: 1 | 111168 / 114272 | training loss: 0.008834442123770714\n",
      "epoch: 1 | 111200 / 114272 | training loss: 0.0691637396812439\n",
      "epoch: 1 | 111232 / 114272 | training loss: 0.10815678536891937\n",
      "epoch: 1 | 111264 / 114272 | training loss: 0.011762039735913277\n",
      "epoch: 1 | 111296 / 114272 | training loss: 0.2896572947502136\n",
      "epoch: 1 | 111328 / 114272 | training loss: 0.03727254271507263\n",
      "epoch: 1 | 111360 / 114272 | training loss: 0.0073565333150327206\n",
      "epoch: 1 | 111392 / 114272 | training loss: 0.03907665237784386\n",
      "epoch: 1 | 111424 / 114272 | training loss: 0.018012745305895805\n",
      "epoch: 1 | 111456 / 114272 | training loss: 0.030550364404916763\n",
      "epoch: 1 | 111488 / 114272 | training loss: 0.16040144860744476\n",
      "epoch: 1 | 111520 / 114272 | training loss: 0.05719596892595291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | 111552 / 114272 | training loss: 0.045932989567518234\n",
      "epoch: 1 | 111584 / 114272 | training loss: 0.22501835227012634\n",
      "epoch: 1 | 111616 / 114272 | training loss: 0.21101608872413635\n",
      "epoch: 1 | 111648 / 114272 | training loss: 0.05832543596625328\n",
      "epoch: 1 | 111680 / 114272 | training loss: 0.2735695242881775\n",
      "epoch: 1 | 111712 / 114272 | training loss: 0.17644448578357697\n",
      "epoch: 1 | 111744 / 114272 | training loss: 0.1060640960931778\n",
      "epoch: 1 | 111776 / 114272 | training loss: 0.029904428869485855\n",
      "epoch: 1 | 111808 / 114272 | training loss: 0.2901190221309662\n",
      "epoch: 1 | 111840 / 114272 | training loss: 0.2388782501220703\n",
      "epoch: 1 | 111872 / 114272 | training loss: 0.008218835107982159\n",
      "epoch: 1 | 111904 / 114272 | training loss: 0.21223676204681396\n",
      "epoch: 1 | 111936 / 114272 | training loss: 0.0757441371679306\n",
      "epoch: 1 | 111968 / 114272 | training loss: 0.01803627610206604\n",
      "epoch: 1 | 112000 / 114272 | training loss: 0.11763551831245422\n",
      "epoch: 1 | 112032 / 114272 | training loss: 0.20137423276901245\n",
      "epoch: 1 | 112064 / 114272 | training loss: 0.1842237412929535\n",
      "epoch: 1 | 112096 / 114272 | training loss: 0.3500064015388489\n",
      "epoch: 1 | 112128 / 114272 | training loss: 0.2689671516418457\n",
      "epoch: 1 | 112160 / 114272 | training loss: 0.33795371651649475\n",
      "epoch: 1 | 112192 / 114272 | training loss: 0.03227740526199341\n",
      "epoch: 1 | 112224 / 114272 | training loss: 0.2112639844417572\n",
      "epoch: 1 | 112256 / 114272 | training loss: 0.20454391837120056\n",
      "epoch: 1 | 112288 / 114272 | training loss: 0.13565130531787872\n",
      "epoch: 1 | 112320 / 114272 | training loss: 0.010641228407621384\n",
      "epoch: 1 | 112352 / 114272 | training loss: 0.04821541905403137\n",
      "epoch: 1 | 112384 / 114272 | training loss: 0.025959065183997154\n",
      "epoch: 1 | 112416 / 114272 | training loss: 0.02926739491522312\n",
      "epoch: 1 | 112448 / 114272 | training loss: 0.2243840992450714\n",
      "epoch: 1 | 112480 / 114272 | training loss: 0.06899608671665192\n",
      "epoch: 1 | 112512 / 114272 | training loss: 0.09375303238630295\n",
      "epoch: 1 | 112544 / 114272 | training loss: 0.2711504399776459\n",
      "epoch: 1 | 112576 / 114272 | training loss: 0.04560377076268196\n",
      "epoch: 1 | 112608 / 114272 | training loss: 0.07114061713218689\n",
      "epoch: 1 | 112640 / 114272 | training loss: 0.3381120562553406\n",
      "epoch: 1 | 112672 / 114272 | training loss: 0.11794880777597427\n",
      "epoch: 1 | 112704 / 114272 | training loss: 0.024839483201503754\n",
      "epoch: 1 | 112736 / 114272 | training loss: 0.10986397415399551\n",
      "epoch: 1 | 112768 / 114272 | training loss: 0.18851222097873688\n",
      "epoch: 1 | 112800 / 114272 | training loss: 0.08439914882183075\n",
      "epoch: 1 | 112832 / 114272 | training loss: 0.17704446613788605\n",
      "epoch: 1 | 112864 / 114272 | training loss: 0.22629737854003906\n",
      "epoch: 1 | 112896 / 114272 | training loss: 0.011607534252107143\n",
      "epoch: 1 | 112928 / 114272 | training loss: 0.06948331743478775\n",
      "epoch: 1 | 112960 / 114272 | training loss: 0.02806316502392292\n",
      "epoch: 1 | 112992 / 114272 | training loss: 0.11386985331773758\n",
      "epoch: 1 | 113024 / 114272 | training loss: 0.07699502259492874\n",
      "epoch: 1 | 113056 / 114272 | training loss: 0.18645857274532318\n",
      "epoch: 1 | 113088 / 114272 | training loss: 0.12519285082817078\n",
      "epoch: 1 | 113120 / 114272 | training loss: 0.2501815855503082\n",
      "epoch: 1 | 113152 / 114272 | training loss: 0.1482352763414383\n",
      "epoch: 1 | 113184 / 114272 | training loss: 0.14123229682445526\n",
      "epoch: 1 | 113216 / 114272 | training loss: 0.21104800701141357\n",
      "epoch: 1 | 113248 / 114272 | training loss: 0.1350153535604477\n",
      "epoch: 1 | 113280 / 114272 | training loss: 0.1540497988462448\n",
      "epoch: 1 | 113312 / 114272 | training loss: 0.11123010516166687\n",
      "epoch: 1 | 113344 / 114272 | training loss: 0.04576028138399124\n",
      "epoch: 1 | 113376 / 114272 | training loss: 0.2094193696975708\n",
      "epoch: 1 | 113408 / 114272 | training loss: 0.49612852931022644\n",
      "epoch: 1 | 113440 / 114272 | training loss: 0.06947056204080582\n",
      "epoch: 1 | 113472 / 114272 | training loss: 0.21001885831356049\n",
      "epoch: 1 | 113504 / 114272 | training loss: 0.05425872653722763\n",
      "epoch: 1 | 113536 / 114272 | training loss: 0.10794751346111298\n",
      "epoch: 1 | 113568 / 114272 | training loss: 0.19030138850212097\n",
      "epoch: 1 | 113600 / 114272 | training loss: 0.2506798207759857\n",
      "epoch: 1 | 113632 / 114272 | training loss: 0.12075920403003693\n",
      "epoch: 1 | 113664 / 114272 | training loss: 0.07883183658123016\n",
      "epoch: 1 | 113696 / 114272 | training loss: 0.09248648583889008\n",
      "epoch: 1 | 113728 / 114272 | training loss: 0.13378527760505676\n",
      "epoch: 1 | 113760 / 114272 | training loss: 0.06156494840979576\n",
      "epoch: 1 | 113792 / 114272 | training loss: 0.2861008942127228\n",
      "epoch: 1 | 113824 / 114272 | training loss: 0.32764628529548645\n",
      "epoch: 1 | 113856 / 114272 | training loss: 0.014097738079726696\n",
      "epoch: 1 | 113888 / 114272 | training loss: 0.2039088010787964\n",
      "epoch: 1 | 113920 / 114272 | training loss: 0.017506049945950508\n",
      "epoch: 1 | 113952 / 114272 | training loss: 0.08194339275360107\n",
      "epoch: 1 | 113984 / 114272 | training loss: 0.09358978271484375\n",
      "epoch: 1 | 114016 / 114272 | training loss: 0.17450553178787231\n",
      "epoch: 1 | 114048 / 114272 | training loss: 0.03511452674865723\n",
      "epoch: 1 | 114080 / 114272 | training loss: 0.1237787976861\n",
      "epoch: 1 | 114112 / 114272 | training loss: 0.017951838672161102\n",
      "epoch: 1 | 114144 / 114272 | training loss: 0.33343878388404846\n",
      "epoch: 1 | 114176 / 114272 | training loss: 0.03759423643350601\n",
      "epoch: 1 | 114208 / 114272 | training loss: 0.28179624676704407\n",
      "epoch: 1 | 114240 / 114272 | training loss: 0.15333689749240875\n",
      "Training epoch 1 done! Average loss: 0.11737468675987363. Accuracy: 0.9612941052926351\n",
      "Validation epoch 1 done! Average loss: 0.14283176324636543. Accurage: 0.9525307606263982\n",
      "Epoch 3 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 2 | 0 / 114272 | training loss: 0.028731724247336388\n",
      "epoch: 2 | 32 / 114272 | training loss: 0.07765030115842819\n",
      "epoch: 2 | 64 / 114272 | training loss: 0.015224454924464226\n",
      "epoch: 2 | 96 / 114272 | training loss: 0.35102924704551697\n",
      "epoch: 2 | 128 / 114272 | training loss: 0.3326215147972107\n",
      "epoch: 2 | 160 / 114272 | training loss: 0.08333810418844223\n",
      "epoch: 2 | 192 / 114272 | training loss: 0.00924661010503769\n",
      "epoch: 2 | 224 / 114272 | training loss: 0.048807524144649506\n",
      "epoch: 2 | 256 / 114272 | training loss: 0.032282017171382904\n",
      "epoch: 2 | 288 / 114272 | training loss: 0.16891594231128693\n",
      "epoch: 2 | 320 / 114272 | training loss: 0.07010120898485184\n",
      "epoch: 2 | 352 / 114272 | training loss: 0.11396940052509308\n",
      "epoch: 2 | 384 / 114272 | training loss: 0.019987188279628754\n",
      "epoch: 2 | 416 / 114272 | training loss: 0.1855594664812088\n",
      "epoch: 2 | 448 / 114272 | training loss: 0.013354947790503502\n",
      "epoch: 2 | 480 / 114272 | training loss: 0.02675996534526348\n",
      "epoch: 2 | 512 / 114272 | training loss: 0.012186801061034203\n",
      "epoch: 2 | 544 / 114272 | training loss: 0.03169173747301102\n",
      "epoch: 2 | 576 / 114272 | training loss: 0.14095649123191833\n",
      "epoch: 2 | 608 / 114272 | training loss: 0.21621675789356232\n",
      "epoch: 2 | 640 / 114272 | training loss: 0.07218483090400696\n",
      "epoch: 2 | 672 / 114272 | training loss: 0.13348393142223358\n",
      "epoch: 2 | 704 / 114272 | training loss: 0.008175010792911053\n",
      "epoch: 2 | 736 / 114272 | training loss: 0.3512172996997833\n",
      "epoch: 2 | 768 / 114272 | training loss: 0.01591748557984829\n",
      "epoch: 2 | 800 / 114272 | training loss: 0.018225064501166344\n",
      "epoch: 2 | 832 / 114272 | training loss: 0.03643547371029854\n",
      "epoch: 2 | 864 / 114272 | training loss: 0.014744342304766178\n",
      "epoch: 2 | 896 / 114272 | training loss: 0.01014694944024086\n",
      "epoch: 2 | 928 / 114272 | training loss: 0.14310279488563538\n",
      "epoch: 2 | 960 / 114272 | training loss: 0.12821093201637268\n",
      "epoch: 2 | 992 / 114272 | training loss: 0.05869489908218384\n",
      "epoch: 2 | 1024 / 114272 | training loss: 0.09055731445550919\n",
      "epoch: 2 | 1056 / 114272 | training loss: 0.23873190581798553\n",
      "epoch: 2 | 1088 / 114272 | training loss: 0.01494046300649643\n",
      "epoch: 2 | 1120 / 114272 | training loss: 0.009250938892364502\n",
      "epoch: 2 | 1152 / 114272 | training loss: 0.14176243543624878\n",
      "epoch: 2 | 1184 / 114272 | training loss: 0.03167741373181343\n",
      "epoch: 2 | 1216 / 114272 | training loss: 0.12501110136508942\n",
      "epoch: 2 | 1248 / 114272 | training loss: 0.024476848542690277\n",
      "epoch: 2 | 1280 / 114272 | training loss: 0.3659448027610779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 1312 / 114272 | training loss: 0.005377381108701229\n",
      "epoch: 2 | 1344 / 114272 | training loss: 0.24126730859279633\n",
      "epoch: 2 | 1376 / 114272 | training loss: 0.021853378042578697\n",
      "epoch: 2 | 1408 / 114272 | training loss: 0.1298956573009491\n",
      "epoch: 2 | 1440 / 114272 | training loss: 0.019454678520560265\n",
      "epoch: 2 | 1472 / 114272 | training loss: 0.005144437309354544\n",
      "epoch: 2 | 1504 / 114272 | training loss: 0.036705926060676575\n",
      "epoch: 2 | 1536 / 114272 | training loss: 0.23768387734889984\n",
      "epoch: 2 | 1568 / 114272 | training loss: 0.05735538899898529\n",
      "epoch: 2 | 1600 / 114272 | training loss: 0.10462579131126404\n",
      "epoch: 2 | 1632 / 114272 | training loss: 0.010648849420249462\n",
      "epoch: 2 | 1664 / 114272 | training loss: 0.2204391211271286\n",
      "epoch: 2 | 1696 / 114272 | training loss: 0.12284430861473083\n",
      "epoch: 2 | 1728 / 114272 | training loss: 0.005535766948014498\n",
      "epoch: 2 | 1760 / 114272 | training loss: 0.036691874265670776\n",
      "epoch: 2 | 1792 / 114272 | training loss: 0.12753745913505554\n",
      "epoch: 2 | 1824 / 114272 | training loss: 0.13943760097026825\n",
      "epoch: 2 | 1856 / 114272 | training loss: 0.09313417971134186\n",
      "epoch: 2 | 1888 / 114272 | training loss: 0.044437941163778305\n",
      "epoch: 2 | 1920 / 114272 | training loss: 0.20520532131195068\n",
      "epoch: 2 | 1952 / 114272 | training loss: 0.022467169910669327\n",
      "epoch: 2 | 1984 / 114272 | training loss: 0.05323977768421173\n",
      "epoch: 2 | 2016 / 114272 | training loss: 0.06420861929655075\n",
      "epoch: 2 | 2048 / 114272 | training loss: 0.03418509289622307\n",
      "epoch: 2 | 2080 / 114272 | training loss: 0.1835852414369583\n",
      "epoch: 2 | 2112 / 114272 | training loss: 0.052122391760349274\n",
      "epoch: 2 | 2144 / 114272 | training loss: 0.0851532518863678\n",
      "epoch: 2 | 2176 / 114272 | training loss: 0.023655684664845467\n",
      "epoch: 2 | 2208 / 114272 | training loss: 0.20067906379699707\n",
      "epoch: 2 | 2240 / 114272 | training loss: 0.07855063676834106\n",
      "epoch: 2 | 2272 / 114272 | training loss: 0.028896957635879517\n",
      "epoch: 2 | 2304 / 114272 | training loss: 0.04749155044555664\n",
      "epoch: 2 | 2336 / 114272 | training loss: 0.04228295758366585\n",
      "epoch: 2 | 2368 / 114272 | training loss: 0.0065419659949839115\n",
      "epoch: 2 | 2400 / 114272 | training loss: 0.008966845460236073\n",
      "epoch: 2 | 2432 / 114272 | training loss: 0.013624688610434532\n",
      "epoch: 2 | 2464 / 114272 | training loss: 0.024503855034708977\n",
      "epoch: 2 | 2496 / 114272 | training loss: 0.078716941177845\n",
      "epoch: 2 | 2528 / 114272 | training loss: 0.013242526911199093\n",
      "epoch: 2 | 2560 / 114272 | training loss: 0.04704825580120087\n",
      "epoch: 2 | 2592 / 114272 | training loss: 0.34051311016082764\n",
      "epoch: 2 | 2624 / 114272 | training loss: 0.10280440747737885\n",
      "epoch: 2 | 2656 / 114272 | training loss: 0.005984129849821329\n",
      "epoch: 2 | 2688 / 114272 | training loss: 0.011681620962917805\n",
      "epoch: 2 | 2720 / 114272 | training loss: 0.01874297298491001\n",
      "epoch: 2 | 2752 / 114272 | training loss: 0.2583700716495514\n",
      "epoch: 2 | 2784 / 114272 | training loss: 0.08417145162820816\n",
      "epoch: 2 | 2816 / 114272 | training loss: 0.0382230281829834\n",
      "epoch: 2 | 2848 / 114272 | training loss: 0.0650259330868721\n",
      "epoch: 2 | 2880 / 114272 | training loss: 0.025627024471759796\n",
      "epoch: 2 | 2912 / 114272 | training loss: 0.08858297765254974\n",
      "epoch: 2 | 2944 / 114272 | training loss: 0.009048919193446636\n",
      "epoch: 2 | 2976 / 114272 | training loss: 0.035533782094717026\n",
      "epoch: 2 | 3008 / 114272 | training loss: 0.11403907835483551\n",
      "epoch: 2 | 3040 / 114272 | training loss: 0.01164103951305151\n",
      "epoch: 2 | 3072 / 114272 | training loss: 0.03512071818113327\n",
      "epoch: 2 | 3104 / 114272 | training loss: 0.0027992944233119488\n",
      "epoch: 2 | 3136 / 114272 | training loss: 0.010486376471817493\n",
      "epoch: 2 | 3168 / 114272 | training loss: 0.19099734723567963\n",
      "epoch: 2 | 3200 / 114272 | training loss: 0.02790067158639431\n",
      "epoch: 2 | 3232 / 114272 | training loss: 0.0036188012454658747\n",
      "epoch: 2 | 3264 / 114272 | training loss: 0.002620602259412408\n",
      "epoch: 2 | 3296 / 114272 | training loss: 0.043236132711172104\n",
      "epoch: 2 | 3328 / 114272 | training loss: 0.21379387378692627\n",
      "epoch: 2 | 3360 / 114272 | training loss: 0.014752487652003765\n",
      "epoch: 2 | 3392 / 114272 | training loss: 0.07129961252212524\n",
      "epoch: 2 | 3424 / 114272 | training loss: 0.2665151059627533\n",
      "epoch: 2 | 3456 / 114272 | training loss: 0.014542864635586739\n",
      "epoch: 2 | 3488 / 114272 | training loss: 0.0729200467467308\n",
      "epoch: 2 | 3520 / 114272 | training loss: 0.13932131230831146\n",
      "epoch: 2 | 3552 / 114272 | training loss: 0.005846634041517973\n",
      "epoch: 2 | 3584 / 114272 | training loss: 0.15337011218070984\n",
      "epoch: 2 | 3616 / 114272 | training loss: 0.06731853634119034\n",
      "epoch: 2 | 3648 / 114272 | training loss: 0.5330040454864502\n",
      "epoch: 2 | 3680 / 114272 | training loss: 0.005415992345660925\n",
      "epoch: 2 | 3712 / 114272 | training loss: 0.020246369764208794\n",
      "epoch: 2 | 3744 / 114272 | training loss: 0.018772440031170845\n",
      "epoch: 2 | 3776 / 114272 | training loss: 0.26739761233329773\n",
      "epoch: 2 | 3808 / 114272 | training loss: 0.004110016394406557\n",
      "epoch: 2 | 3840 / 114272 | training loss: 0.005833589471876621\n",
      "epoch: 2 | 3872 / 114272 | training loss: 0.0035631200298666954\n",
      "epoch: 2 | 3904 / 114272 | training loss: 0.007560711354017258\n",
      "epoch: 2 | 3936 / 114272 | training loss: 0.04858807474374771\n",
      "epoch: 2 | 3968 / 114272 | training loss: 0.17500318586826324\n",
      "epoch: 2 | 4000 / 114272 | training loss: 0.2431032955646515\n",
      "epoch: 2 | 4032 / 114272 | training loss: 0.021028807386755943\n",
      "epoch: 2 | 4064 / 114272 | training loss: 0.048138294368982315\n",
      "epoch: 2 | 4096 / 114272 | training loss: 0.08791275322437286\n",
      "epoch: 2 | 4128 / 114272 | training loss: 0.004223663359880447\n",
      "epoch: 2 | 4160 / 114272 | training loss: 0.06407300382852554\n",
      "epoch: 2 | 4192 / 114272 | training loss: 0.039559200406074524\n",
      "epoch: 2 | 4224 / 114272 | training loss: 0.044492341578006744\n",
      "epoch: 2 | 4256 / 114272 | training loss: 0.019675612449645996\n",
      "epoch: 2 | 4288 / 114272 | training loss: 0.05061681941151619\n",
      "epoch: 2 | 4320 / 114272 | training loss: 0.014907239004969597\n",
      "epoch: 2 | 4352 / 114272 | training loss: 0.010622984729707241\n",
      "epoch: 2 | 4384 / 114272 | training loss: 0.02946310117840767\n",
      "epoch: 2 | 4416 / 114272 | training loss: 0.0068645114079117775\n",
      "epoch: 2 | 4448 / 114272 | training loss: 0.008428709581494331\n",
      "epoch: 2 | 4480 / 114272 | training loss: 0.03729996085166931\n",
      "epoch: 2 | 4512 / 114272 | training loss: 0.0014256109716370702\n",
      "epoch: 2 | 4544 / 114272 | training loss: 0.10994292795658112\n",
      "epoch: 2 | 4576 / 114272 | training loss: 0.05678555369377136\n",
      "epoch: 2 | 4608 / 114272 | training loss: 0.10452647507190704\n",
      "epoch: 2 | 4640 / 114272 | training loss: 0.09502043575048447\n",
      "epoch: 2 | 4672 / 114272 | training loss: 0.2025946080684662\n",
      "epoch: 2 | 4704 / 114272 | training loss: 0.2954641282558441\n",
      "epoch: 2 | 4736 / 114272 | training loss: 0.33726683259010315\n",
      "epoch: 2 | 4768 / 114272 | training loss: 0.15679213404655457\n",
      "epoch: 2 | 4800 / 114272 | training loss: 0.0026166962925344706\n",
      "epoch: 2 | 4832 / 114272 | training loss: 0.18665063381195068\n",
      "epoch: 2 | 4864 / 114272 | training loss: 0.1503814309835434\n",
      "epoch: 2 | 4896 / 114272 | training loss: 0.005384326446801424\n",
      "epoch: 2 | 4928 / 114272 | training loss: 0.16251546144485474\n",
      "epoch: 2 | 4960 / 114272 | training loss: 0.10366649180650711\n",
      "epoch: 2 | 4992 / 114272 | training loss: 0.17882974445819855\n",
      "epoch: 2 | 5024 / 114272 | training loss: 0.03535086661577225\n",
      "epoch: 2 | 5056 / 114272 | training loss: 0.09695342183113098\n",
      "epoch: 2 | 5088 / 114272 | training loss: 0.13525111973285675\n",
      "epoch: 2 | 5120 / 114272 | training loss: 0.03786160424351692\n",
      "epoch: 2 | 5152 / 114272 | training loss: 0.005244225263595581\n",
      "epoch: 2 | 5184 / 114272 | training loss: 0.06478425115346909\n",
      "epoch: 2 | 5216 / 114272 | training loss: 0.0038273967802524567\n",
      "epoch: 2 | 5248 / 114272 | training loss: 0.2238755077123642\n",
      "epoch: 2 | 5280 / 114272 | training loss: 0.009576073847711086\n",
      "epoch: 2 | 5312 / 114272 | training loss: 0.06935287266969681\n",
      "epoch: 2 | 5344 / 114272 | training loss: 0.09637747704982758\n",
      "epoch: 2 | 5376 / 114272 | training loss: 0.0057220119051635265\n",
      "epoch: 2 | 5408 / 114272 | training loss: 0.05011802539229393\n",
      "epoch: 2 | 5440 / 114272 | training loss: 0.30226072669029236\n",
      "epoch: 2 | 5472 / 114272 | training loss: 0.030915003269910812\n",
      "epoch: 2 | 5504 / 114272 | training loss: 0.1838657706975937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 5536 / 114272 | training loss: 0.0591367743909359\n",
      "epoch: 2 | 5568 / 114272 | training loss: 0.09838148206472397\n",
      "epoch: 2 | 5600 / 114272 | training loss: 0.10593872517347336\n",
      "epoch: 2 | 5632 / 114272 | training loss: 0.010002861730754375\n",
      "epoch: 2 | 5664 / 114272 | training loss: 0.2135138213634491\n",
      "epoch: 2 | 5696 / 114272 | training loss: 0.0035976681392639875\n",
      "epoch: 2 | 5728 / 114272 | training loss: 0.1303275227546692\n",
      "epoch: 2 | 5760 / 114272 | training loss: 0.011876476928591728\n",
      "epoch: 2 | 5792 / 114272 | training loss: 0.04157804697751999\n",
      "epoch: 2 | 5824 / 114272 | training loss: 0.029202954843640327\n",
      "epoch: 2 | 5856 / 114272 | training loss: 0.18525764346122742\n",
      "epoch: 2 | 5888 / 114272 | training loss: 0.007951942272484303\n",
      "epoch: 2 | 5920 / 114272 | training loss: 0.0983889102935791\n",
      "epoch: 2 | 5952 / 114272 | training loss: 0.20379580557346344\n",
      "epoch: 2 | 5984 / 114272 | training loss: 0.15440791845321655\n",
      "epoch: 2 | 6016 / 114272 | training loss: 0.005031456705182791\n",
      "epoch: 2 | 6048 / 114272 | training loss: 0.00935436598956585\n",
      "epoch: 2 | 6080 / 114272 | training loss: 0.027667846530675888\n",
      "epoch: 2 | 6112 / 114272 | training loss: 0.37570270895957947\n",
      "epoch: 2 | 6144 / 114272 | training loss: 0.11079546809196472\n",
      "epoch: 2 | 6176 / 114272 | training loss: 0.3073091506958008\n",
      "epoch: 2 | 6208 / 114272 | training loss: 0.05237220972776413\n",
      "epoch: 2 | 6240 / 114272 | training loss: 0.09727781265974045\n",
      "epoch: 2 | 6272 / 114272 | training loss: 0.022284869104623795\n",
      "epoch: 2 | 6304 / 114272 | training loss: 0.003447643481194973\n",
      "epoch: 2 | 6336 / 114272 | training loss: 0.1375596970319748\n",
      "epoch: 2 | 6368 / 114272 | training loss: 0.26899707317352295\n",
      "epoch: 2 | 6400 / 114272 | training loss: 0.06196397542953491\n",
      "epoch: 2 | 6432 / 114272 | training loss: 0.13437898457050323\n",
      "epoch: 2 | 6464 / 114272 | training loss: 0.10408402979373932\n",
      "epoch: 2 | 6496 / 114272 | training loss: 0.16798372566699982\n",
      "epoch: 2 | 6528 / 114272 | training loss: 0.12386105954647064\n",
      "epoch: 2 | 6560 / 114272 | training loss: 0.007770801894366741\n",
      "epoch: 2 | 6592 / 114272 | training loss: 0.12647230923175812\n",
      "epoch: 2 | 6624 / 114272 | training loss: 0.017299996688961983\n",
      "epoch: 2 | 6656 / 114272 | training loss: 0.18054430186748505\n",
      "epoch: 2 | 6688 / 114272 | training loss: 0.036185190081596375\n",
      "epoch: 2 | 6720 / 114272 | training loss: 0.0121591966599226\n",
      "epoch: 2 | 6752 / 114272 | training loss: 0.020503221079707146\n",
      "epoch: 2 | 6784 / 114272 | training loss: 0.05387810990214348\n",
      "epoch: 2 | 6816 / 114272 | training loss: 0.044610559940338135\n",
      "epoch: 2 | 6848 / 114272 | training loss: 0.08746317028999329\n",
      "epoch: 2 | 6880 / 114272 | training loss: 0.006294110789895058\n",
      "epoch: 2 | 6912 / 114272 | training loss: 0.008379733189940453\n",
      "epoch: 2 | 6944 / 114272 | training loss: 0.0325729101896286\n",
      "epoch: 2 | 6976 / 114272 | training loss: 0.09978923201560974\n",
      "epoch: 2 | 7008 / 114272 | training loss: 0.09287900477647781\n",
      "epoch: 2 | 7040 / 114272 | training loss: 0.012687725014984608\n",
      "epoch: 2 | 7072 / 114272 | training loss: 0.025690017268061638\n",
      "epoch: 2 | 7104 / 114272 | training loss: 0.09225758910179138\n",
      "epoch: 2 | 7136 / 114272 | training loss: 0.1720769703388214\n",
      "epoch: 2 | 7168 / 114272 | training loss: 0.08731921762228012\n",
      "epoch: 2 | 7200 / 114272 | training loss: 0.0604877769947052\n",
      "epoch: 2 | 7232 / 114272 | training loss: 0.17472577095031738\n",
      "epoch: 2 | 7264 / 114272 | training loss: 0.08315028995275497\n",
      "epoch: 2 | 7296 / 114272 | training loss: 0.11804793775081635\n",
      "epoch: 2 | 7328 / 114272 | training loss: 0.37619680166244507\n",
      "epoch: 2 | 7360 / 114272 | training loss: 0.06310316175222397\n",
      "epoch: 2 | 7392 / 114272 | training loss: 0.15520122647285461\n",
      "epoch: 2 | 7424 / 114272 | training loss: 0.021428672596812248\n",
      "epoch: 2 | 7456 / 114272 | training loss: 0.24690820276737213\n",
      "epoch: 2 | 7488 / 114272 | training loss: 0.03257036954164505\n",
      "epoch: 2 | 7520 / 114272 | training loss: 0.16602212190628052\n",
      "epoch: 2 | 7552 / 114272 | training loss: 0.018566925078630447\n",
      "epoch: 2 | 7584 / 114272 | training loss: 0.02227954752743244\n",
      "epoch: 2 | 7616 / 114272 | training loss: 0.14196999371051788\n",
      "epoch: 2 | 7648 / 114272 | training loss: 0.01126423291862011\n",
      "epoch: 2 | 7680 / 114272 | training loss: 0.005862116813659668\n",
      "epoch: 2 | 7712 / 114272 | training loss: 0.32627731561660767\n",
      "epoch: 2 | 7744 / 114272 | training loss: 0.10403113067150116\n",
      "epoch: 2 | 7776 / 114272 | training loss: 0.05922102928161621\n",
      "epoch: 2 | 7808 / 114272 | training loss: 0.10756124556064606\n",
      "epoch: 2 | 7840 / 114272 | training loss: 0.17684710025787354\n",
      "epoch: 2 | 7872 / 114272 | training loss: 0.009715108200907707\n",
      "epoch: 2 | 7904 / 114272 | training loss: 0.09666360914707184\n",
      "epoch: 2 | 7936 / 114272 | training loss: 0.09779655933380127\n",
      "epoch: 2 | 7968 / 114272 | training loss: 0.3811092972755432\n",
      "epoch: 2 | 8000 / 114272 | training loss: 0.07675085961818695\n",
      "epoch: 2 | 8032 / 114272 | training loss: 0.025562983006238937\n",
      "epoch: 2 | 8064 / 114272 | training loss: 0.15496017038822174\n",
      "epoch: 2 | 8096 / 114272 | training loss: 0.1675013154745102\n",
      "epoch: 2 | 8128 / 114272 | training loss: 0.014996618032455444\n",
      "epoch: 2 | 8160 / 114272 | training loss: 0.09542227536439896\n",
      "epoch: 2 | 8192 / 114272 | training loss: 0.014222993515431881\n",
      "epoch: 2 | 8224 / 114272 | training loss: 0.03854667395353317\n",
      "epoch: 2 | 8256 / 114272 | training loss: 0.10797242820262909\n",
      "epoch: 2 | 8288 / 114272 | training loss: 0.055098649114370346\n",
      "epoch: 2 | 8320 / 114272 | training loss: 0.2137618511915207\n",
      "epoch: 2 | 8352 / 114272 | training loss: 0.06608152389526367\n",
      "epoch: 2 | 8384 / 114272 | training loss: 0.007689214777201414\n",
      "epoch: 2 | 8416 / 114272 | training loss: 0.03603260591626167\n",
      "epoch: 2 | 8448 / 114272 | training loss: 0.14071378111839294\n",
      "epoch: 2 | 8480 / 114272 | training loss: 0.026028171181678772\n",
      "epoch: 2 | 8512 / 114272 | training loss: 0.0033778022043406963\n",
      "epoch: 2 | 8544 / 114272 | training loss: 0.009142712689936161\n",
      "epoch: 2 | 8576 / 114272 | training loss: 0.12448254972696304\n",
      "epoch: 2 | 8608 / 114272 | training loss: 0.02732505090534687\n",
      "epoch: 2 | 8640 / 114272 | training loss: 0.2720111906528473\n",
      "epoch: 2 | 8672 / 114272 | training loss: 0.05479288101196289\n",
      "epoch: 2 | 8704 / 114272 | training loss: 0.006003550253808498\n",
      "epoch: 2 | 8736 / 114272 | training loss: 0.12023439258337021\n",
      "epoch: 2 | 8768 / 114272 | training loss: 0.1753290295600891\n",
      "epoch: 2 | 8800 / 114272 | training loss: 0.013105439022183418\n",
      "epoch: 2 | 8832 / 114272 | training loss: 0.01657448522746563\n",
      "epoch: 2 | 8864 / 114272 | training loss: 0.15737077593803406\n",
      "epoch: 2 | 8896 / 114272 | training loss: 0.026262957602739334\n",
      "epoch: 2 | 8928 / 114272 | training loss: 0.030361684039235115\n",
      "epoch: 2 | 8960 / 114272 | training loss: 0.038675904273986816\n",
      "epoch: 2 | 8992 / 114272 | training loss: 0.005270918365567923\n",
      "epoch: 2 | 9024 / 114272 | training loss: 0.10170046985149384\n",
      "epoch: 2 | 9056 / 114272 | training loss: 0.00623728521168232\n",
      "epoch: 2 | 9088 / 114272 | training loss: 0.17182859778404236\n",
      "epoch: 2 | 9120 / 114272 | training loss: 0.021029844880104065\n",
      "epoch: 2 | 9152 / 114272 | training loss: 0.015215539373457432\n",
      "epoch: 2 | 9184 / 114272 | training loss: 0.008895350620150566\n",
      "epoch: 2 | 9216 / 114272 | training loss: 0.11542434245347977\n",
      "epoch: 2 | 9248 / 114272 | training loss: 0.260037362575531\n",
      "epoch: 2 | 9280 / 114272 | training loss: 0.1614706963300705\n",
      "epoch: 2 | 9312 / 114272 | training loss: 0.2477371096611023\n",
      "epoch: 2 | 9344 / 114272 | training loss: 0.0073116617277264595\n",
      "epoch: 2 | 9376 / 114272 | training loss: 0.08757232874631882\n",
      "epoch: 2 | 9408 / 114272 | training loss: 0.06402266025543213\n",
      "epoch: 2 | 9440 / 114272 | training loss: 0.17882373929023743\n",
      "epoch: 2 | 9472 / 114272 | training loss: 0.0106856320053339\n",
      "epoch: 2 | 9504 / 114272 | training loss: 0.1513948291540146\n",
      "epoch: 2 | 9536 / 114272 | training loss: 0.33259040117263794\n",
      "epoch: 2 | 9568 / 114272 | training loss: 0.21000489592552185\n",
      "epoch: 2 | 9600 / 114272 | training loss: 0.25972846150398254\n",
      "epoch: 2 | 9632 / 114272 | training loss: 0.07018639147281647\n",
      "epoch: 2 | 9664 / 114272 | training loss: 0.008830584585666656\n",
      "epoch: 2 | 9696 / 114272 | training loss: 0.04949575290083885\n",
      "epoch: 2 | 9728 / 114272 | training loss: 0.017207680270075798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 9760 / 114272 | training loss: 0.006647791713476181\n",
      "epoch: 2 | 9792 / 114272 | training loss: 0.06615831702947617\n",
      "epoch: 2 | 9824 / 114272 | training loss: 0.011413303203880787\n",
      "epoch: 2 | 9856 / 114272 | training loss: 0.06925283372402191\n",
      "epoch: 2 | 9888 / 114272 | training loss: 0.11951620131731033\n",
      "epoch: 2 | 9920 / 114272 | training loss: 0.03404029831290245\n",
      "epoch: 2 | 9952 / 114272 | training loss: 0.00734320655465126\n",
      "epoch: 2 | 9984 / 114272 | training loss: 0.0034158453345298767\n",
      "epoch: 2 | 10016 / 114272 | training loss: 0.003603593213483691\n",
      "epoch: 2 | 10048 / 114272 | training loss: 0.014309238642454147\n",
      "epoch: 2 | 10080 / 114272 | training loss: 0.008878634311258793\n",
      "epoch: 2 | 10112 / 114272 | training loss: 0.009305007755756378\n",
      "epoch: 2 | 10144 / 114272 | training loss: 0.05671287328004837\n",
      "epoch: 2 | 10176 / 114272 | training loss: 0.16001497209072113\n",
      "epoch: 2 | 10208 / 114272 | training loss: 0.2531120777130127\n",
      "epoch: 2 | 10240 / 114272 | training loss: 0.04829167574644089\n",
      "epoch: 2 | 10272 / 114272 | training loss: 0.20773085951805115\n",
      "epoch: 2 | 10304 / 114272 | training loss: 0.002367208246141672\n",
      "epoch: 2 | 10336 / 114272 | training loss: 0.0032507379073649645\n",
      "epoch: 2 | 10368 / 114272 | training loss: 0.01032214518636465\n",
      "epoch: 2 | 10400 / 114272 | training loss: 0.003842826932668686\n",
      "epoch: 2 | 10432 / 114272 | training loss: 0.09355271607637405\n",
      "epoch: 2 | 10464 / 114272 | training loss: 0.004603290464729071\n",
      "epoch: 2 | 10496 / 114272 | training loss: 0.1617407500743866\n",
      "epoch: 2 | 10528 / 114272 | training loss: 0.10243124514818192\n",
      "epoch: 2 | 10560 / 114272 | training loss: 0.08271903544664383\n",
      "epoch: 2 | 10592 / 114272 | training loss: 0.03703460469841957\n",
      "epoch: 2 | 10624 / 114272 | training loss: 0.2053314596414566\n",
      "epoch: 2 | 10656 / 114272 | training loss: 0.52391517162323\n",
      "epoch: 2 | 10688 / 114272 | training loss: 0.02108507603406906\n",
      "epoch: 2 | 10720 / 114272 | training loss: 0.009983880445361137\n",
      "epoch: 2 | 10752 / 114272 | training loss: 0.010388853959739208\n",
      "epoch: 2 | 10784 / 114272 | training loss: 0.06808484345674515\n",
      "epoch: 2 | 10816 / 114272 | training loss: 0.2801786959171295\n",
      "epoch: 2 | 10848 / 114272 | training loss: 0.03290801867842674\n",
      "epoch: 2 | 10880 / 114272 | training loss: 0.3641297221183777\n",
      "epoch: 2 | 10912 / 114272 | training loss: 0.012699640356004238\n",
      "epoch: 2 | 10944 / 114272 | training loss: 0.26331937313079834\n",
      "epoch: 2 | 10976 / 114272 | training loss: 0.24837401509284973\n",
      "epoch: 2 | 11008 / 114272 | training loss: 0.2589195668697357\n",
      "epoch: 2 | 11040 / 114272 | training loss: 0.00883000623434782\n",
      "epoch: 2 | 11072 / 114272 | training loss: 0.014965235255658627\n",
      "epoch: 2 | 11104 / 114272 | training loss: 0.05528995767235756\n",
      "epoch: 2 | 11136 / 114272 | training loss: 0.037742629647254944\n",
      "epoch: 2 | 11168 / 114272 | training loss: 0.043409690260887146\n",
      "epoch: 2 | 11200 / 114272 | training loss: 0.016778625547885895\n",
      "epoch: 2 | 11232 / 114272 | training loss: 0.20717135071754456\n",
      "epoch: 2 | 11264 / 114272 | training loss: 0.0961030051112175\n",
      "epoch: 2 | 11296 / 114272 | training loss: 0.13988594710826874\n",
      "epoch: 2 | 11328 / 114272 | training loss: 0.02929302491247654\n",
      "epoch: 2 | 11360 / 114272 | training loss: 0.012439755722880363\n",
      "epoch: 2 | 11392 / 114272 | training loss: 0.08826786279678345\n",
      "epoch: 2 | 11424 / 114272 | training loss: 0.2235470414161682\n",
      "epoch: 2 | 11456 / 114272 | training loss: 0.006223763804882765\n",
      "epoch: 2 | 11488 / 114272 | training loss: 0.1769097000360489\n",
      "epoch: 2 | 11520 / 114272 | training loss: 0.005892574321478605\n",
      "epoch: 2 | 11552 / 114272 | training loss: 0.04302210733294487\n",
      "epoch: 2 | 11584 / 114272 | training loss: 0.036454662680625916\n",
      "epoch: 2 | 11616 / 114272 | training loss: 0.010536112822592258\n",
      "epoch: 2 | 11648 / 114272 | training loss: 0.08085569739341736\n",
      "epoch: 2 | 11680 / 114272 | training loss: 0.0369253046810627\n",
      "epoch: 2 | 11712 / 114272 | training loss: 0.0176058579236269\n",
      "epoch: 2 | 11744 / 114272 | training loss: 0.24982500076293945\n",
      "epoch: 2 | 11776 / 114272 | training loss: 0.09766075015068054\n",
      "epoch: 2 | 11808 / 114272 | training loss: 0.03386029973626137\n",
      "epoch: 2 | 11840 / 114272 | training loss: 0.02014138177037239\n",
      "epoch: 2 | 11872 / 114272 | training loss: 0.06774099916219711\n",
      "epoch: 2 | 11904 / 114272 | training loss: 0.010649623349308968\n",
      "epoch: 2 | 11936 / 114272 | training loss: 0.039829663932323456\n",
      "epoch: 2 | 11968 / 114272 | training loss: 0.011240242049098015\n",
      "epoch: 2 | 12000 / 114272 | training loss: 0.1836232841014862\n",
      "epoch: 2 | 12032 / 114272 | training loss: 0.16599076986312866\n",
      "epoch: 2 | 12064 / 114272 | training loss: 0.02319147437810898\n",
      "epoch: 2 | 12096 / 114272 | training loss: 0.06101778522133827\n",
      "epoch: 2 | 12128 / 114272 | training loss: 0.07256552577018738\n",
      "epoch: 2 | 12160 / 114272 | training loss: 0.10275972634553909\n",
      "epoch: 2 | 12192 / 114272 | training loss: 0.004847150761634111\n",
      "epoch: 2 | 12224 / 114272 | training loss: 0.06771320849657059\n",
      "epoch: 2 | 12256 / 114272 | training loss: 0.19375140964984894\n",
      "epoch: 2 | 12288 / 114272 | training loss: 0.013422539457678795\n",
      "epoch: 2 | 12320 / 114272 | training loss: 0.008469650521874428\n",
      "epoch: 2 | 12352 / 114272 | training loss: 0.18791541457176208\n",
      "epoch: 2 | 12384 / 114272 | training loss: 0.01654362864792347\n",
      "epoch: 2 | 12416 / 114272 | training loss: 0.26669758558273315\n",
      "epoch: 2 | 12448 / 114272 | training loss: 0.03671322762966156\n",
      "epoch: 2 | 12480 / 114272 | training loss: 0.004837564658373594\n",
      "epoch: 2 | 12512 / 114272 | training loss: 0.0792188048362732\n",
      "epoch: 2 | 12544 / 114272 | training loss: 0.028688713908195496\n",
      "epoch: 2 | 12576 / 114272 | training loss: 0.11242517828941345\n",
      "epoch: 2 | 12608 / 114272 | training loss: 0.012690769508481026\n",
      "epoch: 2 | 12640 / 114272 | training loss: 0.026476895436644554\n",
      "epoch: 2 | 12672 / 114272 | training loss: 0.20307005941867828\n",
      "epoch: 2 | 12704 / 114272 | training loss: 0.2908826768398285\n",
      "epoch: 2 | 12736 / 114272 | training loss: 0.1568620800971985\n",
      "epoch: 2 | 12768 / 114272 | training loss: 0.0016850698739290237\n",
      "epoch: 2 | 12800 / 114272 | training loss: 0.03103821910917759\n",
      "epoch: 2 | 12832 / 114272 | training loss: 0.08954831957817078\n",
      "epoch: 2 | 12864 / 114272 | training loss: 0.06637920439243317\n",
      "epoch: 2 | 12896 / 114272 | training loss: 0.11634939908981323\n",
      "epoch: 2 | 12928 / 114272 | training loss: 0.286286324262619\n",
      "epoch: 2 | 12960 / 114272 | training loss: 0.004801658913493156\n",
      "epoch: 2 | 12992 / 114272 | training loss: 0.2016080915927887\n",
      "epoch: 2 | 13024 / 114272 | training loss: 0.009484880603849888\n",
      "epoch: 2 | 13056 / 114272 | training loss: 0.010569487698376179\n",
      "epoch: 2 | 13088 / 114272 | training loss: 0.03802991285920143\n",
      "epoch: 2 | 13120 / 114272 | training loss: 0.014832689426839352\n",
      "epoch: 2 | 13152 / 114272 | training loss: 0.1460503786802292\n",
      "epoch: 2 | 13184 / 114272 | training loss: 0.13314653933048248\n",
      "epoch: 2 | 13216 / 114272 | training loss: 0.017571058124303818\n",
      "epoch: 2 | 13248 / 114272 | training loss: 0.19558022916316986\n",
      "epoch: 2 | 13280 / 114272 | training loss: 0.03608646243810654\n",
      "epoch: 2 | 13312 / 114272 | training loss: 0.03353801369667053\n",
      "epoch: 2 | 13344 / 114272 | training loss: 0.005122130271047354\n",
      "epoch: 2 | 13376 / 114272 | training loss: 0.04495445638895035\n",
      "epoch: 2 | 13408 / 114272 | training loss: 0.003116249805316329\n",
      "epoch: 2 | 13440 / 114272 | training loss: 0.020300982519984245\n",
      "epoch: 2 | 13472 / 114272 | training loss: 0.1385604292154312\n",
      "epoch: 2 | 13504 / 114272 | training loss: 0.016912270337343216\n",
      "epoch: 2 | 13536 / 114272 | training loss: 0.24694645404815674\n",
      "epoch: 2 | 13568 / 114272 | training loss: 0.05493348091840744\n",
      "epoch: 2 | 13600 / 114272 | training loss: 0.012109383940696716\n",
      "epoch: 2 | 13632 / 114272 | training loss: 0.02549627237021923\n",
      "epoch: 2 | 13664 / 114272 | training loss: 0.09051425755023956\n",
      "epoch: 2 | 13696 / 114272 | training loss: 0.08940629661083221\n",
      "epoch: 2 | 13728 / 114272 | training loss: 0.018801484256982803\n",
      "epoch: 2 | 13760 / 114272 | training loss: 0.1416071057319641\n",
      "epoch: 2 | 13792 / 114272 | training loss: 0.008689123205840588\n",
      "epoch: 2 | 13824 / 114272 | training loss: 0.013776208274066448\n",
      "epoch: 2 | 13856 / 114272 | training loss: 0.22363735735416412\n",
      "epoch: 2 | 13888 / 114272 | training loss: 0.16104404628276825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 13920 / 114272 | training loss: 0.20353911817073822\n",
      "epoch: 2 | 13952 / 114272 | training loss: 0.03957627713680267\n",
      "epoch: 2 | 13984 / 114272 | training loss: 0.007007950451225042\n",
      "epoch: 2 | 14016 / 114272 | training loss: 0.26973244547843933\n",
      "epoch: 2 | 14048 / 114272 | training loss: 0.07156549394130707\n",
      "epoch: 2 | 14080 / 114272 | training loss: 0.00538810808211565\n",
      "epoch: 2 | 14112 / 114272 | training loss: 0.007268359884619713\n",
      "epoch: 2 | 14144 / 114272 | training loss: 0.06207631155848503\n",
      "epoch: 2 | 14176 / 114272 | training loss: 0.05970393121242523\n",
      "epoch: 2 | 14208 / 114272 | training loss: 0.005815566051751375\n",
      "epoch: 2 | 14240 / 114272 | training loss: 0.009943928569555283\n",
      "epoch: 2 | 14272 / 114272 | training loss: 0.25704818964004517\n",
      "epoch: 2 | 14304 / 114272 | training loss: 0.011150235310196877\n",
      "epoch: 2 | 14336 / 114272 | training loss: 0.004652874544262886\n",
      "epoch: 2 | 14368 / 114272 | training loss: 0.12640801072120667\n",
      "epoch: 2 | 14400 / 114272 | training loss: 0.16085223853588104\n",
      "epoch: 2 | 14432 / 114272 | training loss: 0.15510916709899902\n",
      "epoch: 2 | 14464 / 114272 | training loss: 0.10302366316318512\n",
      "epoch: 2 | 14496 / 114272 | training loss: 0.2389902025461197\n",
      "epoch: 2 | 14528 / 114272 | training loss: 0.13450956344604492\n",
      "epoch: 2 | 14560 / 114272 | training loss: 0.01631523109972477\n",
      "epoch: 2 | 14592 / 114272 | training loss: 0.008464278653264046\n",
      "epoch: 2 | 14624 / 114272 | training loss: 0.35960787534713745\n",
      "epoch: 2 | 14656 / 114272 | training loss: 0.11187417060136795\n",
      "epoch: 2 | 14688 / 114272 | training loss: 0.1128673404455185\n",
      "epoch: 2 | 14720 / 114272 | training loss: 0.0561530701816082\n",
      "epoch: 2 | 14752 / 114272 | training loss: 0.21409404277801514\n",
      "epoch: 2 | 14784 / 114272 | training loss: 0.0029672577511519194\n",
      "epoch: 2 | 14816 / 114272 | training loss: 0.17365378141403198\n",
      "epoch: 2 | 14848 / 114272 | training loss: 0.160480335354805\n",
      "epoch: 2 | 14880 / 114272 | training loss: 0.036858368664979935\n",
      "epoch: 2 | 14912 / 114272 | training loss: 0.1885586827993393\n",
      "epoch: 2 | 14944 / 114272 | training loss: 0.008809449151158333\n",
      "epoch: 2 | 14976 / 114272 | training loss: 0.09198969602584839\n",
      "epoch: 2 | 15008 / 114272 | training loss: 0.3346927762031555\n",
      "epoch: 2 | 15040 / 114272 | training loss: 0.06376715004444122\n",
      "epoch: 2 | 15072 / 114272 | training loss: 0.00396885396912694\n",
      "epoch: 2 | 15104 / 114272 | training loss: 0.4233836233615875\n",
      "epoch: 2 | 15136 / 114272 | training loss: 0.016543569043278694\n",
      "epoch: 2 | 15168 / 114272 | training loss: 0.018482882529497147\n",
      "epoch: 2 | 15200 / 114272 | training loss: 0.08906199783086777\n",
      "epoch: 2 | 15232 / 114272 | training loss: 0.055197782814502716\n",
      "epoch: 2 | 15264 / 114272 | training loss: 0.1613415777683258\n",
      "epoch: 2 | 15296 / 114272 | training loss: 0.05255164951086044\n",
      "epoch: 2 | 15328 / 114272 | training loss: 0.09186006337404251\n",
      "epoch: 2 | 15360 / 114272 | training loss: 0.010752681642770767\n",
      "epoch: 2 | 15392 / 114272 | training loss: 0.16324692964553833\n",
      "epoch: 2 | 15424 / 114272 | training loss: 0.02636905014514923\n",
      "epoch: 2 | 15456 / 114272 | training loss: 0.26221778988838196\n",
      "epoch: 2 | 15488 / 114272 | training loss: 0.08403433859348297\n",
      "epoch: 2 | 15520 / 114272 | training loss: 0.13900695741176605\n",
      "epoch: 2 | 15552 / 114272 | training loss: 0.17512229084968567\n",
      "epoch: 2 | 15584 / 114272 | training loss: 0.012424892745912075\n",
      "epoch: 2 | 15616 / 114272 | training loss: 0.04590742290019989\n",
      "epoch: 2 | 15648 / 114272 | training loss: 0.021754251793026924\n",
      "epoch: 2 | 15680 / 114272 | training loss: 0.07506121695041656\n",
      "epoch: 2 | 15712 / 114272 | training loss: 0.11884061992168427\n",
      "epoch: 2 | 15744 / 114272 | training loss: 0.14238247275352478\n",
      "epoch: 2 | 15776 / 114272 | training loss: 0.1742338091135025\n",
      "epoch: 2 | 15808 / 114272 | training loss: 0.03555286303162575\n",
      "epoch: 2 | 15840 / 114272 | training loss: 0.010927023366093636\n",
      "epoch: 2 | 15872 / 114272 | training loss: 0.10360616445541382\n",
      "epoch: 2 | 15904 / 114272 | training loss: 0.015979403629899025\n",
      "epoch: 2 | 15936 / 114272 | training loss: 0.26138627529144287\n",
      "epoch: 2 | 15968 / 114272 | training loss: 0.144060879945755\n",
      "epoch: 2 | 16000 / 114272 | training loss: 0.1988717019557953\n",
      "epoch: 2 | 16032 / 114272 | training loss: 0.02040068805217743\n",
      "epoch: 2 | 16064 / 114272 | training loss: 0.01987595297396183\n",
      "epoch: 2 | 16096 / 114272 | training loss: 0.1342737078666687\n",
      "epoch: 2 | 16128 / 114272 | training loss: 0.053933076560497284\n",
      "epoch: 2 | 16160 / 114272 | training loss: 0.010533147491514683\n",
      "epoch: 2 | 16192 / 114272 | training loss: 0.24803444743156433\n",
      "epoch: 2 | 16224 / 114272 | training loss: 0.022293400019407272\n",
      "epoch: 2 | 16256 / 114272 | training loss: 0.04948198422789574\n",
      "epoch: 2 | 16288 / 114272 | training loss: 0.0847947970032692\n",
      "epoch: 2 | 16320 / 114272 | training loss: 0.02652331255376339\n",
      "epoch: 2 | 16352 / 114272 | training loss: 0.09237316250801086\n",
      "epoch: 2 | 16384 / 114272 | training loss: 0.009554564021527767\n",
      "epoch: 2 | 16416 / 114272 | training loss: 0.10563598573207855\n",
      "epoch: 2 | 16448 / 114272 | training loss: 0.012264544144272804\n",
      "epoch: 2 | 16480 / 114272 | training loss: 0.07711631059646606\n",
      "epoch: 2 | 16512 / 114272 | training loss: 0.06875621527433395\n",
      "epoch: 2 | 16544 / 114272 | training loss: 0.005372603889554739\n",
      "epoch: 2 | 16576 / 114272 | training loss: 0.0233146995306015\n",
      "epoch: 2 | 16608 / 114272 | training loss: 0.026944328099489212\n",
      "epoch: 2 | 16640 / 114272 | training loss: 0.21273921430110931\n",
      "epoch: 2 | 16672 / 114272 | training loss: 0.4491721987724304\n",
      "epoch: 2 | 16704 / 114272 | training loss: 0.13263221085071564\n",
      "epoch: 2 | 16736 / 114272 | training loss: 0.028528306633234024\n",
      "epoch: 2 | 16768 / 114272 | training loss: 0.1256650984287262\n",
      "epoch: 2 | 16800 / 114272 | training loss: 0.04846478998661041\n",
      "epoch: 2 | 16832 / 114272 | training loss: 0.061201054602861404\n",
      "epoch: 2 | 16864 / 114272 | training loss: 0.01910736970603466\n",
      "epoch: 2 | 16896 / 114272 | training loss: 0.13813912868499756\n",
      "epoch: 2 | 16928 / 114272 | training loss: 0.0058496431447565556\n",
      "epoch: 2 | 16960 / 114272 | training loss: 0.04164660722017288\n",
      "epoch: 2 | 16992 / 114272 | training loss: 0.022449495270848274\n",
      "epoch: 2 | 17024 / 114272 | training loss: 0.020411046221852303\n",
      "epoch: 2 | 17056 / 114272 | training loss: 0.037748754024505615\n",
      "epoch: 2 | 17088 / 114272 | training loss: 0.004265546333044767\n",
      "epoch: 2 | 17120 / 114272 | training loss: 0.012701276689767838\n",
      "epoch: 2 | 17152 / 114272 | training loss: 0.1000233143568039\n",
      "epoch: 2 | 17184 / 114272 | training loss: 0.03764525055885315\n",
      "epoch: 2 | 17216 / 114272 | training loss: 0.0879230871796608\n",
      "epoch: 2 | 17248 / 114272 | training loss: 0.19286751747131348\n",
      "epoch: 2 | 17280 / 114272 | training loss: 0.1458018571138382\n",
      "epoch: 2 | 17312 / 114272 | training loss: 0.14636288583278656\n",
      "epoch: 2 | 17344 / 114272 | training loss: 0.06858918070793152\n",
      "epoch: 2 | 17376 / 114272 | training loss: 0.01639995165169239\n",
      "epoch: 2 | 17408 / 114272 | training loss: 0.11908399313688278\n",
      "epoch: 2 | 17440 / 114272 | training loss: 0.004698912147432566\n",
      "epoch: 2 | 17472 / 114272 | training loss: 0.2203710973262787\n",
      "epoch: 2 | 17504 / 114272 | training loss: 0.11752171069383621\n",
      "epoch: 2 | 17536 / 114272 | training loss: 0.16881383955478668\n",
      "epoch: 2 | 17568 / 114272 | training loss: 0.1008797138929367\n",
      "epoch: 2 | 17600 / 114272 | training loss: 0.2057257741689682\n",
      "epoch: 2 | 17632 / 114272 | training loss: 0.23824431002140045\n",
      "epoch: 2 | 17664 / 114272 | training loss: 0.15076936781406403\n",
      "epoch: 2 | 17696 / 114272 | training loss: 0.017386419698596\n",
      "epoch: 2 | 17728 / 114272 | training loss: 0.05499296635389328\n",
      "epoch: 2 | 17760 / 114272 | training loss: 0.06736230850219727\n",
      "epoch: 2 | 17792 / 114272 | training loss: 0.06072600558400154\n",
      "epoch: 2 | 17824 / 114272 | training loss: 0.008628069423139095\n",
      "epoch: 2 | 17856 / 114272 | training loss: 0.032759543508291245\n",
      "epoch: 2 | 17888 / 114272 | training loss: 0.12886245548725128\n",
      "epoch: 2 | 17920 / 114272 | training loss: 0.3155772387981415\n",
      "epoch: 2 | 17952 / 114272 | training loss: 0.24007734656333923\n",
      "epoch: 2 | 17984 / 114272 | training loss: 0.17674006521701813\n",
      "epoch: 2 | 18016 / 114272 | training loss: 0.014871194958686829\n",
      "epoch: 2 | 18048 / 114272 | training loss: 0.1297004669904709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 18080 / 114272 | training loss: 0.011448375880718231\n",
      "epoch: 2 | 18112 / 114272 | training loss: 0.022279147058725357\n",
      "epoch: 2 | 18144 / 114272 | training loss: 0.202779620885849\n",
      "epoch: 2 | 18176 / 114272 | training loss: 0.14219072461128235\n",
      "epoch: 2 | 18208 / 114272 | training loss: 0.013147106394171715\n",
      "epoch: 2 | 18240 / 114272 | training loss: 0.08950424939393997\n",
      "epoch: 2 | 18272 / 114272 | training loss: 0.01284774485975504\n",
      "epoch: 2 | 18304 / 114272 | training loss: 0.019835611805319786\n",
      "epoch: 2 | 18336 / 114272 | training loss: 0.023283781483769417\n",
      "epoch: 2 | 18368 / 114272 | training loss: 0.09585627913475037\n",
      "epoch: 2 | 18400 / 114272 | training loss: 0.2032822072505951\n",
      "epoch: 2 | 18432 / 114272 | training loss: 0.020631834864616394\n",
      "epoch: 2 | 18464 / 114272 | training loss: 0.05825500190258026\n",
      "epoch: 2 | 18496 / 114272 | training loss: 0.18267939984798431\n",
      "epoch: 2 | 18528 / 114272 | training loss: 0.03451795503497124\n",
      "epoch: 2 | 18560 / 114272 | training loss: 0.025154409930109978\n",
      "epoch: 2 | 18592 / 114272 | training loss: 0.055099766701459885\n",
      "epoch: 2 | 18624 / 114272 | training loss: 0.10769819468259811\n",
      "epoch: 2 | 18656 / 114272 | training loss: 0.006029798649251461\n",
      "epoch: 2 | 18688 / 114272 | training loss: 0.3859752118587494\n",
      "epoch: 2 | 18720 / 114272 | training loss: 0.18567290902137756\n",
      "epoch: 2 | 18752 / 114272 | training loss: 0.08960288763046265\n",
      "epoch: 2 | 18784 / 114272 | training loss: 0.19420085847377777\n",
      "epoch: 2 | 18816 / 114272 | training loss: 0.08698464184999466\n",
      "epoch: 2 | 18848 / 114272 | training loss: 0.34243372082710266\n",
      "epoch: 2 | 18880 / 114272 | training loss: 0.13319866359233856\n",
      "epoch: 2 | 18912 / 114272 | training loss: 0.1982191801071167\n",
      "epoch: 2 | 18944 / 114272 | training loss: 0.05298726633191109\n",
      "epoch: 2 | 18976 / 114272 | training loss: 0.050817202776670456\n",
      "epoch: 2 | 19008 / 114272 | training loss: 0.009352241642773151\n",
      "epoch: 2 | 19040 / 114272 | training loss: 0.1102241575717926\n",
      "epoch: 2 | 19072 / 114272 | training loss: 0.25735414028167725\n",
      "epoch: 2 | 19104 / 114272 | training loss: 0.0875442624092102\n",
      "epoch: 2 | 19136 / 114272 | training loss: 0.08633099496364594\n",
      "epoch: 2 | 19168 / 114272 | training loss: 0.01864399015903473\n",
      "epoch: 2 | 19200 / 114272 | training loss: 0.04371604695916176\n",
      "epoch: 2 | 19232 / 114272 | training loss: 0.15573304891586304\n",
      "epoch: 2 | 19264 / 114272 | training loss: 0.009680148214101791\n",
      "epoch: 2 | 19296 / 114272 | training loss: 0.012503866106271744\n",
      "epoch: 2 | 19328 / 114272 | training loss: 0.04474439471960068\n",
      "epoch: 2 | 19360 / 114272 | training loss: 0.19861963391304016\n",
      "epoch: 2 | 19392 / 114272 | training loss: 0.1137443259358406\n",
      "epoch: 2 | 19424 / 114272 | training loss: 0.01123040821403265\n",
      "epoch: 2 | 19456 / 114272 | training loss: 0.18317478895187378\n",
      "epoch: 2 | 19488 / 114272 | training loss: 0.06219390034675598\n",
      "epoch: 2 | 19520 / 114272 | training loss: 0.023857463151216507\n",
      "epoch: 2 | 19552 / 114272 | training loss: 0.1224658414721489\n",
      "epoch: 2 | 19584 / 114272 | training loss: 0.11724480241537094\n",
      "epoch: 2 | 19616 / 114272 | training loss: 0.016489438712596893\n",
      "epoch: 2 | 19648 / 114272 | training loss: 0.026645872741937637\n",
      "epoch: 2 | 19680 / 114272 | training loss: 0.011598417535424232\n",
      "epoch: 2 | 19712 / 114272 | training loss: 0.008846370503306389\n",
      "epoch: 2 | 19744 / 114272 | training loss: 0.00772556196898222\n",
      "epoch: 2 | 19776 / 114272 | training loss: 0.008432627655565739\n",
      "epoch: 2 | 19808 / 114272 | training loss: 0.009647303260862827\n",
      "epoch: 2 | 19840 / 114272 | training loss: 0.007039385382086039\n",
      "epoch: 2 | 19872 / 114272 | training loss: 0.019458070397377014\n",
      "epoch: 2 | 19904 / 114272 | training loss: 0.016398830339312553\n",
      "epoch: 2 | 19936 / 114272 | training loss: 0.10043536871671677\n",
      "epoch: 2 | 19968 / 114272 | training loss: 0.21297936141490936\n",
      "epoch: 2 | 20000 / 114272 | training loss: 0.019501378759741783\n",
      "epoch: 2 | 20032 / 114272 | training loss: 0.008394231088459492\n",
      "epoch: 2 | 20064 / 114272 | training loss: 0.015204067341983318\n",
      "epoch: 2 | 20096 / 114272 | training loss: 0.007637385278940201\n",
      "epoch: 2 | 20128 / 114272 | training loss: 0.12926052510738373\n",
      "epoch: 2 | 20160 / 114272 | training loss: 0.012412081472575665\n",
      "epoch: 2 | 20192 / 114272 | training loss: 0.1474130004644394\n",
      "epoch: 2 | 20224 / 114272 | training loss: 0.1721111238002777\n",
      "epoch: 2 | 20256 / 114272 | training loss: 0.012884731404483318\n",
      "epoch: 2 | 20288 / 114272 | training loss: 0.013050139881670475\n",
      "epoch: 2 | 20320 / 114272 | training loss: 0.2094356119632721\n",
      "epoch: 2 | 20352 / 114272 | training loss: 0.09062805026769638\n",
      "epoch: 2 | 20384 / 114272 | training loss: 0.022915378212928772\n",
      "epoch: 2 | 20416 / 114272 | training loss: 0.03485101833939552\n",
      "epoch: 2 | 20448 / 114272 | training loss: 0.12161705642938614\n",
      "epoch: 2 | 20480 / 114272 | training loss: 0.11668335646390915\n",
      "epoch: 2 | 20512 / 114272 | training loss: 0.016728810966014862\n",
      "epoch: 2 | 20544 / 114272 | training loss: 0.09354114532470703\n",
      "epoch: 2 | 20576 / 114272 | training loss: 0.003983126487582922\n",
      "epoch: 2 | 20608 / 114272 | training loss: 0.025584734976291656\n",
      "epoch: 2 | 20640 / 114272 | training loss: 0.006711569614708424\n",
      "epoch: 2 | 20672 / 114272 | training loss: 0.005577920004725456\n",
      "epoch: 2 | 20704 / 114272 | training loss: 0.016969209536910057\n",
      "epoch: 2 | 20736 / 114272 | training loss: 0.17067937552928925\n",
      "epoch: 2 | 20768 / 114272 | training loss: 0.2553049921989441\n",
      "epoch: 2 | 20800 / 114272 | training loss: 0.1547618955373764\n",
      "epoch: 2 | 20832 / 114272 | training loss: 0.00916797760874033\n",
      "epoch: 2 | 20864 / 114272 | training loss: 0.06849244236946106\n",
      "epoch: 2 | 20896 / 114272 | training loss: 0.004151252098381519\n",
      "epoch: 2 | 20928 / 114272 | training loss: 0.061683639883995056\n",
      "epoch: 2 | 20960 / 114272 | training loss: 0.1840352565050125\n",
      "epoch: 2 | 20992 / 114272 | training loss: 0.05817306786775589\n",
      "epoch: 2 | 21024 / 114272 | training loss: 0.1564379781484604\n",
      "epoch: 2 | 21056 / 114272 | training loss: 0.0068787685595452785\n",
      "epoch: 2 | 21088 / 114272 | training loss: 0.008351675234735012\n",
      "epoch: 2 | 21120 / 114272 | training loss: 0.10655061155557632\n",
      "epoch: 2 | 21152 / 114272 | training loss: 0.0050451611168682575\n",
      "epoch: 2 | 21184 / 114272 | training loss: 0.12113048881292343\n",
      "epoch: 2 | 21216 / 114272 | training loss: 0.006415376905351877\n",
      "epoch: 2 | 21248 / 114272 | training loss: 0.07537365704774857\n",
      "epoch: 2 | 21280 / 114272 | training loss: 0.111025370657444\n",
      "epoch: 2 | 21312 / 114272 | training loss: 0.004517116583883762\n",
      "epoch: 2 | 21344 / 114272 | training loss: 0.006238275207579136\n",
      "epoch: 2 | 21376 / 114272 | training loss: 0.03692639246582985\n",
      "epoch: 2 | 21408 / 114272 | training loss: 0.1240016371011734\n",
      "epoch: 2 | 21440 / 114272 | training loss: 0.006011961493641138\n",
      "epoch: 2 | 21472 / 114272 | training loss: 0.05060889944434166\n",
      "epoch: 2 | 21504 / 114272 | training loss: 0.21298685669898987\n",
      "epoch: 2 | 21536 / 114272 | training loss: 0.19172130525112152\n",
      "epoch: 2 | 21568 / 114272 | training loss: 0.05600912123918533\n",
      "epoch: 2 | 21600 / 114272 | training loss: 0.01205466315150261\n",
      "epoch: 2 | 21632 / 114272 | training loss: 0.01475032139569521\n",
      "epoch: 2 | 21664 / 114272 | training loss: 0.009631034918129444\n",
      "epoch: 2 | 21696 / 114272 | training loss: 0.12408234179019928\n",
      "epoch: 2 | 21728 / 114272 | training loss: 0.13792239129543304\n",
      "epoch: 2 | 21760 / 114272 | training loss: 0.06012933701276779\n",
      "epoch: 2 | 21792 / 114272 | training loss: 0.10041127353906631\n",
      "epoch: 2 | 21824 / 114272 | training loss: 0.004133621230721474\n",
      "epoch: 2 | 21856 / 114272 | training loss: 0.07333909720182419\n",
      "epoch: 2 | 21888 / 114272 | training loss: 0.04893733933568001\n",
      "epoch: 2 | 21920 / 114272 | training loss: 0.15709872543811798\n",
      "epoch: 2 | 21952 / 114272 | training loss: 0.014655259437859058\n",
      "epoch: 2 | 21984 / 114272 | training loss: 0.012077724561095238\n",
      "epoch: 2 | 22016 / 114272 | training loss: 0.0027765375562012196\n",
      "epoch: 2 | 22048 / 114272 | training loss: 0.03539402410387993\n",
      "epoch: 2 | 22080 / 114272 | training loss: 0.061532340943813324\n",
      "epoch: 2 | 22112 / 114272 | training loss: 0.009018969722092152\n",
      "epoch: 2 | 22144 / 114272 | training loss: 0.009421036578714848\n",
      "epoch: 2 | 22176 / 114272 | training loss: 0.14009369909763336\n",
      "epoch: 2 | 22208 / 114272 | training loss: 0.02345910295844078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 22240 / 114272 | training loss: 0.03210217505693436\n",
      "epoch: 2 | 22272 / 114272 | training loss: 0.03471149131655693\n",
      "epoch: 2 | 22304 / 114272 | training loss: 0.0068686907179653645\n",
      "epoch: 2 | 22336 / 114272 | training loss: 0.11477167159318924\n",
      "epoch: 2 | 22368 / 114272 | training loss: 0.041775889694690704\n",
      "epoch: 2 | 22400 / 114272 | training loss: 0.04451083391904831\n",
      "epoch: 2 | 22432 / 114272 | training loss: 0.01120511069893837\n",
      "epoch: 2 | 22464 / 114272 | training loss: 0.00935429334640503\n",
      "epoch: 2 | 22496 / 114272 | training loss: 0.06392811983823776\n",
      "epoch: 2 | 22528 / 114272 | training loss: 0.19038034975528717\n",
      "epoch: 2 | 22560 / 114272 | training loss: 0.006654169410467148\n",
      "epoch: 2 | 22592 / 114272 | training loss: 0.006358928978443146\n",
      "epoch: 2 | 22624 / 114272 | training loss: 0.03432275354862213\n",
      "epoch: 2 | 22656 / 114272 | training loss: 0.05700845271348953\n",
      "epoch: 2 | 22688 / 114272 | training loss: 0.23484529554843903\n",
      "epoch: 2 | 22720 / 114272 | training loss: 0.1733580380678177\n",
      "epoch: 2 | 22752 / 114272 | training loss: 0.035993922501802444\n",
      "epoch: 2 | 22784 / 114272 | training loss: 0.10219883918762207\n",
      "epoch: 2 | 22816 / 114272 | training loss: 0.26656392216682434\n",
      "epoch: 2 | 22848 / 114272 | training loss: 0.3026297688484192\n",
      "epoch: 2 | 22880 / 114272 | training loss: 0.031564708799123764\n",
      "epoch: 2 | 22912 / 114272 | training loss: 0.005294745322316885\n",
      "epoch: 2 | 22944 / 114272 | training loss: 0.04562760889530182\n",
      "epoch: 2 | 22976 / 114272 | training loss: 0.0045183016918599606\n",
      "epoch: 2 | 23008 / 114272 | training loss: 0.0029476068448275328\n",
      "epoch: 2 | 23040 / 114272 | training loss: 0.13805851340293884\n",
      "epoch: 2 | 23072 / 114272 | training loss: 0.0025865829084068537\n",
      "epoch: 2 | 23104 / 114272 | training loss: 0.08905672281980515\n",
      "epoch: 2 | 23136 / 114272 | training loss: 0.04142989590764046\n",
      "epoch: 2 | 23168 / 114272 | training loss: 0.02366672083735466\n",
      "epoch: 2 | 23200 / 114272 | training loss: 0.3136093020439148\n",
      "epoch: 2 | 23232 / 114272 | training loss: 0.010743825696408749\n",
      "epoch: 2 | 23264 / 114272 | training loss: 0.027429500594735146\n",
      "epoch: 2 | 23296 / 114272 | training loss: 0.012624948285520077\n",
      "epoch: 2 | 23328 / 114272 | training loss: 0.008100085891783237\n",
      "epoch: 2 | 23360 / 114272 | training loss: 0.09423448145389557\n",
      "epoch: 2 | 23392 / 114272 | training loss: 0.3416127860546112\n",
      "epoch: 2 | 23424 / 114272 | training loss: 0.0823557898402214\n",
      "epoch: 2 | 23456 / 114272 | training loss: 0.008488684892654419\n",
      "epoch: 2 | 23488 / 114272 | training loss: 0.14274153113365173\n",
      "epoch: 2 | 23520 / 114272 | training loss: 0.10069708526134491\n",
      "epoch: 2 | 23552 / 114272 | training loss: 0.1554727703332901\n",
      "epoch: 2 | 23584 / 114272 | training loss: 0.20742923021316528\n",
      "epoch: 2 | 23616 / 114272 | training loss: 0.11159166693687439\n",
      "epoch: 2 | 23648 / 114272 | training loss: 0.05266369879245758\n",
      "epoch: 2 | 23680 / 114272 | training loss: 0.12172654271125793\n",
      "epoch: 2 | 23712 / 114272 | training loss: 0.02243906818330288\n",
      "epoch: 2 | 23744 / 114272 | training loss: 0.10238724946975708\n",
      "epoch: 2 | 23776 / 114272 | training loss: 0.08275943994522095\n",
      "epoch: 2 | 23808 / 114272 | training loss: 0.19594964385032654\n",
      "epoch: 2 | 23840 / 114272 | training loss: 0.007060579489916563\n",
      "epoch: 2 | 23872 / 114272 | training loss: 0.08213775604963303\n",
      "epoch: 2 | 23904 / 114272 | training loss: 0.31130605936050415\n",
      "epoch: 2 | 23936 / 114272 | training loss: 0.06784157454967499\n",
      "epoch: 2 | 23968 / 114272 | training loss: 0.010605039075016975\n",
      "epoch: 2 | 24000 / 114272 | training loss: 0.06393712013959885\n",
      "epoch: 2 | 24032 / 114272 | training loss: 0.14312921464443207\n",
      "epoch: 2 | 24064 / 114272 | training loss: 0.023414043709635735\n",
      "epoch: 2 | 24096 / 114272 | training loss: 0.1826520711183548\n",
      "epoch: 2 | 24128 / 114272 | training loss: 0.017486093565821648\n",
      "epoch: 2 | 24160 / 114272 | training loss: 0.1438630223274231\n",
      "epoch: 2 | 24192 / 114272 | training loss: 0.005837792530655861\n",
      "epoch: 2 | 24224 / 114272 | training loss: 0.0072669400833547115\n",
      "epoch: 2 | 24256 / 114272 | training loss: 0.07183094322681427\n",
      "epoch: 2 | 24288 / 114272 | training loss: 0.013972872868180275\n",
      "epoch: 2 | 24320 / 114272 | training loss: 0.006998445373028517\n",
      "epoch: 2 | 24352 / 114272 | training loss: 0.07098682969808578\n",
      "epoch: 2 | 24384 / 114272 | training loss: 0.05653751641511917\n",
      "epoch: 2 | 24416 / 114272 | training loss: 0.006797082722187042\n",
      "epoch: 2 | 24448 / 114272 | training loss: 0.1555478572845459\n",
      "epoch: 2 | 24480 / 114272 | training loss: 0.35599231719970703\n",
      "epoch: 2 | 24512 / 114272 | training loss: 0.003139964770525694\n",
      "epoch: 2 | 24544 / 114272 | training loss: 0.008510305546224117\n",
      "epoch: 2 | 24576 / 114272 | training loss: 0.005284275859594345\n",
      "epoch: 2 | 24608 / 114272 | training loss: 0.01869233325123787\n",
      "epoch: 2 | 24640 / 114272 | training loss: 0.007877110503613949\n",
      "epoch: 2 | 24672 / 114272 | training loss: 0.3320125639438629\n",
      "epoch: 2 | 24704 / 114272 | training loss: 0.07347329705953598\n",
      "epoch: 2 | 24736 / 114272 | training loss: 0.0663265809416771\n",
      "epoch: 2 | 24768 / 114272 | training loss: 0.2731057107448578\n",
      "epoch: 2 | 24800 / 114272 | training loss: 0.1243671178817749\n",
      "epoch: 2 | 24832 / 114272 | training loss: 0.28549107909202576\n",
      "epoch: 2 | 24864 / 114272 | training loss: 0.15850622951984406\n",
      "epoch: 2 | 24896 / 114272 | training loss: 0.17470094561576843\n",
      "epoch: 2 | 24928 / 114272 | training loss: 0.004340972751379013\n",
      "epoch: 2 | 24960 / 114272 | training loss: 0.044257547706365585\n",
      "epoch: 2 | 24992 / 114272 | training loss: 0.12287894636392593\n",
      "epoch: 2 | 25024 / 114272 | training loss: 0.2438986748456955\n",
      "epoch: 2 | 25056 / 114272 | training loss: 0.005666979588568211\n",
      "epoch: 2 | 25088 / 114272 | training loss: 0.1225195825099945\n",
      "epoch: 2 | 25120 / 114272 | training loss: 0.005000746343284845\n",
      "epoch: 2 | 25152 / 114272 | training loss: 0.012588723562657833\n",
      "epoch: 2 | 25184 / 114272 | training loss: 0.10203265398740768\n",
      "epoch: 2 | 25216 / 114272 | training loss: 0.0721292719244957\n",
      "epoch: 2 | 25248 / 114272 | training loss: 0.05960065498948097\n",
      "epoch: 2 | 25280 / 114272 | training loss: 0.16468635201454163\n",
      "epoch: 2 | 25312 / 114272 | training loss: 0.023668412119150162\n",
      "epoch: 2 | 25344 / 114272 | training loss: 0.17409226298332214\n",
      "epoch: 2 | 25376 / 114272 | training loss: 0.09792616218328476\n",
      "epoch: 2 | 25408 / 114272 | training loss: 0.1425859034061432\n",
      "epoch: 2 | 25440 / 114272 | training loss: 0.07524105906486511\n",
      "epoch: 2 | 25472 / 114272 | training loss: 0.08608633279800415\n",
      "epoch: 2 | 25504 / 114272 | training loss: 0.020902574062347412\n",
      "epoch: 2 | 25536 / 114272 | training loss: 0.09640078991651535\n",
      "epoch: 2 | 25568 / 114272 | training loss: 0.008739855140447617\n",
      "epoch: 2 | 25600 / 114272 | training loss: 0.01041023712605238\n",
      "epoch: 2 | 25632 / 114272 | training loss: 0.012490780092775822\n",
      "epoch: 2 | 25664 / 114272 | training loss: 0.006954991724342108\n",
      "epoch: 2 | 25696 / 114272 | training loss: 0.06433063000440598\n",
      "epoch: 2 | 25728 / 114272 | training loss: 0.06269370764493942\n",
      "epoch: 2 | 25760 / 114272 | training loss: 0.07102435827255249\n",
      "epoch: 2 | 25792 / 114272 | training loss: 0.007437541615217924\n",
      "epoch: 2 | 25824 / 114272 | training loss: 0.10164730995893478\n",
      "epoch: 2 | 25856 / 114272 | training loss: 0.009906267747282982\n",
      "epoch: 2 | 25888 / 114272 | training loss: 0.005617953836917877\n",
      "epoch: 2 | 25920 / 114272 | training loss: 0.06371192634105682\n",
      "epoch: 2 | 25952 / 114272 | training loss: 0.005879068281501532\n",
      "epoch: 2 | 25984 / 114272 | training loss: 0.14160576462745667\n",
      "epoch: 2 | 26016 / 114272 | training loss: 0.03629150241613388\n",
      "epoch: 2 | 26048 / 114272 | training loss: 0.18288059532642365\n",
      "epoch: 2 | 26080 / 114272 | training loss: 0.17684751749038696\n",
      "epoch: 2 | 26112 / 114272 | training loss: 0.074229896068573\n",
      "epoch: 2 | 26144 / 114272 | training loss: 0.09746701270341873\n",
      "epoch: 2 | 26176 / 114272 | training loss: 0.11142304539680481\n",
      "epoch: 2 | 26208 / 114272 | training loss: 0.12163443118333817\n",
      "epoch: 2 | 26240 / 114272 | training loss: 0.1000838577747345\n",
      "epoch: 2 | 26272 / 114272 | training loss: 0.10278884321451187\n",
      "epoch: 2 | 26304 / 114272 | training loss: 0.21381361782550812\n",
      "epoch: 2 | 26336 / 114272 | training loss: 0.0401059091091156\n",
      "epoch: 2 | 26368 / 114272 | training loss: 0.004902328830212355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 26400 / 114272 | training loss: 0.2297501564025879\n",
      "epoch: 2 | 26432 / 114272 | training loss: 0.009093202650547028\n",
      "epoch: 2 | 26464 / 114272 | training loss: 0.19570215046405792\n",
      "epoch: 2 | 26496 / 114272 | training loss: 0.07726091146469116\n",
      "epoch: 2 | 26528 / 114272 | training loss: 0.012864316813647747\n",
      "epoch: 2 | 26560 / 114272 | training loss: 0.04236876964569092\n",
      "epoch: 2 | 26592 / 114272 | training loss: 0.09393566846847534\n",
      "epoch: 2 | 26624 / 114272 | training loss: 0.14305351674556732\n",
      "epoch: 2 | 26656 / 114272 | training loss: 0.12170813977718353\n",
      "epoch: 2 | 26688 / 114272 | training loss: 0.11812550574541092\n",
      "epoch: 2 | 26720 / 114272 | training loss: 0.21096539497375488\n",
      "epoch: 2 | 26752 / 114272 | training loss: 0.006451303604990244\n",
      "epoch: 2 | 26784 / 114272 | training loss: 0.060349155217409134\n",
      "epoch: 2 | 26816 / 114272 | training loss: 0.04612568020820618\n",
      "epoch: 2 | 26848 / 114272 | training loss: 0.014330067671835423\n",
      "epoch: 2 | 26880 / 114272 | training loss: 0.04962187260389328\n",
      "epoch: 2 | 26912 / 114272 | training loss: 0.06137845665216446\n",
      "epoch: 2 | 26944 / 114272 | training loss: 0.04445209354162216\n",
      "epoch: 2 | 26976 / 114272 | training loss: 0.04689275845885277\n",
      "epoch: 2 | 27008 / 114272 | training loss: 0.012687885202467442\n",
      "epoch: 2 | 27040 / 114272 | training loss: 0.2233685851097107\n",
      "epoch: 2 | 27072 / 114272 | training loss: 0.025625312700867653\n",
      "epoch: 2 | 27104 / 114272 | training loss: 0.01703721098601818\n",
      "epoch: 2 | 27136 / 114272 | training loss: 0.00548252509906888\n",
      "epoch: 2 | 27168 / 114272 | training loss: 0.11054748296737671\n",
      "epoch: 2 | 27200 / 114272 | training loss: 0.01067877747118473\n",
      "epoch: 2 | 27232 / 114272 | training loss: 0.01168320793658495\n",
      "epoch: 2 | 27264 / 114272 | training loss: 0.20689420402050018\n",
      "epoch: 2 | 27296 / 114272 | training loss: 0.4237625300884247\n",
      "epoch: 2 | 27328 / 114272 | training loss: 0.008052605204284191\n",
      "epoch: 2 | 27360 / 114272 | training loss: 0.13278469443321228\n",
      "epoch: 2 | 27392 / 114272 | training loss: 0.04971158504486084\n",
      "epoch: 2 | 27424 / 114272 | training loss: 0.13677345216274261\n",
      "epoch: 2 | 27456 / 114272 | training loss: 0.1529843956232071\n",
      "epoch: 2 | 27488 / 114272 | training loss: 0.21102452278137207\n",
      "epoch: 2 | 27520 / 114272 | training loss: 0.021607788279652596\n",
      "epoch: 2 | 27552 / 114272 | training loss: 0.15810073912143707\n",
      "epoch: 2 | 27584 / 114272 | training loss: 0.1629653126001358\n",
      "epoch: 2 | 27616 / 114272 | training loss: 0.004437906667590141\n",
      "epoch: 2 | 27648 / 114272 | training loss: 0.15751861035823822\n",
      "epoch: 2 | 27680 / 114272 | training loss: 0.006526737008243799\n",
      "epoch: 2 | 27712 / 114272 | training loss: 0.0071699232794344425\n",
      "epoch: 2 | 27744 / 114272 | training loss: 0.0059944516979157925\n",
      "epoch: 2 | 27776 / 114272 | training loss: 0.012075420469045639\n",
      "epoch: 2 | 27808 / 114272 | training loss: 0.003312354441732168\n",
      "epoch: 2 | 27840 / 114272 | training loss: 0.008065199479460716\n",
      "epoch: 2 | 27872 / 114272 | training loss: 0.13237139582633972\n",
      "epoch: 2 | 27904 / 114272 | training loss: 0.004180962685495615\n",
      "epoch: 2 | 27936 / 114272 | training loss: 0.03146882355213165\n",
      "epoch: 2 | 27968 / 114272 | training loss: 0.008029802702367306\n",
      "epoch: 2 | 28000 / 114272 | training loss: 0.16657815873622894\n",
      "epoch: 2 | 28032 / 114272 | training loss: 0.10838929563760757\n",
      "epoch: 2 | 28064 / 114272 | training loss: 0.011729307472705841\n",
      "epoch: 2 | 28096 / 114272 | training loss: 0.033278707414865494\n",
      "epoch: 2 | 28128 / 114272 | training loss: 0.064278244972229\n",
      "epoch: 2 | 28160 / 114272 | training loss: 0.005765913054347038\n",
      "epoch: 2 | 28192 / 114272 | training loss: 0.029112979769706726\n",
      "epoch: 2 | 28224 / 114272 | training loss: 0.006562806665897369\n",
      "epoch: 2 | 28256 / 114272 | training loss: 0.043858859688043594\n",
      "epoch: 2 | 28288 / 114272 | training loss: 0.21400713920593262\n",
      "epoch: 2 | 28320 / 114272 | training loss: 0.10401233285665512\n",
      "epoch: 2 | 28352 / 114272 | training loss: 0.003529824549332261\n",
      "epoch: 2 | 28384 / 114272 | training loss: 0.015221676789224148\n",
      "epoch: 2 | 28416 / 114272 | training loss: 0.40911006927490234\n",
      "epoch: 2 | 28448 / 114272 | training loss: 0.22269068658351898\n",
      "epoch: 2 | 28480 / 114272 | training loss: 0.00452784588560462\n",
      "epoch: 2 | 28512 / 114272 | training loss: 0.005355598405003548\n",
      "epoch: 2 | 28544 / 114272 | training loss: 0.1880076378583908\n",
      "epoch: 2 | 28576 / 114272 | training loss: 0.008518420159816742\n",
      "epoch: 2 | 28608 / 114272 | training loss: 0.11881406605243683\n",
      "epoch: 2 | 28640 / 114272 | training loss: 0.2416500449180603\n",
      "epoch: 2 | 28672 / 114272 | training loss: 0.03169313073158264\n",
      "epoch: 2 | 28704 / 114272 | training loss: 0.18929648399353027\n",
      "epoch: 2 | 28736 / 114272 | training loss: 0.0939568355679512\n",
      "epoch: 2 | 28768 / 114272 | training loss: 0.006592456717044115\n",
      "epoch: 2 | 28800 / 114272 | training loss: 0.2475903183221817\n",
      "epoch: 2 | 28832 / 114272 | training loss: 0.058558911085128784\n",
      "epoch: 2 | 28864 / 114272 | training loss: 0.04128296300768852\n",
      "epoch: 2 | 28896 / 114272 | training loss: 0.014421130530536175\n",
      "epoch: 2 | 28928 / 114272 | training loss: 0.18424443900585175\n",
      "epoch: 2 | 28960 / 114272 | training loss: 0.08685240149497986\n",
      "epoch: 2 | 28992 / 114272 | training loss: 0.19280888140201569\n",
      "epoch: 2 | 29024 / 114272 | training loss: 0.029100602492690086\n",
      "epoch: 2 | 29056 / 114272 | training loss: 0.04245925694704056\n",
      "epoch: 2 | 29088 / 114272 | training loss: 0.007901903241872787\n",
      "epoch: 2 | 29120 / 114272 | training loss: 0.013788807205855846\n",
      "epoch: 2 | 29152 / 114272 | training loss: 0.009324904531240463\n",
      "epoch: 2 | 29184 / 114272 | training loss: 0.01836112141609192\n",
      "epoch: 2 | 29216 / 114272 | training loss: 0.05327494814991951\n",
      "epoch: 2 | 29248 / 114272 | training loss: 0.15799172222614288\n",
      "epoch: 2 | 29280 / 114272 | training loss: 0.2856683135032654\n",
      "epoch: 2 | 29312 / 114272 | training loss: 0.004880427848547697\n",
      "epoch: 2 | 29344 / 114272 | training loss: 0.009090924635529518\n",
      "epoch: 2 | 29376 / 114272 | training loss: 0.11573320627212524\n",
      "epoch: 2 | 29408 / 114272 | training loss: 0.013815691694617271\n",
      "epoch: 2 | 29440 / 114272 | training loss: 0.2587979733943939\n",
      "epoch: 2 | 29472 / 114272 | training loss: 0.0263743307441473\n",
      "epoch: 2 | 29504 / 114272 | training loss: 0.055788759142160416\n",
      "epoch: 2 | 29536 / 114272 | training loss: 0.00926970411092043\n",
      "epoch: 2 | 29568 / 114272 | training loss: 0.008429559879004955\n",
      "epoch: 2 | 29600 / 114272 | training loss: 0.2435825914144516\n",
      "epoch: 2 | 29632 / 114272 | training loss: 0.5081852674484253\n",
      "epoch: 2 | 29664 / 114272 | training loss: 0.02775002084672451\n",
      "epoch: 2 | 29696 / 114272 | training loss: 0.025590740144252777\n",
      "epoch: 2 | 29728 / 114272 | training loss: 0.007357514929026365\n",
      "epoch: 2 | 29760 / 114272 | training loss: 0.10230036079883575\n",
      "epoch: 2 | 29792 / 114272 | training loss: 0.05562442168593407\n",
      "epoch: 2 | 29824 / 114272 | training loss: 0.007197779603302479\n",
      "epoch: 2 | 29856 / 114272 | training loss: 0.17057926952838898\n",
      "epoch: 2 | 29888 / 114272 | training loss: 0.1549740433692932\n",
      "epoch: 2 | 29920 / 114272 | training loss: 0.16015619039535522\n",
      "epoch: 2 | 29952 / 114272 | training loss: 0.03407838195562363\n",
      "epoch: 2 | 29984 / 114272 | training loss: 0.020399462431669235\n",
      "epoch: 2 | 30016 / 114272 | training loss: 0.15982018411159515\n",
      "epoch: 2 | 30048 / 114272 | training loss: 0.011871668510138988\n",
      "epoch: 2 | 30080 / 114272 | training loss: 0.16930292546749115\n",
      "epoch: 2 | 30112 / 114272 | training loss: 0.010743534192442894\n",
      "epoch: 2 | 30144 / 114272 | training loss: 0.0762329027056694\n",
      "epoch: 2 | 30176 / 114272 | training loss: 0.025254519656300545\n",
      "epoch: 2 | 30208 / 114272 | training loss: 0.11953045427799225\n",
      "epoch: 2 | 30240 / 114272 | training loss: 0.05379323661327362\n",
      "epoch: 2 | 30272 / 114272 | training loss: 0.021815508604049683\n",
      "epoch: 2 | 30304 / 114272 | training loss: 0.01759060099720955\n",
      "epoch: 2 | 30336 / 114272 | training loss: 0.00570023525506258\n",
      "epoch: 2 | 30368 / 114272 | training loss: 0.07817105203866959\n",
      "epoch: 2 | 30400 / 114272 | training loss: 0.008905143477022648\n",
      "epoch: 2 | 30432 / 114272 | training loss: 0.006333954632282257\n",
      "epoch: 2 | 30464 / 114272 | training loss: 0.01588273048400879\n",
      "epoch: 2 | 30496 / 114272 | training loss: 0.00512290745973587\n",
      "epoch: 2 | 30528 / 114272 | training loss: 0.021735958755016327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 30560 / 114272 | training loss: 0.0075840456411242485\n",
      "epoch: 2 | 30592 / 114272 | training loss: 0.22790037095546722\n",
      "epoch: 2 | 30624 / 114272 | training loss: 0.02149936929345131\n",
      "epoch: 2 | 30656 / 114272 | training loss: 0.31191134452819824\n",
      "epoch: 2 | 30688 / 114272 | training loss: 0.09764620661735535\n",
      "epoch: 2 | 30720 / 114272 | training loss: 0.17026224732398987\n",
      "epoch: 2 | 30752 / 114272 | training loss: 0.15952250361442566\n",
      "epoch: 2 | 30784 / 114272 | training loss: 0.06786325573921204\n",
      "epoch: 2 | 30816 / 114272 | training loss: 0.0062872483395040035\n",
      "epoch: 2 | 30848 / 114272 | training loss: 0.05376911163330078\n",
      "epoch: 2 | 30880 / 114272 | training loss: 0.006531239952892065\n",
      "epoch: 2 | 30912 / 114272 | training loss: 0.1011732667684555\n",
      "epoch: 2 | 30944 / 114272 | training loss: 0.020514117553830147\n",
      "epoch: 2 | 30976 / 114272 | training loss: 0.12553924322128296\n",
      "epoch: 2 | 31008 / 114272 | training loss: 0.19025343656539917\n",
      "epoch: 2 | 31040 / 114272 | training loss: 0.14923545718193054\n",
      "epoch: 2 | 31072 / 114272 | training loss: 0.009648751467466354\n",
      "epoch: 2 | 31104 / 114272 | training loss: 0.008657608181238174\n",
      "epoch: 2 | 31136 / 114272 | training loss: 0.03610862419009209\n",
      "epoch: 2 | 31168 / 114272 | training loss: 0.013371450826525688\n",
      "epoch: 2 | 31200 / 114272 | training loss: 0.009070152416825294\n",
      "epoch: 2 | 31232 / 114272 | training loss: 0.0595138855278492\n",
      "epoch: 2 | 31264 / 114272 | training loss: 0.09975763410329819\n",
      "epoch: 2 | 31296 / 114272 | training loss: 0.006421887315809727\n",
      "epoch: 2 | 31328 / 114272 | training loss: 0.19962084293365479\n",
      "epoch: 2 | 31360 / 114272 | training loss: 0.3184681832790375\n",
      "epoch: 2 | 31392 / 114272 | training loss: 0.009798754006624222\n",
      "epoch: 2 | 31424 / 114272 | training loss: 0.008581317961215973\n",
      "epoch: 2 | 31456 / 114272 | training loss: 0.16161412000656128\n",
      "epoch: 2 | 31488 / 114272 | training loss: 0.010339666157960892\n",
      "epoch: 2 | 31520 / 114272 | training loss: 0.16244743764400482\n",
      "epoch: 2 | 31552 / 114272 | training loss: 0.02735607698559761\n",
      "epoch: 2 | 31584 / 114272 | training loss: 0.20124247670173645\n",
      "epoch: 2 | 31616 / 114272 | training loss: 0.013389932923018932\n",
      "epoch: 2 | 31648 / 114272 | training loss: 0.007247052155435085\n",
      "epoch: 2 | 31680 / 114272 | training loss: 0.1715160608291626\n",
      "epoch: 2 | 31712 / 114272 | training loss: 0.012884424068033695\n",
      "epoch: 2 | 31744 / 114272 | training loss: 0.004379671532660723\n",
      "epoch: 2 | 31776 / 114272 | training loss: 0.03373737260699272\n",
      "epoch: 2 | 31808 / 114272 | training loss: 0.009688707068562508\n",
      "epoch: 2 | 31840 / 114272 | training loss: 0.05637592077255249\n",
      "epoch: 2 | 31872 / 114272 | training loss: 0.017489109188318253\n",
      "epoch: 2 | 31904 / 114272 | training loss: 0.08393754810094833\n",
      "epoch: 2 | 31936 / 114272 | training loss: 0.10218600928783417\n",
      "epoch: 2 | 31968 / 114272 | training loss: 0.09418416768312454\n",
      "epoch: 2 | 32000 / 114272 | training loss: 0.003274612594395876\n",
      "epoch: 2 | 32032 / 114272 | training loss: 0.3895927667617798\n",
      "epoch: 2 | 32064 / 114272 | training loss: 0.007938246242702007\n",
      "epoch: 2 | 32096 / 114272 | training loss: 0.13709115982055664\n",
      "epoch: 2 | 32128 / 114272 | training loss: 0.08987608551979065\n",
      "epoch: 2 | 32160 / 114272 | training loss: 0.026121944189071655\n",
      "epoch: 2 | 32192 / 114272 | training loss: 0.21406328678131104\n",
      "epoch: 2 | 32224 / 114272 | training loss: 0.2018062025308609\n",
      "epoch: 2 | 32256 / 114272 | training loss: 0.1275269240140915\n",
      "epoch: 2 | 32288 / 114272 | training loss: 0.010553035885095596\n",
      "epoch: 2 | 32320 / 114272 | training loss: 0.006517606321722269\n",
      "epoch: 2 | 32352 / 114272 | training loss: 0.018432864919304848\n",
      "epoch: 2 | 32384 / 114272 | training loss: 0.16964349150657654\n",
      "epoch: 2 | 32416 / 114272 | training loss: 0.4482974410057068\n",
      "epoch: 2 | 32448 / 114272 | training loss: 0.10386007279157639\n",
      "epoch: 2 | 32480 / 114272 | training loss: 0.09637130051851273\n",
      "epoch: 2 | 32512 / 114272 | training loss: 0.29335513710975647\n",
      "epoch: 2 | 32544 / 114272 | training loss: 0.07951720058917999\n",
      "epoch: 2 | 32576 / 114272 | training loss: 0.08301851898431778\n",
      "epoch: 2 | 32608 / 114272 | training loss: 0.222944438457489\n",
      "epoch: 2 | 32640 / 114272 | training loss: 0.011571578681468964\n",
      "epoch: 2 | 32672 / 114272 | training loss: 0.16379617154598236\n",
      "epoch: 2 | 32704 / 114272 | training loss: 0.07233463227748871\n",
      "epoch: 2 | 32736 / 114272 | training loss: 0.14294399321079254\n",
      "epoch: 2 | 32768 / 114272 | training loss: 0.029414189979434013\n",
      "epoch: 2 | 32800 / 114272 | training loss: 0.17828890681266785\n",
      "epoch: 2 | 32832 / 114272 | training loss: 0.14988906681537628\n",
      "epoch: 2 | 32864 / 114272 | training loss: 0.04480661451816559\n",
      "epoch: 2 | 32896 / 114272 | training loss: 0.30563560128211975\n",
      "epoch: 2 | 32928 / 114272 | training loss: 0.05087246373295784\n",
      "epoch: 2 | 32960 / 114272 | training loss: 0.0577823743224144\n",
      "epoch: 2 | 32992 / 114272 | training loss: 0.10536383092403412\n",
      "epoch: 2 | 33024 / 114272 | training loss: 0.091382697224617\n",
      "epoch: 2 | 33056 / 114272 | training loss: 0.010579102672636509\n",
      "epoch: 2 | 33088 / 114272 | training loss: 0.09385836869478226\n",
      "epoch: 2 | 33120 / 114272 | training loss: 0.02952766604721546\n",
      "epoch: 2 | 33152 / 114272 | training loss: 0.008822389878332615\n",
      "epoch: 2 | 33184 / 114272 | training loss: 0.01198522187769413\n",
      "epoch: 2 | 33216 / 114272 | training loss: 0.013829243369400501\n",
      "epoch: 2 | 33248 / 114272 | training loss: 0.15985503792762756\n",
      "epoch: 2 | 33280 / 114272 | training loss: 0.008645758032798767\n",
      "epoch: 2 | 33312 / 114272 | training loss: 0.13803257048130035\n",
      "epoch: 2 | 33344 / 114272 | training loss: 0.06230632960796356\n",
      "epoch: 2 | 33376 / 114272 | training loss: 0.06353524327278137\n",
      "epoch: 2 | 33408 / 114272 | training loss: 0.3086874485015869\n",
      "epoch: 2 | 33440 / 114272 | training loss: 0.008308427408337593\n",
      "epoch: 2 | 33472 / 114272 | training loss: 0.13867513835430145\n",
      "epoch: 2 | 33504 / 114272 | training loss: 0.010911798104643822\n",
      "epoch: 2 | 33536 / 114272 | training loss: 0.19790251553058624\n",
      "epoch: 2 | 33568 / 114272 | training loss: 0.056144896894693375\n",
      "epoch: 2 | 33600 / 114272 | training loss: 0.10319241881370544\n",
      "epoch: 2 | 33632 / 114272 | training loss: 0.009328898042440414\n",
      "epoch: 2 | 33664 / 114272 | training loss: 0.029840268194675446\n",
      "epoch: 2 | 33696 / 114272 | training loss: 0.12136252224445343\n",
      "epoch: 2 | 33728 / 114272 | training loss: 0.16860122978687286\n",
      "epoch: 2 | 33760 / 114272 | training loss: 0.17404158413410187\n",
      "epoch: 2 | 33792 / 114272 | training loss: 0.12524043023586273\n",
      "epoch: 2 | 33824 / 114272 | training loss: 0.021300362423062325\n",
      "epoch: 2 | 33856 / 114272 | training loss: 0.03094826638698578\n",
      "epoch: 2 | 33888 / 114272 | training loss: 0.013507063500583172\n",
      "epoch: 2 | 33920 / 114272 | training loss: 0.32684531807899475\n",
      "epoch: 2 | 33952 / 114272 | training loss: 0.008016749285161495\n",
      "epoch: 2 | 33984 / 114272 | training loss: 0.06016472727060318\n",
      "epoch: 2 | 34016 / 114272 | training loss: 0.005509434267878532\n",
      "epoch: 2 | 34048 / 114272 | training loss: 0.0035638040862977505\n",
      "epoch: 2 | 34080 / 114272 | training loss: 0.056343987584114075\n",
      "epoch: 2 | 34112 / 114272 | training loss: 0.04236841946840286\n",
      "epoch: 2 | 34144 / 114272 | training loss: 0.014210675843060017\n",
      "epoch: 2 | 34176 / 114272 | training loss: 0.02490747720003128\n",
      "epoch: 2 | 34208 / 114272 | training loss: 0.02938496135175228\n",
      "epoch: 2 | 34240 / 114272 | training loss: 0.16913773119449615\n",
      "epoch: 2 | 34272 / 114272 | training loss: 0.031186209991574287\n",
      "epoch: 2 | 34304 / 114272 | training loss: 0.020990749821066856\n",
      "epoch: 2 | 34336 / 114272 | training loss: 0.07493598014116287\n",
      "epoch: 2 | 34368 / 114272 | training loss: 0.2781825363636017\n",
      "epoch: 2 | 34400 / 114272 | training loss: 0.09352124482393265\n",
      "epoch: 2 | 34432 / 114272 | training loss: 0.006459275726228952\n",
      "epoch: 2 | 34464 / 114272 | training loss: 0.12761040031909943\n",
      "epoch: 2 | 34496 / 114272 | training loss: 0.006496935151517391\n",
      "epoch: 2 | 34528 / 114272 | training loss: 0.02939775213599205\n",
      "epoch: 2 | 34560 / 114272 | training loss: 0.01312391646206379\n",
      "epoch: 2 | 34592 / 114272 | training loss: 0.07843293249607086\n",
      "epoch: 2 | 34624 / 114272 | training loss: 0.005623664706945419\n",
      "epoch: 2 | 34656 / 114272 | training loss: 0.020892782136797905\n",
      "epoch: 2 | 34688 / 114272 | training loss: 0.020742638036608696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 34720 / 114272 | training loss: 0.010902592912316322\n",
      "epoch: 2 | 34752 / 114272 | training loss: 0.13203729689121246\n",
      "epoch: 2 | 34784 / 114272 | training loss: 0.17083942890167236\n",
      "epoch: 2 | 34816 / 114272 | training loss: 0.1451738029718399\n",
      "epoch: 2 | 34848 / 114272 | training loss: 0.05966832861304283\n",
      "epoch: 2 | 34880 / 114272 | training loss: 0.3889411985874176\n",
      "epoch: 2 | 34912 / 114272 | training loss: 0.06055532768368721\n",
      "epoch: 2 | 34944 / 114272 | training loss: 0.0672655999660492\n",
      "epoch: 2 | 34976 / 114272 | training loss: 0.15284766256809235\n",
      "epoch: 2 | 35008 / 114272 | training loss: 0.006141590885818005\n",
      "epoch: 2 | 35040 / 114272 | training loss: 0.22913192212581635\n",
      "epoch: 2 | 35072 / 114272 | training loss: 0.11666417866945267\n",
      "epoch: 2 | 35104 / 114272 | training loss: 0.01373668760061264\n",
      "epoch: 2 | 35136 / 114272 | training loss: 0.02156621217727661\n",
      "epoch: 2 | 35168 / 114272 | training loss: 0.26187026500701904\n",
      "epoch: 2 | 35200 / 114272 | training loss: 0.012143050320446491\n",
      "epoch: 2 | 35232 / 114272 | training loss: 0.011600544676184654\n",
      "epoch: 2 | 35264 / 114272 | training loss: 0.28223344683647156\n",
      "epoch: 2 | 35296 / 114272 | training loss: 0.024432513862848282\n",
      "epoch: 2 | 35328 / 114272 | training loss: 0.14168529212474823\n",
      "epoch: 2 | 35360 / 114272 | training loss: 0.13628527522087097\n",
      "epoch: 2 | 35392 / 114272 | training loss: 0.022285861894488335\n",
      "epoch: 2 | 35424 / 114272 | training loss: 0.008817999623715878\n",
      "epoch: 2 | 35456 / 114272 | training loss: 0.008497966453433037\n",
      "epoch: 2 | 35488 / 114272 | training loss: 0.006436542607843876\n",
      "epoch: 2 | 35520 / 114272 | training loss: 0.1576521396636963\n",
      "epoch: 2 | 35552 / 114272 | training loss: 0.23378317058086395\n",
      "epoch: 2 | 35584 / 114272 | training loss: 0.0081145279109478\n",
      "epoch: 2 | 35616 / 114272 | training loss: 0.39238879084587097\n",
      "epoch: 2 | 35648 / 114272 | training loss: 0.016287580132484436\n",
      "epoch: 2 | 35680 / 114272 | training loss: 0.17190080881118774\n",
      "epoch: 2 | 35712 / 114272 | training loss: 0.07197968661785126\n",
      "epoch: 2 | 35744 / 114272 | training loss: 0.1776377111673355\n",
      "epoch: 2 | 35776 / 114272 | training loss: 0.005501477047801018\n",
      "epoch: 2 | 35808 / 114272 | training loss: 0.005593371111899614\n",
      "epoch: 2 | 35840 / 114272 | training loss: 0.5640035271644592\n",
      "epoch: 2 | 35872 / 114272 | training loss: 0.13280341029167175\n",
      "epoch: 2 | 35904 / 114272 | training loss: 0.023727377876639366\n",
      "epoch: 2 | 35936 / 114272 | training loss: 0.0924164354801178\n",
      "epoch: 2 | 35968 / 114272 | training loss: 0.012192289344966412\n",
      "epoch: 2 | 36000 / 114272 | training loss: 0.021834081038832664\n",
      "epoch: 2 | 36032 / 114272 | training loss: 0.1875215321779251\n",
      "epoch: 2 | 36064 / 114272 | training loss: 0.003207055851817131\n",
      "epoch: 2 | 36096 / 114272 | training loss: 0.006348154507577419\n",
      "epoch: 2 | 36128 / 114272 | training loss: 0.14802467823028564\n",
      "epoch: 2 | 36160 / 114272 | training loss: 0.018571779131889343\n",
      "epoch: 2 | 36192 / 114272 | training loss: 0.1796802282333374\n",
      "epoch: 2 | 36224 / 114272 | training loss: 0.007886731997132301\n",
      "epoch: 2 | 36256 / 114272 | training loss: 0.011302310973405838\n",
      "epoch: 2 | 36288 / 114272 | training loss: 0.3860718011856079\n",
      "epoch: 2 | 36320 / 114272 | training loss: 0.2298632115125656\n",
      "epoch: 2 | 36352 / 114272 | training loss: 0.006544860545545816\n",
      "epoch: 2 | 36384 / 114272 | training loss: 0.01297160517424345\n",
      "epoch: 2 | 36416 / 114272 | training loss: 0.059762243181467056\n",
      "epoch: 2 | 36448 / 114272 | training loss: 0.12099310010671616\n",
      "epoch: 2 | 36480 / 114272 | training loss: 0.00716656306758523\n",
      "epoch: 2 | 36512 / 114272 | training loss: 0.0964464619755745\n",
      "epoch: 2 | 36544 / 114272 | training loss: 0.007166016846895218\n",
      "epoch: 2 | 36576 / 114272 | training loss: 0.04268090799450874\n",
      "epoch: 2 | 36608 / 114272 | training loss: 0.10253310203552246\n",
      "epoch: 2 | 36640 / 114272 | training loss: 0.005778331775218248\n",
      "epoch: 2 | 36672 / 114272 | training loss: 0.26216328144073486\n",
      "epoch: 2 | 36704 / 114272 | training loss: 0.03947564586997032\n",
      "epoch: 2 | 36736 / 114272 | training loss: 0.13515877723693848\n",
      "epoch: 2 | 36768 / 114272 | training loss: 0.08212690055370331\n",
      "epoch: 2 | 36800 / 114272 | training loss: 0.08019332587718964\n",
      "epoch: 2 | 36832 / 114272 | training loss: 0.008167698048055172\n",
      "epoch: 2 | 36864 / 114272 | training loss: 0.06318429112434387\n",
      "epoch: 2 | 36896 / 114272 | training loss: 0.5100123882293701\n",
      "epoch: 2 | 36928 / 114272 | training loss: 0.1543866991996765\n",
      "epoch: 2 | 36960 / 114272 | training loss: 0.1590392142534256\n",
      "epoch: 2 | 36992 / 114272 | training loss: 0.034165266901254654\n",
      "epoch: 2 | 37024 / 114272 | training loss: 0.010835944674909115\n",
      "epoch: 2 | 37056 / 114272 | training loss: 0.13139018416404724\n",
      "epoch: 2 | 37088 / 114272 | training loss: 0.10538098961114883\n",
      "epoch: 2 | 37120 / 114272 | training loss: 0.053679317235946655\n",
      "epoch: 2 | 37152 / 114272 | training loss: 0.062119580805301666\n",
      "epoch: 2 | 37184 / 114272 | training loss: 0.22944407165050507\n",
      "epoch: 2 | 37216 / 114272 | training loss: 0.016214463859796524\n",
      "epoch: 2 | 37248 / 114272 | training loss: 0.09337259083986282\n",
      "epoch: 2 | 37280 / 114272 | training loss: 0.017822861671447754\n",
      "epoch: 2 | 37312 / 114272 | training loss: 0.12070003151893616\n",
      "epoch: 2 | 37344 / 114272 | training loss: 0.08741728216409683\n",
      "epoch: 2 | 37376 / 114272 | training loss: 0.0997968316078186\n",
      "epoch: 2 | 37408 / 114272 | training loss: 0.119107685983181\n",
      "epoch: 2 | 37440 / 114272 | training loss: 0.03727644309401512\n",
      "epoch: 2 | 37472 / 114272 | training loss: 0.024607151746749878\n",
      "epoch: 2 | 37504 / 114272 | training loss: 0.09722146391868591\n",
      "epoch: 2 | 37536 / 114272 | training loss: 0.03470908850431442\n",
      "epoch: 2 | 37568 / 114272 | training loss: 0.009972123429179192\n",
      "epoch: 2 | 37600 / 114272 | training loss: 0.5075640082359314\n",
      "epoch: 2 | 37632 / 114272 | training loss: 0.0640401616692543\n",
      "epoch: 2 | 37664 / 114272 | training loss: 0.19602976739406586\n",
      "epoch: 2 | 37696 / 114272 | training loss: 0.01336557324975729\n",
      "epoch: 2 | 37728 / 114272 | training loss: 0.10442669689655304\n",
      "epoch: 2 | 37760 / 114272 | training loss: 0.07034388929605484\n",
      "epoch: 2 | 37792 / 114272 | training loss: 0.009660501964390278\n",
      "epoch: 2 | 37824 / 114272 | training loss: 0.14779748022556305\n",
      "epoch: 2 | 37856 / 114272 | training loss: 0.03939170762896538\n",
      "epoch: 2 | 37888 / 114272 | training loss: 0.01752161793410778\n",
      "epoch: 2 | 37920 / 114272 | training loss: 0.054757796227931976\n",
      "epoch: 2 | 37952 / 114272 | training loss: 0.24958863854408264\n",
      "epoch: 2 | 37984 / 114272 | training loss: 0.166993647813797\n",
      "epoch: 2 | 38016 / 114272 | training loss: 0.1009199395775795\n",
      "epoch: 2 | 38048 / 114272 | training loss: 0.17134258151054382\n",
      "epoch: 2 | 38080 / 114272 | training loss: 0.12366750836372375\n",
      "epoch: 2 | 38112 / 114272 | training loss: 0.007331656292080879\n",
      "epoch: 2 | 38144 / 114272 | training loss: 0.008292279206216335\n",
      "epoch: 2 | 38176 / 114272 | training loss: 0.10909883677959442\n",
      "epoch: 2 | 38208 / 114272 | training loss: 0.0938437283039093\n",
      "epoch: 2 | 38240 / 114272 | training loss: 0.326019287109375\n",
      "epoch: 2 | 38272 / 114272 | training loss: 0.12477542459964752\n",
      "epoch: 2 | 38304 / 114272 | training loss: 0.07632549107074738\n",
      "epoch: 2 | 38336 / 114272 | training loss: 0.1420641839504242\n",
      "epoch: 2 | 38368 / 114272 | training loss: 0.027664538472890854\n",
      "epoch: 2 | 38400 / 114272 | training loss: 0.08813063055276871\n",
      "epoch: 2 | 38432 / 114272 | training loss: 0.013238864950835705\n",
      "epoch: 2 | 38464 / 114272 | training loss: 0.33180859684944153\n",
      "epoch: 2 | 38496 / 114272 | training loss: 0.04224422201514244\n",
      "epoch: 2 | 38528 / 114272 | training loss: 0.14122168719768524\n",
      "epoch: 2 | 38560 / 114272 | training loss: 0.15741290152072906\n",
      "epoch: 2 | 38592 / 114272 | training loss: 0.16087332367897034\n",
      "epoch: 2 | 38624 / 114272 | training loss: 0.14136828482151031\n",
      "epoch: 2 | 38656 / 114272 | training loss: 0.016449788585305214\n",
      "epoch: 2 | 38688 / 114272 | training loss: 0.10757796466350555\n",
      "epoch: 2 | 38720 / 114272 | training loss: 0.21970967948436737\n",
      "epoch: 2 | 38752 / 114272 | training loss: 0.00919826328754425\n",
      "epoch: 2 | 38784 / 114272 | training loss: 0.04746938869357109\n",
      "epoch: 2 | 38816 / 114272 | training loss: 0.10310370475053787\n",
      "epoch: 2 | 38848 / 114272 | training loss: 0.0774010643362999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 38880 / 114272 | training loss: 0.012489679269492626\n",
      "epoch: 2 | 38912 / 114272 | training loss: 0.0866524800658226\n",
      "epoch: 2 | 38944 / 114272 | training loss: 0.013267382979393005\n",
      "epoch: 2 | 38976 / 114272 | training loss: 0.04924948513507843\n",
      "epoch: 2 | 39008 / 114272 | training loss: 0.017118753865361214\n",
      "epoch: 2 | 39040 / 114272 | training loss: 0.10365410894155502\n",
      "epoch: 2 | 39072 / 114272 | training loss: 0.03162437304854393\n",
      "epoch: 2 | 39104 / 114272 | training loss: 0.07731832563877106\n",
      "epoch: 2 | 39136 / 114272 | training loss: 0.050229430198669434\n",
      "epoch: 2 | 39168 / 114272 | training loss: 0.14572103321552277\n",
      "epoch: 2 | 39200 / 114272 | training loss: 0.22715668380260468\n",
      "epoch: 2 | 39232 / 114272 | training loss: 0.238664910197258\n",
      "epoch: 2 | 39264 / 114272 | training loss: 0.004632021300494671\n",
      "epoch: 2 | 39296 / 114272 | training loss: 0.26524803042411804\n",
      "epoch: 2 | 39328 / 114272 | training loss: 0.05801744386553764\n",
      "epoch: 2 | 39360 / 114272 | training loss: 0.006637383718043566\n",
      "epoch: 2 | 39392 / 114272 | training loss: 0.13003261387348175\n",
      "epoch: 2 | 39424 / 114272 | training loss: 0.2259693741798401\n",
      "epoch: 2 | 39456 / 114272 | training loss: 0.005082127638161182\n",
      "epoch: 2 | 39488 / 114272 | training loss: 0.07818971574306488\n",
      "epoch: 2 | 39520 / 114272 | training loss: 0.0424344465136528\n",
      "epoch: 2 | 39552 / 114272 | training loss: 0.1963975578546524\n",
      "epoch: 2 | 39584 / 114272 | training loss: 0.25812026858329773\n",
      "epoch: 2 | 39616 / 114272 | training loss: 0.05484917387366295\n",
      "epoch: 2 | 39648 / 114272 | training loss: 0.12243078649044037\n",
      "epoch: 2 | 39680 / 114272 | training loss: 0.03845237195491791\n",
      "epoch: 2 | 39712 / 114272 | training loss: 0.07291341572999954\n",
      "epoch: 2 | 39744 / 114272 | training loss: 0.16973264515399933\n",
      "epoch: 2 | 39776 / 114272 | training loss: 0.018053840845823288\n",
      "epoch: 2 | 39808 / 114272 | training loss: 0.041915010660886765\n",
      "epoch: 2 | 39840 / 114272 | training loss: 0.07171826809644699\n",
      "epoch: 2 | 39872 / 114272 | training loss: 0.13045811653137207\n",
      "epoch: 2 | 39904 / 114272 | training loss: 0.1255980283021927\n",
      "epoch: 2 | 39936 / 114272 | training loss: 0.05206962674856186\n",
      "epoch: 2 | 39968 / 114272 | training loss: 0.0414755679666996\n",
      "epoch: 2 | 40000 / 114272 | training loss: 0.1220010444521904\n",
      "epoch: 2 | 40032 / 114272 | training loss: 0.019478926435112953\n",
      "epoch: 2 | 40064 / 114272 | training loss: 0.03837428614497185\n",
      "epoch: 2 | 40096 / 114272 | training loss: 0.1418408900499344\n",
      "epoch: 2 | 40128 / 114272 | training loss: 0.025137536227703094\n",
      "epoch: 2 | 40160 / 114272 | training loss: 0.0128939775750041\n",
      "epoch: 2 | 40192 / 114272 | training loss: 0.1723075807094574\n",
      "epoch: 2 | 40224 / 114272 | training loss: 0.11590253561735153\n",
      "epoch: 2 | 40256 / 114272 | training loss: 0.20376421511173248\n",
      "epoch: 2 | 40288 / 114272 | training loss: 0.01888013631105423\n",
      "epoch: 2 | 40320 / 114272 | training loss: 0.17482544481754303\n",
      "epoch: 2 | 40352 / 114272 | training loss: 0.3222913444042206\n",
      "epoch: 2 | 40384 / 114272 | training loss: 0.17222221195697784\n",
      "epoch: 2 | 40416 / 114272 | training loss: 0.07724039256572723\n",
      "epoch: 2 | 40448 / 114272 | training loss: 0.00932767428457737\n",
      "epoch: 2 | 40480 / 114272 | training loss: 0.05037067085504532\n",
      "epoch: 2 | 40512 / 114272 | training loss: 0.08957771211862564\n",
      "epoch: 2 | 40544 / 114272 | training loss: 0.025005368515849113\n",
      "epoch: 2 | 40576 / 114272 | training loss: 0.03872587904334068\n",
      "epoch: 2 | 40608 / 114272 | training loss: 0.1053948700428009\n",
      "epoch: 2 | 40640 / 114272 | training loss: 0.026772543787956238\n",
      "epoch: 2 | 40672 / 114272 | training loss: 0.04155641794204712\n",
      "epoch: 2 | 40704 / 114272 | training loss: 0.0816698744893074\n",
      "epoch: 2 | 40736 / 114272 | training loss: 0.05099054425954819\n",
      "epoch: 2 | 40768 / 114272 | training loss: 0.08531306684017181\n",
      "epoch: 2 | 40800 / 114272 | training loss: 0.017327019944787025\n",
      "epoch: 2 | 40832 / 114272 | training loss: 0.043144889175891876\n",
      "epoch: 2 | 40864 / 114272 | training loss: 0.23851048946380615\n",
      "epoch: 2 | 40896 / 114272 | training loss: 0.11183543503284454\n",
      "epoch: 2 | 40928 / 114272 | training loss: 0.025225313380360603\n",
      "epoch: 2 | 40960 / 114272 | training loss: 0.09729987382888794\n",
      "epoch: 2 | 40992 / 114272 | training loss: 0.26891952753067017\n",
      "epoch: 2 | 41024 / 114272 | training loss: 0.044627852737903595\n",
      "epoch: 2 | 41056 / 114272 | training loss: 0.08638979494571686\n",
      "epoch: 2 | 41088 / 114272 | training loss: 0.11150375008583069\n",
      "epoch: 2 | 41120 / 114272 | training loss: 0.11656547337770462\n",
      "epoch: 2 | 41152 / 114272 | training loss: 0.061820752918720245\n",
      "epoch: 2 | 41184 / 114272 | training loss: 0.07068559527397156\n",
      "epoch: 2 | 41216 / 114272 | training loss: 0.06849528104066849\n",
      "epoch: 2 | 41248 / 114272 | training loss: 0.07976488769054413\n",
      "epoch: 2 | 41280 / 114272 | training loss: 0.22158026695251465\n",
      "epoch: 2 | 41312 / 114272 | training loss: 0.005312249530106783\n",
      "epoch: 2 | 41344 / 114272 | training loss: 0.10682663321495056\n",
      "epoch: 2 | 41376 / 114272 | training loss: 0.40841999650001526\n",
      "epoch: 2 | 41408 / 114272 | training loss: 0.008823808282613754\n",
      "epoch: 2 | 41440 / 114272 | training loss: 0.08524976670742035\n",
      "epoch: 2 | 41472 / 114272 | training loss: 0.12673285603523254\n",
      "epoch: 2 | 41504 / 114272 | training loss: 0.2564558684825897\n",
      "epoch: 2 | 41536 / 114272 | training loss: 0.2564200162887573\n",
      "epoch: 2 | 41568 / 114272 | training loss: 0.22847971320152283\n",
      "epoch: 2 | 41600 / 114272 | training loss: 0.03557158634066582\n",
      "epoch: 2 | 41632 / 114272 | training loss: 0.007470195181667805\n",
      "epoch: 2 | 41664 / 114272 | training loss: 0.1190497875213623\n",
      "epoch: 2 | 41696 / 114272 | training loss: 0.024808960035443306\n",
      "epoch: 2 | 41728 / 114272 | training loss: 0.015862947329878807\n",
      "epoch: 2 | 41760 / 114272 | training loss: 0.025767724961042404\n",
      "epoch: 2 | 41792 / 114272 | training loss: 0.07899869978427887\n",
      "epoch: 2 | 41824 / 114272 | training loss: 0.2149297147989273\n",
      "epoch: 2 | 41856 / 114272 | training loss: 0.2784019410610199\n",
      "epoch: 2 | 41888 / 114272 | training loss: 0.00937909446656704\n",
      "epoch: 2 | 41920 / 114272 | training loss: 0.2038770616054535\n",
      "epoch: 2 | 41952 / 114272 | training loss: 0.09166041016578674\n",
      "epoch: 2 | 41984 / 114272 | training loss: 0.054347310215234756\n",
      "epoch: 2 | 42016 / 114272 | training loss: 0.2093646228313446\n",
      "epoch: 2 | 42048 / 114272 | training loss: 0.016179082915186882\n",
      "epoch: 2 | 42080 / 114272 | training loss: 0.040570713579654694\n",
      "epoch: 2 | 42112 / 114272 | training loss: 0.021430131047964096\n",
      "epoch: 2 | 42144 / 114272 | training loss: 0.054041896015405655\n",
      "epoch: 2 | 42176 / 114272 | training loss: 0.11584574729204178\n",
      "epoch: 2 | 42208 / 114272 | training loss: 0.16253957152366638\n",
      "epoch: 2 | 42240 / 114272 | training loss: 0.19003480672836304\n",
      "epoch: 2 | 42272 / 114272 | training loss: 0.1730044037103653\n",
      "epoch: 2 | 42304 / 114272 | training loss: 0.014120607636868954\n",
      "epoch: 2 | 42336 / 114272 | training loss: 0.059101175516843796\n",
      "epoch: 2 | 42368 / 114272 | training loss: 0.050988029688596725\n",
      "epoch: 2 | 42400 / 114272 | training loss: 0.017078682780265808\n",
      "epoch: 2 | 42432 / 114272 | training loss: 0.08699672669172287\n",
      "epoch: 2 | 42464 / 114272 | training loss: 0.14442448318004608\n",
      "epoch: 2 | 42496 / 114272 | training loss: 0.10108708590269089\n",
      "epoch: 2 | 42528 / 114272 | training loss: 0.2932063043117523\n",
      "epoch: 2 | 42560 / 114272 | training loss: 0.04713248088955879\n",
      "epoch: 2 | 42592 / 114272 | training loss: 0.200170636177063\n",
      "epoch: 2 | 42624 / 114272 | training loss: 0.0047373385168612\n",
      "epoch: 2 | 42656 / 114272 | training loss: 0.017369644716382027\n",
      "epoch: 2 | 42688 / 114272 | training loss: 0.06430899351835251\n",
      "epoch: 2 | 42720 / 114272 | training loss: 0.2552318274974823\n",
      "epoch: 2 | 42752 / 114272 | training loss: 0.010135152377188206\n",
      "epoch: 2 | 42784 / 114272 | training loss: 0.12941458821296692\n",
      "epoch: 2 | 42816 / 114272 | training loss: 0.07498358190059662\n",
      "epoch: 2 | 42848 / 114272 | training loss: 0.13515132665634155\n",
      "epoch: 2 | 42880 / 114272 | training loss: 0.14725038409233093\n",
      "epoch: 2 | 42912 / 114272 | training loss: 0.0048585920594632626\n",
      "epoch: 2 | 42944 / 114272 | training loss: 0.13282625377178192\n",
      "epoch: 2 | 42976 / 114272 | training loss: 0.00518959853798151\n",
      "epoch: 2 | 43008 / 114272 | training loss: 0.023121226578950882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 43040 / 114272 | training loss: 0.005113854538649321\n",
      "epoch: 2 | 43072 / 114272 | training loss: 0.1378718465566635\n",
      "epoch: 2 | 43104 / 114272 | training loss: 0.008106818422675133\n",
      "epoch: 2 | 43136 / 114272 | training loss: 0.008247923105955124\n",
      "epoch: 2 | 43168 / 114272 | training loss: 0.2600489854812622\n",
      "epoch: 2 | 43200 / 114272 | training loss: 0.07508506625890732\n",
      "epoch: 2 | 43232 / 114272 | training loss: 0.029562117531895638\n",
      "epoch: 2 | 43264 / 114272 | training loss: 0.03638280928134918\n",
      "epoch: 2 | 43296 / 114272 | training loss: 0.13420258462429047\n",
      "epoch: 2 | 43328 / 114272 | training loss: 0.07427307218313217\n",
      "epoch: 2 | 43360 / 114272 | training loss: 0.020999902859330177\n",
      "epoch: 2 | 43392 / 114272 | training loss: 0.215861976146698\n",
      "epoch: 2 | 43424 / 114272 | training loss: 0.05280071124434471\n",
      "epoch: 2 | 43456 / 114272 | training loss: 0.11486997455358505\n",
      "epoch: 2 | 43488 / 114272 | training loss: 0.09297476708889008\n",
      "epoch: 2 | 43520 / 114272 | training loss: 0.025563731789588928\n",
      "epoch: 2 | 43552 / 114272 | training loss: 0.17728927731513977\n",
      "epoch: 2 | 43584 / 114272 | training loss: 0.0662665143609047\n",
      "epoch: 2 | 43616 / 114272 | training loss: 0.0158364400267601\n",
      "epoch: 2 | 43648 / 114272 | training loss: 0.020532436668872833\n",
      "epoch: 2 | 43680 / 114272 | training loss: 0.09160853922367096\n",
      "epoch: 2 | 43712 / 114272 | training loss: 0.19167548418045044\n",
      "epoch: 2 | 43744 / 114272 | training loss: 0.11635754257440567\n",
      "epoch: 2 | 43776 / 114272 | training loss: 0.033323366194963455\n",
      "epoch: 2 | 43808 / 114272 | training loss: 0.030038801953196526\n",
      "epoch: 2 | 43840 / 114272 | training loss: 0.002478259149938822\n",
      "epoch: 2 | 43872 / 114272 | training loss: 0.047819312661886215\n",
      "epoch: 2 | 43904 / 114272 | training loss: 0.11747760325670242\n",
      "epoch: 2 | 43936 / 114272 | training loss: 0.10783492028713226\n",
      "epoch: 2 | 43968 / 114272 | training loss: 0.12455333769321442\n",
      "epoch: 2 | 44000 / 114272 | training loss: 0.007857219316065311\n",
      "epoch: 2 | 44032 / 114272 | training loss: 0.02785295620560646\n",
      "epoch: 2 | 44064 / 114272 | training loss: 0.007625109050422907\n",
      "epoch: 2 | 44096 / 114272 | training loss: 0.05554678663611412\n",
      "epoch: 2 | 44128 / 114272 | training loss: 0.14118458330631256\n",
      "epoch: 2 | 44160 / 114272 | training loss: 0.21509097516536713\n",
      "epoch: 2 | 44192 / 114272 | training loss: 0.15066806972026825\n",
      "epoch: 2 | 44224 / 114272 | training loss: 0.08135450631380081\n",
      "epoch: 2 | 44256 / 114272 | training loss: 0.09004185348749161\n",
      "epoch: 2 | 44288 / 114272 | training loss: 0.1868673712015152\n",
      "epoch: 2 | 44320 / 114272 | training loss: 0.3276594877243042\n",
      "epoch: 2 | 44352 / 114272 | training loss: 0.004575527273118496\n",
      "epoch: 2 | 44384 / 114272 | training loss: 0.010094876401126385\n",
      "epoch: 2 | 44416 / 114272 | training loss: 0.043158695101737976\n",
      "epoch: 2 | 44448 / 114272 | training loss: 0.01654871739447117\n",
      "epoch: 2 | 44480 / 114272 | training loss: 0.015928782522678375\n",
      "epoch: 2 | 44512 / 114272 | training loss: 0.010090003721415997\n",
      "epoch: 2 | 44544 / 114272 | training loss: 0.009931274689733982\n",
      "epoch: 2 | 44576 / 114272 | training loss: 0.04094497486948967\n",
      "epoch: 2 | 44608 / 114272 | training loss: 0.0603799968957901\n",
      "epoch: 2 | 44640 / 114272 | training loss: 0.4026619493961334\n",
      "epoch: 2 | 44672 / 114272 | training loss: 0.12005771696567535\n",
      "epoch: 2 | 44704 / 114272 | training loss: 0.12371865659952164\n",
      "epoch: 2 | 44736 / 114272 | training loss: 0.0079607293009758\n",
      "epoch: 2 | 44768 / 114272 | training loss: 0.05920838564634323\n",
      "epoch: 2 | 44800 / 114272 | training loss: 0.25294235348701477\n",
      "epoch: 2 | 44832 / 114272 | training loss: 0.004846016876399517\n",
      "epoch: 2 | 44864 / 114272 | training loss: 0.0843704417347908\n",
      "epoch: 2 | 44896 / 114272 | training loss: 0.014246753416955471\n",
      "epoch: 2 | 44928 / 114272 | training loss: 0.08551301807165146\n",
      "epoch: 2 | 44960 / 114272 | training loss: 0.0069703697226941586\n",
      "epoch: 2 | 44992 / 114272 | training loss: 0.05661208927631378\n",
      "epoch: 2 | 45024 / 114272 | training loss: 0.07150804996490479\n",
      "epoch: 2 | 45056 / 114272 | training loss: 0.015421521849930286\n",
      "epoch: 2 | 45088 / 114272 | training loss: 0.011383850127458572\n",
      "epoch: 2 | 45120 / 114272 | training loss: 0.14396634697914124\n",
      "epoch: 2 | 45152 / 114272 | training loss: 0.014058645814657211\n",
      "epoch: 2 | 45184 / 114272 | training loss: 0.37655186653137207\n",
      "epoch: 2 | 45216 / 114272 | training loss: 0.1069493368268013\n",
      "epoch: 2 | 45248 / 114272 | training loss: 0.01013065967708826\n",
      "epoch: 2 | 45280 / 114272 | training loss: 0.007290596142411232\n",
      "epoch: 2 | 45312 / 114272 | training loss: 0.014053115621209145\n",
      "epoch: 2 | 45344 / 114272 | training loss: 0.043979350477457047\n",
      "epoch: 2 | 45376 / 114272 | training loss: 0.170730859041214\n",
      "epoch: 2 | 45408 / 114272 | training loss: 0.16125263273715973\n",
      "epoch: 2 | 45440 / 114272 | training loss: 0.1553369164466858\n",
      "epoch: 2 | 45472 / 114272 | training loss: 0.03243033587932587\n",
      "epoch: 2 | 45504 / 114272 | training loss: 0.018149279057979584\n",
      "epoch: 2 | 45536 / 114272 | training loss: 0.10582410544157028\n",
      "epoch: 2 | 45568 / 114272 | training loss: 0.005485869944095612\n",
      "epoch: 2 | 45600 / 114272 | training loss: 0.0355740562081337\n",
      "epoch: 2 | 45632 / 114272 | training loss: 0.018063798546791077\n",
      "epoch: 2 | 45664 / 114272 | training loss: 0.3138869106769562\n",
      "epoch: 2 | 45696 / 114272 | training loss: 0.1756085455417633\n",
      "epoch: 2 | 45728 / 114272 | training loss: 0.006437417585402727\n",
      "epoch: 2 | 45760 / 114272 | training loss: 0.28500449657440186\n",
      "epoch: 2 | 45792 / 114272 | training loss: 0.055116064846515656\n",
      "epoch: 2 | 45824 / 114272 | training loss: 0.2242627739906311\n",
      "epoch: 2 | 45856 / 114272 | training loss: 0.09120801836252213\n",
      "epoch: 2 | 45888 / 114272 | training loss: 0.02162359282374382\n",
      "epoch: 2 | 45920 / 114272 | training loss: 0.11052655428647995\n",
      "epoch: 2 | 45952 / 114272 | training loss: 0.012497727759182453\n",
      "epoch: 2 | 45984 / 114272 | training loss: 0.0691031664609909\n",
      "epoch: 2 | 46016 / 114272 | training loss: 0.05470765009522438\n",
      "epoch: 2 | 46048 / 114272 | training loss: 0.19822296500205994\n",
      "epoch: 2 | 46080 / 114272 | training loss: 0.06078662723302841\n",
      "epoch: 2 | 46112 / 114272 | training loss: 0.00829458050429821\n",
      "epoch: 2 | 46144 / 114272 | training loss: 0.0076249102130532265\n",
      "epoch: 2 | 46176 / 114272 | training loss: 0.32510215044021606\n",
      "epoch: 2 | 46208 / 114272 | training loss: 0.06792938709259033\n",
      "epoch: 2 | 46240 / 114272 | training loss: 0.06565946340560913\n",
      "epoch: 2 | 46272 / 114272 | training loss: 0.041196536272764206\n",
      "epoch: 2 | 46304 / 114272 | training loss: 0.3747531771659851\n",
      "epoch: 2 | 46336 / 114272 | training loss: 0.10196655988693237\n",
      "epoch: 2 | 46368 / 114272 | training loss: 0.23220521211624146\n",
      "epoch: 2 | 46400 / 114272 | training loss: 0.007300434168428183\n",
      "epoch: 2 | 46432 / 114272 | training loss: 0.1026928648352623\n",
      "epoch: 2 | 46464 / 114272 | training loss: 0.07718100398778915\n",
      "epoch: 2 | 46496 / 114272 | training loss: 0.026683863252401352\n",
      "epoch: 2 | 46528 / 114272 | training loss: 0.008829331025481224\n",
      "epoch: 2 | 46560 / 114272 | training loss: 0.05341244488954544\n",
      "epoch: 2 | 46592 / 114272 | training loss: 0.055962637066841125\n",
      "epoch: 2 | 46624 / 114272 | training loss: 0.1589069366455078\n",
      "epoch: 2 | 46656 / 114272 | training loss: 0.0053933290764689445\n",
      "epoch: 2 | 46688 / 114272 | training loss: 0.24812398850917816\n",
      "epoch: 2 | 46720 / 114272 | training loss: 0.08983749151229858\n",
      "epoch: 2 | 46752 / 114272 | training loss: 0.009550237096846104\n",
      "epoch: 2 | 46784 / 114272 | training loss: 0.11914239823818207\n",
      "epoch: 2 | 46816 / 114272 | training loss: 0.05401413515210152\n",
      "epoch: 2 | 46848 / 114272 | training loss: 0.011931967921555042\n",
      "epoch: 2 | 46880 / 114272 | training loss: 0.014630386605858803\n",
      "epoch: 2 | 46912 / 114272 | training loss: 0.18414081633090973\n",
      "epoch: 2 | 46944 / 114272 | training loss: 0.1042274534702301\n",
      "epoch: 2 | 46976 / 114272 | training loss: 0.062246520072221756\n",
      "epoch: 2 | 47008 / 114272 | training loss: 0.06327842175960541\n",
      "epoch: 2 | 47040 / 114272 | training loss: 0.12593300640583038\n",
      "epoch: 2 | 47072 / 114272 | training loss: 0.12557947635650635\n",
      "epoch: 2 | 47104 / 114272 | training loss: 0.04760260134935379\n",
      "epoch: 2 | 47136 / 114272 | training loss: 0.0728997141122818\n",
      "epoch: 2 | 47168 / 114272 | training loss: 0.04663122072815895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 47200 / 114272 | training loss: 0.006944065447896719\n",
      "epoch: 2 | 47232 / 114272 | training loss: 0.116363525390625\n",
      "epoch: 2 | 47264 / 114272 | training loss: 0.12618699669837952\n",
      "epoch: 2 | 47296 / 114272 | training loss: 0.10084417462348938\n",
      "epoch: 2 | 47328 / 114272 | training loss: 0.10036690533161163\n",
      "epoch: 2 | 47360 / 114272 | training loss: 0.029586169868707657\n",
      "epoch: 2 | 47392 / 114272 | training loss: 0.01189856231212616\n",
      "epoch: 2 | 47424 / 114272 | training loss: 0.07351384311914444\n",
      "epoch: 2 | 47456 / 114272 | training loss: 0.022457566112279892\n",
      "epoch: 2 | 47488 / 114272 | training loss: 0.06778261065483093\n",
      "epoch: 2 | 47520 / 114272 | training loss: 0.08849947899580002\n",
      "epoch: 2 | 47552 / 114272 | training loss: 0.013348840177059174\n",
      "epoch: 2 | 47584 / 114272 | training loss: 0.109763003885746\n",
      "epoch: 2 | 47616 / 114272 | training loss: 0.06413816660642624\n",
      "epoch: 2 | 47648 / 114272 | training loss: 0.11408822238445282\n",
      "epoch: 2 | 47680 / 114272 | training loss: 0.03933383896946907\n",
      "epoch: 2 | 47712 / 114272 | training loss: 0.025284823030233383\n",
      "epoch: 2 | 47744 / 114272 | training loss: 0.010450122877955437\n",
      "epoch: 2 | 47776 / 114272 | training loss: 0.005668723955750465\n",
      "epoch: 2 | 47808 / 114272 | training loss: 0.13573475182056427\n",
      "epoch: 2 | 47840 / 114272 | training loss: 0.04867277294397354\n",
      "epoch: 2 | 47872 / 114272 | training loss: 0.056308504194021225\n",
      "epoch: 2 | 47904 / 114272 | training loss: 0.007106720935553312\n",
      "epoch: 2 | 47936 / 114272 | training loss: 0.08579456806182861\n",
      "epoch: 2 | 47968 / 114272 | training loss: 0.025331176817417145\n",
      "epoch: 2 | 48000 / 114272 | training loss: 0.03357546031475067\n",
      "epoch: 2 | 48032 / 114272 | training loss: 0.2919228971004486\n",
      "epoch: 2 | 48064 / 114272 | training loss: 0.017895618453621864\n",
      "epoch: 2 | 48096 / 114272 | training loss: 0.008563834242522717\n",
      "epoch: 2 | 48128 / 114272 | training loss: 0.025691481307148933\n",
      "epoch: 2 | 48160 / 114272 | training loss: 0.024025997146964073\n",
      "epoch: 2 | 48192 / 114272 | training loss: 0.007401270791888237\n",
      "epoch: 2 | 48224 / 114272 | training loss: 0.002781301038339734\n",
      "epoch: 2 | 48256 / 114272 | training loss: 0.06933483481407166\n",
      "epoch: 2 | 48288 / 114272 | training loss: 0.008731779642403126\n",
      "epoch: 2 | 48320 / 114272 | training loss: 0.0060976361855864525\n",
      "epoch: 2 | 48352 / 114272 | training loss: 0.07817589491605759\n",
      "epoch: 2 | 48384 / 114272 | training loss: 0.005678308662027121\n",
      "epoch: 2 | 48416 / 114272 | training loss: 0.06575248390436172\n",
      "epoch: 2 | 48448 / 114272 | training loss: 0.005910912994295359\n",
      "epoch: 2 | 48480 / 114272 | training loss: 0.20857514441013336\n",
      "epoch: 2 | 48512 / 114272 | training loss: 0.002369506750255823\n",
      "epoch: 2 | 48544 / 114272 | training loss: 0.014871614053845406\n",
      "epoch: 2 | 48576 / 114272 | training loss: 0.013136681169271469\n",
      "epoch: 2 | 48608 / 114272 | training loss: 0.16894406080245972\n",
      "epoch: 2 | 48640 / 114272 | training loss: 0.013162241317331791\n",
      "epoch: 2 | 48672 / 114272 | training loss: 0.02893509715795517\n",
      "epoch: 2 | 48704 / 114272 | training loss: 0.013889824040234089\n",
      "epoch: 2 | 48736 / 114272 | training loss: 0.0041945939883589745\n",
      "epoch: 2 | 48768 / 114272 | training loss: 0.06480895727872849\n",
      "epoch: 2 | 48800 / 114272 | training loss: 0.014001849107444286\n",
      "epoch: 2 | 48832 / 114272 | training loss: 0.002812817459926009\n",
      "epoch: 2 | 48864 / 114272 | training loss: 0.005678445566445589\n",
      "epoch: 2 | 48896 / 114272 | training loss: 0.25232863426208496\n",
      "epoch: 2 | 48928 / 114272 | training loss: 0.003403925336897373\n",
      "epoch: 2 | 48960 / 114272 | training loss: 0.009389775805175304\n",
      "epoch: 2 | 48992 / 114272 | training loss: 0.003940478898584843\n",
      "epoch: 2 | 49024 / 114272 | training loss: 0.003102178918197751\n",
      "epoch: 2 | 49056 / 114272 | training loss: 0.002726504113525152\n",
      "epoch: 2 | 49088 / 114272 | training loss: 0.007512926589697599\n",
      "epoch: 2 | 49120 / 114272 | training loss: 0.029276415705680847\n",
      "epoch: 2 | 49152 / 114272 | training loss: 0.15584507584571838\n",
      "epoch: 2 | 49184 / 114272 | training loss: 0.09579398483037949\n",
      "epoch: 2 | 49216 / 114272 | training loss: 0.15070894360542297\n",
      "epoch: 2 | 49248 / 114272 | training loss: 0.4585459530353546\n",
      "epoch: 2 | 49280 / 114272 | training loss: 0.0046624974347651005\n",
      "epoch: 2 | 49312 / 114272 | training loss: 0.41838154196739197\n",
      "epoch: 2 | 49344 / 114272 | training loss: 0.005106075666844845\n",
      "epoch: 2 | 49376 / 114272 | training loss: 0.29221010208129883\n",
      "epoch: 2 | 49408 / 114272 | training loss: 0.002335677156224847\n",
      "epoch: 2 | 49440 / 114272 | training loss: 0.04082193970680237\n",
      "epoch: 2 | 49472 / 114272 | training loss: 0.37417149543762207\n",
      "epoch: 2 | 49504 / 114272 | training loss: 0.08113788068294525\n",
      "epoch: 2 | 49536 / 114272 | training loss: 0.4271755516529083\n",
      "epoch: 2 | 49568 / 114272 | training loss: 0.09011324495077133\n",
      "epoch: 2 | 49600 / 114272 | training loss: 0.11569090187549591\n",
      "epoch: 2 | 49632 / 114272 | training loss: 0.019697634503245354\n",
      "epoch: 2 | 49664 / 114272 | training loss: 0.021325046196579933\n",
      "epoch: 2 | 49696 / 114272 | training loss: 0.10272646695375443\n",
      "epoch: 2 | 49728 / 114272 | training loss: 0.003245995379984379\n",
      "epoch: 2 | 49760 / 114272 | training loss: 0.00635432219132781\n",
      "epoch: 2 | 49792 / 114272 | training loss: 0.056489113718271255\n",
      "epoch: 2 | 49824 / 114272 | training loss: 0.11064421385526657\n",
      "epoch: 2 | 49856 / 114272 | training loss: 0.05355839058756828\n",
      "epoch: 2 | 49888 / 114272 | training loss: 0.3204391896724701\n",
      "epoch: 2 | 49920 / 114272 | training loss: 0.22898079454898834\n",
      "epoch: 2 | 49952 / 114272 | training loss: 0.046815089881420135\n",
      "epoch: 2 | 49984 / 114272 | training loss: 0.11094006896018982\n",
      "epoch: 2 | 50016 / 114272 | training loss: 0.02753671444952488\n",
      "epoch: 2 | 50048 / 114272 | training loss: 0.07396247982978821\n",
      "epoch: 2 | 50080 / 114272 | training loss: 0.027627618983387947\n",
      "epoch: 2 | 50112 / 114272 | training loss: 0.003983789123594761\n",
      "epoch: 2 | 50144 / 114272 | training loss: 0.05478845536708832\n",
      "epoch: 2 | 50176 / 114272 | training loss: 0.030996110290288925\n",
      "epoch: 2 | 50208 / 114272 | training loss: 0.17410844564437866\n",
      "epoch: 2 | 50240 / 114272 | training loss: 0.1183222085237503\n",
      "epoch: 2 | 50272 / 114272 | training loss: 0.06679464876651764\n",
      "epoch: 2 | 50304 / 114272 | training loss: 0.003544407431036234\n",
      "epoch: 2 | 50336 / 114272 | training loss: 0.2099614292383194\n",
      "epoch: 2 | 50368 / 114272 | training loss: 0.023479551076889038\n",
      "epoch: 2 | 50400 / 114272 | training loss: 0.011988765560090542\n",
      "epoch: 2 | 50432 / 114272 | training loss: 0.2837317883968353\n",
      "epoch: 2 | 50464 / 114272 | training loss: 0.051407404243946075\n",
      "epoch: 2 | 50496 / 114272 | training loss: 0.06666132062673569\n",
      "epoch: 2 | 50528 / 114272 | training loss: 0.016644081100821495\n",
      "epoch: 2 | 50560 / 114272 | training loss: 0.15994924306869507\n",
      "epoch: 2 | 50592 / 114272 | training loss: 0.12312888354063034\n",
      "epoch: 2 | 50624 / 114272 | training loss: 0.005868543405085802\n",
      "epoch: 2 | 50656 / 114272 | training loss: 0.003870108863338828\n",
      "epoch: 2 | 50688 / 114272 | training loss: 0.009346307255327702\n",
      "epoch: 2 | 50720 / 114272 | training loss: 0.11967182904481888\n",
      "epoch: 2 | 50752 / 114272 | training loss: 0.1860773265361786\n",
      "epoch: 2 | 50784 / 114272 | training loss: 0.13876263797283173\n",
      "epoch: 2 | 50816 / 114272 | training loss: 0.19411863386631012\n",
      "epoch: 2 | 50848 / 114272 | training loss: 0.14306697249412537\n",
      "epoch: 2 | 50880 / 114272 | training loss: 0.005736700724810362\n",
      "epoch: 2 | 50912 / 114272 | training loss: 0.2108669877052307\n",
      "epoch: 2 | 50944 / 114272 | training loss: 0.013930379413068295\n",
      "epoch: 2 | 50976 / 114272 | training loss: 0.005768973845988512\n",
      "epoch: 2 | 51008 / 114272 | training loss: 0.00876639038324356\n",
      "epoch: 2 | 51040 / 114272 | training loss: 0.024734023958444595\n",
      "epoch: 2 | 51072 / 114272 | training loss: 0.03771515190601349\n",
      "epoch: 2 | 51104 / 114272 | training loss: 0.08303867280483246\n",
      "epoch: 2 | 51136 / 114272 | training loss: 0.0924856886267662\n",
      "epoch: 2 | 51168 / 114272 | training loss: 0.11373993009328842\n",
      "epoch: 2 | 51200 / 114272 | training loss: 0.14106230437755585\n",
      "epoch: 2 | 51232 / 114272 | training loss: 0.10726981610059738\n",
      "epoch: 2 | 51264 / 114272 | training loss: 0.0891788974404335\n",
      "epoch: 2 | 51296 / 114272 | training loss: 0.13114741444587708\n",
      "epoch: 2 | 51328 / 114272 | training loss: 0.00979318656027317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 51360 / 114272 | training loss: 0.2008146494626999\n",
      "epoch: 2 | 51392 / 114272 | training loss: 0.25065818428993225\n",
      "epoch: 2 | 51424 / 114272 | training loss: 0.3140597641468048\n",
      "epoch: 2 | 51456 / 114272 | training loss: 0.005115088541060686\n",
      "epoch: 2 | 51488 / 114272 | training loss: 0.008044262416660786\n",
      "epoch: 2 | 51520 / 114272 | training loss: 0.0260920412838459\n",
      "epoch: 2 | 51552 / 114272 | training loss: 0.010606661438941956\n",
      "epoch: 2 | 51584 / 114272 | training loss: 0.2191941738128662\n",
      "epoch: 2 | 51616 / 114272 | training loss: 0.03167160972952843\n",
      "epoch: 2 | 51648 / 114272 | training loss: 0.03832876309752464\n",
      "epoch: 2 | 51680 / 114272 | training loss: 0.1382988691329956\n",
      "epoch: 2 | 51712 / 114272 | training loss: 0.009613352827727795\n",
      "epoch: 2 | 51744 / 114272 | training loss: 0.022451169788837433\n",
      "epoch: 2 | 51776 / 114272 | training loss: 0.02060907706618309\n",
      "epoch: 2 | 51808 / 114272 | training loss: 0.021670252084732056\n",
      "epoch: 2 | 51840 / 114272 | training loss: 0.010637195780873299\n",
      "epoch: 2 | 51872 / 114272 | training loss: 0.0092310244217515\n",
      "epoch: 2 | 51904 / 114272 | training loss: 0.007741712033748627\n",
      "epoch: 2 | 51936 / 114272 | training loss: 0.3446514904499054\n",
      "epoch: 2 | 51968 / 114272 | training loss: 0.05821852385997772\n",
      "epoch: 2 | 52000 / 114272 | training loss: 0.005712145008146763\n",
      "epoch: 2 | 52032 / 114272 | training loss: 0.0351739302277565\n",
      "epoch: 2 | 52064 / 114272 | training loss: 0.2620810568332672\n",
      "epoch: 2 | 52096 / 114272 | training loss: 0.021099139004945755\n",
      "epoch: 2 | 52128 / 114272 | training loss: 0.11111798137426376\n",
      "epoch: 2 | 52160 / 114272 | training loss: 0.07446940988302231\n",
      "epoch: 2 | 52192 / 114272 | training loss: 0.050397563725709915\n",
      "epoch: 2 | 52224 / 114272 | training loss: 0.009682729840278625\n",
      "epoch: 2 | 52256 / 114272 | training loss: 0.17480920255184174\n",
      "epoch: 2 | 52288 / 114272 | training loss: 0.027042673900723457\n",
      "epoch: 2 | 52320 / 114272 | training loss: 0.09628321975469589\n",
      "epoch: 2 | 52352 / 114272 | training loss: 0.10679883509874344\n",
      "epoch: 2 | 52384 / 114272 | training loss: 0.008869769982993603\n",
      "epoch: 2 | 52416 / 114272 | training loss: 0.005785777699202299\n",
      "epoch: 2 | 52448 / 114272 | training loss: 0.008863410912454128\n",
      "epoch: 2 | 52480 / 114272 | training loss: 0.2546517252922058\n",
      "epoch: 2 | 52512 / 114272 | training loss: 0.04451365023851395\n",
      "epoch: 2 | 52544 / 114272 | training loss: 0.08242318034172058\n",
      "epoch: 2 | 52576 / 114272 | training loss: 0.14535591006278992\n",
      "epoch: 2 | 52608 / 114272 | training loss: 0.1812550276517868\n",
      "epoch: 2 | 52640 / 114272 | training loss: 0.004619866143912077\n",
      "epoch: 2 | 52672 / 114272 | training loss: 0.20426848530769348\n",
      "epoch: 2 | 52704 / 114272 | training loss: 0.1886327713727951\n",
      "epoch: 2 | 52736 / 114272 | training loss: 0.31166619062423706\n",
      "epoch: 2 | 52768 / 114272 | training loss: 0.019870281219482422\n",
      "epoch: 2 | 52800 / 114272 | training loss: 0.07379642874002457\n",
      "epoch: 2 | 52832 / 114272 | training loss: 0.11467026174068451\n",
      "epoch: 2 | 52864 / 114272 | training loss: 0.08038792014122009\n",
      "epoch: 2 | 52896 / 114272 | training loss: 0.0668664276599884\n",
      "epoch: 2 | 52928 / 114272 | training loss: 0.11567143350839615\n",
      "epoch: 2 | 52960 / 114272 | training loss: 0.05806656926870346\n",
      "epoch: 2 | 52992 / 114272 | training loss: 0.12390115112066269\n",
      "epoch: 2 | 53024 / 114272 | training loss: 0.10016853362321854\n",
      "epoch: 2 | 53056 / 114272 | training loss: 0.06364979594945908\n",
      "epoch: 2 | 53088 / 114272 | training loss: 0.10703954100608826\n",
      "epoch: 2 | 53120 / 114272 | training loss: 0.019461849704384804\n",
      "epoch: 2 | 53152 / 114272 | training loss: 0.01146303303539753\n",
      "epoch: 2 | 53184 / 114272 | training loss: 0.19691170752048492\n",
      "epoch: 2 | 53216 / 114272 | training loss: 0.008842450566589832\n",
      "epoch: 2 | 53248 / 114272 | training loss: 0.19858667254447937\n",
      "epoch: 2 | 53280 / 114272 | training loss: 0.0070602125488221645\n",
      "epoch: 2 | 53312 / 114272 | training loss: 0.26139968633651733\n",
      "epoch: 2 | 53344 / 114272 | training loss: 0.14043231308460236\n",
      "epoch: 2 | 53376 / 114272 | training loss: 0.010143032297492027\n",
      "epoch: 2 | 53408 / 114272 | training loss: 0.15995046496391296\n",
      "epoch: 2 | 53440 / 114272 | training loss: 0.09830992668867111\n",
      "epoch: 2 | 53472 / 114272 | training loss: 0.04945702105760574\n",
      "epoch: 2 | 53504 / 114272 | training loss: 0.3480790853500366\n",
      "epoch: 2 | 53536 / 114272 | training loss: 0.07366906851530075\n",
      "epoch: 2 | 53568 / 114272 | training loss: 0.007352810353040695\n",
      "epoch: 2 | 53600 / 114272 | training loss: 0.2766420543193817\n",
      "epoch: 2 | 53632 / 114272 | training loss: 0.004998613614588976\n",
      "epoch: 2 | 53664 / 114272 | training loss: 0.088907890021801\n",
      "epoch: 2 | 53696 / 114272 | training loss: 0.14200973510742188\n",
      "epoch: 2 | 53728 / 114272 | training loss: 0.06708458811044693\n",
      "epoch: 2 | 53760 / 114272 | training loss: 0.08772815018892288\n",
      "epoch: 2 | 53792 / 114272 | training loss: 0.07389567047357559\n",
      "epoch: 2 | 53824 / 114272 | training loss: 0.009977951645851135\n",
      "epoch: 2 | 53856 / 114272 | training loss: 0.13773119449615479\n",
      "epoch: 2 | 53888 / 114272 | training loss: 0.09921840578317642\n",
      "epoch: 2 | 53920 / 114272 | training loss: 0.07527987658977509\n",
      "epoch: 2 | 53952 / 114272 | training loss: 0.22552742063999176\n",
      "epoch: 2 | 53984 / 114272 | training loss: 0.13036172091960907\n",
      "epoch: 2 | 54016 / 114272 | training loss: 0.13824397325515747\n",
      "epoch: 2 | 54048 / 114272 | training loss: 0.025667386129498482\n",
      "epoch: 2 | 54080 / 114272 | training loss: 0.17674502730369568\n",
      "epoch: 2 | 54112 / 114272 | training loss: 0.022250056266784668\n",
      "epoch: 2 | 54144 / 114272 | training loss: 0.08509882539510727\n",
      "epoch: 2 | 54176 / 114272 | training loss: 0.08370359987020493\n",
      "epoch: 2 | 54208 / 114272 | training loss: 0.012787903659045696\n",
      "epoch: 2 | 54240 / 114272 | training loss: 0.13685815036296844\n",
      "epoch: 2 | 54272 / 114272 | training loss: 0.01593579351902008\n",
      "epoch: 2 | 54304 / 114272 | training loss: 0.21640004217624664\n",
      "epoch: 2 | 54336 / 114272 | training loss: 0.13762551546096802\n",
      "epoch: 2 | 54368 / 114272 | training loss: 0.04822592809796333\n",
      "epoch: 2 | 54400 / 114272 | training loss: 0.012548866681754589\n",
      "epoch: 2 | 54432 / 114272 | training loss: 0.1383787989616394\n",
      "epoch: 2 | 54464 / 114272 | training loss: 0.08114779740571976\n",
      "epoch: 2 | 54496 / 114272 | training loss: 0.034280408173799515\n",
      "epoch: 2 | 54528 / 114272 | training loss: 0.12253854423761368\n",
      "epoch: 2 | 54560 / 114272 | training loss: 0.08452719449996948\n",
      "epoch: 2 | 54592 / 114272 | training loss: 0.06667929887771606\n",
      "epoch: 2 | 54624 / 114272 | training loss: 0.038360100239515305\n",
      "epoch: 2 | 54656 / 114272 | training loss: 0.027270767837762833\n",
      "epoch: 2 | 54688 / 114272 | training loss: 0.10249871760606766\n",
      "epoch: 2 | 54720 / 114272 | training loss: 0.08097852766513824\n",
      "epoch: 2 | 54752 / 114272 | training loss: 0.03413394093513489\n",
      "epoch: 2 | 54784 / 114272 | training loss: 0.0925600677728653\n",
      "epoch: 2 | 54816 / 114272 | training loss: 0.0053174919448792934\n",
      "epoch: 2 | 54848 / 114272 | training loss: 0.015539383515715599\n",
      "epoch: 2 | 54880 / 114272 | training loss: 0.05404204875230789\n",
      "epoch: 2 | 54912 / 114272 | training loss: 0.03623078018426895\n",
      "epoch: 2 | 54944 / 114272 | training loss: 0.01495810691267252\n",
      "epoch: 2 | 54976 / 114272 | training loss: 0.08018738776445389\n",
      "epoch: 2 | 55008 / 114272 | training loss: 0.03493206575512886\n",
      "epoch: 2 | 55040 / 114272 | training loss: 0.1790599822998047\n",
      "epoch: 2 | 55072 / 114272 | training loss: 0.047651272267103195\n",
      "epoch: 2 | 55104 / 114272 | training loss: 0.02459147572517395\n",
      "epoch: 2 | 55136 / 114272 | training loss: 0.022653652355074883\n",
      "epoch: 2 | 55168 / 114272 | training loss: 0.4108414351940155\n",
      "epoch: 2 | 55200 / 114272 | training loss: 0.008165461011230946\n",
      "epoch: 2 | 55232 / 114272 | training loss: 0.41567543148994446\n",
      "epoch: 2 | 55264 / 114272 | training loss: 0.010242674499750137\n",
      "epoch: 2 | 55296 / 114272 | training loss: 0.1649993658065796\n",
      "epoch: 2 | 55328 / 114272 | training loss: 0.01141093298792839\n",
      "epoch: 2 | 55360 / 114272 | training loss: 0.003323368960991502\n",
      "epoch: 2 | 55392 / 114272 | training loss: 0.08050881326198578\n",
      "epoch: 2 | 55424 / 114272 | training loss: 0.015778284519910812\n",
      "epoch: 2 | 55456 / 114272 | training loss: 0.016906902194023132\n",
      "epoch: 2 | 55488 / 114272 | training loss: 0.09820989519357681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 55520 / 114272 | training loss: 0.007373076863586903\n",
      "epoch: 2 | 55552 / 114272 | training loss: 0.047843966633081436\n",
      "epoch: 2 | 55584 / 114272 | training loss: 0.02491895854473114\n",
      "epoch: 2 | 55616 / 114272 | training loss: 0.0036058456171303988\n",
      "epoch: 2 | 55648 / 114272 | training loss: 0.059481050819158554\n",
      "epoch: 2 | 55680 / 114272 | training loss: 0.19537194073200226\n",
      "epoch: 2 | 55712 / 114272 | training loss: 0.09658348560333252\n",
      "epoch: 2 | 55744 / 114272 | training loss: 0.4914151430130005\n",
      "epoch: 2 | 55776 / 114272 | training loss: 0.013175548054277897\n",
      "epoch: 2 | 55808 / 114272 | training loss: 0.007851995527744293\n",
      "epoch: 2 | 55840 / 114272 | training loss: 0.3741096258163452\n",
      "epoch: 2 | 55872 / 114272 | training loss: 0.15422579646110535\n",
      "epoch: 2 | 55904 / 114272 | training loss: 0.09374070167541504\n",
      "epoch: 2 | 55936 / 114272 | training loss: 0.08736709505319595\n",
      "epoch: 2 | 55968 / 114272 | training loss: 0.10610105097293854\n",
      "epoch: 2 | 56000 / 114272 | training loss: 0.138499453663826\n",
      "epoch: 2 | 56032 / 114272 | training loss: 0.10804199427366257\n",
      "epoch: 2 | 56064 / 114272 | training loss: 0.02527647465467453\n",
      "epoch: 2 | 56096 / 114272 | training loss: 0.10304474830627441\n",
      "epoch: 2 | 56128 / 114272 | training loss: 0.10920147597789764\n",
      "epoch: 2 | 56160 / 114272 | training loss: 0.37515807151794434\n",
      "epoch: 2 | 56192 / 114272 | training loss: 0.005242497660219669\n",
      "epoch: 2 | 56224 / 114272 | training loss: 0.022325759753584862\n",
      "epoch: 2 | 56256 / 114272 | training loss: 0.23422536253929138\n",
      "epoch: 2 | 56288 / 114272 | training loss: 0.0269318874925375\n",
      "epoch: 2 | 56320 / 114272 | training loss: 0.03937741741538048\n",
      "epoch: 2 | 56352 / 114272 | training loss: 0.08593423664569855\n",
      "epoch: 2 | 56384 / 114272 | training loss: 0.021199289709329605\n",
      "epoch: 2 | 56416 / 114272 | training loss: 0.012096114456653595\n",
      "epoch: 2 | 56448 / 114272 | training loss: 0.02193649485707283\n",
      "epoch: 2 | 56480 / 114272 | training loss: 0.41316843032836914\n",
      "epoch: 2 | 56512 / 114272 | training loss: 0.0087205171585083\n",
      "epoch: 2 | 56544 / 114272 | training loss: 0.007471722085028887\n",
      "epoch: 2 | 56576 / 114272 | training loss: 0.11593256145715714\n",
      "epoch: 2 | 56608 / 114272 | training loss: 0.10538139194250107\n",
      "epoch: 2 | 56640 / 114272 | training loss: 0.005751193035393953\n",
      "epoch: 2 | 56672 / 114272 | training loss: 0.11488565057516098\n",
      "epoch: 2 | 56704 / 114272 | training loss: 0.011835861951112747\n",
      "epoch: 2 | 56736 / 114272 | training loss: 0.038875821977853775\n",
      "epoch: 2 | 56768 / 114272 | training loss: 0.057159487158060074\n",
      "epoch: 2 | 56800 / 114272 | training loss: 0.0326768197119236\n",
      "epoch: 2 | 56832 / 114272 | training loss: 0.05940936505794525\n",
      "epoch: 2 | 56864 / 114272 | training loss: 0.10080906748771667\n",
      "epoch: 2 | 56896 / 114272 | training loss: 0.0895448699593544\n",
      "epoch: 2 | 56928 / 114272 | training loss: 0.011789700947701931\n",
      "epoch: 2 | 56960 / 114272 | training loss: 0.23730279505252838\n",
      "epoch: 2 | 56992 / 114272 | training loss: 0.28085941076278687\n",
      "epoch: 2 | 57024 / 114272 | training loss: 0.19253955781459808\n",
      "epoch: 2 | 57056 / 114272 | training loss: 0.11492109298706055\n",
      "epoch: 2 | 57088 / 114272 | training loss: 0.07700585573911667\n",
      "epoch: 2 | 57120 / 114272 | training loss: 0.1198805496096611\n",
      "epoch: 2 | 57152 / 114272 | training loss: 0.15366055071353912\n",
      "epoch: 2 | 57184 / 114272 | training loss: 0.012927219271659851\n",
      "epoch: 2 | 57216 / 114272 | training loss: 0.017129916697740555\n",
      "epoch: 2 | 57248 / 114272 | training loss: 0.084693543612957\n",
      "epoch: 2 | 57280 / 114272 | training loss: 0.05238298326730728\n",
      "epoch: 2 | 57312 / 114272 | training loss: 0.00649116700515151\n",
      "epoch: 2 | 57344 / 114272 | training loss: 0.07697881013154984\n",
      "epoch: 2 | 57376 / 114272 | training loss: 0.0061342231929302216\n",
      "epoch: 2 | 57408 / 114272 | training loss: 0.062347330152988434\n",
      "epoch: 2 | 57440 / 114272 | training loss: 0.01061861403286457\n",
      "epoch: 2 | 57472 / 114272 | training loss: 0.46820858120918274\n",
      "epoch: 2 | 57504 / 114272 | training loss: 0.15336079895496368\n",
      "epoch: 2 | 57536 / 114272 | training loss: 0.08047895133495331\n",
      "epoch: 2 | 57568 / 114272 | training loss: 0.07410766184329987\n",
      "epoch: 2 | 57600 / 114272 | training loss: 0.01983087696135044\n",
      "epoch: 2 | 57632 / 114272 | training loss: 0.007315340451896191\n",
      "epoch: 2 | 57664 / 114272 | training loss: 0.32913607358932495\n",
      "epoch: 2 | 57696 / 114272 | training loss: 0.016481813043355942\n",
      "epoch: 2 | 57728 / 114272 | training loss: 0.14168821275234222\n",
      "epoch: 2 | 57760 / 114272 | training loss: 0.13759972155094147\n",
      "epoch: 2 | 57792 / 114272 | training loss: 0.013801556080579758\n",
      "epoch: 2 | 57824 / 114272 | training loss: 0.10948468744754791\n",
      "epoch: 2 | 57856 / 114272 | training loss: 0.030455566942691803\n",
      "epoch: 2 | 57888 / 114272 | training loss: 0.008073169738054276\n",
      "epoch: 2 | 57920 / 114272 | training loss: 0.006775637157261372\n",
      "epoch: 2 | 57952 / 114272 | training loss: 0.10103791207075119\n",
      "epoch: 2 | 57984 / 114272 | training loss: 0.24183739721775055\n",
      "epoch: 2 | 58016 / 114272 | training loss: 0.018846293911337852\n",
      "epoch: 2 | 58048 / 114272 | training loss: 0.16050642728805542\n",
      "epoch: 2 | 58080 / 114272 | training loss: 0.1119745746254921\n",
      "epoch: 2 | 58112 / 114272 | training loss: 0.00814294908195734\n",
      "epoch: 2 | 58144 / 114272 | training loss: 0.025190772488713264\n",
      "epoch: 2 | 58176 / 114272 | training loss: 0.054879773408174515\n",
      "epoch: 2 | 58208 / 114272 | training loss: 0.012366699054837227\n",
      "epoch: 2 | 58240 / 114272 | training loss: 0.01692991703748703\n",
      "epoch: 2 | 58272 / 114272 | training loss: 0.044320907443761826\n",
      "epoch: 2 | 58304 / 114272 | training loss: 0.12531068921089172\n",
      "epoch: 2 | 58336 / 114272 | training loss: 0.12498994916677475\n",
      "epoch: 2 | 58368 / 114272 | training loss: 0.09459517151117325\n",
      "epoch: 2 | 58400 / 114272 | training loss: 0.010211669839918613\n",
      "epoch: 2 | 58432 / 114272 | training loss: 0.19053632020950317\n",
      "epoch: 2 | 58464 / 114272 | training loss: 0.14485281705856323\n",
      "epoch: 2 | 58496 / 114272 | training loss: 0.20336347818374634\n",
      "epoch: 2 | 58528 / 114272 | training loss: 0.05119571089744568\n",
      "epoch: 2 | 58560 / 114272 | training loss: 0.2374066561460495\n",
      "epoch: 2 | 58592 / 114272 | training loss: 0.08115985989570618\n",
      "epoch: 2 | 58624 / 114272 | training loss: 0.054393526166677475\n",
      "epoch: 2 | 58656 / 114272 | training loss: 0.19783300161361694\n",
      "epoch: 2 | 58688 / 114272 | training loss: 0.03044031374156475\n",
      "epoch: 2 | 58720 / 114272 | training loss: 0.30955222249031067\n",
      "epoch: 2 | 58752 / 114272 | training loss: 0.054028160870075226\n",
      "epoch: 2 | 58784 / 114272 | training loss: 0.03409337252378464\n",
      "epoch: 2 | 58816 / 114272 | training loss: 0.09675528109073639\n",
      "epoch: 2 | 58848 / 114272 | training loss: 0.06325571238994598\n",
      "epoch: 2 | 58880 / 114272 | training loss: 0.02367096021771431\n",
      "epoch: 2 | 58912 / 114272 | training loss: 0.15256863832473755\n",
      "epoch: 2 | 58944 / 114272 | training loss: 0.04502938687801361\n",
      "epoch: 2 | 58976 / 114272 | training loss: 0.10243984311819077\n",
      "epoch: 2 | 59008 / 114272 | training loss: 0.021244391798973083\n",
      "epoch: 2 | 59040 / 114272 | training loss: 0.033828433603048325\n",
      "epoch: 2 | 59072 / 114272 | training loss: 0.05648140236735344\n",
      "epoch: 2 | 59104 / 114272 | training loss: 0.018559282645583153\n",
      "epoch: 2 | 59136 / 114272 | training loss: 0.0902615562081337\n",
      "epoch: 2 | 59168 / 114272 | training loss: 0.023373428732156754\n",
      "epoch: 2 | 59200 / 114272 | training loss: 0.04478989541530609\n",
      "epoch: 2 | 59232 / 114272 | training loss: 0.010639035142958164\n",
      "epoch: 2 | 59264 / 114272 | training loss: 0.026029011234641075\n",
      "epoch: 2 | 59296 / 114272 | training loss: 0.04617570340633392\n",
      "epoch: 2 | 59328 / 114272 | training loss: 0.13718083500862122\n",
      "epoch: 2 | 59360 / 114272 | training loss: 0.14669232070446014\n",
      "epoch: 2 | 59392 / 114272 | training loss: 0.11075076460838318\n",
      "epoch: 2 | 59424 / 114272 | training loss: 0.011341804638504982\n",
      "epoch: 2 | 59456 / 114272 | training loss: 0.03249197453260422\n",
      "epoch: 2 | 59488 / 114272 | training loss: 0.007309806067496538\n",
      "epoch: 2 | 59520 / 114272 | training loss: 0.1395142674446106\n",
      "epoch: 2 | 59552 / 114272 | training loss: 0.17132140696048737\n",
      "epoch: 2 | 59584 / 114272 | training loss: 0.01019357331097126\n",
      "epoch: 2 | 59616 / 114272 | training loss: 0.016644738614559174\n",
      "epoch: 2 | 59648 / 114272 | training loss: 0.12278859317302704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 59680 / 114272 | training loss: 0.013193481601774693\n",
      "epoch: 2 | 59712 / 114272 | training loss: 0.033215753734111786\n",
      "epoch: 2 | 59744 / 114272 | training loss: 0.006317658815532923\n",
      "epoch: 2 | 59776 / 114272 | training loss: 0.013282738626003265\n",
      "epoch: 2 | 59808 / 114272 | training loss: 0.013992044143378735\n",
      "epoch: 2 | 59840 / 114272 | training loss: 0.007275797892361879\n",
      "epoch: 2 | 59872 / 114272 | training loss: 0.006067116279155016\n",
      "epoch: 2 | 59904 / 114272 | training loss: 0.021964315325021744\n",
      "epoch: 2 | 59936 / 114272 | training loss: 0.1378791779279709\n",
      "epoch: 2 | 59968 / 114272 | training loss: 0.0064888643100857735\n",
      "epoch: 2 | 60000 / 114272 | training loss: 0.2286229431629181\n",
      "epoch: 2 | 60032 / 114272 | training loss: 0.0875922366976738\n",
      "epoch: 2 | 60064 / 114272 | training loss: 0.007001906167715788\n",
      "epoch: 2 | 60096 / 114272 | training loss: 0.09954329580068588\n",
      "epoch: 2 | 60128 / 114272 | training loss: 0.007004109676927328\n",
      "epoch: 2 | 60160 / 114272 | training loss: 0.25453850626945496\n",
      "epoch: 2 | 60192 / 114272 | training loss: 0.00606507295742631\n",
      "epoch: 2 | 60224 / 114272 | training loss: 0.006499116774648428\n",
      "epoch: 2 | 60256 / 114272 | training loss: 0.09412240236997604\n",
      "epoch: 2 | 60288 / 114272 | training loss: 0.009467237628996372\n",
      "epoch: 2 | 60320 / 114272 | training loss: 0.006743552163243294\n",
      "epoch: 2 | 60352 / 114272 | training loss: 0.10264596343040466\n",
      "epoch: 2 | 60384 / 114272 | training loss: 0.00582301989197731\n",
      "epoch: 2 | 60416 / 114272 | training loss: 0.04175525903701782\n",
      "epoch: 2 | 60448 / 114272 | training loss: 0.009997340850532055\n",
      "epoch: 2 | 60480 / 114272 | training loss: 0.09060433506965637\n",
      "epoch: 2 | 60512 / 114272 | training loss: 0.026747535914182663\n",
      "epoch: 2 | 60544 / 114272 | training loss: 0.09843415021896362\n",
      "epoch: 2 | 60576 / 114272 | training loss: 0.0049817971885204315\n",
      "epoch: 2 | 60608 / 114272 | training loss: 0.1586456149816513\n",
      "epoch: 2 | 60640 / 114272 | training loss: 0.19250436127185822\n",
      "epoch: 2 | 60672 / 114272 | training loss: 0.1182238832116127\n",
      "epoch: 2 | 60704 / 114272 | training loss: 0.012876555323600769\n",
      "epoch: 2 | 60736 / 114272 | training loss: 0.008040950633585453\n",
      "epoch: 2 | 60768 / 114272 | training loss: 0.0061323740519583225\n",
      "epoch: 2 | 60800 / 114272 | training loss: 0.005813878029584885\n",
      "epoch: 2 | 60832 / 114272 | training loss: 0.0023701030295342207\n",
      "epoch: 2 | 60864 / 114272 | training loss: 0.11711914092302322\n",
      "epoch: 2 | 60896 / 114272 | training loss: 0.2162938266992569\n",
      "epoch: 2 | 60928 / 114272 | training loss: 0.0058241127990186214\n",
      "epoch: 2 | 60960 / 114272 | training loss: 0.2104755938053131\n",
      "epoch: 2 | 60992 / 114272 | training loss: 0.32548052072525024\n",
      "epoch: 2 | 61024 / 114272 | training loss: 0.21009889245033264\n",
      "epoch: 2 | 61056 / 114272 | training loss: 0.00865795649588108\n",
      "epoch: 2 | 61088 / 114272 | training loss: 0.00782096479088068\n",
      "epoch: 2 | 61120 / 114272 | training loss: 0.2763676047325134\n",
      "epoch: 2 | 61152 / 114272 | training loss: 0.2330259084701538\n",
      "epoch: 2 | 61184 / 114272 | training loss: 0.14836804568767548\n",
      "epoch: 2 | 61216 / 114272 | training loss: 0.00520281819626689\n",
      "epoch: 2 | 61248 / 114272 | training loss: 0.010146723128855228\n",
      "epoch: 2 | 61280 / 114272 | training loss: 0.01392106432467699\n",
      "epoch: 2 | 61312 / 114272 | training loss: 0.0060059321112930775\n",
      "epoch: 2 | 61344 / 114272 | training loss: 0.01400747150182724\n",
      "epoch: 2 | 61376 / 114272 | training loss: 0.00755942752584815\n",
      "epoch: 2 | 61408 / 114272 | training loss: 0.010929634794592857\n",
      "epoch: 2 | 61440 / 114272 | training loss: 0.0023422446101903915\n",
      "epoch: 2 | 61472 / 114272 | training loss: 0.06818177551031113\n",
      "epoch: 2 | 61504 / 114272 | training loss: 0.31382235884666443\n",
      "epoch: 2 | 61536 / 114272 | training loss: 0.058139510452747345\n",
      "epoch: 2 | 61568 / 114272 | training loss: 0.21414463222026825\n",
      "epoch: 2 | 61600 / 114272 | training loss: 0.01896432414650917\n",
      "epoch: 2 | 61632 / 114272 | training loss: 0.03484772518277168\n",
      "epoch: 2 | 61664 / 114272 | training loss: 0.0011290962575003505\n",
      "epoch: 2 | 61696 / 114272 | training loss: 0.21482081711292267\n",
      "epoch: 2 | 61728 / 114272 | training loss: 0.09500952810049057\n",
      "epoch: 2 | 61760 / 114272 | training loss: 0.01570427604019642\n",
      "epoch: 2 | 61792 / 114272 | training loss: 0.2445940226316452\n",
      "epoch: 2 | 61824 / 114272 | training loss: 0.13808849453926086\n",
      "epoch: 2 | 61856 / 114272 | training loss: 0.003857341827824712\n",
      "epoch: 2 | 61888 / 114272 | training loss: 0.17166444659233093\n",
      "epoch: 2 | 61920 / 114272 | training loss: 0.10225503891706467\n",
      "epoch: 2 | 61952 / 114272 | training loss: 0.16277901828289032\n",
      "epoch: 2 | 61984 / 114272 | training loss: 0.1488753706216812\n",
      "epoch: 2 | 62016 / 114272 | training loss: 0.0219667199999094\n",
      "epoch: 2 | 62048 / 114272 | training loss: 0.16656160354614258\n",
      "epoch: 2 | 62080 / 114272 | training loss: 0.0043916115537285805\n",
      "epoch: 2 | 62112 / 114272 | training loss: 0.06248888745903969\n",
      "epoch: 2 | 62144 / 114272 | training loss: 0.3290179967880249\n",
      "epoch: 2 | 62176 / 114272 | training loss: 0.21835891902446747\n",
      "epoch: 2 | 62208 / 114272 | training loss: 0.10973712801933289\n",
      "epoch: 2 | 62240 / 114272 | training loss: 0.02531132660806179\n",
      "epoch: 2 | 62272 / 114272 | training loss: 0.33361539244651794\n",
      "epoch: 2 | 62304 / 114272 | training loss: 0.089354507625103\n",
      "epoch: 2 | 62336 / 114272 | training loss: 0.01086428388953209\n",
      "epoch: 2 | 62368 / 114272 | training loss: 0.008574520237743855\n",
      "epoch: 2 | 62400 / 114272 | training loss: 0.15511132776737213\n",
      "epoch: 2 | 62432 / 114272 | training loss: 0.08773503452539444\n",
      "epoch: 2 | 62464 / 114272 | training loss: 0.08214983344078064\n",
      "epoch: 2 | 62496 / 114272 | training loss: 0.007637667004019022\n",
      "epoch: 2 | 62528 / 114272 | training loss: 0.007617726922035217\n",
      "epoch: 2 | 62560 / 114272 | training loss: 0.13855841755867004\n",
      "epoch: 2 | 62592 / 114272 | training loss: 0.005163808818906546\n",
      "epoch: 2 | 62624 / 114272 | training loss: 0.02819807454943657\n",
      "epoch: 2 | 62656 / 114272 | training loss: 0.02593381144106388\n",
      "epoch: 2 | 62688 / 114272 | training loss: 0.2746603190898895\n",
      "epoch: 2 | 62720 / 114272 | training loss: 0.019660597667098045\n",
      "epoch: 2 | 62752 / 114272 | training loss: 0.01089019700884819\n",
      "epoch: 2 | 62784 / 114272 | training loss: 0.23532342910766602\n",
      "epoch: 2 | 62816 / 114272 | training loss: 0.009296832606196404\n",
      "epoch: 2 | 62848 / 114272 | training loss: 0.014570306986570358\n",
      "epoch: 2 | 62880 / 114272 | training loss: 0.05502792075276375\n",
      "epoch: 2 | 62912 / 114272 | training loss: 0.0663953423500061\n",
      "epoch: 2 | 62944 / 114272 | training loss: 0.041648928076028824\n",
      "epoch: 2 | 62976 / 114272 | training loss: 0.3117639422416687\n",
      "epoch: 2 | 63008 / 114272 | training loss: 0.025391269475221634\n",
      "epoch: 2 | 63040 / 114272 | training loss: 0.005518682301044464\n",
      "epoch: 2 | 63072 / 114272 | training loss: 0.11366823315620422\n",
      "epoch: 2 | 63104 / 114272 | training loss: 0.21675948798656464\n",
      "epoch: 2 | 63136 / 114272 | training loss: 0.010590496473014355\n",
      "epoch: 2 | 63168 / 114272 | training loss: 0.16483809053897858\n",
      "epoch: 2 | 63200 / 114272 | training loss: 0.23731468617916107\n",
      "epoch: 2 | 63232 / 114272 | training loss: 0.007670920342206955\n",
      "epoch: 2 | 63264 / 114272 | training loss: 0.24555160105228424\n",
      "epoch: 2 | 63296 / 114272 | training loss: 0.004029271192848682\n",
      "epoch: 2 | 63328 / 114272 | training loss: 0.03019074909389019\n",
      "epoch: 2 | 63360 / 114272 | training loss: 0.015538034960627556\n",
      "epoch: 2 | 63392 / 114272 | training loss: 0.009488378651440144\n",
      "epoch: 2 | 63424 / 114272 | training loss: 0.011918455362319946\n",
      "epoch: 2 | 63456 / 114272 | training loss: 0.007323877420276403\n",
      "epoch: 2 | 63488 / 114272 | training loss: 0.23370854556560516\n",
      "epoch: 2 | 63520 / 114272 | training loss: 0.41790735721588135\n",
      "epoch: 2 | 63552 / 114272 | training loss: 0.0026825021486729383\n",
      "epoch: 2 | 63584 / 114272 | training loss: 0.11816900968551636\n",
      "epoch: 2 | 63616 / 114272 | training loss: 0.30475908517837524\n",
      "epoch: 2 | 63648 / 114272 | training loss: 0.2728341817855835\n",
      "epoch: 2 | 63680 / 114272 | training loss: 0.01043633371591568\n",
      "epoch: 2 | 63712 / 114272 | training loss: 0.005692800506949425\n",
      "epoch: 2 | 63744 / 114272 | training loss: 0.009301593527197838\n",
      "epoch: 2 | 63776 / 114272 | training loss: 0.06966502964496613\n",
      "epoch: 2 | 63808 / 114272 | training loss: 0.009781384840607643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 63840 / 114272 | training loss: 0.09938624501228333\n",
      "epoch: 2 | 63872 / 114272 | training loss: 0.193965882062912\n",
      "epoch: 2 | 63904 / 114272 | training loss: 0.09372875094413757\n",
      "epoch: 2 | 63936 / 114272 | training loss: 0.24853388965129852\n",
      "epoch: 2 | 63968 / 114272 | training loss: 0.06391976028680801\n",
      "epoch: 2 | 64000 / 114272 | training loss: 0.009172492660582066\n",
      "epoch: 2 | 64032 / 114272 | training loss: 0.01436743512749672\n",
      "epoch: 2 | 64064 / 114272 | training loss: 0.040835101157426834\n",
      "epoch: 2 | 64096 / 114272 | training loss: 0.005240199621766806\n",
      "epoch: 2 | 64128 / 114272 | training loss: 0.10349355638027191\n",
      "epoch: 2 | 64160 / 114272 | training loss: 0.1159682348370552\n",
      "epoch: 2 | 64192 / 114272 | training loss: 0.007553575560450554\n",
      "epoch: 2 | 64224 / 114272 | training loss: 0.046698976308107376\n",
      "epoch: 2 | 64256 / 114272 | training loss: 0.11220023036003113\n",
      "epoch: 2 | 64288 / 114272 | training loss: 0.06417832523584366\n",
      "epoch: 2 | 64320 / 114272 | training loss: 0.0582607202231884\n",
      "epoch: 2 | 64352 / 114272 | training loss: 0.0570063479244709\n",
      "epoch: 2 | 64384 / 114272 | training loss: 0.012566483579576015\n",
      "epoch: 2 | 64416 / 114272 | training loss: 0.12132715433835983\n",
      "epoch: 2 | 64448 / 114272 | training loss: 0.010830318555235863\n",
      "epoch: 2 | 64480 / 114272 | training loss: 0.1034230962395668\n",
      "epoch: 2 | 64512 / 114272 | training loss: 0.00785591546446085\n",
      "epoch: 2 | 64544 / 114272 | training loss: 0.01877252198755741\n",
      "epoch: 2 | 64576 / 114272 | training loss: 0.00514605175703764\n",
      "epoch: 2 | 64608 / 114272 | training loss: 0.013810102827847004\n",
      "epoch: 2 | 64640 / 114272 | training loss: 0.043937090784311295\n",
      "epoch: 2 | 64672 / 114272 | training loss: 0.06197714805603027\n",
      "epoch: 2 | 64704 / 114272 | training loss: 0.00954513531178236\n",
      "epoch: 2 | 64736 / 114272 | training loss: 0.012951546348631382\n",
      "epoch: 2 | 64768 / 114272 | training loss: 0.3077514171600342\n",
      "epoch: 2 | 64800 / 114272 | training loss: 0.015603253617882729\n",
      "epoch: 2 | 64832 / 114272 | training loss: 0.0051308791153132915\n",
      "epoch: 2 | 64864 / 114272 | training loss: 0.22050213813781738\n",
      "epoch: 2 | 64896 / 114272 | training loss: 0.08007004857063293\n",
      "epoch: 2 | 64928 / 114272 | training loss: 0.0840756967663765\n",
      "epoch: 2 | 64960 / 114272 | training loss: 0.03794590383768082\n",
      "epoch: 2 | 64992 / 114272 | training loss: 0.1324986219406128\n",
      "epoch: 2 | 65024 / 114272 | training loss: 0.013296290300786495\n",
      "epoch: 2 | 65056 / 114272 | training loss: 0.007255093660205603\n",
      "epoch: 2 | 65088 / 114272 | training loss: 0.005630990955978632\n",
      "epoch: 2 | 65120 / 114272 | training loss: 0.010286491364240646\n",
      "epoch: 2 | 65152 / 114272 | training loss: 0.1353667974472046\n",
      "epoch: 2 | 65184 / 114272 | training loss: 0.09898214787244797\n",
      "epoch: 2 | 65216 / 114272 | training loss: 0.013798686675727367\n",
      "epoch: 2 | 65248 / 114272 | training loss: 0.003600731026381254\n",
      "epoch: 2 | 65280 / 114272 | training loss: 0.049580447375774384\n",
      "epoch: 2 | 65312 / 114272 | training loss: 0.10105462372303009\n",
      "epoch: 2 | 65344 / 114272 | training loss: 0.14058907330036163\n",
      "epoch: 2 | 65376 / 114272 | training loss: 0.005452710669487715\n",
      "epoch: 2 | 65408 / 114272 | training loss: 0.007605079561471939\n",
      "epoch: 2 | 65440 / 114272 | training loss: 0.09251095354557037\n",
      "epoch: 2 | 65472 / 114272 | training loss: 0.01091456413269043\n",
      "epoch: 2 | 65504 / 114272 | training loss: 0.15643832087516785\n",
      "epoch: 2 | 65536 / 114272 | training loss: 0.006062387488782406\n",
      "epoch: 2 | 65568 / 114272 | training loss: 0.08161094784736633\n",
      "epoch: 2 | 65600 / 114272 | training loss: 0.01251712255179882\n",
      "epoch: 2 | 65632 / 114272 | training loss: 0.00461951270699501\n",
      "epoch: 2 | 65664 / 114272 | training loss: 0.17041152715682983\n",
      "epoch: 2 | 65696 / 114272 | training loss: 0.009532847441732883\n",
      "epoch: 2 | 65728 / 114272 | training loss: 0.3813363015651703\n",
      "epoch: 2 | 65760 / 114272 | training loss: 0.41822507977485657\n",
      "epoch: 2 | 65792 / 114272 | training loss: 0.08564185351133347\n",
      "epoch: 2 | 65824 / 114272 | training loss: 0.03370978310704231\n",
      "epoch: 2 | 65856 / 114272 | training loss: 0.006261185742914677\n",
      "epoch: 2 | 65888 / 114272 | training loss: 0.2918723225593567\n",
      "epoch: 2 | 65920 / 114272 | training loss: 0.1194169744849205\n",
      "epoch: 2 | 65952 / 114272 | training loss: 0.01669846847653389\n",
      "epoch: 2 | 65984 / 114272 | training loss: 0.006823144853115082\n",
      "epoch: 2 | 66016 / 114272 | training loss: 0.13611549139022827\n",
      "epoch: 2 | 66048 / 114272 | training loss: 0.1607227623462677\n",
      "epoch: 2 | 66080 / 114272 | training loss: 0.11449932307004929\n",
      "epoch: 2 | 66112 / 114272 | training loss: 0.2994118630886078\n",
      "epoch: 2 | 66144 / 114272 | training loss: 0.011351313441991806\n",
      "epoch: 2 | 66176 / 114272 | training loss: 0.016624629497528076\n",
      "epoch: 2 | 66208 / 114272 | training loss: 0.01373908668756485\n",
      "epoch: 2 | 66240 / 114272 | training loss: 0.013605225831270218\n",
      "epoch: 2 | 66272 / 114272 | training loss: 0.09578349441289902\n",
      "epoch: 2 | 66304 / 114272 | training loss: 0.07920201867818832\n",
      "epoch: 2 | 66336 / 114272 | training loss: 0.010547018609941006\n",
      "epoch: 2 | 66368 / 114272 | training loss: 0.08462648093700409\n",
      "epoch: 2 | 66400 / 114272 | training loss: 0.07474303245544434\n",
      "epoch: 2 | 66432 / 114272 | training loss: 0.15942327678203583\n",
      "epoch: 2 | 66464 / 114272 | training loss: 0.20922915637493134\n",
      "epoch: 2 | 66496 / 114272 | training loss: 0.009644937701523304\n",
      "epoch: 2 | 66528 / 114272 | training loss: 0.031133372336626053\n",
      "epoch: 2 | 66560 / 114272 | training loss: 0.15034019947052002\n",
      "epoch: 2 | 66592 / 114272 | training loss: 0.012789146043360233\n",
      "epoch: 2 | 66624 / 114272 | training loss: 0.2704440951347351\n",
      "epoch: 2 | 66656 / 114272 | training loss: 0.09694886952638626\n",
      "epoch: 2 | 66688 / 114272 | training loss: 0.009971469640731812\n",
      "epoch: 2 | 66720 / 114272 | training loss: 0.006508763413876295\n",
      "epoch: 2 | 66752 / 114272 | training loss: 0.08580642193555832\n",
      "epoch: 2 | 66784 / 114272 | training loss: 0.12999911606311798\n",
      "epoch: 2 | 66816 / 114272 | training loss: 0.18347688019275665\n",
      "epoch: 2 | 66848 / 114272 | training loss: 0.0643436461687088\n",
      "epoch: 2 | 66880 / 114272 | training loss: 0.1480773538351059\n",
      "epoch: 2 | 66912 / 114272 | training loss: 0.010540664196014404\n",
      "epoch: 2 | 66944 / 114272 | training loss: 0.217183455824852\n",
      "epoch: 2 | 66976 / 114272 | training loss: 0.009122161194682121\n",
      "epoch: 2 | 67008 / 114272 | training loss: 0.11322934180498123\n",
      "epoch: 2 | 67040 / 114272 | training loss: 0.010716693475842476\n",
      "epoch: 2 | 67072 / 114272 | training loss: 0.10498511791229248\n",
      "epoch: 2 | 67104 / 114272 | training loss: 0.1996016800403595\n",
      "epoch: 2 | 67136 / 114272 | training loss: 0.03564532473683357\n",
      "epoch: 2 | 67168 / 114272 | training loss: 0.13310794532299042\n",
      "epoch: 2 | 67200 / 114272 | training loss: 0.03770707920193672\n",
      "epoch: 2 | 67232 / 114272 | training loss: 0.10360383987426758\n",
      "epoch: 2 | 67264 / 114272 | training loss: 0.09782902151346207\n",
      "epoch: 2 | 67296 / 114272 | training loss: 0.32655981183052063\n",
      "epoch: 2 | 67328 / 114272 | training loss: 0.022930361330509186\n",
      "epoch: 2 | 67360 / 114272 | training loss: 0.018951909616589546\n",
      "epoch: 2 | 67392 / 114272 | training loss: 0.01056902389973402\n",
      "epoch: 2 | 67424 / 114272 | training loss: 0.012465233914554119\n",
      "epoch: 2 | 67456 / 114272 | training loss: 0.033199161291122437\n",
      "epoch: 2 | 67488 / 114272 | training loss: 0.17698189616203308\n",
      "epoch: 2 | 67520 / 114272 | training loss: 0.009015791118144989\n",
      "epoch: 2 | 67552 / 114272 | training loss: 0.11277813464403152\n",
      "epoch: 2 | 67584 / 114272 | training loss: 0.027371549978852272\n",
      "epoch: 2 | 67616 / 114272 | training loss: 0.005410117097198963\n",
      "epoch: 2 | 67648 / 114272 | training loss: 0.10355833917856216\n",
      "epoch: 2 | 67680 / 114272 | training loss: 0.008896760642528534\n",
      "epoch: 2 | 67712 / 114272 | training loss: 0.011090694926679134\n",
      "epoch: 2 | 67744 / 114272 | training loss: 0.008197859860956669\n",
      "epoch: 2 | 67776 / 114272 | training loss: 0.18755564093589783\n",
      "epoch: 2 | 67808 / 114272 | training loss: 0.09133446961641312\n",
      "epoch: 2 | 67840 / 114272 | training loss: 0.00639859214425087\n",
      "epoch: 2 | 67872 / 114272 | training loss: 0.007118311710655689\n",
      "epoch: 2 | 67904 / 114272 | training loss: 0.014949114061892033\n",
      "epoch: 2 | 67936 / 114272 | training loss: 0.2023906707763672\n",
      "epoch: 2 | 67968 / 114272 | training loss: 0.27206262946128845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 68000 / 114272 | training loss: 0.10043583065271378\n",
      "epoch: 2 | 68032 / 114272 | training loss: 0.186013862490654\n",
      "epoch: 2 | 68064 / 114272 | training loss: 0.01511906087398529\n",
      "epoch: 2 | 68096 / 114272 | training loss: 0.0043197874911129475\n",
      "epoch: 2 | 68128 / 114272 | training loss: 0.22613686323165894\n",
      "epoch: 2 | 68160 / 114272 | training loss: 0.014565817080438137\n",
      "epoch: 2 | 68192 / 114272 | training loss: 0.15281236171722412\n",
      "epoch: 2 | 68224 / 114272 | training loss: 0.11691231280565262\n",
      "epoch: 2 | 68256 / 114272 | training loss: 0.03308722376823425\n",
      "epoch: 2 | 68288 / 114272 | training loss: 0.11298363655805588\n",
      "epoch: 2 | 68320 / 114272 | training loss: 0.024048950523138046\n",
      "epoch: 2 | 68352 / 114272 | training loss: 0.1304825246334076\n",
      "epoch: 2 | 68384 / 114272 | training loss: 0.020126692950725555\n",
      "epoch: 2 | 68416 / 114272 | training loss: 0.061038725078105927\n",
      "epoch: 2 | 68448 / 114272 | training loss: 0.00481894938275218\n",
      "epoch: 2 | 68480 / 114272 | training loss: 0.0038129889871925116\n",
      "epoch: 2 | 68512 / 114272 | training loss: 0.1984989494085312\n",
      "epoch: 2 | 68544 / 114272 | training loss: 0.17521932721138\n",
      "epoch: 2 | 68576 / 114272 | training loss: 0.006846567150205374\n",
      "epoch: 2 | 68608 / 114272 | training loss: 0.36163997650146484\n",
      "epoch: 2 | 68640 / 114272 | training loss: 0.0034057623706758022\n",
      "epoch: 2 | 68672 / 114272 | training loss: 0.09603757411241531\n",
      "epoch: 2 | 68704 / 114272 | training loss: 0.056408848613500595\n",
      "epoch: 2 | 68736 / 114272 | training loss: 0.07439611107110977\n",
      "epoch: 2 | 68768 / 114272 | training loss: 0.01771780475974083\n",
      "epoch: 2 | 68800 / 114272 | training loss: 0.004727409221231937\n",
      "epoch: 2 | 68832 / 114272 | training loss: 0.258645236492157\n",
      "epoch: 2 | 68864 / 114272 | training loss: 0.08382971584796906\n",
      "epoch: 2 | 68896 / 114272 | training loss: 0.19466763734817505\n",
      "epoch: 2 | 68928 / 114272 | training loss: 0.01290915533900261\n",
      "epoch: 2 | 68960 / 114272 | training loss: 0.06354070454835892\n",
      "epoch: 2 | 68992 / 114272 | training loss: 0.012884790077805519\n",
      "epoch: 2 | 69024 / 114272 | training loss: 0.08056329935789108\n",
      "epoch: 2 | 69056 / 114272 | training loss: 0.06157803535461426\n",
      "epoch: 2 | 69088 / 114272 | training loss: 0.144266739487648\n",
      "epoch: 2 | 69120 / 114272 | training loss: 0.0945831835269928\n",
      "epoch: 2 | 69152 / 114272 | training loss: 0.012528925202786922\n",
      "epoch: 2 | 69184 / 114272 | training loss: 0.07815375924110413\n",
      "epoch: 2 | 69216 / 114272 | training loss: 0.08798841387033463\n",
      "epoch: 2 | 69248 / 114272 | training loss: 0.3347119987010956\n",
      "epoch: 2 | 69280 / 114272 | training loss: 0.004846812225878239\n",
      "epoch: 2 | 69312 / 114272 | training loss: 0.023913932964205742\n",
      "epoch: 2 | 69344 / 114272 | training loss: 0.004771541804075241\n",
      "epoch: 2 | 69376 / 114272 | training loss: 0.005397634580731392\n",
      "epoch: 2 | 69408 / 114272 | training loss: 0.06699416041374207\n",
      "epoch: 2 | 69440 / 114272 | training loss: 0.26903513073921204\n",
      "epoch: 2 | 69472 / 114272 | training loss: 0.0047162557020783424\n",
      "epoch: 2 | 69504 / 114272 | training loss: 0.0836728885769844\n",
      "epoch: 2 | 69536 / 114272 | training loss: 0.195058673620224\n",
      "epoch: 2 | 69568 / 114272 | training loss: 0.09492458403110504\n",
      "epoch: 2 | 69600 / 114272 | training loss: 0.018408942967653275\n",
      "epoch: 2 | 69632 / 114272 | training loss: 0.015237603336572647\n",
      "epoch: 2 | 69664 / 114272 | training loss: 0.00663666520267725\n",
      "epoch: 2 | 69696 / 114272 | training loss: 0.09145437926054001\n",
      "epoch: 2 | 69728 / 114272 | training loss: 0.005328177008777857\n",
      "epoch: 2 | 69760 / 114272 | training loss: 0.1598869115114212\n",
      "epoch: 2 | 69792 / 114272 | training loss: 0.08996573090553284\n",
      "epoch: 2 | 69824 / 114272 | training loss: 0.007157546002417803\n",
      "epoch: 2 | 69856 / 114272 | training loss: 0.1334981620311737\n",
      "epoch: 2 | 69888 / 114272 | training loss: 0.15651923418045044\n",
      "epoch: 2 | 69920 / 114272 | training loss: 0.04529731720685959\n",
      "epoch: 2 | 69952 / 114272 | training loss: 0.011836481280624866\n",
      "epoch: 2 | 69984 / 114272 | training loss: 0.014274932444095612\n",
      "epoch: 2 | 70016 / 114272 | training loss: 0.12050517648458481\n",
      "epoch: 2 | 70048 / 114272 | training loss: 0.13983547687530518\n",
      "epoch: 2 | 70080 / 114272 | training loss: 0.021665750071406364\n",
      "epoch: 2 | 70112 / 114272 | training loss: 0.5513394474983215\n",
      "epoch: 2 | 70144 / 114272 | training loss: 0.1397910863161087\n",
      "epoch: 2 | 70176 / 114272 | training loss: 0.06690863519906998\n",
      "epoch: 2 | 70208 / 114272 | training loss: 0.21741023659706116\n",
      "epoch: 2 | 70240 / 114272 | training loss: 0.21408787369728088\n",
      "epoch: 2 | 70272 / 114272 | training loss: 0.1729155033826828\n",
      "epoch: 2 | 70304 / 114272 | training loss: 0.008113071322441101\n",
      "epoch: 2 | 70336 / 114272 | training loss: 0.011628803797066212\n",
      "epoch: 2 | 70368 / 114272 | training loss: 0.009310130961239338\n",
      "epoch: 2 | 70400 / 114272 | training loss: 0.01354785356670618\n",
      "epoch: 2 | 70432 / 114272 | training loss: 0.06219343841075897\n",
      "epoch: 2 | 70464 / 114272 | training loss: 0.12105165421962738\n",
      "epoch: 2 | 70496 / 114272 | training loss: 0.014273499138653278\n",
      "epoch: 2 | 70528 / 114272 | training loss: 0.2658490538597107\n",
      "epoch: 2 | 70560 / 114272 | training loss: 0.11946866661310196\n",
      "epoch: 2 | 70592 / 114272 | training loss: 0.16661283373832703\n",
      "epoch: 2 | 70624 / 114272 | training loss: 0.05305139720439911\n",
      "epoch: 2 | 70656 / 114272 | training loss: 0.006427541375160217\n",
      "epoch: 2 | 70688 / 114272 | training loss: 0.016052300110459328\n",
      "epoch: 2 | 70720 / 114272 | training loss: 0.039689354598522186\n",
      "epoch: 2 | 70752 / 114272 | training loss: 0.09850645065307617\n",
      "epoch: 2 | 70784 / 114272 | training loss: 0.07703143358230591\n",
      "epoch: 2 | 70816 / 114272 | training loss: 0.0034187063574790955\n",
      "epoch: 2 | 70848 / 114272 | training loss: 0.006133512128144503\n",
      "epoch: 2 | 70880 / 114272 | training loss: 0.012884118594229221\n",
      "epoch: 2 | 70912 / 114272 | training loss: 0.0864238515496254\n",
      "epoch: 2 | 70944 / 114272 | training loss: 0.0633947104215622\n",
      "epoch: 2 | 70976 / 114272 | training loss: 0.3328400254249573\n",
      "epoch: 2 | 71008 / 114272 | training loss: 0.00479007326066494\n",
      "epoch: 2 | 71040 / 114272 | training loss: 0.05896639823913574\n",
      "epoch: 2 | 71072 / 114272 | training loss: 0.002955106785520911\n",
      "epoch: 2 | 71104 / 114272 | training loss: 0.25854042172431946\n",
      "epoch: 2 | 71136 / 114272 | training loss: 0.04306226223707199\n",
      "epoch: 2 | 71168 / 114272 | training loss: 0.013045256026089191\n",
      "epoch: 2 | 71200 / 114272 | training loss: 0.1582079827785492\n",
      "epoch: 2 | 71232 / 114272 | training loss: 0.12191445380449295\n",
      "epoch: 2 | 71264 / 114272 | training loss: 0.19592660665512085\n",
      "epoch: 2 | 71296 / 114272 | training loss: 0.07712257653474808\n",
      "epoch: 2 | 71328 / 114272 | training loss: 0.11901689320802689\n",
      "epoch: 2 | 71360 / 114272 | training loss: 0.0061415452510118484\n",
      "epoch: 2 | 71392 / 114272 | training loss: 0.01605689711868763\n",
      "epoch: 2 | 71424 / 114272 | training loss: 0.0645071342587471\n",
      "epoch: 2 | 71456 / 114272 | training loss: 0.07255429029464722\n",
      "epoch: 2 | 71488 / 114272 | training loss: 0.00765303336083889\n",
      "epoch: 2 | 71520 / 114272 | training loss: 0.04134855046868324\n",
      "epoch: 2 | 71552 / 114272 | training loss: 0.28558698296546936\n",
      "epoch: 2 | 71584 / 114272 | training loss: 0.38162463903427124\n",
      "epoch: 2 | 71616 / 114272 | training loss: 0.12544545531272888\n",
      "epoch: 2 | 71648 / 114272 | training loss: 0.007400264032185078\n",
      "epoch: 2 | 71680 / 114272 | training loss: 0.011973323300480843\n",
      "epoch: 2 | 71712 / 114272 | training loss: 0.06456233561038971\n",
      "epoch: 2 | 71744 / 114272 | training loss: 0.021545253694057465\n",
      "epoch: 2 | 71776 / 114272 | training loss: 0.025755703449249268\n",
      "epoch: 2 | 71808 / 114272 | training loss: 0.08476410806179047\n",
      "epoch: 2 | 71840 / 114272 | training loss: 0.0187518373131752\n",
      "epoch: 2 | 71872 / 114272 | training loss: 0.016703087836503983\n",
      "epoch: 2 | 71904 / 114272 | training loss: 0.009508877992630005\n",
      "epoch: 2 | 71936 / 114272 | training loss: 0.005066275130957365\n",
      "epoch: 2 | 71968 / 114272 | training loss: 0.1133260577917099\n",
      "epoch: 2 | 72000 / 114272 | training loss: 0.00581294996663928\n",
      "epoch: 2 | 72032 / 114272 | training loss: 0.339689701795578\n",
      "epoch: 2 | 72064 / 114272 | training loss: 0.019561463966965675\n",
      "epoch: 2 | 72096 / 114272 | training loss: 0.2766686677932739\n",
      "epoch: 2 | 72128 / 114272 | training loss: 0.005064457654953003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 72160 / 114272 | training loss: 0.005962195340543985\n",
      "epoch: 2 | 72192 / 114272 | training loss: 0.14498913288116455\n",
      "epoch: 2 | 72224 / 114272 | training loss: 0.011232299730181694\n",
      "epoch: 2 | 72256 / 114272 | training loss: 0.06950206309556961\n",
      "epoch: 2 | 72288 / 114272 | training loss: 0.16098517179489136\n",
      "epoch: 2 | 72320 / 114272 | training loss: 0.043540168553590775\n",
      "epoch: 2 | 72352 / 114272 | training loss: 0.05828205123543739\n",
      "epoch: 2 | 72384 / 114272 | training loss: 0.21582597494125366\n",
      "epoch: 2 | 72416 / 114272 | training loss: 0.08786457777023315\n",
      "epoch: 2 | 72448 / 114272 | training loss: 0.18774104118347168\n",
      "epoch: 2 | 72480 / 114272 | training loss: 0.22795641422271729\n",
      "epoch: 2 | 72512 / 114272 | training loss: 0.2271835058927536\n",
      "epoch: 2 | 72544 / 114272 | training loss: 0.15247604250907898\n",
      "epoch: 2 | 72576 / 114272 | training loss: 0.11114292591810226\n",
      "epoch: 2 | 72608 / 114272 | training loss: 0.02691507712006569\n",
      "epoch: 2 | 72640 / 114272 | training loss: 0.016288498416543007\n",
      "epoch: 2 | 72672 / 114272 | training loss: 0.15681901574134827\n",
      "epoch: 2 | 72704 / 114272 | training loss: 0.002656331518664956\n",
      "epoch: 2 | 72736 / 114272 | training loss: 0.003247898770496249\n",
      "epoch: 2 | 72768 / 114272 | training loss: 0.21152327954769135\n",
      "epoch: 2 | 72800 / 114272 | training loss: 0.043194182217121124\n",
      "epoch: 2 | 72832 / 114272 | training loss: 0.11173826456069946\n",
      "epoch: 2 | 72864 / 114272 | training loss: 0.15917731821537018\n",
      "epoch: 2 | 72896 / 114272 | training loss: 0.1340392827987671\n",
      "epoch: 2 | 72928 / 114272 | training loss: 0.13485603034496307\n",
      "epoch: 2 | 72960 / 114272 | training loss: 0.07706669718027115\n",
      "epoch: 2 | 72992 / 114272 | training loss: 0.039827488362789154\n",
      "epoch: 2 | 73024 / 114272 | training loss: 0.01570797711610794\n",
      "epoch: 2 | 73056 / 114272 | training loss: 0.1874239444732666\n",
      "epoch: 2 | 73088 / 114272 | training loss: 0.081684909760952\n",
      "epoch: 2 | 73120 / 114272 | training loss: 0.01751198060810566\n",
      "epoch: 2 | 73152 / 114272 | training loss: 0.13383758068084717\n",
      "epoch: 2 | 73184 / 114272 | training loss: 0.03035491146147251\n",
      "epoch: 2 | 73216 / 114272 | training loss: 0.07243956625461578\n",
      "epoch: 2 | 73248 / 114272 | training loss: 0.04296033829450607\n",
      "epoch: 2 | 73280 / 114272 | training loss: 0.11477212607860565\n",
      "epoch: 2 | 73312 / 114272 | training loss: 0.13493472337722778\n",
      "epoch: 2 | 73344 / 114272 | training loss: 0.3006499111652374\n",
      "epoch: 2 | 73376 / 114272 | training loss: 0.0649479553103447\n",
      "epoch: 2 | 73408 / 114272 | training loss: 0.09997357428073883\n",
      "epoch: 2 | 73440 / 114272 | training loss: 0.16706284880638123\n",
      "epoch: 2 | 73472 / 114272 | training loss: 0.1574428528547287\n",
      "epoch: 2 | 73504 / 114272 | training loss: 0.03312864527106285\n",
      "epoch: 2 | 73536 / 114272 | training loss: 0.03236531466245651\n",
      "epoch: 2 | 73568 / 114272 | training loss: 0.008046137169003487\n",
      "epoch: 2 | 73600 / 114272 | training loss: 0.009752001613378525\n",
      "epoch: 2 | 73632 / 114272 | training loss: 0.3206799328327179\n",
      "epoch: 2 | 73664 / 114272 | training loss: 0.07891925424337387\n",
      "epoch: 2 | 73696 / 114272 | training loss: 0.022526202723383904\n",
      "epoch: 2 | 73728 / 114272 | training loss: 0.008233007974922657\n",
      "epoch: 2 | 73760 / 114272 | training loss: 0.022622978314757347\n",
      "epoch: 2 | 73792 / 114272 | training loss: 0.1420178860425949\n",
      "epoch: 2 | 73824 / 114272 | training loss: 0.18663571774959564\n",
      "epoch: 2 | 73856 / 114272 | training loss: 0.09022250026464462\n",
      "epoch: 2 | 73888 / 114272 | training loss: 0.010925320908427238\n",
      "epoch: 2 | 73920 / 114272 | training loss: 0.029538653790950775\n",
      "epoch: 2 | 73952 / 114272 | training loss: 0.12317273020744324\n",
      "epoch: 2 | 73984 / 114272 | training loss: 0.009588712826371193\n",
      "epoch: 2 | 74016 / 114272 | training loss: 0.017720859497785568\n",
      "epoch: 2 | 74048 / 114272 | training loss: 0.16356247663497925\n",
      "epoch: 2 | 74080 / 114272 | training loss: 0.010915921069681644\n",
      "epoch: 2 | 74112 / 114272 | training loss: 0.06397838890552521\n",
      "epoch: 2 | 74144 / 114272 | training loss: 0.010350722819566727\n",
      "epoch: 2 | 74176 / 114272 | training loss: 0.05728629231452942\n",
      "epoch: 2 | 74208 / 114272 | training loss: 0.19726985692977905\n",
      "epoch: 2 | 74240 / 114272 | training loss: 0.025049204006791115\n",
      "epoch: 2 | 74272 / 114272 | training loss: 0.0721718817949295\n",
      "epoch: 2 | 74304 / 114272 | training loss: 0.01545146107673645\n",
      "epoch: 2 | 74336 / 114272 | training loss: 0.13067573308944702\n",
      "epoch: 2 | 74368 / 114272 | training loss: 0.23804312944412231\n",
      "epoch: 2 | 74400 / 114272 | training loss: 0.2903274595737457\n",
      "epoch: 2 | 74432 / 114272 | training loss: 0.01069378200918436\n",
      "epoch: 2 | 74464 / 114272 | training loss: 0.10681287199258804\n",
      "epoch: 2 | 74496 / 114272 | training loss: 0.20989467203617096\n",
      "epoch: 2 | 74528 / 114272 | training loss: 0.06700631231069565\n",
      "epoch: 2 | 74560 / 114272 | training loss: 0.09180162101984024\n",
      "epoch: 2 | 74592 / 114272 | training loss: 0.02017834037542343\n",
      "epoch: 2 | 74624 / 114272 | training loss: 0.03496066853404045\n",
      "epoch: 2 | 74656 / 114272 | training loss: 0.16880692541599274\n",
      "epoch: 2 | 74688 / 114272 | training loss: 0.004123490769416094\n",
      "epoch: 2 | 74720 / 114272 | training loss: 0.020192602649331093\n",
      "epoch: 2 | 74752 / 114272 | training loss: 0.04646071791648865\n",
      "epoch: 2 | 74784 / 114272 | training loss: 0.007932226173579693\n",
      "epoch: 2 | 74816 / 114272 | training loss: 0.11818692088127136\n",
      "epoch: 2 | 74848 / 114272 | training loss: 0.013108352199196815\n",
      "epoch: 2 | 74880 / 114272 | training loss: 0.008770773187279701\n",
      "epoch: 2 | 74912 / 114272 | training loss: 0.020638123154640198\n",
      "epoch: 2 | 74944 / 114272 | training loss: 0.015170133672654629\n",
      "epoch: 2 | 74976 / 114272 | training loss: 0.005631800275295973\n",
      "epoch: 2 | 75008 / 114272 | training loss: 0.16598930954933167\n",
      "epoch: 2 | 75040 / 114272 | training loss: 0.012744294479489326\n",
      "epoch: 2 | 75072 / 114272 | training loss: 0.01125640794634819\n",
      "epoch: 2 | 75104 / 114272 | training loss: 0.1432023048400879\n",
      "epoch: 2 | 75136 / 114272 | training loss: 0.15489372611045837\n",
      "epoch: 2 | 75168 / 114272 | training loss: 0.19597741961479187\n",
      "epoch: 2 | 75200 / 114272 | training loss: 0.2059607207775116\n",
      "epoch: 2 | 75232 / 114272 | training loss: 0.006543692201375961\n",
      "epoch: 2 | 75264 / 114272 | training loss: 0.16898393630981445\n",
      "epoch: 2 | 75296 / 114272 | training loss: 0.0048277913592755795\n",
      "epoch: 2 | 75328 / 114272 | training loss: 0.09011188894510269\n",
      "epoch: 2 | 75360 / 114272 | training loss: 0.06074092909693718\n",
      "epoch: 2 | 75392 / 114272 | training loss: 0.006389823276549578\n",
      "epoch: 2 | 75424 / 114272 | training loss: 0.00640921201556921\n",
      "epoch: 2 | 75456 / 114272 | training loss: 0.01662575826048851\n",
      "epoch: 2 | 75488 / 114272 | training loss: 0.2349725216627121\n",
      "epoch: 2 | 75520 / 114272 | training loss: 0.09681166708469391\n",
      "epoch: 2 | 75552 / 114272 | training loss: 0.1259869486093521\n",
      "epoch: 2 | 75584 / 114272 | training loss: 0.008867030963301659\n",
      "epoch: 2 | 75616 / 114272 | training loss: 0.11078823357820511\n",
      "epoch: 2 | 75648 / 114272 | training loss: 0.15659135580062866\n",
      "epoch: 2 | 75680 / 114272 | training loss: 0.006638463120907545\n",
      "epoch: 2 | 75712 / 114272 | training loss: 0.008329512551426888\n",
      "epoch: 2 | 75744 / 114272 | training loss: 0.24346785247325897\n",
      "epoch: 2 | 75776 / 114272 | training loss: 0.0618591345846653\n",
      "epoch: 2 | 75808 / 114272 | training loss: 0.07988215237855911\n",
      "epoch: 2 | 75840 / 114272 | training loss: 0.005758348852396011\n",
      "epoch: 2 | 75872 / 114272 | training loss: 0.12781190872192383\n",
      "epoch: 2 | 75904 / 114272 | training loss: 0.2527926564216614\n",
      "epoch: 2 | 75936 / 114272 | training loss: 0.155495285987854\n",
      "epoch: 2 | 75968 / 114272 | training loss: 0.017352256923913956\n",
      "epoch: 2 | 76000 / 114272 | training loss: 0.10778962820768356\n",
      "epoch: 2 | 76032 / 114272 | training loss: 0.31596601009368896\n",
      "epoch: 2 | 76064 / 114272 | training loss: 0.023568183183670044\n",
      "epoch: 2 | 76096 / 114272 | training loss: 0.008797976188361645\n",
      "epoch: 2 | 76128 / 114272 | training loss: 0.0909968689084053\n",
      "epoch: 2 | 76160 / 114272 | training loss: 0.07350282371044159\n",
      "epoch: 2 | 76192 / 114272 | training loss: 0.10315882414579391\n",
      "epoch: 2 | 76224 / 114272 | training loss: 0.14470112323760986\n",
      "epoch: 2 | 76256 / 114272 | training loss: 0.043043527752161026\n",
      "epoch: 2 | 76288 / 114272 | training loss: 0.0403224341571331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 76320 / 114272 | training loss: 0.2544477581977844\n",
      "epoch: 2 | 76352 / 114272 | training loss: 0.09564588963985443\n",
      "epoch: 2 | 76384 / 114272 | training loss: 0.12211715430021286\n",
      "epoch: 2 | 76416 / 114272 | training loss: 0.043601423501968384\n",
      "epoch: 2 | 76448 / 114272 | training loss: 0.09091173857450485\n",
      "epoch: 2 | 76480 / 114272 | training loss: 0.03527328372001648\n",
      "epoch: 2 | 76512 / 114272 | training loss: 0.02926606498658657\n",
      "epoch: 2 | 76544 / 114272 | training loss: 0.0750570297241211\n",
      "epoch: 2 | 76576 / 114272 | training loss: 0.050039444118738174\n",
      "epoch: 2 | 76608 / 114272 | training loss: 0.019946960732340813\n",
      "epoch: 2 | 76640 / 114272 | training loss: 0.007007799576967955\n",
      "epoch: 2 | 76672 / 114272 | training loss: 0.025017227977514267\n",
      "epoch: 2 | 76704 / 114272 | training loss: 0.0473899282515049\n",
      "epoch: 2 | 76736 / 114272 | training loss: 0.1486051231622696\n",
      "epoch: 2 | 76768 / 114272 | training loss: 0.008623126894235611\n",
      "epoch: 2 | 76800 / 114272 | training loss: 0.025230243802070618\n",
      "epoch: 2 | 76832 / 114272 | training loss: 0.007866776548326015\n",
      "epoch: 2 | 76864 / 114272 | training loss: 0.08745654672384262\n",
      "epoch: 2 | 76896 / 114272 | training loss: 0.022561680525541306\n",
      "epoch: 2 | 76928 / 114272 | training loss: 0.007175699342042208\n",
      "epoch: 2 | 76960 / 114272 | training loss: 0.005935657303780317\n",
      "epoch: 2 | 76992 / 114272 | training loss: 0.008944454602897167\n",
      "epoch: 2 | 77024 / 114272 | training loss: 0.016650589182972908\n",
      "epoch: 2 | 77056 / 114272 | training loss: 0.11970007419586182\n",
      "epoch: 2 | 77088 / 114272 | training loss: 0.02342110499739647\n",
      "epoch: 2 | 77120 / 114272 | training loss: 0.09332635253667831\n",
      "epoch: 2 | 77152 / 114272 | training loss: 0.015140415169298649\n",
      "epoch: 2 | 77184 / 114272 | training loss: 0.016972284764051437\n",
      "epoch: 2 | 77216 / 114272 | training loss: 0.0076257591135799885\n",
      "epoch: 2 | 77248 / 114272 | training loss: 0.13895538449287415\n",
      "epoch: 2 | 77280 / 114272 | training loss: 0.012805079109966755\n",
      "epoch: 2 | 77312 / 114272 | training loss: 0.25157293677330017\n",
      "epoch: 2 | 77344 / 114272 | training loss: 0.15609511733055115\n",
      "epoch: 2 | 77376 / 114272 | training loss: 0.22790095210075378\n",
      "epoch: 2 | 77408 / 114272 | training loss: 0.007644812110811472\n",
      "epoch: 2 | 77440 / 114272 | training loss: 0.3182999789714813\n",
      "epoch: 2 | 77472 / 114272 | training loss: 0.0028706183657050133\n",
      "epoch: 2 | 77504 / 114272 | training loss: 0.005092013161629438\n",
      "epoch: 2 | 77536 / 114272 | training loss: 0.0030976319685578346\n",
      "epoch: 2 | 77568 / 114272 | training loss: 0.010545588098466396\n",
      "epoch: 2 | 77600 / 114272 | training loss: 0.18243449926376343\n",
      "epoch: 2 | 77632 / 114272 | training loss: 0.014658835716545582\n",
      "epoch: 2 | 77664 / 114272 | training loss: 0.0064077978022396564\n",
      "epoch: 2 | 77696 / 114272 | training loss: 0.06517817080020905\n",
      "epoch: 2 | 77728 / 114272 | training loss: 0.009649352170526981\n",
      "epoch: 2 | 77760 / 114272 | training loss: 0.008812498301267624\n",
      "epoch: 2 | 77792 / 114272 | training loss: 0.20836670696735382\n",
      "epoch: 2 | 77824 / 114272 | training loss: 0.017761191353201866\n",
      "epoch: 2 | 77856 / 114272 | training loss: 0.03703932464122772\n",
      "epoch: 2 | 77888 / 114272 | training loss: 0.05041918531060219\n",
      "epoch: 2 | 77920 / 114272 | training loss: 0.04844040423631668\n",
      "epoch: 2 | 77952 / 114272 | training loss: 0.23145724833011627\n",
      "epoch: 2 | 77984 / 114272 | training loss: 0.09241513907909393\n",
      "epoch: 2 | 78016 / 114272 | training loss: 0.0093464907258749\n",
      "epoch: 2 | 78048 / 114272 | training loss: 0.44222089648246765\n",
      "epoch: 2 | 78080 / 114272 | training loss: 0.30284383893013\n",
      "epoch: 2 | 78112 / 114272 | training loss: 0.13543745875358582\n",
      "epoch: 2 | 78144 / 114272 | training loss: 0.005414078943431377\n",
      "epoch: 2 | 78176 / 114272 | training loss: 0.01896098628640175\n",
      "epoch: 2 | 78208 / 114272 | training loss: 0.055850762873888016\n",
      "epoch: 2 | 78240 / 114272 | training loss: 0.10406768321990967\n",
      "epoch: 2 | 78272 / 114272 | training loss: 0.054933346807956696\n",
      "epoch: 2 | 78304 / 114272 | training loss: 0.24695244431495667\n",
      "epoch: 2 | 78336 / 114272 | training loss: 0.14473380148410797\n",
      "epoch: 2 | 78368 / 114272 | training loss: 0.024296630173921585\n",
      "epoch: 2 | 78400 / 114272 | training loss: 0.10533633083105087\n",
      "epoch: 2 | 78432 / 114272 | training loss: 0.012703467160463333\n",
      "epoch: 2 | 78464 / 114272 | training loss: 0.01127273216843605\n",
      "epoch: 2 | 78496 / 114272 | training loss: 0.2045203596353531\n",
      "epoch: 2 | 78528 / 114272 | training loss: 0.17365486919879913\n",
      "epoch: 2 | 78560 / 114272 | training loss: 0.011102959513664246\n",
      "epoch: 2 | 78592 / 114272 | training loss: 0.0021684812381863594\n",
      "epoch: 2 | 78624 / 114272 | training loss: 0.024717072024941444\n",
      "epoch: 2 | 78656 / 114272 | training loss: 0.007673652842640877\n",
      "epoch: 2 | 78688 / 114272 | training loss: 0.01520321425050497\n",
      "epoch: 2 | 78720 / 114272 | training loss: 0.0116731571033597\n",
      "epoch: 2 | 78752 / 114272 | training loss: 0.024783266708254814\n",
      "epoch: 2 | 78784 / 114272 | training loss: 0.04854429140686989\n",
      "epoch: 2 | 78816 / 114272 | training loss: 0.025712981820106506\n",
      "epoch: 2 | 78848 / 114272 | training loss: 0.09552717208862305\n",
      "epoch: 2 | 78880 / 114272 | training loss: 0.014619315043091774\n",
      "epoch: 2 | 78912 / 114272 | training loss: 0.011016727425158024\n",
      "epoch: 2 | 78944 / 114272 | training loss: 0.08600527793169022\n",
      "epoch: 2 | 78976 / 114272 | training loss: 0.016107967123389244\n",
      "epoch: 2 | 79008 / 114272 | training loss: 0.05952389910817146\n",
      "epoch: 2 | 79040 / 114272 | training loss: 0.011320782825350761\n",
      "epoch: 2 | 79072 / 114272 | training loss: 0.19593974947929382\n",
      "epoch: 2 | 79104 / 114272 | training loss: 0.01374991424381733\n",
      "epoch: 2 | 79136 / 114272 | training loss: 0.0925426036119461\n",
      "epoch: 2 | 79168 / 114272 | training loss: 0.028607862070202827\n",
      "epoch: 2 | 79200 / 114272 | training loss: 0.1304779052734375\n",
      "epoch: 2 | 79232 / 114272 | training loss: 0.305043488740921\n",
      "epoch: 2 | 79264 / 114272 | training loss: 0.17636820673942566\n",
      "epoch: 2 | 79296 / 114272 | training loss: 0.013262258842587471\n",
      "epoch: 2 | 79328 / 114272 | training loss: 0.23764900863170624\n",
      "epoch: 2 | 79360 / 114272 | training loss: 0.08472375571727753\n",
      "epoch: 2 | 79392 / 114272 | training loss: 0.04447745904326439\n",
      "epoch: 2 | 79424 / 114272 | training loss: 0.011654573492705822\n",
      "epoch: 2 | 79456 / 114272 | training loss: 0.018797485157847404\n",
      "epoch: 2 | 79488 / 114272 | training loss: 0.01400827057659626\n",
      "epoch: 2 | 79520 / 114272 | training loss: 0.009171328507363796\n",
      "epoch: 2 | 79552 / 114272 | training loss: 0.16719801723957062\n",
      "epoch: 2 | 79584 / 114272 | training loss: 0.01391760352998972\n",
      "epoch: 2 | 79616 / 114272 | training loss: 0.09083927422761917\n",
      "epoch: 2 | 79648 / 114272 | training loss: 0.009251328185200691\n",
      "epoch: 2 | 79680 / 114272 | training loss: 0.007354242727160454\n",
      "epoch: 2 | 79712 / 114272 | training loss: 0.0038976131472736597\n",
      "epoch: 2 | 79744 / 114272 | training loss: 0.19304245710372925\n",
      "epoch: 2 | 79776 / 114272 | training loss: 0.08885178714990616\n",
      "epoch: 2 | 79808 / 114272 | training loss: 0.33282577991485596\n",
      "epoch: 2 | 79840 / 114272 | training loss: 0.08161104470491409\n",
      "epoch: 2 | 79872 / 114272 | training loss: 0.009147238917648792\n",
      "epoch: 2 | 79904 / 114272 | training loss: 0.0033138722646981478\n",
      "epoch: 2 | 79936 / 114272 | training loss: 0.3353838622570038\n",
      "epoch: 2 | 79968 / 114272 | training loss: 0.27584967017173767\n",
      "epoch: 2 | 80000 / 114272 | training loss: 0.2252711057662964\n",
      "epoch: 2 | 80032 / 114272 | training loss: 0.15507100522518158\n",
      "epoch: 2 | 80064 / 114272 | training loss: 0.10328374058008194\n",
      "epoch: 2 | 80096 / 114272 | training loss: 0.020611174404621124\n",
      "epoch: 2 | 80128 / 114272 | training loss: 0.0035419221967458725\n",
      "epoch: 2 | 80160 / 114272 | training loss: 0.08611686527729034\n",
      "epoch: 2 | 80192 / 114272 | training loss: 0.241798996925354\n",
      "epoch: 2 | 80224 / 114272 | training loss: 0.006718365475535393\n",
      "epoch: 2 | 80256 / 114272 | training loss: 0.0046949018724262714\n",
      "epoch: 2 | 80288 / 114272 | training loss: 0.01594296097755432\n",
      "epoch: 2 | 80320 / 114272 | training loss: 0.09345903992652893\n",
      "epoch: 2 | 80352 / 114272 | training loss: 0.05220227688550949\n",
      "epoch: 2 | 80384 / 114272 | training loss: 0.16963480412960052\n",
      "epoch: 2 | 80416 / 114272 | training loss: 0.008121655322611332\n",
      "epoch: 2 | 80448 / 114272 | training loss: 0.0065111215226352215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 80480 / 114272 | training loss: 0.007479079067707062\n",
      "epoch: 2 | 80512 / 114272 | training loss: 0.054759472608566284\n",
      "epoch: 2 | 80544 / 114272 | training loss: 0.2750159800052643\n",
      "epoch: 2 | 80576 / 114272 | training loss: 0.0035614969674497843\n",
      "epoch: 2 | 80608 / 114272 | training loss: 0.023092467337846756\n",
      "epoch: 2 | 80640 / 114272 | training loss: 0.18153831362724304\n",
      "epoch: 2 | 80672 / 114272 | training loss: 0.286525696516037\n",
      "epoch: 2 | 80704 / 114272 | training loss: 0.00912588182836771\n",
      "epoch: 2 | 80736 / 114272 | training loss: 0.27781543135643005\n",
      "epoch: 2 | 80768 / 114272 | training loss: 0.08365791290998459\n",
      "epoch: 2 | 80800 / 114272 | training loss: 0.011573867872357368\n",
      "epoch: 2 | 80832 / 114272 | training loss: 0.017369352281093597\n",
      "epoch: 2 | 80864 / 114272 | training loss: 0.051590900868177414\n",
      "epoch: 2 | 80896 / 114272 | training loss: 0.043220680207014084\n",
      "epoch: 2 | 80928 / 114272 | training loss: 0.010500137694180012\n",
      "epoch: 2 | 80960 / 114272 | training loss: 0.006225781049579382\n",
      "epoch: 2 | 80992 / 114272 | training loss: 0.19102410972118378\n",
      "epoch: 2 | 81024 / 114272 | training loss: 0.0389443039894104\n",
      "epoch: 2 | 81056 / 114272 | training loss: 0.03420808166265488\n",
      "epoch: 2 | 81088 / 114272 | training loss: 0.006154546979814768\n",
      "epoch: 2 | 81120 / 114272 | training loss: 0.41333580017089844\n",
      "epoch: 2 | 81152 / 114272 | training loss: 0.004385549575090408\n",
      "epoch: 2 | 81184 / 114272 | training loss: 0.22941440343856812\n",
      "epoch: 2 | 81216 / 114272 | training loss: 0.005036217160522938\n",
      "epoch: 2 | 81248 / 114272 | training loss: 0.15494193136692047\n",
      "epoch: 2 | 81280 / 114272 | training loss: 0.008845881558954716\n",
      "epoch: 2 | 81312 / 114272 | training loss: 0.004651837982237339\n",
      "epoch: 2 | 81344 / 114272 | training loss: 0.09678180515766144\n",
      "epoch: 2 | 81376 / 114272 | training loss: 0.07563905417919159\n",
      "epoch: 2 | 81408 / 114272 | training loss: 0.007141765672713518\n",
      "epoch: 2 | 81440 / 114272 | training loss: 0.07673007994890213\n",
      "epoch: 2 | 81472 / 114272 | training loss: 0.20689921081066132\n",
      "epoch: 2 | 81504 / 114272 | training loss: 0.0057681468315422535\n",
      "epoch: 2 | 81536 / 114272 | training loss: 0.006978780031204224\n",
      "epoch: 2 | 81568 / 114272 | training loss: 0.05530286952853203\n",
      "epoch: 2 | 81600 / 114272 | training loss: 0.15336109697818756\n",
      "epoch: 2 | 81632 / 114272 | training loss: 0.34971970319747925\n",
      "epoch: 2 | 81664 / 114272 | training loss: 0.07324918359518051\n",
      "epoch: 2 | 81696 / 114272 | training loss: 0.08578523993492126\n",
      "epoch: 2 | 81728 / 114272 | training loss: 0.009212353266775608\n",
      "epoch: 2 | 81760 / 114272 | training loss: 0.011561564169824123\n",
      "epoch: 2 | 81792 / 114272 | training loss: 0.1367713361978531\n",
      "epoch: 2 | 81824 / 114272 | training loss: 0.01521836593747139\n",
      "epoch: 2 | 81856 / 114272 | training loss: 0.03205700218677521\n",
      "epoch: 2 | 81888 / 114272 | training loss: 0.16328023374080658\n",
      "epoch: 2 | 81920 / 114272 | training loss: 0.014352613128721714\n",
      "epoch: 2 | 81952 / 114272 | training loss: 0.08534520119428635\n",
      "epoch: 2 | 81984 / 114272 | training loss: 0.012993186712265015\n",
      "epoch: 2 | 82016 / 114272 | training loss: 0.016503456979990005\n",
      "epoch: 2 | 82048 / 114272 | training loss: 0.007255209609866142\n",
      "epoch: 2 | 82080 / 114272 | training loss: 0.010429725050926208\n",
      "epoch: 2 | 82112 / 114272 | training loss: 0.2488643378019333\n",
      "epoch: 2 | 82144 / 114272 | training loss: 0.07367973774671555\n",
      "epoch: 2 | 82176 / 114272 | training loss: 0.00788191333413124\n",
      "epoch: 2 | 82208 / 114272 | training loss: 0.05199526995420456\n",
      "epoch: 2 | 82240 / 114272 | training loss: 0.0077247219160199165\n",
      "epoch: 2 | 82272 / 114272 | training loss: 0.008510734885931015\n",
      "epoch: 2 | 82304 / 114272 | training loss: 0.06420153379440308\n",
      "epoch: 2 | 82336 / 114272 | training loss: 0.006563758011907339\n",
      "epoch: 2 | 82368 / 114272 | training loss: 0.00971262902021408\n",
      "epoch: 2 | 82400 / 114272 | training loss: 0.02996252104640007\n",
      "epoch: 2 | 82432 / 114272 | training loss: 0.019174087792634964\n",
      "epoch: 2 | 82464 / 114272 | training loss: 0.0057028234004974365\n",
      "epoch: 2 | 82496 / 114272 | training loss: 0.13464516401290894\n",
      "epoch: 2 | 82528 / 114272 | training loss: 0.09535594284534454\n",
      "epoch: 2 | 82560 / 114272 | training loss: 0.004324636422097683\n",
      "epoch: 2 | 82592 / 114272 | training loss: 0.04806000366806984\n",
      "epoch: 2 | 82624 / 114272 | training loss: 0.16955475509166718\n",
      "epoch: 2 | 82656 / 114272 | training loss: 0.15679813921451569\n",
      "epoch: 2 | 82688 / 114272 | training loss: 0.1340698003768921\n",
      "epoch: 2 | 82720 / 114272 | training loss: 0.10314333438873291\n",
      "epoch: 2 | 82752 / 114272 | training loss: 0.02210106886923313\n",
      "epoch: 2 | 82784 / 114272 | training loss: 0.07092443108558655\n",
      "epoch: 2 | 82816 / 114272 | training loss: 0.3486822247505188\n",
      "epoch: 2 | 82848 / 114272 | training loss: 0.08738895505666733\n",
      "epoch: 2 | 82880 / 114272 | training loss: 0.005893890745937824\n",
      "epoch: 2 | 82912 / 114272 | training loss: 0.020150944590568542\n",
      "epoch: 2 | 82944 / 114272 | training loss: 0.20336765050888062\n",
      "epoch: 2 | 82976 / 114272 | training loss: 0.007000564131885767\n",
      "epoch: 2 | 83008 / 114272 | training loss: 0.012457646429538727\n",
      "epoch: 2 | 83040 / 114272 | training loss: 0.01240268349647522\n",
      "epoch: 2 | 83072 / 114272 | training loss: 0.1816544085741043\n",
      "epoch: 2 | 83104 / 114272 | training loss: 0.09148986637592316\n",
      "epoch: 2 | 83136 / 114272 | training loss: 0.003879215568304062\n",
      "epoch: 2 | 83168 / 114272 | training loss: 0.0031301453709602356\n",
      "epoch: 2 | 83200 / 114272 | training loss: 0.006091318093240261\n",
      "epoch: 2 | 83232 / 114272 | training loss: 0.06575622409582138\n",
      "epoch: 2 | 83264 / 114272 | training loss: 0.2018006443977356\n",
      "epoch: 2 | 83296 / 114272 | training loss: 0.03482490032911301\n",
      "epoch: 2 | 83328 / 114272 | training loss: 0.11787592619657516\n",
      "epoch: 2 | 83360 / 114272 | training loss: 0.2114403396844864\n",
      "epoch: 2 | 83392 / 114272 | training loss: 0.0472649484872818\n",
      "epoch: 2 | 83424 / 114272 | training loss: 0.1378316730260849\n",
      "epoch: 2 | 83456 / 114272 | training loss: 0.057776495814323425\n",
      "epoch: 2 | 83488 / 114272 | training loss: 0.03938550129532814\n",
      "epoch: 2 | 83520 / 114272 | training loss: 0.021188879385590553\n",
      "epoch: 2 | 83552 / 114272 | training loss: 0.0071294293738901615\n",
      "epoch: 2 | 83584 / 114272 | training loss: 0.06305782496929169\n",
      "epoch: 2 | 83616 / 114272 | training loss: 0.03488814830780029\n",
      "epoch: 2 | 83648 / 114272 | training loss: 0.2177528291940689\n",
      "epoch: 2 | 83680 / 114272 | training loss: 0.014700189232826233\n",
      "epoch: 2 | 83712 / 114272 | training loss: 0.22244501113891602\n",
      "epoch: 2 | 83744 / 114272 | training loss: 0.008540757931768894\n",
      "epoch: 2 | 83776 / 114272 | training loss: 0.15782128274440765\n",
      "epoch: 2 | 83808 / 114272 | training loss: 0.006404565181583166\n",
      "epoch: 2 | 83840 / 114272 | training loss: 0.0025054672732949257\n",
      "epoch: 2 | 83872 / 114272 | training loss: 0.00986449234187603\n",
      "epoch: 2 | 83904 / 114272 | training loss: 0.14930281043052673\n",
      "epoch: 2 | 83936 / 114272 | training loss: 0.19115284085273743\n",
      "epoch: 2 | 83968 / 114272 | training loss: 0.1578064262866974\n",
      "epoch: 2 | 84000 / 114272 | training loss: 0.1510472148656845\n",
      "epoch: 2 | 84032 / 114272 | training loss: 0.011282642371952534\n",
      "epoch: 2 | 84064 / 114272 | training loss: 0.02556605078279972\n",
      "epoch: 2 | 84096 / 114272 | training loss: 0.003169057425111532\n",
      "epoch: 2 | 84128 / 114272 | training loss: 0.07841790467500687\n",
      "epoch: 2 | 84160 / 114272 | training loss: 0.09547802805900574\n",
      "epoch: 2 | 84192 / 114272 | training loss: 0.1324334740638733\n",
      "epoch: 2 | 84224 / 114272 | training loss: 0.0026856104377657175\n",
      "epoch: 2 | 84256 / 114272 | training loss: 0.21958473324775696\n",
      "epoch: 2 | 84288 / 114272 | training loss: 0.14901241660118103\n",
      "epoch: 2 | 84320 / 114272 | training loss: 0.35177019238471985\n",
      "epoch: 2 | 84352 / 114272 | training loss: 0.014830468222498894\n",
      "epoch: 2 | 84384 / 114272 | training loss: 0.03589092567563057\n",
      "epoch: 2 | 84416 / 114272 | training loss: 0.11273893713951111\n",
      "epoch: 2 | 84448 / 114272 | training loss: 0.04209015518426895\n",
      "epoch: 2 | 84480 / 114272 | training loss: 0.013377855531871319\n",
      "epoch: 2 | 84512 / 114272 | training loss: 0.14090664684772491\n",
      "epoch: 2 | 84544 / 114272 | training loss: 0.21216879785060883\n",
      "epoch: 2 | 84576 / 114272 | training loss: 0.21238477528095245\n",
      "epoch: 2 | 84608 / 114272 | training loss: 0.10246457159519196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 84640 / 114272 | training loss: 0.03395825996994972\n",
      "epoch: 2 | 84672 / 114272 | training loss: 0.3174405097961426\n",
      "epoch: 2 | 84704 / 114272 | training loss: 0.08424566686153412\n",
      "epoch: 2 | 84736 / 114272 | training loss: 0.17120501399040222\n",
      "epoch: 2 | 84768 / 114272 | training loss: 0.023693732917308807\n",
      "epoch: 2 | 84800 / 114272 | training loss: 0.2953381836414337\n",
      "epoch: 2 | 84832 / 114272 | training loss: 0.011667166836559772\n",
      "epoch: 2 | 84864 / 114272 | training loss: 0.06559526175260544\n",
      "epoch: 2 | 84896 / 114272 | training loss: 0.18958567082881927\n",
      "epoch: 2 | 84928 / 114272 | training loss: 0.023372406139969826\n",
      "epoch: 2 | 84960 / 114272 | training loss: 0.11394897848367691\n",
      "epoch: 2 | 84992 / 114272 | training loss: 0.23648826777935028\n",
      "epoch: 2 | 85024 / 114272 | training loss: 0.07444465905427933\n",
      "epoch: 2 | 85056 / 114272 | training loss: 0.2029275745153427\n",
      "epoch: 2 | 85088 / 114272 | training loss: 0.03189137205481529\n",
      "epoch: 2 | 85120 / 114272 | training loss: 0.22366833686828613\n",
      "epoch: 2 | 85152 / 114272 | training loss: 0.2962900400161743\n",
      "epoch: 2 | 85184 / 114272 | training loss: 0.02110956236720085\n",
      "epoch: 2 | 85216 / 114272 | training loss: 0.06902940571308136\n",
      "epoch: 2 | 85248 / 114272 | training loss: 0.042603131383657455\n",
      "epoch: 2 | 85280 / 114272 | training loss: 0.27845433354377747\n",
      "epoch: 2 | 85312 / 114272 | training loss: 0.026104040443897247\n",
      "epoch: 2 | 85344 / 114272 | training loss: 0.10762771964073181\n",
      "epoch: 2 | 85376 / 114272 | training loss: 0.08876335620880127\n",
      "epoch: 2 | 85408 / 114272 | training loss: 0.01721067540347576\n",
      "epoch: 2 | 85440 / 114272 | training loss: 0.019734691828489304\n",
      "epoch: 2 | 85472 / 114272 | training loss: 0.2196718007326126\n",
      "epoch: 2 | 85504 / 114272 | training loss: 0.01853042095899582\n",
      "epoch: 2 | 85536 / 114272 | training loss: 0.0932527408003807\n",
      "epoch: 2 | 85568 / 114272 | training loss: 0.0774209201335907\n",
      "epoch: 2 | 85600 / 114272 | training loss: 0.22502899169921875\n",
      "epoch: 2 | 85632 / 114272 | training loss: 0.05259181559085846\n",
      "epoch: 2 | 85664 / 114272 | training loss: 0.4820806682109833\n",
      "epoch: 2 | 85696 / 114272 | training loss: 0.1746664047241211\n",
      "epoch: 2 | 85728 / 114272 | training loss: 0.08237989246845245\n",
      "epoch: 2 | 85760 / 114272 | training loss: 0.21494749188423157\n",
      "epoch: 2 | 85792 / 114272 | training loss: 0.3176504671573639\n",
      "epoch: 2 | 85824 / 114272 | training loss: 0.10966065526008606\n",
      "epoch: 2 | 85856 / 114272 | training loss: 0.07985910028219223\n",
      "epoch: 2 | 85888 / 114272 | training loss: 0.13463714718818665\n",
      "epoch: 2 | 85920 / 114272 | training loss: 0.2671286463737488\n",
      "epoch: 2 | 85952 / 114272 | training loss: 0.17055253684520721\n",
      "epoch: 2 | 85984 / 114272 | training loss: 0.28055617213249207\n",
      "epoch: 2 | 86016 / 114272 | training loss: 0.3023674786090851\n",
      "epoch: 2 | 86048 / 114272 | training loss: 0.2754170894622803\n",
      "epoch: 2 | 86080 / 114272 | training loss: 0.10790817439556122\n",
      "epoch: 2 | 86112 / 114272 | training loss: 0.015022438950836658\n",
      "epoch: 2 | 86144 / 114272 | training loss: 0.014712240546941757\n",
      "epoch: 2 | 86176 / 114272 | training loss: 0.03455992415547371\n",
      "epoch: 2 | 86208 / 114272 | training loss: 0.1011204645037651\n",
      "epoch: 2 | 86240 / 114272 | training loss: 0.023011617362499237\n",
      "epoch: 2 | 86272 / 114272 | training loss: 0.15217779576778412\n",
      "epoch: 2 | 86304 / 114272 | training loss: 0.051768798381090164\n",
      "epoch: 2 | 86336 / 114272 | training loss: 0.03734105825424194\n",
      "epoch: 2 | 86368 / 114272 | training loss: 0.02305874228477478\n",
      "epoch: 2 | 86400 / 114272 | training loss: 0.019018154591321945\n",
      "epoch: 2 | 86432 / 114272 | training loss: 0.014506535604596138\n",
      "epoch: 2 | 86464 / 114272 | training loss: 0.13543152809143066\n",
      "epoch: 2 | 86496 / 114272 | training loss: 0.09213915467262268\n",
      "epoch: 2 | 86528 / 114272 | training loss: 0.1667979508638382\n",
      "epoch: 2 | 86560 / 114272 | training loss: 0.15118299424648285\n",
      "epoch: 2 | 86592 / 114272 | training loss: 0.014520617201924324\n",
      "epoch: 2 | 86624 / 114272 | training loss: 0.11044730991125107\n",
      "epoch: 2 | 86656 / 114272 | training loss: 0.19852527976036072\n",
      "epoch: 2 | 86688 / 114272 | training loss: 0.0884167030453682\n",
      "epoch: 2 | 86720 / 114272 | training loss: 0.07100538909435272\n",
      "epoch: 2 | 86752 / 114272 | training loss: 0.19934594631195068\n",
      "epoch: 2 | 86784 / 114272 | training loss: 0.10193517059087753\n",
      "epoch: 2 | 86816 / 114272 | training loss: 0.013780390843749046\n",
      "epoch: 2 | 86848 / 114272 | training loss: 0.010863219387829304\n",
      "epoch: 2 | 86880 / 114272 | training loss: 0.15907935798168182\n",
      "epoch: 2 | 86912 / 114272 | training loss: 0.0835973471403122\n",
      "epoch: 2 | 86944 / 114272 | training loss: 0.09926437586545944\n",
      "epoch: 2 | 86976 / 114272 | training loss: 0.022814784198999405\n",
      "epoch: 2 | 87008 / 114272 | training loss: 0.018326733261346817\n",
      "epoch: 2 | 87040 / 114272 | training loss: 0.1107206791639328\n",
      "epoch: 2 | 87072 / 114272 | training loss: 0.13787062466144562\n",
      "epoch: 2 | 87104 / 114272 | training loss: 0.15498419106006622\n",
      "epoch: 2 | 87136 / 114272 | training loss: 0.014054267667233944\n",
      "epoch: 2 | 87168 / 114272 | training loss: 0.02781720645725727\n",
      "epoch: 2 | 87200 / 114272 | training loss: 0.1468074917793274\n",
      "epoch: 2 | 87232 / 114272 | training loss: 0.21551379561424255\n",
      "epoch: 2 | 87264 / 114272 | training loss: 0.045722682029008865\n",
      "epoch: 2 | 87296 / 114272 | training loss: 0.011579236015677452\n",
      "epoch: 2 | 87328 / 114272 | training loss: 0.046862583607435226\n",
      "epoch: 2 | 87360 / 114272 | training loss: 0.03189827501773834\n",
      "epoch: 2 | 87392 / 114272 | training loss: 0.005612235516309738\n",
      "epoch: 2 | 87424 / 114272 | training loss: 0.01341824047267437\n",
      "epoch: 2 | 87456 / 114272 | training loss: 0.09342782199382782\n",
      "epoch: 2 | 87488 / 114272 | training loss: 0.1760713905096054\n",
      "epoch: 2 | 87520 / 114272 | training loss: 0.06034485623240471\n",
      "epoch: 2 | 87552 / 114272 | training loss: 0.017658669501543045\n",
      "epoch: 2 | 87584 / 114272 | training loss: 0.14788931608200073\n",
      "epoch: 2 | 87616 / 114272 | training loss: 0.006837139837443829\n",
      "epoch: 2 | 87648 / 114272 | training loss: 0.04805506765842438\n",
      "epoch: 2 | 87680 / 114272 | training loss: 0.00466735428199172\n",
      "epoch: 2 | 87712 / 114272 | training loss: 0.007285150699317455\n",
      "epoch: 2 | 87744 / 114272 | training loss: 0.036152184009552\n",
      "epoch: 2 | 87776 / 114272 | training loss: 0.10844118893146515\n",
      "epoch: 2 | 87808 / 114272 | training loss: 0.008687981404364109\n",
      "epoch: 2 | 87840 / 114272 | training loss: 0.28001707792282104\n",
      "epoch: 2 | 87872 / 114272 | training loss: 0.16182737052440643\n",
      "epoch: 2 | 87904 / 114272 | training loss: 0.254252165555954\n",
      "epoch: 2 | 87936 / 114272 | training loss: 0.11986920982599258\n",
      "epoch: 2 | 87968 / 114272 | training loss: 0.010337693616747856\n",
      "epoch: 2 | 88000 / 114272 | training loss: 0.28611916303634644\n",
      "epoch: 2 | 88032 / 114272 | training loss: 0.03724266216158867\n",
      "epoch: 2 | 88064 / 114272 | training loss: 0.23665069043636322\n",
      "epoch: 2 | 88096 / 114272 | training loss: 0.06394393742084503\n",
      "epoch: 2 | 88128 / 114272 | training loss: 0.08758436143398285\n",
      "epoch: 2 | 88160 / 114272 | training loss: 0.060155171900987625\n",
      "epoch: 2 | 88192 / 114272 | training loss: 0.1964486539363861\n",
      "epoch: 2 | 88224 / 114272 | training loss: 0.010272415354847908\n",
      "epoch: 2 | 88256 / 114272 | training loss: 0.08539904654026031\n",
      "epoch: 2 | 88288 / 114272 | training loss: 0.031975679099559784\n",
      "epoch: 2 | 88320 / 114272 | training loss: 0.0437636598944664\n",
      "epoch: 2 | 88352 / 114272 | training loss: 0.008260926231741905\n",
      "epoch: 2 | 88384 / 114272 | training loss: 0.09510957449674606\n",
      "epoch: 2 | 88416 / 114272 | training loss: 0.10743597894906998\n",
      "epoch: 2 | 88448 / 114272 | training loss: 0.044811274856328964\n",
      "epoch: 2 | 88480 / 114272 | training loss: 0.17472368478775024\n",
      "epoch: 2 | 88512 / 114272 | training loss: 0.033371806144714355\n",
      "epoch: 2 | 88544 / 114272 | training loss: 0.20852309465408325\n",
      "epoch: 2 | 88576 / 114272 | training loss: 0.04457496851682663\n",
      "epoch: 2 | 88608 / 114272 | training loss: 0.06200048699975014\n",
      "epoch: 2 | 88640 / 114272 | training loss: 0.3340306580066681\n",
      "epoch: 2 | 88672 / 114272 | training loss: 0.06047038361430168\n",
      "epoch: 2 | 88704 / 114272 | training loss: 0.12360608577728271\n",
      "epoch: 2 | 88736 / 114272 | training loss: 0.22003614902496338\n",
      "epoch: 2 | 88768 / 114272 | training loss: 0.12172704190015793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 88800 / 114272 | training loss: 0.010434187948703766\n",
      "epoch: 2 | 88832 / 114272 | training loss: 0.16895204782485962\n",
      "epoch: 2 | 88864 / 114272 | training loss: 0.38678881525993347\n",
      "epoch: 2 | 88896 / 114272 | training loss: 0.030361071228981018\n",
      "epoch: 2 | 88928 / 114272 | training loss: 0.13465288281440735\n",
      "epoch: 2 | 88960 / 114272 | training loss: 0.04035072401165962\n",
      "epoch: 2 | 88992 / 114272 | training loss: 0.09228341281414032\n",
      "epoch: 2 | 89024 / 114272 | training loss: 0.08325141668319702\n",
      "epoch: 2 | 89056 / 114272 | training loss: 0.023832302540540695\n",
      "epoch: 2 | 89088 / 114272 | training loss: 0.08418982475996017\n",
      "epoch: 2 | 89120 / 114272 | training loss: 0.12659333646297455\n",
      "epoch: 2 | 89152 / 114272 | training loss: 0.11390599608421326\n",
      "epoch: 2 | 89184 / 114272 | training loss: 0.020888298749923706\n",
      "epoch: 2 | 89216 / 114272 | training loss: 0.05495774745941162\n",
      "epoch: 2 | 89248 / 114272 | training loss: 0.027740629389882088\n",
      "epoch: 2 | 89280 / 114272 | training loss: 0.048730649054050446\n",
      "epoch: 2 | 89312 / 114272 | training loss: 0.06573684513568878\n",
      "epoch: 2 | 89344 / 114272 | training loss: 0.09548776596784592\n",
      "epoch: 2 | 89376 / 114272 | training loss: 0.13108457624912262\n",
      "epoch: 2 | 89408 / 114272 | training loss: 0.31528007984161377\n",
      "epoch: 2 | 89440 / 114272 | training loss: 0.03269953280687332\n",
      "epoch: 2 | 89472 / 114272 | training loss: 0.09984798729419708\n",
      "epoch: 2 | 89504 / 114272 | training loss: 0.01587761379778385\n",
      "epoch: 2 | 89536 / 114272 | training loss: 0.06909871846437454\n",
      "epoch: 2 | 89568 / 114272 | training loss: 0.08510645478963852\n",
      "epoch: 2 | 89600 / 114272 | training loss: 0.014601192437112331\n",
      "epoch: 2 | 89632 / 114272 | training loss: 0.13822071254253387\n",
      "epoch: 2 | 89664 / 114272 | training loss: 0.12279443442821503\n",
      "epoch: 2 | 89696 / 114272 | training loss: 0.017910433933138847\n",
      "epoch: 2 | 89728 / 114272 | training loss: 0.08694221079349518\n",
      "epoch: 2 | 89760 / 114272 | training loss: 0.37297695875167847\n",
      "epoch: 2 | 89792 / 114272 | training loss: 0.20246970653533936\n",
      "epoch: 2 | 89824 / 114272 | training loss: 0.021652569994330406\n",
      "epoch: 2 | 89856 / 114272 | training loss: 0.04232318326830864\n",
      "epoch: 2 | 89888 / 114272 | training loss: 0.08092670887708664\n",
      "epoch: 2 | 89920 / 114272 | training loss: 0.1272009015083313\n",
      "epoch: 2 | 89952 / 114272 | training loss: 0.00803431961685419\n",
      "epoch: 2 | 89984 / 114272 | training loss: 0.057854361832141876\n",
      "epoch: 2 | 90016 / 114272 | training loss: 0.187728151679039\n",
      "epoch: 2 | 90048 / 114272 | training loss: 0.2808872163295746\n",
      "epoch: 2 | 90080 / 114272 | training loss: 0.011884411796927452\n",
      "epoch: 2 | 90112 / 114272 | training loss: 0.08926349133253098\n",
      "epoch: 2 | 90144 / 114272 | training loss: 0.11670994758605957\n",
      "epoch: 2 | 90176 / 114272 | training loss: 0.014298765920102596\n",
      "epoch: 2 | 90208 / 114272 | training loss: 0.009259472601115704\n",
      "epoch: 2 | 90240 / 114272 | training loss: 0.05772162228822708\n",
      "epoch: 2 | 90272 / 114272 | training loss: 0.07555913925170898\n",
      "epoch: 2 | 90304 / 114272 | training loss: 0.11117665469646454\n",
      "epoch: 2 | 90336 / 114272 | training loss: 0.006147794425487518\n",
      "epoch: 2 | 90368 / 114272 | training loss: 0.08330540359020233\n",
      "epoch: 2 | 90400 / 114272 | training loss: 0.014489931985735893\n",
      "epoch: 2 | 90432 / 114272 | training loss: 0.009864818304777145\n",
      "epoch: 2 | 90464 / 114272 | training loss: 0.12210500985383987\n",
      "epoch: 2 | 90496 / 114272 | training loss: 0.17132961750030518\n",
      "epoch: 2 | 90528 / 114272 | training loss: 0.07974471151828766\n",
      "epoch: 2 | 90560 / 114272 | training loss: 0.05129140987992287\n",
      "epoch: 2 | 90592 / 114272 | training loss: 0.38037213683128357\n",
      "epoch: 2 | 90624 / 114272 | training loss: 0.20398558676242828\n",
      "epoch: 2 | 90656 / 114272 | training loss: 0.29006943106651306\n",
      "epoch: 2 | 90688 / 114272 | training loss: 0.05450460687279701\n",
      "epoch: 2 | 90720 / 114272 | training loss: 0.104186050593853\n",
      "epoch: 2 | 90752 / 114272 | training loss: 0.004492140840739012\n",
      "epoch: 2 | 90784 / 114272 | training loss: 0.17989912629127502\n",
      "epoch: 2 | 90816 / 114272 | training loss: 0.020380506291985512\n",
      "epoch: 2 | 90848 / 114272 | training loss: 0.28318342566490173\n",
      "epoch: 2 | 90880 / 114272 | training loss: 0.006414768751710653\n",
      "epoch: 2 | 90912 / 114272 | training loss: 0.049997374415397644\n",
      "epoch: 2 | 90944 / 114272 | training loss: 0.12904316186904907\n",
      "epoch: 2 | 90976 / 114272 | training loss: 0.012683381326496601\n",
      "epoch: 2 | 91008 / 114272 | training loss: 0.016689492389559746\n",
      "epoch: 2 | 91040 / 114272 | training loss: 0.07364664226770401\n",
      "epoch: 2 | 91072 / 114272 | training loss: 0.01652659848332405\n",
      "epoch: 2 | 91104 / 114272 | training loss: 0.015004684217274189\n",
      "epoch: 2 | 91136 / 114272 | training loss: 0.17422331869602203\n",
      "epoch: 2 | 91168 / 114272 | training loss: 0.011917922645807266\n",
      "epoch: 2 | 91200 / 114272 | training loss: 0.18803970515727997\n",
      "epoch: 2 | 91232 / 114272 | training loss: 0.046161308884620667\n",
      "epoch: 2 | 91264 / 114272 | training loss: 0.006762482691556215\n",
      "epoch: 2 | 91296 / 114272 | training loss: 0.09646344184875488\n",
      "epoch: 2 | 91328 / 114272 | training loss: 0.0940915048122406\n",
      "epoch: 2 | 91360 / 114272 | training loss: 0.004817730747163296\n",
      "epoch: 2 | 91392 / 114272 | training loss: 0.03319738805294037\n",
      "epoch: 2 | 91424 / 114272 | training loss: 0.049236003309488297\n",
      "epoch: 2 | 91456 / 114272 | training loss: 0.007596482988446951\n",
      "epoch: 2 | 91488 / 114272 | training loss: 0.01264258474111557\n",
      "epoch: 2 | 91520 / 114272 | training loss: 0.010137107223272324\n",
      "epoch: 2 | 91552 / 114272 | training loss: 0.11026013642549515\n",
      "epoch: 2 | 91584 / 114272 | training loss: 0.02679312601685524\n",
      "epoch: 2 | 91616 / 114272 | training loss: 0.22188711166381836\n",
      "epoch: 2 | 91648 / 114272 | training loss: 0.010266376659274101\n",
      "epoch: 2 | 91680 / 114272 | training loss: 0.007670700084418058\n",
      "epoch: 2 | 91712 / 114272 | training loss: 0.009990428574383259\n",
      "epoch: 2 | 91744 / 114272 | training loss: 0.10838048905134201\n",
      "epoch: 2 | 91776 / 114272 | training loss: 0.0092907240614295\n",
      "epoch: 2 | 91808 / 114272 | training loss: 0.06446036696434021\n",
      "epoch: 2 | 91840 / 114272 | training loss: 0.006010597106069326\n",
      "epoch: 2 | 91872 / 114272 | training loss: 0.10265810787677765\n",
      "epoch: 2 | 91904 / 114272 | training loss: 0.045533668249845505\n",
      "epoch: 2 | 91936 / 114272 | training loss: 0.010384978726506233\n",
      "epoch: 2 | 91968 / 114272 | training loss: 0.2416650950908661\n",
      "epoch: 2 | 92000 / 114272 | training loss: 0.02029532939195633\n",
      "epoch: 2 | 92032 / 114272 | training loss: 0.29039689898490906\n",
      "epoch: 2 | 92064 / 114272 | training loss: 0.011707109399139881\n",
      "epoch: 2 | 92096 / 114272 | training loss: 0.19199363887310028\n",
      "epoch: 2 | 92128 / 114272 | training loss: 0.014707107096910477\n",
      "epoch: 2 | 92160 / 114272 | training loss: 0.010953454300761223\n",
      "epoch: 2 | 92192 / 114272 | training loss: 0.13231024146080017\n",
      "epoch: 2 | 92224 / 114272 | training loss: 0.08268339186906815\n",
      "epoch: 2 | 92256 / 114272 | training loss: 0.007895857095718384\n",
      "epoch: 2 | 92288 / 114272 | training loss: 0.017665620893239975\n",
      "epoch: 2 | 92320 / 114272 | training loss: 0.19027811288833618\n",
      "epoch: 2 | 92352 / 114272 | training loss: 0.24061675369739532\n",
      "epoch: 2 | 92384 / 114272 | training loss: 0.010118132457137108\n",
      "epoch: 2 | 92416 / 114272 | training loss: 0.24502035975456238\n",
      "epoch: 2 | 92448 / 114272 | training loss: 0.010468677617609501\n",
      "epoch: 2 | 92480 / 114272 | training loss: 0.005706298630684614\n",
      "epoch: 2 | 92512 / 114272 | training loss: 0.09174483269453049\n",
      "epoch: 2 | 92544 / 114272 | training loss: 0.11538849025964737\n",
      "epoch: 2 | 92576 / 114272 | training loss: 0.0038692099042236805\n",
      "epoch: 2 | 92608 / 114272 | training loss: 0.007310320623219013\n",
      "epoch: 2 | 92640 / 114272 | training loss: 0.07342421263456345\n",
      "epoch: 2 | 92672 / 114272 | training loss: 0.11336658149957657\n",
      "epoch: 2 | 92704 / 114272 | training loss: 0.2910875082015991\n",
      "epoch: 2 | 92736 / 114272 | training loss: 0.012603905983269215\n",
      "epoch: 2 | 92768 / 114272 | training loss: 0.018045367673039436\n",
      "epoch: 2 | 92800 / 114272 | training loss: 0.08214450627565384\n",
      "epoch: 2 | 92832 / 114272 | training loss: 0.21444836258888245\n",
      "epoch: 2 | 92864 / 114272 | training loss: 0.22039756178855896\n",
      "epoch: 2 | 92896 / 114272 | training loss: 0.009794269688427448\n",
      "epoch: 2 | 92928 / 114272 | training loss: 0.2180117964744568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 92960 / 114272 | training loss: 0.02097293548285961\n",
      "epoch: 2 | 92992 / 114272 | training loss: 0.03083362802863121\n",
      "epoch: 2 | 93024 / 114272 | training loss: 0.020092977210879326\n",
      "epoch: 2 | 93056 / 114272 | training loss: 0.011505691334605217\n",
      "epoch: 2 | 93088 / 114272 | training loss: 0.09012927860021591\n",
      "epoch: 2 | 93120 / 114272 | training loss: 0.11584682762622833\n",
      "epoch: 2 | 93152 / 114272 | training loss: 0.006456214003264904\n",
      "epoch: 2 | 93184 / 114272 | training loss: 0.1509210467338562\n",
      "epoch: 2 | 93216 / 114272 | training loss: 0.012128951027989388\n",
      "epoch: 2 | 93248 / 114272 | training loss: 0.06951390206813812\n",
      "epoch: 2 | 93280 / 114272 | training loss: 0.09156523644924164\n",
      "epoch: 2 | 93312 / 114272 | training loss: 0.034066759049892426\n",
      "epoch: 2 | 93344 / 114272 | training loss: 0.014548258855938911\n",
      "epoch: 2 | 93376 / 114272 | training loss: 0.19611725211143494\n",
      "epoch: 2 | 93408 / 114272 | training loss: 0.015741590410470963\n",
      "epoch: 2 | 93440 / 114272 | training loss: 0.2561153471469879\n",
      "epoch: 2 | 93472 / 114272 | training loss: 0.15684956312179565\n",
      "epoch: 2 | 93504 / 114272 | training loss: 0.24138987064361572\n",
      "epoch: 2 | 93536 / 114272 | training loss: 0.0896666944026947\n",
      "epoch: 2 | 93568 / 114272 | training loss: 0.05767030641436577\n",
      "epoch: 2 | 93600 / 114272 | training loss: 0.014659447595477104\n",
      "epoch: 2 | 93632 / 114272 | training loss: 0.19598232209682465\n",
      "epoch: 2 | 93664 / 114272 | training loss: 0.012511730194091797\n",
      "epoch: 2 | 93696 / 114272 | training loss: 0.0597933754324913\n",
      "epoch: 2 | 93728 / 114272 | training loss: 0.10529482364654541\n",
      "epoch: 2 | 93760 / 114272 | training loss: 0.12476298213005066\n",
      "epoch: 2 | 93792 / 114272 | training loss: 0.008917738683521748\n",
      "epoch: 2 | 93824 / 114272 | training loss: 0.24936515092849731\n",
      "epoch: 2 | 93856 / 114272 | training loss: 0.2351997345685959\n",
      "epoch: 2 | 93888 / 114272 | training loss: 0.010928984731435776\n",
      "epoch: 2 | 93920 / 114272 | training loss: 0.2202640026807785\n",
      "epoch: 2 | 93952 / 114272 | training loss: 0.006582843139767647\n",
      "epoch: 2 | 93984 / 114272 | training loss: 0.009483740665018559\n",
      "epoch: 2 | 94016 / 114272 | training loss: 0.008933400735259056\n",
      "epoch: 2 | 94048 / 114272 | training loss: 0.007476447150111198\n",
      "epoch: 2 | 94080 / 114272 | training loss: 0.016631491482257843\n",
      "epoch: 2 | 94112 / 114272 | training loss: 0.013987120240926743\n",
      "epoch: 2 | 94144 / 114272 | training loss: 0.08550551533699036\n",
      "epoch: 2 | 94176 / 114272 | training loss: 0.12239643931388855\n",
      "epoch: 2 | 94208 / 114272 | training loss: 0.009065424092113972\n",
      "epoch: 2 | 94240 / 114272 | training loss: 0.01626327447593212\n",
      "epoch: 2 | 94272 / 114272 | training loss: 0.14510032534599304\n",
      "epoch: 2 | 94304 / 114272 | training loss: 0.030931970104575157\n",
      "epoch: 2 | 94336 / 114272 | training loss: 0.030600344762206078\n",
      "epoch: 2 | 94368 / 114272 | training loss: 0.27581411600112915\n",
      "epoch: 2 | 94400 / 114272 | training loss: 0.012527955695986748\n",
      "epoch: 2 | 94432 / 114272 | training loss: 0.006151607260107994\n",
      "epoch: 2 | 94464 / 114272 | training loss: 0.10614342242479324\n",
      "epoch: 2 | 94496 / 114272 | training loss: 0.12557366490364075\n",
      "epoch: 2 | 94528 / 114272 | training loss: 0.012681480497121811\n",
      "epoch: 2 | 94560 / 114272 | training loss: 0.006149142049252987\n",
      "epoch: 2 | 94592 / 114272 | training loss: 0.10320278257131577\n",
      "epoch: 2 | 94624 / 114272 | training loss: 0.010445712134242058\n",
      "epoch: 2 | 94656 / 114272 | training loss: 0.02330385148525238\n",
      "epoch: 2 | 94688 / 114272 | training loss: 0.27605319023132324\n",
      "epoch: 2 | 94720 / 114272 | training loss: 0.10113617777824402\n",
      "epoch: 2 | 94752 / 114272 | training loss: 0.0038317013531923294\n",
      "epoch: 2 | 94784 / 114272 | training loss: 0.021772639825940132\n",
      "epoch: 2 | 94816 / 114272 | training loss: 0.16517424583435059\n",
      "epoch: 2 | 94848 / 114272 | training loss: 0.21936899423599243\n",
      "epoch: 2 | 94880 / 114272 | training loss: 0.17343293130397797\n",
      "epoch: 2 | 94912 / 114272 | training loss: 0.007775844540446997\n",
      "epoch: 2 | 94944 / 114272 | training loss: 0.07704843580722809\n",
      "epoch: 2 | 94976 / 114272 | training loss: 0.007161913439631462\n",
      "epoch: 2 | 95008 / 114272 | training loss: 0.016852473840117455\n",
      "epoch: 2 | 95040 / 114272 | training loss: 0.1073944941163063\n",
      "epoch: 2 | 95072 / 114272 | training loss: 0.049363017082214355\n",
      "epoch: 2 | 95104 / 114272 | training loss: 0.026674412190914154\n",
      "epoch: 2 | 95136 / 114272 | training loss: 0.029767246916890144\n",
      "epoch: 2 | 95168 / 114272 | training loss: 0.14046767354011536\n",
      "epoch: 2 | 95200 / 114272 | training loss: 0.1326388120651245\n",
      "epoch: 2 | 95232 / 114272 | training loss: 0.006637641228735447\n",
      "epoch: 2 | 95264 / 114272 | training loss: 0.022397544234991074\n",
      "epoch: 2 | 95296 / 114272 | training loss: 0.04524621739983559\n",
      "epoch: 2 | 95328 / 114272 | training loss: 0.008400474674999714\n",
      "epoch: 2 | 95360 / 114272 | training loss: 0.30224892497062683\n",
      "epoch: 2 | 95392 / 114272 | training loss: 0.033512309193611145\n",
      "epoch: 2 | 95424 / 114272 | training loss: 0.05413123965263367\n",
      "epoch: 2 | 95456 / 114272 | training loss: 0.04370417073369026\n",
      "epoch: 2 | 95488 / 114272 | training loss: 0.04761509969830513\n",
      "epoch: 2 | 95520 / 114272 | training loss: 0.10911384969949722\n",
      "epoch: 2 | 95552 / 114272 | training loss: 0.02406827174127102\n",
      "epoch: 2 | 95584 / 114272 | training loss: 0.26469355821609497\n",
      "epoch: 2 | 95616 / 114272 | training loss: 0.007599383592605591\n",
      "epoch: 2 | 95648 / 114272 | training loss: 0.003570107975974679\n",
      "epoch: 2 | 95680 / 114272 | training loss: 0.08325359225273132\n",
      "epoch: 2 | 95712 / 114272 | training loss: 0.25532835721969604\n",
      "epoch: 2 | 95744 / 114272 | training loss: 0.006879390683025122\n",
      "epoch: 2 | 95776 / 114272 | training loss: 0.16194939613342285\n",
      "epoch: 2 | 95808 / 114272 | training loss: 0.11377507448196411\n",
      "epoch: 2 | 95840 / 114272 | training loss: 0.10382207483053207\n",
      "epoch: 2 | 95872 / 114272 | training loss: 0.04124234616756439\n",
      "epoch: 2 | 95904 / 114272 | training loss: 0.1602196842432022\n",
      "epoch: 2 | 95936 / 114272 | training loss: 0.011233002878725529\n",
      "epoch: 2 | 95968 / 114272 | training loss: 0.005211008246988058\n",
      "epoch: 2 | 96000 / 114272 | training loss: 0.013048183172941208\n",
      "epoch: 2 | 96032 / 114272 | training loss: 0.21405890583992004\n",
      "epoch: 2 | 96064 / 114272 | training loss: 0.16706985235214233\n",
      "epoch: 2 | 96096 / 114272 | training loss: 0.011705215089023113\n",
      "epoch: 2 | 96128 / 114272 | training loss: 0.11531361937522888\n",
      "epoch: 2 | 96160 / 114272 | training loss: 0.08438427001237869\n",
      "epoch: 2 | 96192 / 114272 | training loss: 0.04060829058289528\n",
      "epoch: 2 | 96224 / 114272 | training loss: 0.00764692947268486\n",
      "epoch: 2 | 96256 / 114272 | training loss: 0.06800273805856705\n",
      "epoch: 2 | 96288 / 114272 | training loss: 0.12147995829582214\n",
      "epoch: 2 | 96320 / 114272 | training loss: 0.005472380667924881\n",
      "epoch: 2 | 96352 / 114272 | training loss: 0.029141567647457123\n",
      "epoch: 2 | 96384 / 114272 | training loss: 0.004817160312086344\n",
      "epoch: 2 | 96416 / 114272 | training loss: 0.08798136562108994\n",
      "epoch: 2 | 96448 / 114272 | training loss: 0.04977398365736008\n",
      "epoch: 2 | 96480 / 114272 | training loss: 0.1768830120563507\n",
      "epoch: 2 | 96512 / 114272 | training loss: 0.01969585195183754\n",
      "epoch: 2 | 96544 / 114272 | training loss: 0.16248567402362823\n",
      "epoch: 2 | 96576 / 114272 | training loss: 0.16631777584552765\n",
      "epoch: 2 | 96608 / 114272 | training loss: 0.03478826582431793\n",
      "epoch: 2 | 96640 / 114272 | training loss: 0.12483980506658554\n",
      "epoch: 2 | 96672 / 114272 | training loss: 0.1337260603904724\n",
      "epoch: 2 | 96704 / 114272 | training loss: 0.29404157400131226\n",
      "epoch: 2 | 96736 / 114272 | training loss: 0.1475774198770523\n",
      "epoch: 2 | 96768 / 114272 | training loss: 0.07318904250860214\n",
      "epoch: 2 | 96800 / 114272 | training loss: 0.0076867626048624516\n",
      "epoch: 2 | 96832 / 114272 | training loss: 0.10337810218334198\n",
      "epoch: 2 | 96864 / 114272 | training loss: 0.0046158162876963615\n",
      "epoch: 2 | 96896 / 114272 | training loss: 0.34688353538513184\n",
      "epoch: 2 | 96928 / 114272 | training loss: 0.17439386248588562\n",
      "epoch: 2 | 96960 / 114272 | training loss: 0.012415425851941109\n",
      "epoch: 2 | 96992 / 114272 | training loss: 0.013243027031421661\n",
      "epoch: 2 | 97024 / 114272 | training loss: 0.00759697612375021\n",
      "epoch: 2 | 97056 / 114272 | training loss: 0.1433190405368805\n",
      "epoch: 2 | 97088 / 114272 | training loss: 0.10260327905416489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 97120 / 114272 | training loss: 0.015356140211224556\n",
      "epoch: 2 | 97152 / 114272 | training loss: 0.02136785164475441\n",
      "epoch: 2 | 97184 / 114272 | training loss: 0.03451240062713623\n",
      "epoch: 2 | 97216 / 114272 | training loss: 0.02072766423225403\n",
      "epoch: 2 | 97248 / 114272 | training loss: 0.2333846092224121\n",
      "epoch: 2 | 97280 / 114272 | training loss: 0.014382878318428993\n",
      "epoch: 2 | 97312 / 114272 | training loss: 0.011486188508570194\n",
      "epoch: 2 | 97344 / 114272 | training loss: 0.026736466214060783\n",
      "epoch: 2 | 97376 / 114272 | training loss: 0.14993701875209808\n",
      "epoch: 2 | 97408 / 114272 | training loss: 0.019468145444989204\n",
      "epoch: 2 | 97440 / 114272 | training loss: 0.13960182666778564\n",
      "epoch: 2 | 97472 / 114272 | training loss: 0.07450327277183533\n",
      "epoch: 2 | 97504 / 114272 | training loss: 0.12132270634174347\n",
      "epoch: 2 | 97536 / 114272 | training loss: 0.01473610382527113\n",
      "epoch: 2 | 97568 / 114272 | training loss: 0.007187045644968748\n",
      "epoch: 2 | 97600 / 114272 | training loss: 0.2597765624523163\n",
      "epoch: 2 | 97632 / 114272 | training loss: 0.009674559347331524\n",
      "epoch: 2 | 97664 / 114272 | training loss: 0.09147201478481293\n",
      "epoch: 2 | 97696 / 114272 | training loss: 0.14366210997104645\n",
      "epoch: 2 | 97728 / 114272 | training loss: 0.007235195487737656\n",
      "epoch: 2 | 97760 / 114272 | training loss: 0.29342079162597656\n",
      "epoch: 2 | 97792 / 114272 | training loss: 0.00553101347759366\n",
      "epoch: 2 | 97824 / 114272 | training loss: 0.22714774310588837\n",
      "epoch: 2 | 97856 / 114272 | training loss: 0.11948445439338684\n",
      "epoch: 2 | 97888 / 114272 | training loss: 0.22723133862018585\n",
      "epoch: 2 | 97920 / 114272 | training loss: 0.13301977515220642\n",
      "epoch: 2 | 97952 / 114272 | training loss: 0.08061298727989197\n",
      "epoch: 2 | 97984 / 114272 | training loss: 0.007871647365391254\n",
      "epoch: 2 | 98016 / 114272 | training loss: 0.13222692906856537\n",
      "epoch: 2 | 98048 / 114272 | training loss: 0.06941702216863632\n",
      "epoch: 2 | 98080 / 114272 | training loss: 0.006235688924789429\n",
      "epoch: 2 | 98112 / 114272 | training loss: 0.0132803525775671\n",
      "epoch: 2 | 98144 / 114272 | training loss: 0.009009606204926968\n",
      "epoch: 2 | 98176 / 114272 | training loss: 0.0066587310284376144\n",
      "epoch: 2 | 98208 / 114272 | training loss: 0.06302950531244278\n",
      "epoch: 2 | 98240 / 114272 | training loss: 0.004687300883233547\n",
      "epoch: 2 | 98272 / 114272 | training loss: 0.06794194877147675\n",
      "epoch: 2 | 98304 / 114272 | training loss: 0.15525516867637634\n",
      "epoch: 2 | 98336 / 114272 | training loss: 0.16813839972019196\n",
      "epoch: 2 | 98368 / 114272 | training loss: 0.009661246091127396\n",
      "epoch: 2 | 98400 / 114272 | training loss: 0.01727788709104061\n",
      "epoch: 2 | 98432 / 114272 | training loss: 0.0642833337187767\n",
      "epoch: 2 | 98464 / 114272 | training loss: 0.0039915721863508224\n",
      "epoch: 2 | 98496 / 114272 | training loss: 0.007757945451885462\n",
      "epoch: 2 | 98528 / 114272 | training loss: 0.16501156985759735\n",
      "epoch: 2 | 98560 / 114272 | training loss: 0.012040392495691776\n",
      "epoch: 2 | 98592 / 114272 | training loss: 0.008551953360438347\n",
      "epoch: 2 | 98624 / 114272 | training loss: 0.08096145838499069\n",
      "epoch: 2 | 98656 / 114272 | training loss: 0.20912742614746094\n",
      "epoch: 2 | 98688 / 114272 | training loss: 0.05361190065741539\n",
      "epoch: 2 | 98720 / 114272 | training loss: 0.06708741188049316\n",
      "epoch: 2 | 98752 / 114272 | training loss: 0.0828009694814682\n",
      "epoch: 2 | 98784 / 114272 | training loss: 0.005669275298714638\n",
      "epoch: 2 | 98816 / 114272 | training loss: 0.0048644267953932285\n",
      "epoch: 2 | 98848 / 114272 | training loss: 0.004593180492520332\n",
      "epoch: 2 | 98880 / 114272 | training loss: 0.15053154528141022\n",
      "epoch: 2 | 98912 / 114272 | training loss: 0.026871422305703163\n",
      "epoch: 2 | 98944 / 114272 | training loss: 0.15443278849124908\n",
      "epoch: 2 | 98976 / 114272 | training loss: 0.016612473875284195\n",
      "epoch: 2 | 99008 / 114272 | training loss: 0.0028146812692284584\n",
      "epoch: 2 | 99040 / 114272 | training loss: 0.19578762352466583\n",
      "epoch: 2 | 99072 / 114272 | training loss: 0.06116104871034622\n",
      "epoch: 2 | 99104 / 114272 | training loss: 0.004374637734144926\n",
      "epoch: 2 | 99136 / 114272 | training loss: 0.005933721084147692\n",
      "epoch: 2 | 99168 / 114272 | training loss: 0.01149477157741785\n",
      "epoch: 2 | 99200 / 114272 | training loss: 0.06288343667984009\n",
      "epoch: 2 | 99232 / 114272 | training loss: 0.005225894972681999\n",
      "epoch: 2 | 99264 / 114272 | training loss: 0.011762785725295544\n",
      "epoch: 2 | 99296 / 114272 | training loss: 0.2235247790813446\n",
      "epoch: 2 | 99328 / 114272 | training loss: 0.026226796209812164\n",
      "epoch: 2 | 99360 / 114272 | training loss: 0.10401658713817596\n",
      "epoch: 2 | 99392 / 114272 | training loss: 0.00535993929952383\n",
      "epoch: 2 | 99424 / 114272 | training loss: 0.03235432133078575\n",
      "epoch: 2 | 99456 / 114272 | training loss: 0.010585169307887554\n",
      "epoch: 2 | 99488 / 114272 | training loss: 0.018675420433282852\n",
      "epoch: 2 | 99520 / 114272 | training loss: 0.02188017964363098\n",
      "epoch: 2 | 99552 / 114272 | training loss: 0.13340485095977783\n",
      "epoch: 2 | 99584 / 114272 | training loss: 0.022618794813752174\n",
      "epoch: 2 | 99616 / 114272 | training loss: 0.05889059603214264\n",
      "epoch: 2 | 99648 / 114272 | training loss: 0.008840196765959263\n",
      "epoch: 2 | 99680 / 114272 | training loss: 0.009828454814851284\n",
      "epoch: 2 | 99712 / 114272 | training loss: 0.004565588664263487\n",
      "epoch: 2 | 99744 / 114272 | training loss: 0.11899265646934509\n",
      "epoch: 2 | 99776 / 114272 | training loss: 0.017465226352214813\n",
      "epoch: 2 | 99808 / 114272 | training loss: 0.36382004618644714\n",
      "epoch: 2 | 99840 / 114272 | training loss: 0.003343533258885145\n",
      "epoch: 2 | 99872 / 114272 | training loss: 0.004342203494161367\n",
      "epoch: 2 | 99904 / 114272 | training loss: 0.13961443305015564\n",
      "epoch: 2 | 99936 / 114272 | training loss: 0.1147347167134285\n",
      "epoch: 2 | 99968 / 114272 | training loss: 0.3099697232246399\n",
      "epoch: 2 | 100000 / 114272 | training loss: 0.18080158531665802\n",
      "epoch: 2 | 100032 / 114272 | training loss: 0.019217155873775482\n",
      "epoch: 2 | 100064 / 114272 | training loss: 0.17181630432605743\n",
      "epoch: 2 | 100096 / 114272 | training loss: 0.06694499403238297\n",
      "epoch: 2 | 100128 / 114272 | training loss: 0.08946182578802109\n",
      "epoch: 2 | 100160 / 114272 | training loss: 0.40358683466911316\n",
      "epoch: 2 | 100192 / 114272 | training loss: 0.01192783284932375\n",
      "epoch: 2 | 100224 / 114272 | training loss: 0.0022855158895254135\n",
      "epoch: 2 | 100256 / 114272 | training loss: 0.030134953558444977\n",
      "epoch: 2 | 100288 / 114272 | training loss: 0.007734300568699837\n",
      "epoch: 2 | 100320 / 114272 | training loss: 0.10073968023061752\n",
      "epoch: 2 | 100352 / 114272 | training loss: 0.10126959532499313\n",
      "epoch: 2 | 100384 / 114272 | training loss: 0.1917184293270111\n",
      "epoch: 2 | 100416 / 114272 | training loss: 0.05222143232822418\n",
      "epoch: 2 | 100448 / 114272 | training loss: 0.1600692719221115\n",
      "epoch: 2 | 100480 / 114272 | training loss: 0.03954610601067543\n",
      "epoch: 2 | 100512 / 114272 | training loss: 0.038847994059324265\n",
      "epoch: 2 | 100544 / 114272 | training loss: 0.005295092705637217\n",
      "epoch: 2 | 100576 / 114272 | training loss: 0.07859037816524506\n",
      "epoch: 2 | 100608 / 114272 | training loss: 0.19183440506458282\n",
      "epoch: 2 | 100640 / 114272 | training loss: 0.017749298363924026\n",
      "epoch: 2 | 100672 / 114272 | training loss: 0.1670563668012619\n",
      "epoch: 2 | 100704 / 114272 | training loss: 0.03618305176496506\n",
      "epoch: 2 | 100736 / 114272 | training loss: 0.1567826122045517\n",
      "epoch: 2 | 100768 / 114272 | training loss: 0.005115925334393978\n",
      "epoch: 2 | 100800 / 114272 | training loss: 0.08139727264642715\n",
      "epoch: 2 | 100832 / 114272 | training loss: 0.06000695005059242\n",
      "epoch: 2 | 100864 / 114272 | training loss: 0.23031353950500488\n",
      "epoch: 2 | 100896 / 114272 | training loss: 0.09855328500270844\n",
      "epoch: 2 | 100928 / 114272 | training loss: 0.06963545829057693\n",
      "epoch: 2 | 100960 / 114272 | training loss: 0.011236624792218208\n",
      "epoch: 2 | 100992 / 114272 | training loss: 0.008204006589949131\n",
      "epoch: 2 | 101024 / 114272 | training loss: 0.12523271143436432\n",
      "epoch: 2 | 101056 / 114272 | training loss: 0.013040028512477875\n",
      "epoch: 2 | 101088 / 114272 | training loss: 0.13654033839702606\n",
      "epoch: 2 | 101120 / 114272 | training loss: 0.005882131867110729\n",
      "epoch: 2 | 101152 / 114272 | training loss: 0.009533677250146866\n",
      "epoch: 2 | 101184 / 114272 | training loss: 0.15321148931980133\n",
      "epoch: 2 | 101216 / 114272 | training loss: 0.16468308866024017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 101248 / 114272 | training loss: 0.04606851562857628\n",
      "epoch: 2 | 101280 / 114272 | training loss: 0.04290260374546051\n",
      "epoch: 2 | 101312 / 114272 | training loss: 0.15712538361549377\n",
      "epoch: 2 | 101344 / 114272 | training loss: 0.00675693666562438\n",
      "epoch: 2 | 101376 / 114272 | training loss: 0.2281261533498764\n",
      "epoch: 2 | 101408 / 114272 | training loss: 0.11087508499622345\n",
      "epoch: 2 | 101440 / 114272 | training loss: 0.29613202810287476\n",
      "epoch: 2 | 101472 / 114272 | training loss: 0.012169464491307735\n",
      "epoch: 2 | 101504 / 114272 | training loss: 0.22607186436653137\n",
      "epoch: 2 | 101536 / 114272 | training loss: 0.27861487865448\n",
      "epoch: 2 | 101568 / 114272 | training loss: 0.07460374385118484\n",
      "epoch: 2 | 101600 / 114272 | training loss: 0.10608179122209549\n",
      "epoch: 2 | 101632 / 114272 | training loss: 0.09501881897449493\n",
      "epoch: 2 | 101664 / 114272 | training loss: 0.07259020209312439\n",
      "epoch: 2 | 101696 / 114272 | training loss: 0.18626335263252258\n",
      "epoch: 2 | 101728 / 114272 | training loss: 0.019473960623145103\n",
      "epoch: 2 | 101760 / 114272 | training loss: 0.006815850269049406\n",
      "epoch: 2 | 101792 / 114272 | training loss: 0.007006821688264608\n",
      "epoch: 2 | 101824 / 114272 | training loss: 0.03522519767284393\n",
      "epoch: 2 | 101856 / 114272 | training loss: 0.012361948378384113\n",
      "epoch: 2 | 101888 / 114272 | training loss: 0.0038752167019993067\n",
      "epoch: 2 | 101920 / 114272 | training loss: 0.02871152199804783\n",
      "epoch: 2 | 101952 / 114272 | training loss: 0.13589663803577423\n",
      "epoch: 2 | 101984 / 114272 | training loss: 0.024369949474930763\n",
      "epoch: 2 | 102016 / 114272 | training loss: 0.005373392719775438\n",
      "epoch: 2 | 102048 / 114272 | training loss: 0.14262650907039642\n",
      "epoch: 2 | 102080 / 114272 | training loss: 0.1134234219789505\n",
      "epoch: 2 | 102112 / 114272 | training loss: 0.34600046277046204\n",
      "epoch: 2 | 102144 / 114272 | training loss: 0.1938384473323822\n",
      "epoch: 2 | 102176 / 114272 | training loss: 0.0051917084492743015\n",
      "epoch: 2 | 102208 / 114272 | training loss: 0.13849064707756042\n",
      "epoch: 2 | 102240 / 114272 | training loss: 0.27479249238967896\n",
      "epoch: 2 | 102272 / 114272 | training loss: 0.14271564781665802\n",
      "epoch: 2 | 102304 / 114272 | training loss: 0.1326068788766861\n",
      "epoch: 2 | 102336 / 114272 | training loss: 0.01892971806228161\n",
      "epoch: 2 | 102368 / 114272 | training loss: 0.2275526523590088\n",
      "epoch: 2 | 102400 / 114272 | training loss: 0.09858831018209457\n",
      "epoch: 2 | 102432 / 114272 | training loss: 0.008129722438752651\n",
      "epoch: 2 | 102464 / 114272 | training loss: 0.15993914008140564\n",
      "epoch: 2 | 102496 / 114272 | training loss: 0.21416200697422028\n",
      "epoch: 2 | 102528 / 114272 | training loss: 0.2274944931268692\n",
      "epoch: 2 | 102560 / 114272 | training loss: 0.00805902760475874\n",
      "epoch: 2 | 102592 / 114272 | training loss: 0.1747770458459854\n",
      "epoch: 2 | 102624 / 114272 | training loss: 0.0881231352686882\n",
      "epoch: 2 | 102656 / 114272 | training loss: 0.013617878779768944\n",
      "epoch: 2 | 102688 / 114272 | training loss: 0.00986891146749258\n",
      "epoch: 2 | 102720 / 114272 | training loss: 0.017362840473651886\n",
      "epoch: 2 | 102752 / 114272 | training loss: 0.01118384301662445\n",
      "epoch: 2 | 102784 / 114272 | training loss: 0.009557466022670269\n",
      "epoch: 2 | 102816 / 114272 | training loss: 0.11680151522159576\n",
      "epoch: 2 | 102848 / 114272 | training loss: 0.06519340723752975\n",
      "epoch: 2 | 102880 / 114272 | training loss: 0.020857417955994606\n",
      "epoch: 2 | 102912 / 114272 | training loss: 0.10823128372430801\n",
      "epoch: 2 | 102944 / 114272 | training loss: 0.009365128353238106\n",
      "epoch: 2 | 102976 / 114272 | training loss: 0.11530892550945282\n",
      "epoch: 2 | 103008 / 114272 | training loss: 0.0032617119140923023\n",
      "epoch: 2 | 103040 / 114272 | training loss: 0.0284670852124691\n",
      "epoch: 2 | 103072 / 114272 | training loss: 0.0067637343890964985\n",
      "epoch: 2 | 103104 / 114272 | training loss: 0.19027957320213318\n",
      "epoch: 2 | 103136 / 114272 | training loss: 0.004825002979487181\n",
      "epoch: 2 | 103168 / 114272 | training loss: 0.035276271402835846\n",
      "epoch: 2 | 103200 / 114272 | training loss: 0.2754519283771515\n",
      "epoch: 2 | 103232 / 114272 | training loss: 0.0025639990344643593\n",
      "epoch: 2 | 103264 / 114272 | training loss: 0.003599156392738223\n",
      "epoch: 2 | 103296 / 114272 | training loss: 0.23070773482322693\n",
      "epoch: 2 | 103328 / 114272 | training loss: 0.052101828157901764\n",
      "epoch: 2 | 103360 / 114272 | training loss: 0.15322959423065186\n",
      "epoch: 2 | 103392 / 114272 | training loss: 0.1660066395998001\n",
      "epoch: 2 | 103424 / 114272 | training loss: 0.008706197142601013\n",
      "epoch: 2 | 103456 / 114272 | training loss: 0.1338951289653778\n",
      "epoch: 2 | 103488 / 114272 | training loss: 0.00664652232080698\n",
      "epoch: 2 | 103520 / 114272 | training loss: 0.006934077944606543\n",
      "epoch: 2 | 103552 / 114272 | training loss: 0.008588864468038082\n",
      "epoch: 2 | 103584 / 114272 | training loss: 0.06489817053079605\n",
      "epoch: 2 | 103616 / 114272 | training loss: 0.04126175865530968\n",
      "epoch: 2 | 103648 / 114272 | training loss: 0.005067146383225918\n",
      "epoch: 2 | 103680 / 114272 | training loss: 0.004177350550889969\n",
      "epoch: 2 | 103712 / 114272 | training loss: 0.37245872616767883\n",
      "epoch: 2 | 103744 / 114272 | training loss: 0.24005945026874542\n",
      "epoch: 2 | 103776 / 114272 | training loss: 0.04711009934544563\n",
      "epoch: 2 | 103808 / 114272 | training loss: 0.04870188236236572\n",
      "epoch: 2 | 103840 / 114272 | training loss: 0.08295943588018417\n",
      "epoch: 2 | 103872 / 114272 | training loss: 0.3480341136455536\n",
      "epoch: 2 | 103904 / 114272 | training loss: 0.05731742084026337\n",
      "epoch: 2 | 103936 / 114272 | training loss: 0.015296446159482002\n",
      "epoch: 2 | 103968 / 114272 | training loss: 0.25898605585098267\n",
      "epoch: 2 | 104000 / 114272 | training loss: 0.2310565859079361\n",
      "epoch: 2 | 104032 / 114272 | training loss: 0.40064162015914917\n",
      "epoch: 2 | 104064 / 114272 | training loss: 0.0685577541589737\n",
      "epoch: 2 | 104096 / 114272 | training loss: 0.004127582535147667\n",
      "epoch: 2 | 104128 / 114272 | training loss: 0.22691717743873596\n",
      "epoch: 2 | 104160 / 114272 | training loss: 0.2711111903190613\n",
      "epoch: 2 | 104192 / 114272 | training loss: 0.3309346139431\n",
      "epoch: 2 | 104224 / 114272 | training loss: 0.10945075005292892\n",
      "epoch: 2 | 104256 / 114272 | training loss: 0.045861806720495224\n",
      "epoch: 2 | 104288 / 114272 | training loss: 0.11983861029148102\n",
      "epoch: 2 | 104320 / 114272 | training loss: 0.01016127411276102\n",
      "epoch: 2 | 104352 / 114272 | training loss: 0.18348358571529388\n",
      "epoch: 2 | 104384 / 114272 | training loss: 0.00510956346988678\n",
      "epoch: 2 | 104416 / 114272 | training loss: 0.16278018057346344\n",
      "epoch: 2 | 104448 / 114272 | training loss: 0.005548007786273956\n",
      "epoch: 2 | 104480 / 114272 | training loss: 0.04875741899013519\n",
      "epoch: 2 | 104512 / 114272 | training loss: 0.010129456408321857\n",
      "epoch: 2 | 104544 / 114272 | training loss: 0.0893700122833252\n",
      "epoch: 2 | 104576 / 114272 | training loss: 0.08407551795244217\n",
      "epoch: 2 | 104608 / 114272 | training loss: 0.008221677504479885\n",
      "epoch: 2 | 104640 / 114272 | training loss: 0.025466883555054665\n",
      "epoch: 2 | 104672 / 114272 | training loss: 0.03287738561630249\n",
      "epoch: 2 | 104704 / 114272 | training loss: 0.020434698089957237\n",
      "epoch: 2 | 104736 / 114272 | training loss: 0.22900432348251343\n",
      "epoch: 2 | 104768 / 114272 | training loss: 0.07152990251779556\n",
      "epoch: 2 | 104800 / 114272 | training loss: 0.011285415850579739\n",
      "epoch: 2 | 104832 / 114272 | training loss: 0.00873501505702734\n",
      "epoch: 2 | 104864 / 114272 | training loss: 0.006891285534948111\n",
      "epoch: 2 | 104896 / 114272 | training loss: 0.0703771561384201\n",
      "epoch: 2 | 104928 / 114272 | training loss: 0.04644249379634857\n",
      "epoch: 2 | 104960 / 114272 | training loss: 0.11691074073314667\n",
      "epoch: 2 | 104992 / 114272 | training loss: 0.006674540229141712\n",
      "epoch: 2 | 105024 / 114272 | training loss: 0.14070457220077515\n",
      "epoch: 2 | 105056 / 114272 | training loss: 0.015723329037427902\n",
      "epoch: 2 | 105088 / 114272 | training loss: 0.04306790605187416\n",
      "epoch: 2 | 105120 / 114272 | training loss: 0.058537550270557404\n",
      "epoch: 2 | 105152 / 114272 | training loss: 0.04360217601060867\n",
      "epoch: 2 | 105184 / 114272 | training loss: 0.023155342787504196\n",
      "epoch: 2 | 105216 / 114272 | training loss: 0.11324771493673325\n",
      "epoch: 2 | 105248 / 114272 | training loss: 0.08486223965883255\n",
      "epoch: 2 | 105280 / 114272 | training loss: 0.10258135944604874\n",
      "epoch: 2 | 105312 / 114272 | training loss: 0.1960369050502777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 105344 / 114272 | training loss: 0.029963869601488113\n",
      "epoch: 2 | 105376 / 114272 | training loss: 0.05512765422463417\n",
      "epoch: 2 | 105408 / 114272 | training loss: 0.1559375822544098\n",
      "epoch: 2 | 105440 / 114272 | training loss: 0.005722857546061277\n",
      "epoch: 2 | 105472 / 114272 | training loss: 0.0846298336982727\n",
      "epoch: 2 | 105504 / 114272 | training loss: 0.38794800639152527\n",
      "epoch: 2 | 105536 / 114272 | training loss: 0.00992976687848568\n",
      "epoch: 2 | 105568 / 114272 | training loss: 0.10999416559934616\n",
      "epoch: 2 | 105600 / 114272 | training loss: 0.3224998116493225\n",
      "epoch: 2 | 105632 / 114272 | training loss: 0.0029869647696614265\n",
      "epoch: 2 | 105664 / 114272 | training loss: 0.01744035817682743\n",
      "epoch: 2 | 105696 / 114272 | training loss: 0.08025792241096497\n",
      "epoch: 2 | 105728 / 114272 | training loss: 0.18617822229862213\n",
      "epoch: 2 | 105760 / 114272 | training loss: 0.0067505440674722195\n",
      "epoch: 2 | 105792 / 114272 | training loss: 0.046515535563230515\n",
      "epoch: 2 | 105824 / 114272 | training loss: 0.12786199152469635\n",
      "epoch: 2 | 105856 / 114272 | training loss: 0.11759483814239502\n",
      "epoch: 2 | 105888 / 114272 | training loss: 0.04268990457057953\n",
      "epoch: 2 | 105920 / 114272 | training loss: 0.08850929886102676\n",
      "epoch: 2 | 105952 / 114272 | training loss: 0.1947229653596878\n",
      "epoch: 2 | 105984 / 114272 | training loss: 0.0454261489212513\n",
      "epoch: 2 | 106016 / 114272 | training loss: 0.10793941468000412\n",
      "epoch: 2 | 106048 / 114272 | training loss: 0.173850417137146\n",
      "epoch: 2 | 106080 / 114272 | training loss: 0.050238389521837234\n",
      "epoch: 2 | 106112 / 114272 | training loss: 0.27692973613739014\n",
      "epoch: 2 | 106144 / 114272 | training loss: 0.008770396001636982\n",
      "epoch: 2 | 106176 / 114272 | training loss: 0.16947852075099945\n",
      "epoch: 2 | 106208 / 114272 | training loss: 0.03901531174778938\n",
      "epoch: 2 | 106240 / 114272 | training loss: 0.04156722500920296\n",
      "epoch: 2 | 106272 / 114272 | training loss: 0.1283465474843979\n",
      "epoch: 2 | 106304 / 114272 | training loss: 0.012565325945615768\n",
      "epoch: 2 | 106336 / 114272 | training loss: 0.18687275052070618\n",
      "epoch: 2 | 106368 / 114272 | training loss: 0.04798763617873192\n",
      "epoch: 2 | 106400 / 114272 | training loss: 0.014884207397699356\n",
      "epoch: 2 | 106432 / 114272 | training loss: 0.009569067507982254\n",
      "epoch: 2 | 106464 / 114272 | training loss: 0.007985921576619148\n",
      "epoch: 2 | 106496 / 114272 | training loss: 0.11728781461715698\n",
      "epoch: 2 | 106528 / 114272 | training loss: 0.09186369925737381\n",
      "epoch: 2 | 106560 / 114272 | training loss: 0.16799131035804749\n",
      "epoch: 2 | 106592 / 114272 | training loss: 0.07088495045900345\n",
      "epoch: 2 | 106624 / 114272 | training loss: 0.1039692834019661\n",
      "epoch: 2 | 106656 / 114272 | training loss: 0.0858607366681099\n",
      "epoch: 2 | 106688 / 114272 | training loss: 0.026947233825922012\n",
      "epoch: 2 | 106720 / 114272 | training loss: 0.14657793939113617\n",
      "epoch: 2 | 106752 / 114272 | training loss: 0.015390701591968536\n",
      "epoch: 2 | 106784 / 114272 | training loss: 0.18044507503509521\n",
      "epoch: 2 | 106816 / 114272 | training loss: 0.05695391073822975\n",
      "epoch: 2 | 106848 / 114272 | training loss: 0.017789456993341446\n",
      "epoch: 2 | 106880 / 114272 | training loss: 0.011081568896770477\n",
      "epoch: 2 | 106912 / 114272 | training loss: 0.009096830151975155\n",
      "epoch: 2 | 106944 / 114272 | training loss: 0.007778927683830261\n",
      "epoch: 2 | 106976 / 114272 | training loss: 0.10276125371456146\n",
      "epoch: 2 | 107008 / 114272 | training loss: 0.11166132986545563\n",
      "epoch: 2 | 107040 / 114272 | training loss: 0.3119232952594757\n",
      "epoch: 2 | 107072 / 114272 | training loss: 0.008254325948655605\n",
      "epoch: 2 | 107104 / 114272 | training loss: 0.008402496576309204\n",
      "epoch: 2 | 107136 / 114272 | training loss: 0.011984762735664845\n",
      "epoch: 2 | 107168 / 114272 | training loss: 0.011920527555048466\n",
      "epoch: 2 | 107200 / 114272 | training loss: 0.05604115501046181\n",
      "epoch: 2 | 107232 / 114272 | training loss: 0.05774897336959839\n",
      "epoch: 2 | 107264 / 114272 | training loss: 0.008355295285582542\n",
      "epoch: 2 | 107296 / 114272 | training loss: 0.008369144052267075\n",
      "epoch: 2 | 107328 / 114272 | training loss: 0.02129845693707466\n",
      "epoch: 2 | 107360 / 114272 | training loss: 0.03878564015030861\n",
      "epoch: 2 | 107392 / 114272 | training loss: 0.005564281716942787\n",
      "epoch: 2 | 107424 / 114272 | training loss: 0.22302621603012085\n",
      "epoch: 2 | 107456 / 114272 | training loss: 0.1286158561706543\n",
      "epoch: 2 | 107488 / 114272 | training loss: 0.055658288300037384\n",
      "epoch: 2 | 107520 / 114272 | training loss: 0.07782278209924698\n",
      "epoch: 2 | 107552 / 114272 | training loss: 0.029591785743832588\n",
      "epoch: 2 | 107584 / 114272 | training loss: 0.007765127345919609\n",
      "epoch: 2 | 107616 / 114272 | training loss: 0.2396349459886551\n",
      "epoch: 2 | 107648 / 114272 | training loss: 0.09086358547210693\n",
      "epoch: 2 | 107680 / 114272 | training loss: 0.17703108489513397\n",
      "epoch: 2 | 107712 / 114272 | training loss: 0.11065638810396194\n",
      "epoch: 2 | 107744 / 114272 | training loss: 0.006214097607880831\n",
      "epoch: 2 | 107776 / 114272 | training loss: 0.010141536593437195\n",
      "epoch: 2 | 107808 / 114272 | training loss: 0.04408493638038635\n",
      "epoch: 2 | 107840 / 114272 | training loss: 0.22337661683559418\n",
      "epoch: 2 | 107872 / 114272 | training loss: 0.07973970472812653\n",
      "epoch: 2 | 107904 / 114272 | training loss: 0.01055687852203846\n",
      "epoch: 2 | 107936 / 114272 | training loss: 0.041219841688871384\n",
      "epoch: 2 | 107968 / 114272 | training loss: 0.09995118528604507\n",
      "epoch: 2 | 108000 / 114272 | training loss: 0.17384172976016998\n",
      "epoch: 2 | 108032 / 114272 | training loss: 0.005079890601336956\n",
      "epoch: 2 | 108064 / 114272 | training loss: 0.004980797879397869\n",
      "epoch: 2 | 108096 / 114272 | training loss: 0.08800772577524185\n",
      "epoch: 2 | 108128 / 114272 | training loss: 0.017362188547849655\n",
      "epoch: 2 | 108160 / 114272 | training loss: 0.07593101263046265\n",
      "epoch: 2 | 108192 / 114272 | training loss: 0.023504681885242462\n",
      "epoch: 2 | 108224 / 114272 | training loss: 0.30588993430137634\n",
      "epoch: 2 | 108256 / 114272 | training loss: 0.1488669514656067\n",
      "epoch: 2 | 108288 / 114272 | training loss: 0.02153848111629486\n",
      "epoch: 2 | 108320 / 114272 | training loss: 0.26851198077201843\n",
      "epoch: 2 | 108352 / 114272 | training loss: 0.027158508077263832\n",
      "epoch: 2 | 108384 / 114272 | training loss: 0.014483335427939892\n",
      "epoch: 2 | 108416 / 114272 | training loss: 0.19946058094501495\n",
      "epoch: 2 | 108448 / 114272 | training loss: 0.1342613250017166\n",
      "epoch: 2 | 108480 / 114272 | training loss: 0.02486458420753479\n",
      "epoch: 2 | 108512 / 114272 | training loss: 0.01612192578613758\n",
      "epoch: 2 | 108544 / 114272 | training loss: 0.23030102252960205\n",
      "epoch: 2 | 108576 / 114272 | training loss: 0.005727238953113556\n",
      "epoch: 2 | 108608 / 114272 | training loss: 0.09162209182977676\n",
      "epoch: 2 | 108640 / 114272 | training loss: 0.22618284821510315\n",
      "epoch: 2 | 108672 / 114272 | training loss: 0.004497145768254995\n",
      "epoch: 2 | 108704 / 114272 | training loss: 0.0039026832673698664\n",
      "epoch: 2 | 108736 / 114272 | training loss: 0.005546162836253643\n",
      "epoch: 2 | 108768 / 114272 | training loss: 0.023000504821538925\n",
      "epoch: 2 | 108800 / 114272 | training loss: 0.06386560946702957\n",
      "epoch: 2 | 108832 / 114272 | training loss: 0.020943589508533478\n",
      "epoch: 2 | 108864 / 114272 | training loss: 0.1365964114665985\n",
      "epoch: 2 | 108896 / 114272 | training loss: 0.02648066356778145\n",
      "epoch: 2 | 108928 / 114272 | training loss: 0.043713077902793884\n",
      "epoch: 2 | 108960 / 114272 | training loss: 0.04380491375923157\n",
      "epoch: 2 | 108992 / 114272 | training loss: 0.09912233799695969\n",
      "epoch: 2 | 109024 / 114272 | training loss: 0.00315097626298666\n",
      "epoch: 2 | 109056 / 114272 | training loss: 0.012143129482865334\n",
      "epoch: 2 | 109088 / 114272 | training loss: 0.028445430099964142\n",
      "epoch: 2 | 109120 / 114272 | training loss: 0.18395838141441345\n",
      "epoch: 2 | 109152 / 114272 | training loss: 0.023777678608894348\n",
      "epoch: 2 | 109184 / 114272 | training loss: 0.0029168184846639633\n",
      "epoch: 2 | 109216 / 114272 | training loss: 0.18264520168304443\n",
      "epoch: 2 | 109248 / 114272 | training loss: 0.09886901080608368\n",
      "epoch: 2 | 109280 / 114272 | training loss: 0.006355880293995142\n",
      "epoch: 2 | 109312 / 114272 | training loss: 0.131837397813797\n",
      "epoch: 2 | 109344 / 114272 | training loss: 0.008316381834447384\n",
      "epoch: 2 | 109376 / 114272 | training loss: 0.003129680873826146\n",
      "epoch: 2 | 109408 / 114272 | training loss: 0.026655735448002815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 109440 / 114272 | training loss: 0.3004652261734009\n",
      "epoch: 2 | 109472 / 114272 | training loss: 0.1935529261827469\n",
      "epoch: 2 | 109504 / 114272 | training loss: 0.1255105435848236\n",
      "epoch: 2 | 109536 / 114272 | training loss: 0.1094568744301796\n",
      "epoch: 2 | 109568 / 114272 | training loss: 0.06743024289608002\n",
      "epoch: 2 | 109600 / 114272 | training loss: 0.07914070785045624\n",
      "epoch: 2 | 109632 / 114272 | training loss: 0.0198441781103611\n",
      "epoch: 2 | 109664 / 114272 | training loss: 0.028815491124987602\n",
      "epoch: 2 | 109696 / 114272 | training loss: 0.009843605570495129\n",
      "epoch: 2 | 109728 / 114272 | training loss: 0.02667541243135929\n",
      "epoch: 2 | 109760 / 114272 | training loss: 0.014825583435595036\n",
      "epoch: 2 | 109792 / 114272 | training loss: 0.06040888652205467\n",
      "epoch: 2 | 109824 / 114272 | training loss: 0.1979854553937912\n",
      "epoch: 2 | 109856 / 114272 | training loss: 0.1214662566781044\n",
      "epoch: 2 | 109888 / 114272 | training loss: 0.03162448853254318\n",
      "epoch: 2 | 109920 / 114272 | training loss: 0.03643643110990524\n",
      "epoch: 2 | 109952 / 114272 | training loss: 0.07606706768274307\n",
      "epoch: 2 | 109984 / 114272 | training loss: 0.288614958524704\n",
      "epoch: 2 | 110016 / 114272 | training loss: 0.09199481457471848\n",
      "epoch: 2 | 110048 / 114272 | training loss: 0.008509445004165173\n",
      "epoch: 2 | 110080 / 114272 | training loss: 0.15536029636859894\n",
      "epoch: 2 | 110112 / 114272 | training loss: 0.1245390921831131\n",
      "epoch: 2 | 110144 / 114272 | training loss: 0.20668455958366394\n",
      "epoch: 2 | 110176 / 114272 | training loss: 0.08404506742954254\n",
      "epoch: 2 | 110208 / 114272 | training loss: 0.3791216313838959\n",
      "epoch: 2 | 110240 / 114272 | training loss: 0.09587416052818298\n",
      "epoch: 2 | 110272 / 114272 | training loss: 0.014671728946268559\n",
      "epoch: 2 | 110304 / 114272 | training loss: 0.002782768104225397\n",
      "epoch: 2 | 110336 / 114272 | training loss: 0.054532743990421295\n",
      "epoch: 2 | 110368 / 114272 | training loss: 0.116311214864254\n",
      "epoch: 2 | 110400 / 114272 | training loss: 0.13179302215576172\n",
      "epoch: 2 | 110432 / 114272 | training loss: 0.12344563007354736\n",
      "epoch: 2 | 110464 / 114272 | training loss: 0.019496673718094826\n",
      "epoch: 2 | 110496 / 114272 | training loss: 0.1309468299150467\n",
      "epoch: 2 | 110528 / 114272 | training loss: 0.012437639757990837\n",
      "epoch: 2 | 110560 / 114272 | training loss: 0.026070358231663704\n",
      "epoch: 2 | 110592 / 114272 | training loss: 0.00706207612529397\n",
      "epoch: 2 | 110624 / 114272 | training loss: 0.01498078741133213\n",
      "epoch: 2 | 110656 / 114272 | training loss: 0.014451970346271992\n",
      "epoch: 2 | 110688 / 114272 | training loss: 0.1422678679227829\n",
      "epoch: 2 | 110720 / 114272 | training loss: 0.039333850145339966\n",
      "epoch: 2 | 110752 / 114272 | training loss: 0.04506903514266014\n",
      "epoch: 2 | 110784 / 114272 | training loss: 0.07095164805650711\n",
      "epoch: 2 | 110816 / 114272 | training loss: 0.22359253466129303\n",
      "epoch: 2 | 110848 / 114272 | training loss: 0.010240050032734871\n",
      "epoch: 2 | 110880 / 114272 | training loss: 0.009525473229587078\n",
      "epoch: 2 | 110912 / 114272 | training loss: 0.06040656194090843\n",
      "epoch: 2 | 110944 / 114272 | training loss: 0.24651527404785156\n",
      "epoch: 2 | 110976 / 114272 | training loss: 0.040860723704099655\n",
      "epoch: 2 | 111008 / 114272 | training loss: 0.011924576945602894\n",
      "epoch: 2 | 111040 / 114272 | training loss: 0.12475629895925522\n",
      "epoch: 2 | 111072 / 114272 | training loss: 0.006217578426003456\n",
      "epoch: 2 | 111104 / 114272 | training loss: 0.016558127477765083\n",
      "epoch: 2 | 111136 / 114272 | training loss: 0.0024719983339309692\n",
      "epoch: 2 | 111168 / 114272 | training loss: 0.10288973152637482\n",
      "epoch: 2 | 111200 / 114272 | training loss: 0.1322898119688034\n",
      "epoch: 2 | 111232 / 114272 | training loss: 0.10639876872301102\n",
      "epoch: 2 | 111264 / 114272 | training loss: 0.17564734816551208\n",
      "epoch: 2 | 111296 / 114272 | training loss: 0.003008239669725299\n",
      "epoch: 2 | 111328 / 114272 | training loss: 0.2579852044582367\n",
      "epoch: 2 | 111360 / 114272 | training loss: 0.22236129641532898\n",
      "epoch: 2 | 111392 / 114272 | training loss: 0.20590944588184357\n",
      "epoch: 2 | 111424 / 114272 | training loss: 0.3705224394798279\n",
      "epoch: 2 | 111456 / 114272 | training loss: 0.2975430488586426\n",
      "epoch: 2 | 111488 / 114272 | training loss: 0.14699651300907135\n",
      "epoch: 2 | 111520 / 114272 | training loss: 0.1732960194349289\n",
      "epoch: 2 | 111552 / 114272 | training loss: 0.1781318187713623\n",
      "epoch: 2 | 111584 / 114272 | training loss: 0.07960308343172073\n",
      "epoch: 2 | 111616 / 114272 | training loss: 0.022594237700104713\n",
      "epoch: 2 | 111648 / 114272 | training loss: 0.009184916503727436\n",
      "epoch: 2 | 111680 / 114272 | training loss: 0.035920970141887665\n",
      "epoch: 2 | 111712 / 114272 | training loss: 0.012433872558176517\n",
      "epoch: 2 | 111744 / 114272 | training loss: 0.011498203501105309\n",
      "epoch: 2 | 111776 / 114272 | training loss: 0.004994427319616079\n",
      "epoch: 2 | 111808 / 114272 | training loss: 0.005695601459592581\n",
      "epoch: 2 | 111840 / 114272 | training loss: 0.007142356596887112\n",
      "epoch: 2 | 111872 / 114272 | training loss: 0.0474671944975853\n",
      "epoch: 2 | 111904 / 114272 | training loss: 0.09485411643981934\n",
      "epoch: 2 | 111936 / 114272 | training loss: 0.2646144926548004\n",
      "epoch: 2 | 111968 / 114272 | training loss: 0.020042145624756813\n",
      "epoch: 2 | 112000 / 114272 | training loss: 0.15352189540863037\n",
      "epoch: 2 | 112032 / 114272 | training loss: 0.03408635035157204\n",
      "epoch: 2 | 112064 / 114272 | training loss: 0.05457546189427376\n",
      "epoch: 2 | 112096 / 114272 | training loss: 0.14265018701553345\n",
      "epoch: 2 | 112128 / 114272 | training loss: 0.13754157721996307\n",
      "epoch: 2 | 112160 / 114272 | training loss: 0.08070022612810135\n",
      "epoch: 2 | 112192 / 114272 | training loss: 0.1563498079776764\n",
      "epoch: 2 | 112224 / 114272 | training loss: 0.08330387622117996\n",
      "epoch: 2 | 112256 / 114272 | training loss: 0.00694088451564312\n",
      "epoch: 2 | 112288 / 114272 | training loss: 0.14237815141677856\n",
      "epoch: 2 | 112320 / 114272 | training loss: 0.11083267629146576\n",
      "epoch: 2 | 112352 / 114272 | training loss: 0.016548767685890198\n",
      "epoch: 2 | 112384 / 114272 | training loss: 0.09988559782505035\n",
      "epoch: 2 | 112416 / 114272 | training loss: 0.009795727208256721\n",
      "epoch: 2 | 112448 / 114272 | training loss: 0.008315371349453926\n",
      "epoch: 2 | 112480 / 114272 | training loss: 0.06209513545036316\n",
      "epoch: 2 | 112512 / 114272 | training loss: 0.12237201631069183\n",
      "epoch: 2 | 112544 / 114272 | training loss: 0.006855209358036518\n",
      "epoch: 2 | 112576 / 114272 | training loss: 0.14296163618564606\n",
      "epoch: 2 | 112608 / 114272 | training loss: 0.009100514464080334\n",
      "epoch: 2 | 112640 / 114272 | training loss: 0.03564848750829697\n",
      "epoch: 2 | 112672 / 114272 | training loss: 0.07471542805433273\n",
      "epoch: 2 | 112704 / 114272 | training loss: 0.016137653961777687\n",
      "epoch: 2 | 112736 / 114272 | training loss: 0.01903223805129528\n",
      "epoch: 2 | 112768 / 114272 | training loss: 0.010028201155364513\n",
      "epoch: 2 | 112800 / 114272 | training loss: 0.02542402222752571\n",
      "epoch: 2 | 112832 / 114272 | training loss: 0.08953728526830673\n",
      "epoch: 2 | 112864 / 114272 | training loss: 0.0854877158999443\n",
      "epoch: 2 | 112896 / 114272 | training loss: 0.19516153633594513\n",
      "epoch: 2 | 112928 / 114272 | training loss: 0.06902476400136948\n",
      "epoch: 2 | 112960 / 114272 | training loss: 0.00800982303917408\n",
      "epoch: 2 | 112992 / 114272 | training loss: 0.0037779738195240498\n",
      "epoch: 2 | 113024 / 114272 | training loss: 0.3079359829425812\n",
      "epoch: 2 | 113056 / 114272 | training loss: 0.00787486881017685\n",
      "epoch: 2 | 113088 / 114272 | training loss: 0.06527429819107056\n",
      "epoch: 2 | 113120 / 114272 | training loss: 0.1314721554517746\n",
      "epoch: 2 | 113152 / 114272 | training loss: 0.004121613223105669\n",
      "epoch: 2 | 113184 / 114272 | training loss: 0.0014833853347226977\n",
      "epoch: 2 | 113216 / 114272 | training loss: 0.09487669914960861\n",
      "epoch: 2 | 113248 / 114272 | training loss: 0.23685023188591003\n",
      "epoch: 2 | 113280 / 114272 | training loss: 0.10675664991140366\n",
      "epoch: 2 | 113312 / 114272 | training loss: 0.005937460344284773\n",
      "epoch: 2 | 113344 / 114272 | training loss: 0.08776327967643738\n",
      "epoch: 2 | 113376 / 114272 | training loss: 0.02796059474349022\n",
      "epoch: 2 | 113408 / 114272 | training loss: 0.00666265981271863\n",
      "epoch: 2 | 113440 / 114272 | training loss: 0.3334371745586395\n",
      "epoch: 2 | 113472 / 114272 | training loss: 0.07215160876512527\n",
      "epoch: 2 | 113504 / 114272 | training loss: 0.04627247527241707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | 113536 / 114272 | training loss: 0.05851711705327034\n",
      "epoch: 2 | 113568 / 114272 | training loss: 0.009382610209286213\n",
      "epoch: 2 | 113600 / 114272 | training loss: 0.0041696918196976185\n",
      "epoch: 2 | 113632 / 114272 | training loss: 0.02210165187716484\n",
      "epoch: 2 | 113664 / 114272 | training loss: 0.07441861927509308\n",
      "epoch: 2 | 113696 / 114272 | training loss: 0.009004231542348862\n",
      "epoch: 2 | 113728 / 114272 | training loss: 0.10014274716377258\n",
      "epoch: 2 | 113760 / 114272 | training loss: 0.09312444925308228\n",
      "epoch: 2 | 113792 / 114272 | training loss: 0.015816034749150276\n",
      "epoch: 2 | 113824 / 114272 | training loss: 0.1128905862569809\n",
      "epoch: 2 | 113856 / 114272 | training loss: 0.25195157527923584\n",
      "epoch: 2 | 113888 / 114272 | training loss: 0.11090350151062012\n",
      "epoch: 2 | 113920 / 114272 | training loss: 0.1759823113679886\n",
      "epoch: 2 | 113952 / 114272 | training loss: 0.009302536956965923\n",
      "epoch: 2 | 113984 / 114272 | training loss: 0.026601552963256836\n",
      "epoch: 2 | 114016 / 114272 | training loss: 0.05602660030126572\n",
      "epoch: 2 | 114048 / 114272 | training loss: 0.01957646943628788\n",
      "epoch: 2 | 114080 / 114272 | training loss: 0.18433000147342682\n",
      "epoch: 2 | 114112 / 114272 | training loss: 0.30013930797576904\n",
      "epoch: 2 | 114144 / 114272 | training loss: 0.03110601380467415\n",
      "epoch: 2 | 114176 / 114272 | training loss: 0.10853429138660431\n",
      "epoch: 2 | 114208 / 114272 | training loss: 0.008281232789158821\n",
      "epoch: 2 | 114240 / 114272 | training loss: 0.01365178357809782\n",
      "Training epoch 2 done! Average loss: 0.08813350328430873. Accuracy: 0.9729592551106133\n",
      "Validation epoch 2 done! Average loss: 0.14878929807815985. Accurage: 0.9569351230425056\n",
      "Epoch 4 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 3 | 0 / 114272 | training loss: 0.013195041567087173\n",
      "epoch: 3 | 32 / 114272 | training loss: 0.029261590912938118\n",
      "epoch: 3 | 64 / 114272 | training loss: 0.01909017749130726\n",
      "epoch: 3 | 96 / 114272 | training loss: 0.2390213906764984\n",
      "epoch: 3 | 128 / 114272 | training loss: 0.005909382831305265\n",
      "epoch: 3 | 160 / 114272 | training loss: 0.006920597516000271\n",
      "epoch: 3 | 192 / 114272 | training loss: 0.0034624540712684393\n",
      "epoch: 3 | 224 / 114272 | training loss: 0.009249095804989338\n",
      "epoch: 3 | 256 / 114272 | training loss: 0.004200539086014032\n",
      "epoch: 3 | 288 / 114272 | training loss: 0.029086915776133537\n",
      "epoch: 3 | 320 / 114272 | training loss: 0.01189805381000042\n",
      "epoch: 3 | 352 / 114272 | training loss: 0.018909204751253128\n",
      "epoch: 3 | 384 / 114272 | training loss: 0.0023068476002663374\n",
      "epoch: 3 | 416 / 114272 | training loss: 0.005076122470200062\n",
      "epoch: 3 | 448 / 114272 | training loss: 0.1035468727350235\n",
      "epoch: 3 | 480 / 114272 | training loss: 0.002052617957815528\n",
      "epoch: 3 | 512 / 114272 | training loss: 0.004232937935739756\n",
      "epoch: 3 | 544 / 114272 | training loss: 0.014449912123382092\n",
      "epoch: 3 | 576 / 114272 | training loss: 0.1953909546136856\n",
      "epoch: 3 | 608 / 114272 | training loss: 0.09153798222541809\n",
      "epoch: 3 | 640 / 114272 | training loss: 0.00300220912322402\n",
      "epoch: 3 | 672 / 114272 | training loss: 0.057288359850645065\n",
      "epoch: 3 | 704 / 114272 | training loss: 0.16697163879871368\n",
      "epoch: 3 | 736 / 114272 | training loss: 0.1354513317346573\n",
      "epoch: 3 | 768 / 114272 | training loss: 0.005286694038659334\n",
      "epoch: 3 | 800 / 114272 | training loss: 0.07684709131717682\n",
      "epoch: 3 | 832 / 114272 | training loss: 0.008072856813669205\n",
      "epoch: 3 | 864 / 114272 | training loss: 0.2987423241138458\n",
      "epoch: 3 | 896 / 114272 | training loss: 0.012629890814423561\n",
      "epoch: 3 | 928 / 114272 | training loss: 0.007429004181176424\n",
      "epoch: 3 | 960 / 114272 | training loss: 0.08795252442359924\n",
      "epoch: 3 | 992 / 114272 | training loss: 0.0652851089835167\n",
      "epoch: 3 | 1024 / 114272 | training loss: 0.01913665048778057\n",
      "epoch: 3 | 1056 / 114272 | training loss: 0.004466489423066378\n",
      "epoch: 3 | 1088 / 114272 | training loss: 0.05107112228870392\n",
      "epoch: 3 | 1120 / 114272 | training loss: 0.14774073660373688\n",
      "epoch: 3 | 1152 / 114272 | training loss: 0.09217797219753265\n",
      "epoch: 3 | 1184 / 114272 | training loss: 0.006624213885515928\n",
      "epoch: 3 | 1216 / 114272 | training loss: 0.0047973948530852795\n",
      "epoch: 3 | 1248 / 114272 | training loss: 0.30764541029930115\n",
      "epoch: 3 | 1280 / 114272 | training loss: 0.03323282673954964\n",
      "epoch: 3 | 1312 / 114272 | training loss: 0.007366411853581667\n",
      "epoch: 3 | 1344 / 114272 | training loss: 0.015228461474180222\n",
      "epoch: 3 | 1376 / 114272 | training loss: 0.07262422144412994\n",
      "epoch: 3 | 1408 / 114272 | training loss: 0.15857096016407013\n",
      "epoch: 3 | 1440 / 114272 | training loss: 0.11333035677671432\n",
      "epoch: 3 | 1472 / 114272 | training loss: 0.02454039826989174\n",
      "epoch: 3 | 1504 / 114272 | training loss: 0.2107831835746765\n",
      "epoch: 3 | 1536 / 114272 | training loss: 0.009599266573786736\n",
      "epoch: 3 | 1568 / 114272 | training loss: 0.008413642644882202\n",
      "epoch: 3 | 1600 / 114272 | training loss: 0.006339686922729015\n",
      "epoch: 3 | 1632 / 114272 | training loss: 0.015788866207003593\n",
      "epoch: 3 | 1664 / 114272 | training loss: 0.23108302056789398\n",
      "epoch: 3 | 1696 / 114272 | training loss: 0.01310272328555584\n",
      "epoch: 3 | 1728 / 114272 | training loss: 0.019583646208047867\n",
      "epoch: 3 | 1760 / 114272 | training loss: 0.01036982424557209\n",
      "epoch: 3 | 1792 / 114272 | training loss: 0.02129805088043213\n",
      "epoch: 3 | 1824 / 114272 | training loss: 0.003654191503301263\n",
      "epoch: 3 | 1856 / 114272 | training loss: 0.2413690984249115\n",
      "epoch: 3 | 1888 / 114272 | training loss: 0.061968568712472916\n",
      "epoch: 3 | 1920 / 114272 | training loss: 0.009301899932324886\n",
      "epoch: 3 | 1952 / 114272 | training loss: 0.007327674888074398\n",
      "epoch: 3 | 1984 / 114272 | training loss: 0.07969143986701965\n",
      "epoch: 3 | 2016 / 114272 | training loss: 0.02381584607064724\n",
      "epoch: 3 | 2048 / 114272 | training loss: 0.013701734133064747\n",
      "epoch: 3 | 2080 / 114272 | training loss: 0.10672593861818314\n",
      "epoch: 3 | 2112 / 114272 | training loss: 0.007235354278236628\n",
      "epoch: 3 | 2144 / 114272 | training loss: 0.010301303118467331\n",
      "epoch: 3 | 2176 / 114272 | training loss: 0.0012942368630319834\n",
      "epoch: 3 | 2208 / 114272 | training loss: 0.004395381547510624\n",
      "epoch: 3 | 2240 / 114272 | training loss: 0.0039277649484574795\n",
      "epoch: 3 | 2272 / 114272 | training loss: 0.029239045456051826\n",
      "epoch: 3 | 2304 / 114272 | training loss: 0.028876960277557373\n",
      "epoch: 3 | 2336 / 114272 | training loss: 0.039522986859083176\n",
      "epoch: 3 | 2368 / 114272 | training loss: 0.0029252690728753805\n",
      "epoch: 3 | 2400 / 114272 | training loss: 0.501090407371521\n",
      "epoch: 3 | 2432 / 114272 | training loss: 0.02896703965961933\n",
      "epoch: 3 | 2464 / 114272 | training loss: 0.08658195286989212\n",
      "epoch: 3 | 2496 / 114272 | training loss: 0.141523078083992\n",
      "epoch: 3 | 2528 / 114272 | training loss: 0.3289121985435486\n",
      "epoch: 3 | 2560 / 114272 | training loss: 0.00545829301699996\n",
      "epoch: 3 | 2592 / 114272 | training loss: 0.00527361873537302\n",
      "epoch: 3 | 2624 / 114272 | training loss: 0.04121149331331253\n",
      "epoch: 3 | 2656 / 114272 | training loss: 0.034177497029304504\n",
      "epoch: 3 | 2688 / 114272 | training loss: 0.15213675796985626\n",
      "epoch: 3 | 2720 / 114272 | training loss: 0.17950820922851562\n",
      "epoch: 3 | 2752 / 114272 | training loss: 0.21695317327976227\n",
      "epoch: 3 | 2784 / 114272 | training loss: 0.07157143950462341\n",
      "epoch: 3 | 2816 / 114272 | training loss: 0.014802917838096619\n",
      "epoch: 3 | 2848 / 114272 | training loss: 0.006326057016849518\n",
      "epoch: 3 | 2880 / 114272 | training loss: 0.00515366904437542\n",
      "epoch: 3 | 2912 / 114272 | training loss: 0.127511665225029\n",
      "epoch: 3 | 2944 / 114272 | training loss: 0.17962878942489624\n",
      "epoch: 3 | 2976 / 114272 | training loss: 0.00720694474875927\n",
      "epoch: 3 | 3008 / 114272 | training loss: 0.005089195910841227\n",
      "epoch: 3 | 3040 / 114272 | training loss: 0.017671534791588783\n",
      "epoch: 3 | 3072 / 114272 | training loss: 0.025091253221035004\n",
      "epoch: 3 | 3104 / 114272 | training loss: 0.007143298629671335\n",
      "epoch: 3 | 3136 / 114272 | training loss: 0.13360807299613953\n",
      "epoch: 3 | 3168 / 114272 | training loss: 0.022931840270757675\n",
      "epoch: 3 | 3200 / 114272 | training loss: 0.11446139961481094\n",
      "epoch: 3 | 3232 / 114272 | training loss: 0.05232860520482063\n",
      "epoch: 3 | 3264 / 114272 | training loss: 0.004975264426320791\n",
      "epoch: 3 | 3296 / 114272 | training loss: 0.20375116169452667\n",
      "epoch: 3 | 3328 / 114272 | training loss: 0.008966658264398575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 3360 / 114272 | training loss: 0.017193015664815903\n",
      "epoch: 3 | 3392 / 114272 | training loss: 0.010386140085756779\n",
      "epoch: 3 | 3424 / 114272 | training loss: 0.09875523298978806\n",
      "epoch: 3 | 3456 / 114272 | training loss: 0.011961432173848152\n",
      "epoch: 3 | 3488 / 114272 | training loss: 0.0051000965759158134\n",
      "epoch: 3 | 3520 / 114272 | training loss: 0.12497911602258682\n",
      "epoch: 3 | 3552 / 114272 | training loss: 0.0872662141919136\n",
      "epoch: 3 | 3584 / 114272 | training loss: 0.004019147716462612\n",
      "epoch: 3 | 3616 / 114272 | training loss: 0.04238513112068176\n",
      "epoch: 3 | 3648 / 114272 | training loss: 0.0031946103554219007\n",
      "epoch: 3 | 3680 / 114272 | training loss: 0.07927875965833664\n",
      "epoch: 3 | 3712 / 114272 | training loss: 0.0255638025701046\n",
      "epoch: 3 | 3744 / 114272 | training loss: 0.18369238078594208\n",
      "epoch: 3 | 3776 / 114272 | training loss: 0.005108200013637543\n",
      "epoch: 3 | 3808 / 114272 | training loss: 0.12450814247131348\n",
      "epoch: 3 | 3840 / 114272 | training loss: 0.1227918192744255\n",
      "epoch: 3 | 3872 / 114272 | training loss: 0.0026065257843583822\n",
      "epoch: 3 | 3904 / 114272 | training loss: 0.05185101553797722\n",
      "epoch: 3 | 3936 / 114272 | training loss: 0.0051819803193211555\n",
      "epoch: 3 | 3968 / 114272 | training loss: 0.0036650749389082193\n",
      "epoch: 3 | 4000 / 114272 | training loss: 0.07198412716388702\n",
      "epoch: 3 | 4032 / 114272 | training loss: 0.1368291676044464\n",
      "epoch: 3 | 4064 / 114272 | training loss: 0.0647718608379364\n",
      "epoch: 3 | 4096 / 114272 | training loss: 0.11435100436210632\n",
      "epoch: 3 | 4128 / 114272 | training loss: 0.05039980635046959\n",
      "epoch: 3 | 4160 / 114272 | training loss: 0.0916929766535759\n",
      "epoch: 3 | 4192 / 114272 | training loss: 0.06568100303411484\n",
      "epoch: 3 | 4224 / 114272 | training loss: 0.09171070158481598\n",
      "epoch: 3 | 4256 / 114272 | training loss: 0.023534787818789482\n",
      "epoch: 3 | 4288 / 114272 | training loss: 0.008005950599908829\n",
      "epoch: 3 | 4320 / 114272 | training loss: 0.008178845047950745\n",
      "epoch: 3 | 4352 / 114272 | training loss: 0.14343491196632385\n",
      "epoch: 3 | 4384 / 114272 | training loss: 0.01572604291141033\n",
      "epoch: 3 | 4416 / 114272 | training loss: 0.007361600175499916\n",
      "epoch: 3 | 4448 / 114272 | training loss: 0.012608485296368599\n",
      "epoch: 3 | 4480 / 114272 | training loss: 0.21506887674331665\n",
      "epoch: 3 | 4512 / 114272 | training loss: 0.19648168981075287\n",
      "epoch: 3 | 4544 / 114272 | training loss: 0.009725727140903473\n",
      "epoch: 3 | 4576 / 114272 | training loss: 0.008483667857944965\n",
      "epoch: 3 | 4608 / 114272 | training loss: 0.01730426773428917\n",
      "epoch: 3 | 4640 / 114272 | training loss: 0.028750337660312653\n",
      "epoch: 3 | 4672 / 114272 | training loss: 0.05464688315987587\n",
      "epoch: 3 | 4704 / 114272 | training loss: 0.0035625642631202936\n",
      "epoch: 3 | 4736 / 114272 | training loss: 0.007380521856248379\n",
      "epoch: 3 | 4768 / 114272 | training loss: 0.10616034269332886\n",
      "epoch: 3 | 4800 / 114272 | training loss: 0.004257170483469963\n",
      "epoch: 3 | 4832 / 114272 | training loss: 0.011355324648320675\n",
      "epoch: 3 | 4864 / 114272 | training loss: 0.3367060422897339\n",
      "epoch: 3 | 4896 / 114272 | training loss: 0.0023861201480031013\n",
      "epoch: 3 | 4928 / 114272 | training loss: 0.06949033588171005\n",
      "epoch: 3 | 4960 / 114272 | training loss: 0.17596149444580078\n",
      "epoch: 3 | 4992 / 114272 | training loss: 0.0018746214918792248\n",
      "epoch: 3 | 5024 / 114272 | training loss: 0.007513574790209532\n",
      "epoch: 3 | 5056 / 114272 | training loss: 0.054518770426511765\n",
      "epoch: 3 | 5088 / 114272 | training loss: 0.012809854932129383\n",
      "epoch: 3 | 5120 / 114272 | training loss: 0.010144014842808247\n",
      "epoch: 3 | 5152 / 114272 | training loss: 0.0112652238458395\n",
      "epoch: 3 | 5184 / 114272 | training loss: 0.009767329320311546\n",
      "epoch: 3 | 5216 / 114272 | training loss: 0.005658392794430256\n",
      "epoch: 3 | 5248 / 114272 | training loss: 0.19852274656295776\n",
      "epoch: 3 | 5280 / 114272 | training loss: 0.0032586653251200914\n",
      "epoch: 3 | 5312 / 114272 | training loss: 0.006257557310163975\n",
      "epoch: 3 | 5344 / 114272 | training loss: 0.13224944472312927\n",
      "epoch: 3 | 5376 / 114272 | training loss: 0.1604551523923874\n",
      "epoch: 3 | 5408 / 114272 | training loss: 0.14521881937980652\n",
      "epoch: 3 | 5440 / 114272 | training loss: 0.11561481654644012\n",
      "epoch: 3 | 5472 / 114272 | training loss: 0.01050314400345087\n",
      "epoch: 3 | 5504 / 114272 | training loss: 0.11372379958629608\n",
      "epoch: 3 | 5536 / 114272 | training loss: 0.1943339705467224\n",
      "epoch: 3 | 5568 / 114272 | training loss: 0.011452833190560341\n",
      "epoch: 3 | 5600 / 114272 | training loss: 0.004521054681390524\n",
      "epoch: 3 | 5632 / 114272 | training loss: 0.00996096059679985\n",
      "epoch: 3 | 5664 / 114272 | training loss: 0.0073730736039578915\n",
      "epoch: 3 | 5696 / 114272 | training loss: 0.01627914234995842\n",
      "epoch: 3 | 5728 / 114272 | training loss: 0.0054481253027915955\n",
      "epoch: 3 | 5760 / 114272 | training loss: 0.0861796885728836\n",
      "epoch: 3 | 5792 / 114272 | training loss: 0.008304397575557232\n",
      "epoch: 3 | 5824 / 114272 | training loss: 0.010675433091819286\n",
      "epoch: 3 | 5856 / 114272 | training loss: 0.00645049475133419\n",
      "epoch: 3 | 5888 / 114272 | training loss: 0.18850581347942352\n",
      "epoch: 3 | 5920 / 114272 | training loss: 0.01001567393541336\n",
      "epoch: 3 | 5952 / 114272 | training loss: 0.05978066474199295\n",
      "epoch: 3 | 5984 / 114272 | training loss: 0.08945156633853912\n",
      "epoch: 3 | 6016 / 114272 | training loss: 0.016652787104249\n",
      "epoch: 3 | 6048 / 114272 | training loss: 0.16434922814369202\n",
      "epoch: 3 | 6080 / 114272 | training loss: 0.010593942366540432\n",
      "epoch: 3 | 6112 / 114272 | training loss: 0.008477196097373962\n",
      "epoch: 3 | 6144 / 114272 | training loss: 0.031644921749830246\n",
      "epoch: 3 | 6176 / 114272 | training loss: 0.13487586379051208\n",
      "epoch: 3 | 6208 / 114272 | training loss: 0.02757067233324051\n",
      "epoch: 3 | 6240 / 114272 | training loss: 0.0763060674071312\n",
      "epoch: 3 | 6272 / 114272 | training loss: 0.17232997715473175\n",
      "epoch: 3 | 6304 / 114272 | training loss: 0.12184728682041168\n",
      "epoch: 3 | 6336 / 114272 | training loss: 0.007728092838078737\n",
      "epoch: 3 | 6368 / 114272 | training loss: 0.1339084357023239\n",
      "epoch: 3 | 6400 / 114272 | training loss: 0.1030987799167633\n",
      "epoch: 3 | 6432 / 114272 | training loss: 0.07667005062103271\n",
      "epoch: 3 | 6464 / 114272 | training loss: 0.05593451112508774\n",
      "epoch: 3 | 6496 / 114272 | training loss: 0.011375585570931435\n",
      "epoch: 3 | 6528 / 114272 | training loss: 0.1635754406452179\n",
      "epoch: 3 | 6560 / 114272 | training loss: 0.28985795378685\n",
      "epoch: 3 | 6592 / 114272 | training loss: 0.021546512842178345\n",
      "epoch: 3 | 6624 / 114272 | training loss: 0.19382837414741516\n",
      "epoch: 3 | 6656 / 114272 | training loss: 0.0018906124169006944\n",
      "epoch: 3 | 6688 / 114272 | training loss: 0.012092365883290768\n",
      "epoch: 3 | 6720 / 114272 | training loss: 0.012662377208471298\n",
      "epoch: 3 | 6752 / 114272 | training loss: 0.08493869006633759\n",
      "epoch: 3 | 6784 / 114272 | training loss: 0.014393438585102558\n",
      "epoch: 3 | 6816 / 114272 | training loss: 0.012360628694295883\n",
      "epoch: 3 | 6848 / 114272 | training loss: 0.014871821738779545\n",
      "epoch: 3 | 6880 / 114272 | training loss: 0.092348113656044\n",
      "epoch: 3 | 6912 / 114272 | training loss: 0.04659437760710716\n",
      "epoch: 3 | 6944 / 114272 | training loss: 0.024342354387044907\n",
      "epoch: 3 | 6976 / 114272 | training loss: 0.037847090512514114\n",
      "epoch: 3 | 7008 / 114272 | training loss: 0.13217441737651825\n",
      "epoch: 3 | 7040 / 114272 | training loss: 0.22223402559757233\n",
      "epoch: 3 | 7072 / 114272 | training loss: 0.08362094312906265\n",
      "epoch: 3 | 7104 / 114272 | training loss: 0.007186010014265776\n",
      "epoch: 3 | 7136 / 114272 | training loss: 0.010446939617395401\n",
      "epoch: 3 | 7168 / 114272 | training loss: 0.29702818393707275\n",
      "epoch: 3 | 7200 / 114272 | training loss: 0.011469022370874882\n",
      "epoch: 3 | 7232 / 114272 | training loss: 0.008205423131585121\n",
      "epoch: 3 | 7264 / 114272 | training loss: 0.00847187265753746\n",
      "epoch: 3 | 7296 / 114272 | training loss: 0.018187975510954857\n",
      "epoch: 3 | 7328 / 114272 | training loss: 0.012914600782096386\n",
      "epoch: 3 | 7360 / 114272 | training loss: 0.009742663241922855\n",
      "epoch: 3 | 7392 / 114272 | training loss: 0.007186815608292818\n",
      "epoch: 3 | 7424 / 114272 | training loss: 0.022530585527420044\n",
      "epoch: 3 | 7456 / 114272 | training loss: 0.04188559576869011\n",
      "epoch: 3 | 7488 / 114272 | training loss: 0.016022980213165283\n",
      "epoch: 3 | 7520 / 114272 | training loss: 0.11179651319980621\n",
      "epoch: 3 | 7552 / 114272 | training loss: 0.33986201882362366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 7584 / 114272 | training loss: 0.004909307695925236\n",
      "epoch: 3 | 7616 / 114272 | training loss: 0.2818007469177246\n",
      "epoch: 3 | 7648 / 114272 | training loss: 0.0027775608468800783\n",
      "epoch: 3 | 7680 / 114272 | training loss: 0.005189127754420042\n",
      "epoch: 3 | 7712 / 114272 | training loss: 0.002968587214127183\n",
      "epoch: 3 | 7744 / 114272 | training loss: 0.15673595666885376\n",
      "epoch: 3 | 7776 / 114272 | training loss: 0.2901718318462372\n",
      "epoch: 3 | 7808 / 114272 | training loss: 0.010251096449792385\n",
      "epoch: 3 | 7840 / 114272 | training loss: 0.011081209406256676\n",
      "epoch: 3 | 7872 / 114272 | training loss: 0.005713354330509901\n",
      "epoch: 3 | 7904 / 114272 | training loss: 0.07942108064889908\n",
      "epoch: 3 | 7936 / 114272 | training loss: 0.0040291384793818\n",
      "epoch: 3 | 7968 / 114272 | training loss: 0.007410354446619749\n",
      "epoch: 3 | 8000 / 114272 | training loss: 0.0034287150483578444\n",
      "epoch: 3 | 8032 / 114272 | training loss: 0.0166100412607193\n",
      "epoch: 3 | 8064 / 114272 | training loss: 0.01023844350129366\n",
      "epoch: 3 | 8096 / 114272 | training loss: 0.12227895855903625\n",
      "epoch: 3 | 8128 / 114272 | training loss: 0.08885402232408524\n",
      "epoch: 3 | 8160 / 114272 | training loss: 0.014362115412950516\n",
      "epoch: 3 | 8192 / 114272 | training loss: 0.08967667073011398\n",
      "epoch: 3 | 8224 / 114272 | training loss: 0.1227252334356308\n",
      "epoch: 3 | 8256 / 114272 | training loss: 0.002356186043471098\n",
      "epoch: 3 | 8288 / 114272 | training loss: 0.0673152357339859\n",
      "epoch: 3 | 8320 / 114272 | training loss: 0.09813496470451355\n",
      "epoch: 3 | 8352 / 114272 | training loss: 0.2080550640821457\n",
      "epoch: 3 | 8384 / 114272 | training loss: 0.13346463441848755\n",
      "epoch: 3 | 8416 / 114272 | training loss: 0.04547800496220589\n",
      "epoch: 3 | 8448 / 114272 | training loss: 0.06651579588651657\n",
      "epoch: 3 | 8480 / 114272 | training loss: 0.012289787642657757\n",
      "epoch: 3 | 8512 / 114272 | training loss: 0.08876144886016846\n",
      "epoch: 3 | 8544 / 114272 | training loss: 0.01770894229412079\n",
      "epoch: 3 | 8576 / 114272 | training loss: 0.09944529831409454\n",
      "epoch: 3 | 8608 / 114272 | training loss: 0.003947787452489138\n",
      "epoch: 3 | 8640 / 114272 | training loss: 0.07751253992319107\n",
      "epoch: 3 | 8672 / 114272 | training loss: 0.08427021652460098\n",
      "epoch: 3 | 8704 / 114272 | training loss: 0.1158113107085228\n",
      "epoch: 3 | 8736 / 114272 | training loss: 0.020404892042279243\n",
      "epoch: 3 | 8768 / 114272 | training loss: 0.009010476991534233\n",
      "epoch: 3 | 8800 / 114272 | training loss: 0.01889805495738983\n",
      "epoch: 3 | 8832 / 114272 | training loss: 0.06405039876699448\n",
      "epoch: 3 | 8864 / 114272 | training loss: 0.016102079302072525\n",
      "epoch: 3 | 8896 / 114272 | training loss: 0.10957053303718567\n",
      "epoch: 3 | 8928 / 114272 | training loss: 0.11295162886381149\n",
      "epoch: 3 | 8960 / 114272 | training loss: 0.001916606561280787\n",
      "epoch: 3 | 8992 / 114272 | training loss: 0.0730881318449974\n",
      "epoch: 3 | 9024 / 114272 | training loss: 0.09259016066789627\n",
      "epoch: 3 | 9056 / 114272 | training loss: 0.3106161653995514\n",
      "epoch: 3 | 9088 / 114272 | training loss: 0.0029430699069052935\n",
      "epoch: 3 | 9120 / 114272 | training loss: 0.006602189037948847\n",
      "epoch: 3 | 9152 / 114272 | training loss: 0.010834182612597942\n",
      "epoch: 3 | 9184 / 114272 | training loss: 0.059023886919021606\n",
      "epoch: 3 | 9216 / 114272 | training loss: 0.01432895753532648\n",
      "epoch: 3 | 9248 / 114272 | training loss: 0.27738264203071594\n",
      "epoch: 3 | 9280 / 114272 | training loss: 0.005132339429110289\n",
      "epoch: 3 | 9312 / 114272 | training loss: 0.04020567238330841\n",
      "epoch: 3 | 9344 / 114272 | training loss: 0.014248223043978214\n",
      "epoch: 3 | 9376 / 114272 | training loss: 0.40568554401397705\n",
      "epoch: 3 | 9408 / 114272 | training loss: 0.01073860377073288\n",
      "epoch: 3 | 9440 / 114272 | training loss: 0.032656021416187286\n",
      "epoch: 3 | 9472 / 114272 | training loss: 0.08420799672603607\n",
      "epoch: 3 | 9504 / 114272 | training loss: 0.125696063041687\n",
      "epoch: 3 | 9536 / 114272 | training loss: 0.028908468782901764\n",
      "epoch: 3 | 9568 / 114272 | training loss: 0.011318229138851166\n",
      "epoch: 3 | 9600 / 114272 | training loss: 0.08196941763162613\n",
      "epoch: 3 | 9632 / 114272 | training loss: 0.008293082006275654\n",
      "epoch: 3 | 9664 / 114272 | training loss: 0.005291909445077181\n",
      "epoch: 3 | 9696 / 114272 | training loss: 0.007599464152008295\n",
      "epoch: 3 | 9728 / 114272 | training loss: 0.08330077677965164\n",
      "epoch: 3 | 9760 / 114272 | training loss: 0.054178282618522644\n",
      "epoch: 3 | 9792 / 114272 | training loss: 0.2627289295196533\n",
      "epoch: 3 | 9824 / 114272 | training loss: 0.1552049219608307\n",
      "epoch: 3 | 9856 / 114272 | training loss: 0.0020583528093993664\n",
      "epoch: 3 | 9888 / 114272 | training loss: 0.06511670351028442\n",
      "epoch: 3 | 9920 / 114272 | training loss: 0.013407565653324127\n",
      "epoch: 3 | 9952 / 114272 | training loss: 0.07768908888101578\n",
      "epoch: 3 | 9984 / 114272 | training loss: 0.00832457561045885\n",
      "epoch: 3 | 10016 / 114272 | training loss: 0.009760493412613869\n",
      "epoch: 3 | 10048 / 114272 | training loss: 0.009584464132785797\n",
      "epoch: 3 | 10080 / 114272 | training loss: 0.0019045984372496605\n",
      "epoch: 3 | 10112 / 114272 | training loss: 0.0009015958057716489\n",
      "epoch: 3 | 10144 / 114272 | training loss: 0.008684277534484863\n",
      "epoch: 3 | 10176 / 114272 | training loss: 0.08978597074747086\n",
      "epoch: 3 | 10208 / 114272 | training loss: 0.05489273741841316\n",
      "epoch: 3 | 10240 / 114272 | training loss: 0.006467327009886503\n",
      "epoch: 3 | 10272 / 114272 | training loss: 0.15574957430362701\n",
      "epoch: 3 | 10304 / 114272 | training loss: 0.00259964051656425\n",
      "epoch: 3 | 10336 / 114272 | training loss: 0.004970901180058718\n",
      "epoch: 3 | 10368 / 114272 | training loss: 0.006177365314215422\n",
      "epoch: 3 | 10400 / 114272 | training loss: 0.0019161421805620193\n",
      "epoch: 3 | 10432 / 114272 | training loss: 0.10643067210912704\n",
      "epoch: 3 | 10464 / 114272 | training loss: 0.07895847409963608\n",
      "epoch: 3 | 10496 / 114272 | training loss: 0.0061882734298706055\n",
      "epoch: 3 | 10528 / 114272 | training loss: 0.10480506718158722\n",
      "epoch: 3 | 10560 / 114272 | training loss: 0.0333905965089798\n",
      "epoch: 3 | 10592 / 114272 | training loss: 0.07610877603292465\n",
      "epoch: 3 | 10624 / 114272 | training loss: 0.0023910279851406813\n",
      "epoch: 3 | 10656 / 114272 | training loss: 0.006472848821431398\n",
      "epoch: 3 | 10688 / 114272 | training loss: 0.3199474811553955\n",
      "epoch: 3 | 10720 / 114272 | training loss: 0.004480138886719942\n",
      "epoch: 3 | 10752 / 114272 | training loss: 0.002368053188547492\n",
      "epoch: 3 | 10784 / 114272 | training loss: 0.004587673582136631\n",
      "epoch: 3 | 10816 / 114272 | training loss: 0.004469426814466715\n",
      "epoch: 3 | 10848 / 114272 | training loss: 0.026565924286842346\n",
      "epoch: 3 | 10880 / 114272 | training loss: 0.02201635017991066\n",
      "epoch: 3 | 10912 / 114272 | training loss: 0.09382329881191254\n",
      "epoch: 3 | 10944 / 114272 | training loss: 0.03364735096693039\n",
      "epoch: 3 | 10976 / 114272 | training loss: 0.07550214976072311\n",
      "epoch: 3 | 11008 / 114272 | training loss: 0.0038197555113583803\n",
      "epoch: 3 | 11040 / 114272 | training loss: 0.007979007437825203\n",
      "epoch: 3 | 11072 / 114272 | training loss: 0.006996301934123039\n",
      "epoch: 3 | 11104 / 114272 | training loss: 0.012488273903727531\n",
      "epoch: 3 | 11136 / 114272 | training loss: 0.008022764697670937\n",
      "epoch: 3 | 11168 / 114272 | training loss: 0.021501705050468445\n",
      "epoch: 3 | 11200 / 114272 | training loss: 0.004541337955743074\n",
      "epoch: 3 | 11232 / 114272 | training loss: 0.08918938785791397\n",
      "epoch: 3 | 11264 / 114272 | training loss: 0.008074860088527203\n",
      "epoch: 3 | 11296 / 114272 | training loss: 0.08791585266590118\n",
      "epoch: 3 | 11328 / 114272 | training loss: 0.001848582993261516\n",
      "epoch: 3 | 11360 / 114272 | training loss: 0.1330292969942093\n",
      "epoch: 3 | 11392 / 114272 | training loss: 0.002130606910213828\n",
      "epoch: 3 | 11424 / 114272 | training loss: 0.0008955775992944837\n",
      "epoch: 3 | 11456 / 114272 | training loss: 0.23431727290153503\n",
      "epoch: 3 | 11488 / 114272 | training loss: 0.08868774026632309\n",
      "epoch: 3 | 11520 / 114272 | training loss: 0.008856233209371567\n",
      "epoch: 3 | 11552 / 114272 | training loss: 0.003332346910610795\n",
      "epoch: 3 | 11584 / 114272 | training loss: 0.001801146543584764\n",
      "epoch: 3 | 11616 / 114272 | training loss: 0.00786556862294674\n",
      "epoch: 3 | 11648 / 114272 | training loss: 0.013481891714036465\n",
      "epoch: 3 | 11680 / 114272 | training loss: 0.12531664967536926\n",
      "epoch: 3 | 11712 / 114272 | training loss: 0.1555635631084442\n",
      "epoch: 3 | 11744 / 114272 | training loss: 0.01691427081823349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 11776 / 114272 | training loss: 0.10953117161989212\n",
      "epoch: 3 | 11808 / 114272 | training loss: 0.0023704380728304386\n",
      "epoch: 3 | 11840 / 114272 | training loss: 0.012568834237754345\n",
      "epoch: 3 | 11872 / 114272 | training loss: 0.058916304260492325\n",
      "epoch: 3 | 11904 / 114272 | training loss: 0.17065787315368652\n",
      "epoch: 3 | 11936 / 114272 | training loss: 0.1023491621017456\n",
      "epoch: 3 | 11968 / 114272 | training loss: 0.003852104302495718\n",
      "epoch: 3 | 12000 / 114272 | training loss: 0.07166007161140442\n",
      "epoch: 3 | 12032 / 114272 | training loss: 0.005458174739032984\n",
      "epoch: 3 | 12064 / 114272 | training loss: 0.0033287142869085073\n",
      "epoch: 3 | 12096 / 114272 | training loss: 0.00566456001251936\n",
      "epoch: 3 | 12128 / 114272 | training loss: 0.006454437505453825\n",
      "epoch: 3 | 12160 / 114272 | training loss: 0.005528826732188463\n",
      "epoch: 3 | 12192 / 114272 | training loss: 0.03145093470811844\n",
      "epoch: 3 | 12224 / 114272 | training loss: 0.14821919798851013\n",
      "epoch: 3 | 12256 / 114272 | training loss: 0.0018020272254943848\n",
      "epoch: 3 | 12288 / 114272 | training loss: 0.10072750598192215\n",
      "epoch: 3 | 12320 / 114272 | training loss: 0.1604212522506714\n",
      "epoch: 3 | 12352 / 114272 | training loss: 0.014542656019330025\n",
      "epoch: 3 | 12384 / 114272 | training loss: 0.15072154998779297\n",
      "epoch: 3 | 12416 / 114272 | training loss: 0.06793931871652603\n",
      "epoch: 3 | 12448 / 114272 | training loss: 0.06738559901714325\n",
      "epoch: 3 | 12480 / 114272 | training loss: 0.05852187052369118\n",
      "epoch: 3 | 12512 / 114272 | training loss: 0.0026866383850574493\n",
      "epoch: 3 | 12544 / 114272 | training loss: 0.0025271964259445667\n",
      "epoch: 3 | 12576 / 114272 | training loss: 0.05313113331794739\n",
      "epoch: 3 | 12608 / 114272 | training loss: 0.1358826458454132\n",
      "epoch: 3 | 12640 / 114272 | training loss: 0.005560619290918112\n",
      "epoch: 3 | 12672 / 114272 | training loss: 0.016165699809789658\n",
      "epoch: 3 | 12704 / 114272 | training loss: 0.1634717732667923\n",
      "epoch: 3 | 12736 / 114272 | training loss: 0.2172086536884308\n",
      "epoch: 3 | 12768 / 114272 | training loss: 0.004812062252312899\n",
      "epoch: 3 | 12800 / 114272 | training loss: 0.008920714259147644\n",
      "epoch: 3 | 12832 / 114272 | training loss: 0.16452446579933167\n",
      "epoch: 3 | 12864 / 114272 | training loss: 0.0038224076852202415\n",
      "epoch: 3 | 12896 / 114272 | training loss: 0.11394869536161423\n",
      "epoch: 3 | 12928 / 114272 | training loss: 0.09543032944202423\n",
      "epoch: 3 | 12960 / 114272 | training loss: 0.05101138353347778\n",
      "epoch: 3 | 12992 / 114272 | training loss: 0.002475484972819686\n",
      "epoch: 3 | 13024 / 114272 | training loss: 0.07397744059562683\n",
      "epoch: 3 | 13056 / 114272 | training loss: 0.16863927245140076\n",
      "epoch: 3 | 13088 / 114272 | training loss: 0.008607503958046436\n",
      "epoch: 3 | 13120 / 114272 | training loss: 0.09729263931512833\n",
      "epoch: 3 | 13152 / 114272 | training loss: 0.0889943391084671\n",
      "epoch: 3 | 13184 / 114272 | training loss: 0.0017173466039821506\n",
      "epoch: 3 | 13216 / 114272 | training loss: 0.006924119312316179\n",
      "epoch: 3 | 13248 / 114272 | training loss: 0.09795316308736801\n",
      "epoch: 3 | 13280 / 114272 | training loss: 0.014646036550402641\n",
      "epoch: 3 | 13312 / 114272 | training loss: 0.011174662970006466\n",
      "epoch: 3 | 13344 / 114272 | training loss: 0.02531813085079193\n",
      "epoch: 3 | 13376 / 114272 | training loss: 0.1562216579914093\n",
      "epoch: 3 | 13408 / 114272 | training loss: 0.1284325271844864\n",
      "epoch: 3 | 13440 / 114272 | training loss: 0.005556641146540642\n",
      "epoch: 3 | 13472 / 114272 | training loss: 0.04534858837723732\n",
      "epoch: 3 | 13504 / 114272 | training loss: 0.05628384277224541\n",
      "epoch: 3 | 13536 / 114272 | training loss: 0.2732364237308502\n",
      "epoch: 3 | 13568 / 114272 | training loss: 0.004371205810457468\n",
      "epoch: 3 | 13600 / 114272 | training loss: 0.018553003668785095\n",
      "epoch: 3 | 13632 / 114272 | training loss: 0.19472584128379822\n",
      "epoch: 3 | 13664 / 114272 | training loss: 0.010392247699201107\n",
      "epoch: 3 | 13696 / 114272 | training loss: 0.12976764142513275\n",
      "epoch: 3 | 13728 / 114272 | training loss: 0.0019842777401208878\n",
      "epoch: 3 | 13760 / 114272 | training loss: 0.13621416687965393\n",
      "epoch: 3 | 13792 / 114272 | training loss: 0.05428896099328995\n",
      "epoch: 3 | 13824 / 114272 | training loss: 0.021829519420862198\n",
      "epoch: 3 | 13856 / 114272 | training loss: 0.0030204656068235636\n",
      "epoch: 3 | 13888 / 114272 | training loss: 0.003169753821566701\n",
      "epoch: 3 | 13920 / 114272 | training loss: 0.11625266075134277\n",
      "epoch: 3 | 13952 / 114272 | training loss: 0.008531684055924416\n",
      "epoch: 3 | 13984 / 114272 | training loss: 0.11719653755426407\n",
      "epoch: 3 | 14016 / 114272 | training loss: 0.018207382410764694\n",
      "epoch: 3 | 14048 / 114272 | training loss: 0.08554412424564362\n",
      "epoch: 3 | 14080 / 114272 | training loss: 0.003030751831829548\n",
      "epoch: 3 | 14112 / 114272 | training loss: 0.24351400136947632\n",
      "epoch: 3 | 14144 / 114272 | training loss: 0.07337509095668793\n",
      "epoch: 3 | 14176 / 114272 | training loss: 0.3161889910697937\n",
      "epoch: 3 | 14208 / 114272 | training loss: 0.13664919137954712\n",
      "epoch: 3 | 14240 / 114272 | training loss: 0.025112681090831757\n",
      "epoch: 3 | 14272 / 114272 | training loss: 0.004164368379861116\n",
      "epoch: 3 | 14304 / 114272 | training loss: 0.003653088118880987\n",
      "epoch: 3 | 14336 / 114272 | training loss: 0.002817542292177677\n",
      "epoch: 3 | 14368 / 114272 | training loss: 0.03860579431056976\n",
      "epoch: 3 | 14400 / 114272 | training loss: 0.36494892835617065\n",
      "epoch: 3 | 14432 / 114272 | training loss: 0.005384119227528572\n",
      "epoch: 3 | 14464 / 114272 | training loss: 0.12755265831947327\n",
      "epoch: 3 | 14496 / 114272 | training loss: 0.0885624960064888\n",
      "epoch: 3 | 14528 / 114272 | training loss: 0.010219424962997437\n",
      "epoch: 3 | 14560 / 114272 | training loss: 0.11899962276220322\n",
      "epoch: 3 | 14592 / 114272 | training loss: 0.00865702424198389\n",
      "epoch: 3 | 14624 / 114272 | training loss: 0.03311741352081299\n",
      "epoch: 3 | 14656 / 114272 | training loss: 0.00378247513435781\n",
      "epoch: 3 | 14688 / 114272 | training loss: 0.15165376663208008\n",
      "epoch: 3 | 14720 / 114272 | training loss: 0.10512731224298477\n",
      "epoch: 3 | 14752 / 114272 | training loss: 0.008222981356084347\n",
      "epoch: 3 | 14784 / 114272 | training loss: 0.012177841737866402\n",
      "epoch: 3 | 14816 / 114272 | training loss: 0.27022796869277954\n",
      "epoch: 3 | 14848 / 114272 | training loss: 0.05476672574877739\n",
      "epoch: 3 | 14880 / 114272 | training loss: 0.04158783331513405\n",
      "epoch: 3 | 14912 / 114272 | training loss: 0.008981829509139061\n",
      "epoch: 3 | 14944 / 114272 | training loss: 0.007868903689086437\n",
      "epoch: 3 | 14976 / 114272 | training loss: 0.012154077179729939\n",
      "epoch: 3 | 15008 / 114272 | training loss: 0.0036653364077210426\n",
      "epoch: 3 | 15040 / 114272 | training loss: 0.0064587099477648735\n",
      "epoch: 3 | 15072 / 114272 | training loss: 0.18167562782764435\n",
      "epoch: 3 | 15104 / 114272 | training loss: 0.06538359075784683\n",
      "epoch: 3 | 15136 / 114272 | training loss: 0.010005556046962738\n",
      "epoch: 3 | 15168 / 114272 | training loss: 0.1898663192987442\n",
      "epoch: 3 | 15200 / 114272 | training loss: 0.01873943954706192\n",
      "epoch: 3 | 15232 / 114272 | training loss: 0.005224573891609907\n",
      "epoch: 3 | 15264 / 114272 | training loss: 0.08228890597820282\n",
      "epoch: 3 | 15296 / 114272 | training loss: 0.0071396236307919025\n",
      "epoch: 3 | 15328 / 114272 | training loss: 0.12220492959022522\n",
      "epoch: 3 | 15360 / 114272 | training loss: 0.0866357758641243\n",
      "epoch: 3 | 15392 / 114272 | training loss: 0.09814075380563736\n",
      "epoch: 3 | 15424 / 114272 | training loss: 0.0041091968305408955\n",
      "epoch: 3 | 15456 / 114272 | training loss: 0.08906061202287674\n",
      "epoch: 3 | 15488 / 114272 | training loss: 0.07020958513021469\n",
      "epoch: 3 | 15520 / 114272 | training loss: 0.00506421597674489\n",
      "epoch: 3 | 15552 / 114272 | training loss: 0.47992250323295593\n",
      "epoch: 3 | 15584 / 114272 | training loss: 0.0025839165318757296\n",
      "epoch: 3 | 15616 / 114272 | training loss: 0.08507146686315536\n",
      "epoch: 3 | 15648 / 114272 | training loss: 0.0033474937081336975\n",
      "epoch: 3 | 15680 / 114272 | training loss: 0.010052799247205257\n",
      "epoch: 3 | 15712 / 114272 | training loss: 0.002825008938089013\n",
      "epoch: 3 | 15744 / 114272 | training loss: 0.005719340872019529\n",
      "epoch: 3 | 15776 / 114272 | training loss: 0.19302350282669067\n",
      "epoch: 3 | 15808 / 114272 | training loss: 0.10617467761039734\n",
      "epoch: 3 | 15840 / 114272 | training loss: 0.005539391189813614\n",
      "epoch: 3 | 15872 / 114272 | training loss: 0.1672849953174591\n",
      "epoch: 3 | 15904 / 114272 | training loss: 0.009311656467616558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 15936 / 114272 | training loss: 0.005690911319106817\n",
      "epoch: 3 | 15968 / 114272 | training loss: 0.02894190326333046\n",
      "epoch: 3 | 16000 / 114272 | training loss: 0.1142120510339737\n",
      "epoch: 3 | 16032 / 114272 | training loss: 0.08549070358276367\n",
      "epoch: 3 | 16064 / 114272 | training loss: 0.008112264797091484\n",
      "epoch: 3 | 16096 / 114272 | training loss: 0.014826973900198936\n",
      "epoch: 3 | 16128 / 114272 | training loss: 0.004646983928978443\n",
      "epoch: 3 | 16160 / 114272 | training loss: 0.008113362826406956\n",
      "epoch: 3 | 16192 / 114272 | training loss: 0.0091154295951128\n",
      "epoch: 3 | 16224 / 114272 | training loss: 0.010696742683649063\n",
      "epoch: 3 | 16256 / 114272 | training loss: 0.09295397251844406\n",
      "epoch: 3 | 16288 / 114272 | training loss: 0.0021685236133635044\n",
      "epoch: 3 | 16320 / 114272 | training loss: 0.00238729827105999\n",
      "epoch: 3 | 16352 / 114272 | training loss: 0.004618427716195583\n",
      "epoch: 3 | 16384 / 114272 | training loss: 0.12804052233695984\n",
      "epoch: 3 | 16416 / 114272 | training loss: 0.015945453196763992\n",
      "epoch: 3 | 16448 / 114272 | training loss: 0.012939062900841236\n",
      "epoch: 3 | 16480 / 114272 | training loss: 0.006338870618492365\n",
      "epoch: 3 | 16512 / 114272 | training loss: 0.009571349248290062\n",
      "epoch: 3 | 16544 / 114272 | training loss: 0.00413495022803545\n",
      "epoch: 3 | 16576 / 114272 | training loss: 0.00677008181810379\n",
      "epoch: 3 | 16608 / 114272 | training loss: 0.048772137612104416\n",
      "epoch: 3 | 16640 / 114272 | training loss: 0.12786978483200073\n",
      "epoch: 3 | 16672 / 114272 | training loss: 0.002609735820442438\n",
      "epoch: 3 | 16704 / 114272 | training loss: 0.09299230575561523\n",
      "epoch: 3 | 16736 / 114272 | training loss: 0.010749544017016888\n",
      "epoch: 3 | 16768 / 114272 | training loss: 0.07759562879800797\n",
      "epoch: 3 | 16800 / 114272 | training loss: 0.00257663382217288\n",
      "epoch: 3 | 16832 / 114272 | training loss: 0.006094201933592558\n",
      "epoch: 3 | 16864 / 114272 | training loss: 0.0013993728207424283\n",
      "epoch: 3 | 16896 / 114272 | training loss: 0.07209364324808121\n",
      "epoch: 3 | 16928 / 114272 | training loss: 0.09101860970258713\n",
      "epoch: 3 | 16960 / 114272 | training loss: 0.0990648940205574\n",
      "epoch: 3 | 16992 / 114272 | training loss: 0.01170301903039217\n",
      "epoch: 3 | 17024 / 114272 | training loss: 0.006538566201925278\n",
      "epoch: 3 | 17056 / 114272 | training loss: 0.3310388922691345\n",
      "epoch: 3 | 17088 / 114272 | training loss: 0.0015709548024460673\n",
      "epoch: 3 | 17120 / 114272 | training loss: 0.3091244697570801\n",
      "epoch: 3 | 17152 / 114272 | training loss: 0.0035953056067228317\n",
      "epoch: 3 | 17184 / 114272 | training loss: 0.036128539592027664\n",
      "epoch: 3 | 17216 / 114272 | training loss: 0.3468930423259735\n",
      "epoch: 3 | 17248 / 114272 | training loss: 0.005049899220466614\n",
      "epoch: 3 | 17280 / 114272 | training loss: 0.09109941869974136\n",
      "epoch: 3 | 17312 / 114272 | training loss: 0.3110111355781555\n",
      "epoch: 3 | 17344 / 114272 | training loss: 0.008670088835060596\n",
      "epoch: 3 | 17376 / 114272 | training loss: 0.2846021056175232\n",
      "epoch: 3 | 17408 / 114272 | training loss: 0.002463835757225752\n",
      "epoch: 3 | 17440 / 114272 | training loss: 0.009606890380382538\n",
      "epoch: 3 | 17472 / 114272 | training loss: 0.0020811373833566904\n",
      "epoch: 3 | 17504 / 114272 | training loss: 0.005925488192588091\n",
      "epoch: 3 | 17536 / 114272 | training loss: 0.10237550735473633\n",
      "epoch: 3 | 17568 / 114272 | training loss: 0.0958847627043724\n",
      "epoch: 3 | 17600 / 114272 | training loss: 0.030735528096556664\n",
      "epoch: 3 | 17632 / 114272 | training loss: 0.0020235474221408367\n",
      "epoch: 3 | 17664 / 114272 | training loss: 0.016313085332512856\n",
      "epoch: 3 | 17696 / 114272 | training loss: 0.005399024114012718\n",
      "epoch: 3 | 17728 / 114272 | training loss: 0.2282690703868866\n",
      "epoch: 3 | 17760 / 114272 | training loss: 0.006338080391287804\n",
      "epoch: 3 | 17792 / 114272 | training loss: 0.0023760178592056036\n",
      "epoch: 3 | 17824 / 114272 | training loss: 0.002473613014444709\n",
      "epoch: 3 | 17856 / 114272 | training loss: 0.004071375355124474\n",
      "epoch: 3 | 17888 / 114272 | training loss: 0.007511064875870943\n",
      "epoch: 3 | 17920 / 114272 | training loss: 0.003936409950256348\n",
      "epoch: 3 | 17952 / 114272 | training loss: 0.0035075906198471785\n",
      "epoch: 3 | 17984 / 114272 | training loss: 0.06669270992279053\n",
      "epoch: 3 | 18016 / 114272 | training loss: 0.19473250210285187\n",
      "epoch: 3 | 18048 / 114272 | training loss: 0.005065247416496277\n",
      "epoch: 3 | 18080 / 114272 | training loss: 0.0034424979239702225\n",
      "epoch: 3 | 18112 / 114272 | training loss: 0.0032241942826658487\n",
      "epoch: 3 | 18144 / 114272 | training loss: 0.011383305303752422\n",
      "epoch: 3 | 18176 / 114272 | training loss: 0.38500702381134033\n",
      "epoch: 3 | 18208 / 114272 | training loss: 0.0018931113881990314\n",
      "epoch: 3 | 18240 / 114272 | training loss: 0.005493085365742445\n",
      "epoch: 3 | 18272 / 114272 | training loss: 0.1068955510854721\n",
      "epoch: 3 | 18304 / 114272 | training loss: 0.009069652296602726\n",
      "epoch: 3 | 18336 / 114272 | training loss: 0.01589110866189003\n",
      "epoch: 3 | 18368 / 114272 | training loss: 0.004991558846086264\n",
      "epoch: 3 | 18400 / 114272 | training loss: 0.15594807267189026\n",
      "epoch: 3 | 18432 / 114272 | training loss: 0.0017718385206535459\n",
      "epoch: 3 | 18464 / 114272 | training loss: 0.16856858134269714\n",
      "epoch: 3 | 18496 / 114272 | training loss: 0.004119201563298702\n",
      "epoch: 3 | 18528 / 114272 | training loss: 0.004085379187017679\n",
      "epoch: 3 | 18560 / 114272 | training loss: 0.0037987325340509415\n",
      "epoch: 3 | 18592 / 114272 | training loss: 0.0193894412368536\n",
      "epoch: 3 | 18624 / 114272 | training loss: 0.0023758485913276672\n",
      "epoch: 3 | 18656 / 114272 | training loss: 0.0777498185634613\n",
      "epoch: 3 | 18688 / 114272 | training loss: 0.0047453623265028\n",
      "epoch: 3 | 18720 / 114272 | training loss: 0.21066313982009888\n",
      "epoch: 3 | 18752 / 114272 | training loss: 0.21672788262367249\n",
      "epoch: 3 | 18784 / 114272 | training loss: 0.08422982692718506\n",
      "epoch: 3 | 18816 / 114272 | training loss: 0.2815505862236023\n",
      "epoch: 3 | 18848 / 114272 | training loss: 0.004666718654334545\n",
      "epoch: 3 | 18880 / 114272 | training loss: 0.16947774589061737\n",
      "epoch: 3 | 18912 / 114272 | training loss: 0.008436948992311954\n",
      "epoch: 3 | 18944 / 114272 | training loss: 0.036381348967552185\n",
      "epoch: 3 | 18976 / 114272 | training loss: 0.007952127605676651\n",
      "epoch: 3 | 19008 / 114272 | training loss: 0.007582062389701605\n",
      "epoch: 3 | 19040 / 114272 | training loss: 0.0019024040084332228\n",
      "epoch: 3 | 19072 / 114272 | training loss: 0.32128164172172546\n",
      "epoch: 3 | 19104 / 114272 | training loss: 0.006380805280059576\n",
      "epoch: 3 | 19136 / 114272 | training loss: 0.006689378991723061\n",
      "epoch: 3 | 19168 / 114272 | training loss: 0.012322850525379181\n",
      "epoch: 3 | 19200 / 114272 | training loss: 0.024951715022325516\n",
      "epoch: 3 | 19232 / 114272 | training loss: 0.016810288652777672\n",
      "epoch: 3 | 19264 / 114272 | training loss: 0.09267333894968033\n",
      "epoch: 3 | 19296 / 114272 | training loss: 0.005069701932370663\n",
      "epoch: 3 | 19328 / 114272 | training loss: 0.09714274108409882\n",
      "epoch: 3 | 19360 / 114272 | training loss: 0.003661382244899869\n",
      "epoch: 3 | 19392 / 114272 | training loss: 0.004242158494889736\n",
      "epoch: 3 | 19424 / 114272 | training loss: 0.005788519978523254\n",
      "epoch: 3 | 19456 / 114272 | training loss: 0.03908202424645424\n",
      "epoch: 3 | 19488 / 114272 | training loss: 0.15958374738693237\n",
      "epoch: 3 | 19520 / 114272 | training loss: 0.007704886142164469\n",
      "epoch: 3 | 19552 / 114272 | training loss: 0.003824684303253889\n",
      "epoch: 3 | 19584 / 114272 | training loss: 0.004134494345635176\n",
      "epoch: 3 | 19616 / 114272 | training loss: 0.010059114545583725\n",
      "epoch: 3 | 19648 / 114272 | training loss: 0.003571451175957918\n",
      "epoch: 3 | 19680 / 114272 | training loss: 0.018863588571548462\n",
      "epoch: 3 | 19712 / 114272 | training loss: 0.220869243144989\n",
      "epoch: 3 | 19744 / 114272 | training loss: 0.07113304734230042\n",
      "epoch: 3 | 19776 / 114272 | training loss: 0.018490968272089958\n",
      "epoch: 3 | 19808 / 114272 | training loss: 0.0032513521146029234\n",
      "epoch: 3 | 19840 / 114272 | training loss: 0.11463569849729538\n",
      "epoch: 3 | 19872 / 114272 | training loss: 0.06001590937376022\n",
      "epoch: 3 | 19904 / 114272 | training loss: 0.002877364866435528\n",
      "epoch: 3 | 19936 / 114272 | training loss: 0.0018172352574765682\n",
      "epoch: 3 | 19968 / 114272 | training loss: 0.001849608845077455\n",
      "epoch: 3 | 20000 / 114272 | training loss: 0.0838589072227478\n",
      "epoch: 3 | 20032 / 114272 | training loss: 0.1664680540561676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 20064 / 114272 | training loss: 0.03880278393626213\n",
      "epoch: 3 | 20096 / 114272 | training loss: 0.13417786359786987\n",
      "epoch: 3 | 20128 / 114272 | training loss: 0.2799791693687439\n",
      "epoch: 3 | 20160 / 114272 | training loss: 0.29129987955093384\n",
      "epoch: 3 | 20192 / 114272 | training loss: 0.16159401834011078\n",
      "epoch: 3 | 20224 / 114272 | training loss: 0.002930693794041872\n",
      "epoch: 3 | 20256 / 114272 | training loss: 0.008571101352572441\n",
      "epoch: 3 | 20288 / 114272 | training loss: 0.08222440630197525\n",
      "epoch: 3 | 20320 / 114272 | training loss: 0.005121855530887842\n",
      "epoch: 3 | 20352 / 114272 | training loss: 0.03674621134996414\n",
      "epoch: 3 | 20384 / 114272 | training loss: 0.005101248621940613\n",
      "epoch: 3 | 20416 / 114272 | training loss: 0.0037854008842259645\n",
      "epoch: 3 | 20448 / 114272 | training loss: 0.002980303019285202\n",
      "epoch: 3 | 20480 / 114272 | training loss: 0.40106409788131714\n",
      "epoch: 3 | 20512 / 114272 | training loss: 0.002581346081569791\n",
      "epoch: 3 | 20544 / 114272 | training loss: 0.0572144016623497\n",
      "epoch: 3 | 20576 / 114272 | training loss: 0.01100949477404356\n",
      "epoch: 3 | 20608 / 114272 | training loss: 0.00234150025062263\n",
      "epoch: 3 | 20640 / 114272 | training loss: 0.1509617269039154\n",
      "epoch: 3 | 20672 / 114272 | training loss: 0.004898442421108484\n",
      "epoch: 3 | 20704 / 114272 | training loss: 0.17345456779003143\n",
      "epoch: 3 | 20736 / 114272 | training loss: 0.012668890878558159\n",
      "epoch: 3 | 20768 / 114272 | training loss: 0.00872110016644001\n",
      "epoch: 3 | 20800 / 114272 | training loss: 0.08908256888389587\n",
      "epoch: 3 | 20832 / 114272 | training loss: 0.006389050744473934\n",
      "epoch: 3 | 20864 / 114272 | training loss: 0.025156574323773384\n",
      "epoch: 3 | 20896 / 114272 | training loss: 0.1781911998987198\n",
      "epoch: 3 | 20928 / 114272 | training loss: 0.1482151597738266\n",
      "epoch: 3 | 20960 / 114272 | training loss: 0.00564148323610425\n",
      "epoch: 3 | 20992 / 114272 | training loss: 0.0036024912260472775\n",
      "epoch: 3 | 21024 / 114272 | training loss: 0.00249887234531343\n",
      "epoch: 3 | 21056 / 114272 | training loss: 0.002976903459057212\n",
      "epoch: 3 | 21088 / 114272 | training loss: 0.1268923133611679\n",
      "epoch: 3 | 21120 / 114272 | training loss: 0.11315635591745377\n",
      "epoch: 3 | 21152 / 114272 | training loss: 0.10226190835237503\n",
      "epoch: 3 | 21184 / 114272 | training loss: 0.008217030204832554\n",
      "epoch: 3 | 21216 / 114272 | training loss: 0.2046123743057251\n",
      "epoch: 3 | 21248 / 114272 | training loss: 0.007855422794818878\n",
      "epoch: 3 | 21280 / 114272 | training loss: 0.0871458426117897\n",
      "epoch: 3 | 21312 / 114272 | training loss: 0.10130976885557175\n",
      "epoch: 3 | 21344 / 114272 | training loss: 0.08640453964471817\n",
      "epoch: 3 | 21376 / 114272 | training loss: 0.007108580786734819\n",
      "epoch: 3 | 21408 / 114272 | training loss: 0.09504005312919617\n",
      "epoch: 3 | 21440 / 114272 | training loss: 0.04161999747157097\n",
      "epoch: 3 | 21472 / 114272 | training loss: 0.047782644629478455\n",
      "epoch: 3 | 21504 / 114272 | training loss: 0.005775958299636841\n",
      "epoch: 3 | 21536 / 114272 | training loss: 0.009755873121321201\n",
      "epoch: 3 | 21568 / 114272 | training loss: 0.13003471493721008\n",
      "epoch: 3 | 21600 / 114272 | training loss: 0.00328708509914577\n",
      "epoch: 3 | 21632 / 114272 | training loss: 0.00910121574997902\n",
      "epoch: 3 | 21664 / 114272 | training loss: 0.15316665172576904\n",
      "epoch: 3 | 21696 / 114272 | training loss: 0.0035959037486463785\n",
      "epoch: 3 | 21728 / 114272 | training loss: 0.006764059886336327\n",
      "epoch: 3 | 21760 / 114272 | training loss: 0.011536822654306889\n",
      "epoch: 3 | 21792 / 114272 | training loss: 0.004164892714470625\n",
      "epoch: 3 | 21824 / 114272 | training loss: 0.055035244673490524\n",
      "epoch: 3 | 21856 / 114272 | training loss: 0.13621389865875244\n",
      "epoch: 3 | 21888 / 114272 | training loss: 0.044776856899261475\n",
      "epoch: 3 | 21920 / 114272 | training loss: 0.39153924584388733\n",
      "epoch: 3 | 21952 / 114272 | training loss: 0.07566662132740021\n",
      "epoch: 3 | 21984 / 114272 | training loss: 0.1494213491678238\n",
      "epoch: 3 | 22016 / 114272 | training loss: 0.004458561073988676\n",
      "epoch: 3 | 22048 / 114272 | training loss: 0.055060435086488724\n",
      "epoch: 3 | 22080 / 114272 | training loss: 0.0025246767327189445\n",
      "epoch: 3 | 22112 / 114272 | training loss: 0.018047185614705086\n",
      "epoch: 3 | 22144 / 114272 | training loss: 0.2326911836862564\n",
      "epoch: 3 | 22176 / 114272 | training loss: 0.04906989261507988\n",
      "epoch: 3 | 22208 / 114272 | training loss: 0.1686720848083496\n",
      "epoch: 3 | 22240 / 114272 | training loss: 0.013267380185425282\n",
      "epoch: 3 | 22272 / 114272 | training loss: 0.0034469144884496927\n",
      "epoch: 3 | 22304 / 114272 | training loss: 0.033287230879068375\n",
      "epoch: 3 | 22336 / 114272 | training loss: 0.008852413855493069\n",
      "epoch: 3 | 22368 / 114272 | training loss: 0.027358783408999443\n",
      "epoch: 3 | 22400 / 114272 | training loss: 0.17238928377628326\n",
      "epoch: 3 | 22432 / 114272 | training loss: 0.11543963849544525\n",
      "epoch: 3 | 22464 / 114272 | training loss: 0.12503401935100555\n",
      "epoch: 3 | 22496 / 114272 | training loss: 0.00523828761652112\n",
      "epoch: 3 | 22528 / 114272 | training loss: 0.022779248654842377\n",
      "epoch: 3 | 22560 / 114272 | training loss: 0.0072391452267766\n",
      "epoch: 3 | 22592 / 114272 | training loss: 0.004183109849691391\n",
      "epoch: 3 | 22624 / 114272 | training loss: 0.005703748669475317\n",
      "epoch: 3 | 22656 / 114272 | training loss: 0.05626000463962555\n",
      "epoch: 3 | 22688 / 114272 | training loss: 0.002671370515599847\n",
      "epoch: 3 | 22720 / 114272 | training loss: 0.0032369480468332767\n",
      "epoch: 3 | 22752 / 114272 | training loss: 0.10018322616815567\n",
      "epoch: 3 | 22784 / 114272 | training loss: 0.0034156523179262877\n",
      "epoch: 3 | 22816 / 114272 | training loss: 0.09417285770177841\n",
      "epoch: 3 | 22848 / 114272 | training loss: 0.0029335080180317163\n",
      "epoch: 3 | 22880 / 114272 | training loss: 0.11292357742786407\n",
      "epoch: 3 | 22912 / 114272 | training loss: 0.006249725352972746\n",
      "epoch: 3 | 22944 / 114272 | training loss: 0.039653535932302475\n",
      "epoch: 3 | 22976 / 114272 | training loss: 0.005988473538309336\n",
      "epoch: 3 | 23008 / 114272 | training loss: 0.006332307122647762\n",
      "epoch: 3 | 23040 / 114272 | training loss: 0.00579146109521389\n",
      "epoch: 3 | 23072 / 114272 | training loss: 0.005735483020544052\n",
      "epoch: 3 | 23104 / 114272 | training loss: 0.001970723271369934\n",
      "epoch: 3 | 23136 / 114272 | training loss: 0.0016868067905306816\n",
      "epoch: 3 | 23168 / 114272 | training loss: 0.07716524600982666\n",
      "epoch: 3 | 23200 / 114272 | training loss: 0.004383435007184744\n",
      "epoch: 3 | 23232 / 114272 | training loss: 0.25611817836761475\n",
      "epoch: 3 | 23264 / 114272 | training loss: 0.07043442875146866\n",
      "epoch: 3 | 23296 / 114272 | training loss: 0.47328534722328186\n",
      "epoch: 3 | 23328 / 114272 | training loss: 0.006768713239580393\n",
      "epoch: 3 | 23360 / 114272 | training loss: 0.004814422223716974\n",
      "epoch: 3 | 23392 / 114272 | training loss: 0.003582186531275511\n",
      "epoch: 3 | 23424 / 114272 | training loss: 0.15366201102733612\n",
      "epoch: 3 | 23456 / 114272 | training loss: 0.006848908960819244\n",
      "epoch: 3 | 23488 / 114272 | training loss: 0.002997199073433876\n",
      "epoch: 3 | 23520 / 114272 | training loss: 0.0031461610924452543\n",
      "epoch: 3 | 23552 / 114272 | training loss: 0.3507401943206787\n",
      "epoch: 3 | 23584 / 114272 | training loss: 0.0029132631607353687\n",
      "epoch: 3 | 23616 / 114272 | training loss: 0.3494691550731659\n",
      "epoch: 3 | 23648 / 114272 | training loss: 0.0035802279599010944\n",
      "epoch: 3 | 23680 / 114272 | training loss: 0.0011018727673217654\n",
      "epoch: 3 | 23712 / 114272 | training loss: 0.0030394757632166147\n",
      "epoch: 3 | 23744 / 114272 | training loss: 0.0036120014265179634\n",
      "epoch: 3 | 23776 / 114272 | training loss: 0.001611653482541442\n",
      "epoch: 3 | 23808 / 114272 | training loss: 0.00261869584210217\n",
      "epoch: 3 | 23840 / 114272 | training loss: 0.33904552459716797\n",
      "epoch: 3 | 23872 / 114272 | training loss: 0.09018705040216446\n",
      "epoch: 3 | 23904 / 114272 | training loss: 0.003037726739421487\n",
      "epoch: 3 | 23936 / 114272 | training loss: 0.06654943525791168\n",
      "epoch: 3 | 23968 / 114272 | training loss: 0.004486307967454195\n",
      "epoch: 3 | 24000 / 114272 | training loss: 0.002107567386701703\n",
      "epoch: 3 | 24032 / 114272 | training loss: 0.0016772544477134943\n",
      "epoch: 3 | 24064 / 114272 | training loss: 0.003779060672968626\n",
      "epoch: 3 | 24096 / 114272 | training loss: 0.0027427419554442167\n",
      "epoch: 3 | 24128 / 114272 | training loss: 0.0030874242074787617\n",
      "epoch: 3 | 24160 / 114272 | training loss: 0.0029940821696072817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 24192 / 114272 | training loss: 0.45573294162750244\n",
      "epoch: 3 | 24224 / 114272 | training loss: 0.32261624932289124\n",
      "epoch: 3 | 24256 / 114272 | training loss: 0.03919268772006035\n",
      "epoch: 3 | 24288 / 114272 | training loss: 0.004783525597304106\n",
      "epoch: 3 | 24320 / 114272 | training loss: 0.0015586245572194457\n",
      "epoch: 3 | 24352 / 114272 | training loss: 0.01622314378619194\n",
      "epoch: 3 | 24384 / 114272 | training loss: 0.13316607475280762\n",
      "epoch: 3 | 24416 / 114272 | training loss: 0.2024434357881546\n",
      "epoch: 3 | 24448 / 114272 | training loss: 0.017643803730607033\n",
      "epoch: 3 | 24480 / 114272 | training loss: 0.0018205182859674096\n",
      "epoch: 3 | 24512 / 114272 | training loss: 0.010058815591037273\n",
      "epoch: 3 | 24544 / 114272 | training loss: 0.0020471527241170406\n",
      "epoch: 3 | 24576 / 114272 | training loss: 0.11045490205287933\n",
      "epoch: 3 | 24608 / 114272 | training loss: 0.01146681047976017\n",
      "epoch: 3 | 24640 / 114272 | training loss: 0.005470312666147947\n",
      "epoch: 3 | 24672 / 114272 | training loss: 0.4054151475429535\n",
      "epoch: 3 | 24704 / 114272 | training loss: 0.21531888842582703\n",
      "epoch: 3 | 24736 / 114272 | training loss: 0.006690040230751038\n",
      "epoch: 3 | 24768 / 114272 | training loss: 0.03664761409163475\n",
      "epoch: 3 | 24800 / 114272 | training loss: 0.05199733376502991\n",
      "epoch: 3 | 24832 / 114272 | training loss: 0.13431322574615479\n",
      "epoch: 3 | 24864 / 114272 | training loss: 0.0043763467110693455\n",
      "epoch: 3 | 24896 / 114272 | training loss: 0.09357219189405441\n",
      "epoch: 3 | 24928 / 114272 | training loss: 0.007038687821477652\n",
      "epoch: 3 | 24960 / 114272 | training loss: 0.011386127211153507\n",
      "epoch: 3 | 24992 / 114272 | training loss: 0.15568365156650543\n",
      "epoch: 3 | 25024 / 114272 | training loss: 0.2521825134754181\n",
      "epoch: 3 | 25056 / 114272 | training loss: 0.1604071706533432\n",
      "epoch: 3 | 25088 / 114272 | training loss: 0.015524166636168957\n",
      "epoch: 3 | 25120 / 114272 | training loss: 0.004228255711495876\n",
      "epoch: 3 | 25152 / 114272 | training loss: 0.05194386839866638\n",
      "epoch: 3 | 25184 / 114272 | training loss: 0.2775558531284332\n",
      "epoch: 3 | 25216 / 114272 | training loss: 0.1359582394361496\n",
      "epoch: 3 | 25248 / 114272 | training loss: 0.10569649934768677\n",
      "epoch: 3 | 25280 / 114272 | training loss: 0.12023581564426422\n",
      "epoch: 3 | 25312 / 114272 | training loss: 0.08138275146484375\n",
      "epoch: 3 | 25344 / 114272 | training loss: 0.003760406281799078\n",
      "epoch: 3 | 25376 / 114272 | training loss: 0.008801279589533806\n",
      "epoch: 3 | 25408 / 114272 | training loss: 0.011925842612981796\n",
      "epoch: 3 | 25440 / 114272 | training loss: 0.005353602580726147\n",
      "epoch: 3 | 25472 / 114272 | training loss: 0.11912383884191513\n",
      "epoch: 3 | 25504 / 114272 | training loss: 0.08768487721681595\n",
      "epoch: 3 | 25536 / 114272 | training loss: 0.005809514317661524\n",
      "epoch: 3 | 25568 / 114272 | training loss: 0.008265323005616665\n",
      "epoch: 3 | 25600 / 114272 | training loss: 0.23520933091640472\n",
      "epoch: 3 | 25632 / 114272 | training loss: 0.004685272928327322\n",
      "epoch: 3 | 25664 / 114272 | training loss: 0.008558541536331177\n",
      "epoch: 3 | 25696 / 114272 | training loss: 0.0695309042930603\n",
      "epoch: 3 | 25728 / 114272 | training loss: 0.11928728222846985\n",
      "epoch: 3 | 25760 / 114272 | training loss: 0.42000263929367065\n",
      "epoch: 3 | 25792 / 114272 | training loss: 0.008919055573642254\n",
      "epoch: 3 | 25824 / 114272 | training loss: 0.08455225080251694\n",
      "epoch: 3 | 25856 / 114272 | training loss: 0.1758985072374344\n",
      "epoch: 3 | 25888 / 114272 | training loss: 0.3054615259170532\n",
      "epoch: 3 | 25920 / 114272 | training loss: 0.023229364305734634\n",
      "epoch: 3 | 25952 / 114272 | training loss: 0.34620144963264465\n",
      "epoch: 3 | 25984 / 114272 | training loss: 0.005367657635360956\n",
      "epoch: 3 | 26016 / 114272 | training loss: 0.007533470634371042\n",
      "epoch: 3 | 26048 / 114272 | training loss: 0.004803861491382122\n",
      "epoch: 3 | 26080 / 114272 | training loss: 0.006400160025805235\n",
      "epoch: 3 | 26112 / 114272 | training loss: 0.1210947036743164\n",
      "epoch: 3 | 26144 / 114272 | training loss: 0.11061966419219971\n",
      "epoch: 3 | 26176 / 114272 | training loss: 0.0754934623837471\n",
      "epoch: 3 | 26208 / 114272 | training loss: 0.007258579600602388\n",
      "epoch: 3 | 26240 / 114272 | training loss: 0.22068043053150177\n",
      "epoch: 3 | 26272 / 114272 | training loss: 0.01917678490281105\n",
      "epoch: 3 | 26304 / 114272 | training loss: 0.0674777701497078\n",
      "epoch: 3 | 26336 / 114272 | training loss: 0.0087679922580719\n",
      "epoch: 3 | 26368 / 114272 | training loss: 0.0052751824259757996\n",
      "epoch: 3 | 26400 / 114272 | training loss: 0.13392895460128784\n",
      "epoch: 3 | 26432 / 114272 | training loss: 0.10436034947633743\n",
      "epoch: 3 | 26464 / 114272 | training loss: 0.11576495319604874\n",
      "epoch: 3 | 26496 / 114272 | training loss: 0.007215128280222416\n",
      "epoch: 3 | 26528 / 114272 | training loss: 0.0066721150651574135\n",
      "epoch: 3 | 26560 / 114272 | training loss: 0.0038064438849687576\n",
      "epoch: 3 | 26592 / 114272 | training loss: 0.06883112341165543\n",
      "epoch: 3 | 26624 / 114272 | training loss: 0.052258603274822235\n",
      "epoch: 3 | 26656 / 114272 | training loss: 0.21263167262077332\n",
      "epoch: 3 | 26688 / 114272 | training loss: 0.07689079642295837\n",
      "epoch: 3 | 26720 / 114272 | training loss: 0.013034175150096416\n",
      "epoch: 3 | 26752 / 114272 | training loss: 0.06562866270542145\n",
      "epoch: 3 | 26784 / 114272 | training loss: 0.012831968255341053\n",
      "epoch: 3 | 26816 / 114272 | training loss: 0.09079231321811676\n",
      "epoch: 3 | 26848 / 114272 | training loss: 0.004235285799950361\n",
      "epoch: 3 | 26880 / 114272 | training loss: 0.05013975128531456\n",
      "epoch: 3 | 26912 / 114272 | training loss: 0.008470755070447922\n",
      "epoch: 3 | 26944 / 114272 | training loss: 0.006699098274111748\n",
      "epoch: 3 | 26976 / 114272 | training loss: 0.009775502607226372\n",
      "epoch: 3 | 27008 / 114272 | training loss: 0.028422243893146515\n",
      "epoch: 3 | 27040 / 114272 | training loss: 0.012132319621741772\n",
      "epoch: 3 | 27072 / 114272 | training loss: 0.008389689028263092\n",
      "epoch: 3 | 27104 / 114272 | training loss: 0.1347409188747406\n",
      "epoch: 3 | 27136 / 114272 | training loss: 0.012717973440885544\n",
      "epoch: 3 | 27168 / 114272 | training loss: 0.11195850372314453\n",
      "epoch: 3 | 27200 / 114272 | training loss: 0.002616357058286667\n",
      "epoch: 3 | 27232 / 114272 | training loss: 0.004934198688715696\n",
      "epoch: 3 | 27264 / 114272 | training loss: 0.0024090646766126156\n",
      "epoch: 3 | 27296 / 114272 | training loss: 0.003958764020353556\n",
      "epoch: 3 | 27328 / 114272 | training loss: 0.008146344684064388\n",
      "epoch: 3 | 27360 / 114272 | training loss: 0.0035868894774466753\n",
      "epoch: 3 | 27392 / 114272 | training loss: 0.06328938901424408\n",
      "epoch: 3 | 27424 / 114272 | training loss: 0.1064148098230362\n",
      "epoch: 3 | 27456 / 114272 | training loss: 0.011527038179337978\n",
      "epoch: 3 | 27488 / 114272 | training loss: 0.1136881485581398\n",
      "epoch: 3 | 27520 / 114272 | training loss: 0.004334313794970512\n",
      "epoch: 3 | 27552 / 114272 | training loss: 0.13045723736286163\n",
      "epoch: 3 | 27584 / 114272 | training loss: 0.08757739514112473\n",
      "epoch: 3 | 27616 / 114272 | training loss: 0.0026685658376663923\n",
      "epoch: 3 | 27648 / 114272 | training loss: 0.003503403626382351\n",
      "epoch: 3 | 27680 / 114272 | training loss: 0.12274766713380814\n",
      "epoch: 3 | 27712 / 114272 | training loss: 0.008007597178220749\n",
      "epoch: 3 | 27744 / 114272 | training loss: 0.18516972661018372\n",
      "epoch: 3 | 27776 / 114272 | training loss: 0.0015952608082443476\n",
      "epoch: 3 | 27808 / 114272 | training loss: 0.0019255789229646325\n",
      "epoch: 3 | 27840 / 114272 | training loss: 0.004625103436410427\n",
      "epoch: 3 | 27872 / 114272 | training loss: 0.0052419849671423435\n",
      "epoch: 3 | 27904 / 114272 | training loss: 0.09473030269145966\n",
      "epoch: 3 | 27936 / 114272 | training loss: 0.004276203457266092\n",
      "epoch: 3 | 27968 / 114272 | training loss: 0.09548485279083252\n",
      "epoch: 3 | 28000 / 114272 | training loss: 0.04552612826228142\n",
      "epoch: 3 | 28032 / 114272 | training loss: 0.006895969621837139\n",
      "epoch: 3 | 28064 / 114272 | training loss: 0.002905572298914194\n",
      "epoch: 3 | 28096 / 114272 | training loss: 0.212838813662529\n",
      "epoch: 3 | 28128 / 114272 | training loss: 0.007380985654890537\n",
      "epoch: 3 | 28160 / 114272 | training loss: 0.005506386514753103\n",
      "epoch: 3 | 28192 / 114272 | training loss: 0.005223419517278671\n",
      "epoch: 3 | 28224 / 114272 | training loss: 0.003074611769989133\n",
      "epoch: 3 | 28256 / 114272 | training loss: 0.004381847567856312\n",
      "epoch: 3 | 28288 / 114272 | training loss: 0.008311564102768898\n",
      "epoch: 3 | 28320 / 114272 | training loss: 0.0030143512412905693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 28352 / 114272 | training loss: 0.033128395676612854\n",
      "epoch: 3 | 28384 / 114272 | training loss: 0.0014568932820111513\n",
      "epoch: 3 | 28416 / 114272 | training loss: 0.0014041849644854665\n",
      "epoch: 3 | 28448 / 114272 | training loss: 0.009341501630842686\n",
      "epoch: 3 | 28480 / 114272 | training loss: 0.1036497950553894\n",
      "epoch: 3 | 28512 / 114272 | training loss: 0.12276987731456757\n",
      "epoch: 3 | 28544 / 114272 | training loss: 0.12478259205818176\n",
      "epoch: 3 | 28576 / 114272 | training loss: 0.0017627760535106063\n",
      "epoch: 3 | 28608 / 114272 | training loss: 0.005113005638122559\n",
      "epoch: 3 | 28640 / 114272 | training loss: 0.02691284753382206\n",
      "epoch: 3 | 28672 / 114272 | training loss: 0.1066431924700737\n",
      "epoch: 3 | 28704 / 114272 | training loss: 0.002031598472967744\n",
      "epoch: 3 | 28736 / 114272 | training loss: 0.14945881068706512\n",
      "epoch: 3 | 28768 / 114272 | training loss: 0.0030091421213001013\n",
      "epoch: 3 | 28800 / 114272 | training loss: 0.14727000892162323\n",
      "epoch: 3 | 28832 / 114272 | training loss: 0.3262166976928711\n",
      "epoch: 3 | 28864 / 114272 | training loss: 0.010377943515777588\n",
      "epoch: 3 | 28896 / 114272 | training loss: 0.12545417249202728\n",
      "epoch: 3 | 28928 / 114272 | training loss: 0.002542033325880766\n",
      "epoch: 3 | 28960 / 114272 | training loss: 0.046366918832063675\n",
      "epoch: 3 | 28992 / 114272 | training loss: 0.007995441555976868\n",
      "epoch: 3 | 29024 / 114272 | training loss: 0.012651650235056877\n",
      "epoch: 3 | 29056 / 114272 | training loss: 0.003959168680012226\n",
      "epoch: 3 | 29088 / 114272 | training loss: 0.1672174036502838\n",
      "epoch: 3 | 29120 / 114272 | training loss: 0.010530426166951656\n",
      "epoch: 3 | 29152 / 114272 | training loss: 0.013061861507594585\n",
      "epoch: 3 | 29184 / 114272 | training loss: 0.2565457224845886\n",
      "epoch: 3 | 29216 / 114272 | training loss: 0.008573050610721111\n",
      "epoch: 3 | 29248 / 114272 | training loss: 0.0218756515532732\n",
      "epoch: 3 | 29280 / 114272 | training loss: 0.07270889729261398\n",
      "epoch: 3 | 29312 / 114272 | training loss: 0.16365434229373932\n",
      "epoch: 3 | 29344 / 114272 | training loss: 0.0850730687379837\n",
      "epoch: 3 | 29376 / 114272 | training loss: 0.012169471941888332\n",
      "epoch: 3 | 29408 / 114272 | training loss: 0.007193474564701319\n",
      "epoch: 3 | 29440 / 114272 | training loss: 0.008155720308423042\n",
      "epoch: 3 | 29472 / 114272 | training loss: 0.13326537609100342\n",
      "epoch: 3 | 29504 / 114272 | training loss: 0.004895311314612627\n",
      "epoch: 3 | 29536 / 114272 | training loss: 0.0476955808699131\n",
      "epoch: 3 | 29568 / 114272 | training loss: 0.013586148619651794\n",
      "epoch: 3 | 29600 / 114272 | training loss: 0.17239674925804138\n",
      "epoch: 3 | 29632 / 114272 | training loss: 0.12686990201473236\n",
      "epoch: 3 | 29664 / 114272 | training loss: 0.021982215344905853\n",
      "epoch: 3 | 29696 / 114272 | training loss: 0.006948544178158045\n",
      "epoch: 3 | 29728 / 114272 | training loss: 0.0052456362172961235\n",
      "epoch: 3 | 29760 / 114272 | training loss: 0.013158745132386684\n",
      "epoch: 3 | 29792 / 114272 | training loss: 0.002100270241498947\n",
      "epoch: 3 | 29824 / 114272 | training loss: 0.006813944783061743\n",
      "epoch: 3 | 29856 / 114272 | training loss: 0.007966187782585621\n",
      "epoch: 3 | 29888 / 114272 | training loss: 0.0410657674074173\n",
      "epoch: 3 | 29920 / 114272 | training loss: 0.10130957514047623\n",
      "epoch: 3 | 29952 / 114272 | training loss: 0.00930839218199253\n",
      "epoch: 3 | 29984 / 114272 | training loss: 0.12376423180103302\n",
      "epoch: 3 | 30016 / 114272 | training loss: 0.0051195938140153885\n",
      "epoch: 3 | 30048 / 114272 | training loss: 0.17477641999721527\n",
      "epoch: 3 | 30080 / 114272 | training loss: 0.252919465303421\n",
      "epoch: 3 | 30112 / 114272 | training loss: 0.002062377519905567\n",
      "epoch: 3 | 30144 / 114272 | training loss: 0.08856391161680222\n",
      "epoch: 3 | 30176 / 114272 | training loss: 0.007630402222275734\n",
      "epoch: 3 | 30208 / 114272 | training loss: 0.0011375047033652663\n",
      "epoch: 3 | 30240 / 114272 | training loss: 0.16335658729076385\n",
      "epoch: 3 | 30272 / 114272 | training loss: 0.24314118921756744\n",
      "epoch: 3 | 30304 / 114272 | training loss: 0.13610921800136566\n",
      "epoch: 3 | 30336 / 114272 | training loss: 0.16559839248657227\n",
      "epoch: 3 | 30368 / 114272 | training loss: 0.2379118800163269\n",
      "epoch: 3 | 30400 / 114272 | training loss: 0.0014763163635507226\n",
      "epoch: 3 | 30432 / 114272 | training loss: 0.0017526180017739534\n",
      "epoch: 3 | 30464 / 114272 | training loss: 0.21830487251281738\n",
      "epoch: 3 | 30496 / 114272 | training loss: 0.12687060236930847\n",
      "epoch: 3 | 30528 / 114272 | training loss: 0.013344116508960724\n",
      "epoch: 3 | 30560 / 114272 | training loss: 0.04423368349671364\n",
      "epoch: 3 | 30592 / 114272 | training loss: 0.007732745260000229\n",
      "epoch: 3 | 30624 / 114272 | training loss: 0.17343275249004364\n",
      "epoch: 3 | 30656 / 114272 | training loss: 0.1723058521747589\n",
      "epoch: 3 | 30688 / 114272 | training loss: 0.0031569935381412506\n",
      "epoch: 3 | 30720 / 114272 | training loss: 0.3811737298965454\n",
      "epoch: 3 | 30752 / 114272 | training loss: 0.05861485376954079\n",
      "epoch: 3 | 30784 / 114272 | training loss: 0.009809751994907856\n",
      "epoch: 3 | 30816 / 114272 | training loss: 0.06382922828197479\n",
      "epoch: 3 | 30848 / 114272 | training loss: 0.08992661535739899\n",
      "epoch: 3 | 30880 / 114272 | training loss: 0.022986918687820435\n",
      "epoch: 3 | 30912 / 114272 | training loss: 0.027650317177176476\n",
      "epoch: 3 | 30944 / 114272 | training loss: 0.00603085570037365\n",
      "epoch: 3 | 30976 / 114272 | training loss: 0.15481247007846832\n",
      "epoch: 3 | 31008 / 114272 | training loss: 0.007642168086022139\n",
      "epoch: 3 | 31040 / 114272 | training loss: 0.19129861891269684\n",
      "epoch: 3 | 31072 / 114272 | training loss: 0.06570602208375931\n",
      "epoch: 3 | 31104 / 114272 | training loss: 0.2839883267879486\n",
      "epoch: 3 | 31136 / 114272 | training loss: 0.1610831618309021\n",
      "epoch: 3 | 31168 / 114272 | training loss: 0.005312767345458269\n",
      "epoch: 3 | 31200 / 114272 | training loss: 0.0025520187336951494\n",
      "epoch: 3 | 31232 / 114272 | training loss: 0.007087145932018757\n",
      "epoch: 3 | 31264 / 114272 | training loss: 0.08269920200109482\n",
      "epoch: 3 | 31296 / 114272 | training loss: 0.002789698541164398\n",
      "epoch: 3 | 31328 / 114272 | training loss: 0.01356139313429594\n",
      "epoch: 3 | 31360 / 114272 | training loss: 0.10725915431976318\n",
      "epoch: 3 | 31392 / 114272 | training loss: 0.008576605468988419\n",
      "epoch: 3 | 31424 / 114272 | training loss: 0.0937083289027214\n",
      "epoch: 3 | 31456 / 114272 | training loss: 0.02576020359992981\n",
      "epoch: 3 | 31488 / 114272 | training loss: 0.2409157007932663\n",
      "epoch: 3 | 31520 / 114272 | training loss: 0.04234110563993454\n",
      "epoch: 3 | 31552 / 114272 | training loss: 0.004185666795819998\n",
      "epoch: 3 | 31584 / 114272 | training loss: 0.004979063291102648\n",
      "epoch: 3 | 31616 / 114272 | training loss: 0.04969276860356331\n",
      "epoch: 3 | 31648 / 114272 | training loss: 0.0728050097823143\n",
      "epoch: 3 | 31680 / 114272 | training loss: 0.004689124878495932\n",
      "epoch: 3 | 31712 / 114272 | training loss: 0.009985024109482765\n",
      "epoch: 3 | 31744 / 114272 | training loss: 0.10734300315380096\n",
      "epoch: 3 | 31776 / 114272 | training loss: 0.20833735167980194\n",
      "epoch: 3 | 31808 / 114272 | training loss: 0.01718873716890812\n",
      "epoch: 3 | 31840 / 114272 | training loss: 0.004445167258381844\n",
      "epoch: 3 | 31872 / 114272 | training loss: 0.007134904619306326\n",
      "epoch: 3 | 31904 / 114272 | training loss: 0.006274056155234575\n",
      "epoch: 3 | 31936 / 114272 | training loss: 0.009964695200324059\n",
      "epoch: 3 | 31968 / 114272 | training loss: 0.06617926806211472\n",
      "epoch: 3 | 32000 / 114272 | training loss: 0.009032395668327808\n",
      "epoch: 3 | 32032 / 114272 | training loss: 0.10798431932926178\n",
      "epoch: 3 | 32064 / 114272 | training loss: 0.015273414552211761\n",
      "epoch: 3 | 32096 / 114272 | training loss: 0.008188540115952492\n",
      "epoch: 3 | 32128 / 114272 | training loss: 0.009656395763158798\n",
      "epoch: 3 | 32160 / 114272 | training loss: 0.011046153493225574\n",
      "epoch: 3 | 32192 / 114272 | training loss: 0.0052961185574531555\n",
      "epoch: 3 | 32224 / 114272 | training loss: 0.04435936361551285\n",
      "epoch: 3 | 32256 / 114272 | training loss: 0.01247008703649044\n",
      "epoch: 3 | 32288 / 114272 | training loss: 0.00844838097691536\n",
      "epoch: 3 | 32320 / 114272 | training loss: 0.20263947546482086\n",
      "epoch: 3 | 32352 / 114272 | training loss: 0.09860838949680328\n",
      "epoch: 3 | 32384 / 114272 | training loss: 0.004122799262404442\n",
      "epoch: 3 | 32416 / 114272 | training loss: 0.16023437678813934\n",
      "epoch: 3 | 32448 / 114272 | training loss: 0.02231941744685173\n",
      "epoch: 3 | 32480 / 114272 | training loss: 0.07087377458810806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 32512 / 114272 | training loss: 0.005814568605273962\n",
      "epoch: 3 | 32544 / 114272 | training loss: 0.00480563985183835\n",
      "epoch: 3 | 32576 / 114272 | training loss: 0.009388201870024204\n",
      "epoch: 3 | 32608 / 114272 | training loss: 0.11645346879959106\n",
      "epoch: 3 | 32640 / 114272 | training loss: 0.14453871548175812\n",
      "epoch: 3 | 32672 / 114272 | training loss: 0.0485626645386219\n",
      "epoch: 3 | 32704 / 114272 | training loss: 0.131419375538826\n",
      "epoch: 3 | 32736 / 114272 | training loss: 0.059854451566934586\n",
      "epoch: 3 | 32768 / 114272 | training loss: 0.06513424217700958\n",
      "epoch: 3 | 32800 / 114272 | training loss: 0.005394953303039074\n",
      "epoch: 3 | 32832 / 114272 | training loss: 0.02585558593273163\n",
      "epoch: 3 | 32864 / 114272 | training loss: 0.004545224830508232\n",
      "epoch: 3 | 32896 / 114272 | training loss: 0.11822535842657089\n",
      "epoch: 3 | 32928 / 114272 | training loss: 0.018403105437755585\n",
      "epoch: 3 | 32960 / 114272 | training loss: 0.33891159296035767\n",
      "epoch: 3 | 32992 / 114272 | training loss: 0.07199669629335403\n",
      "epoch: 3 | 33024 / 114272 | training loss: 0.011300206184387207\n",
      "epoch: 3 | 33056 / 114272 | training loss: 0.05753723531961441\n",
      "epoch: 3 | 33088 / 114272 | training loss: 0.09102554619312286\n",
      "epoch: 3 | 33120 / 114272 | training loss: 0.006569177843630314\n",
      "epoch: 3 | 33152 / 114272 | training loss: 0.05293268337845802\n",
      "epoch: 3 | 33184 / 114272 | training loss: 0.005536830518394709\n",
      "epoch: 3 | 33216 / 114272 | training loss: 0.0025299207773059607\n",
      "epoch: 3 | 33248 / 114272 | training loss: 0.0037436061538755894\n",
      "epoch: 3 | 33280 / 114272 | training loss: 0.0069013540633022785\n",
      "epoch: 3 | 33312 / 114272 | training loss: 0.014680536463856697\n",
      "epoch: 3 | 33344 / 114272 | training loss: 0.06147870048880577\n",
      "epoch: 3 | 33376 / 114272 | training loss: 0.1379697173833847\n",
      "epoch: 3 | 33408 / 114272 | training loss: 0.04657391831278801\n",
      "epoch: 3 | 33440 / 114272 | training loss: 0.09688879549503326\n",
      "epoch: 3 | 33472 / 114272 | training loss: 0.033901575952768326\n",
      "epoch: 3 | 33504 / 114272 | training loss: 0.00784700270742178\n",
      "epoch: 3 | 33536 / 114272 | training loss: 0.007102218456566334\n",
      "epoch: 3 | 33568 / 114272 | training loss: 0.16965477168560028\n",
      "epoch: 3 | 33600 / 114272 | training loss: 0.004101020283997059\n",
      "epoch: 3 | 33632 / 114272 | training loss: 0.00835267174988985\n",
      "epoch: 3 | 33664 / 114272 | training loss: 0.012831682339310646\n",
      "epoch: 3 | 33696 / 114272 | training loss: 0.015189028345048428\n",
      "epoch: 3 | 33728 / 114272 | training loss: 0.12863443791866302\n",
      "epoch: 3 | 33760 / 114272 | training loss: 0.26347389817237854\n",
      "epoch: 3 | 33792 / 114272 | training loss: 0.008610122837126255\n",
      "epoch: 3 | 33824 / 114272 | training loss: 0.022002682089805603\n",
      "epoch: 3 | 33856 / 114272 | training loss: 0.12204449623823166\n",
      "epoch: 3 | 33888 / 114272 | training loss: 0.009355547837913036\n",
      "epoch: 3 | 33920 / 114272 | training loss: 0.10725098103284836\n",
      "epoch: 3 | 33952 / 114272 | training loss: 0.019018692895770073\n",
      "epoch: 3 | 33984 / 114272 | training loss: 0.006630592048168182\n",
      "epoch: 3 | 34016 / 114272 | training loss: 0.005213234573602676\n",
      "epoch: 3 | 34048 / 114272 | training loss: 0.11283966153860092\n",
      "epoch: 3 | 34080 / 114272 | training loss: 0.004191741347312927\n",
      "epoch: 3 | 34112 / 114272 | training loss: 0.004136508796364069\n",
      "epoch: 3 | 34144 / 114272 | training loss: 0.06611268222332001\n",
      "epoch: 3 | 34176 / 114272 | training loss: 0.12867413461208344\n",
      "epoch: 3 | 34208 / 114272 | training loss: 0.08872823417186737\n",
      "epoch: 3 | 34240 / 114272 | training loss: 0.006876085419207811\n",
      "epoch: 3 | 34272 / 114272 | training loss: 0.00665488513186574\n",
      "epoch: 3 | 34304 / 114272 | training loss: 0.008913093246519566\n",
      "epoch: 3 | 34336 / 114272 | training loss: 0.007223846856504679\n",
      "epoch: 3 | 34368 / 114272 | training loss: 0.006446780636906624\n",
      "epoch: 3 | 34400 / 114272 | training loss: 0.018415236845612526\n",
      "epoch: 3 | 34432 / 114272 | training loss: 0.0033959064166992903\n",
      "epoch: 3 | 34464 / 114272 | training loss: 0.2871507406234741\n",
      "epoch: 3 | 34496 / 114272 | training loss: 0.005813238210976124\n",
      "epoch: 3 | 34528 / 114272 | training loss: 0.21646744012832642\n",
      "epoch: 3 | 34560 / 114272 | training loss: 0.1452474445104599\n",
      "epoch: 3 | 34592 / 114272 | training loss: 0.0036851782351732254\n",
      "epoch: 3 | 34624 / 114272 | training loss: 0.004590318072587252\n",
      "epoch: 3 | 34656 / 114272 | training loss: 0.04371928796172142\n",
      "epoch: 3 | 34688 / 114272 | training loss: 0.13812367618083954\n",
      "epoch: 3 | 34720 / 114272 | training loss: 0.13768884539604187\n",
      "epoch: 3 | 34752 / 114272 | training loss: 0.004815048538148403\n",
      "epoch: 3 | 34784 / 114272 | training loss: 0.05441344529390335\n",
      "epoch: 3 | 34816 / 114272 | training loss: 0.005556697957217693\n",
      "epoch: 3 | 34848 / 114272 | training loss: 0.004515508655458689\n",
      "epoch: 3 | 34880 / 114272 | training loss: 0.009189274162054062\n",
      "epoch: 3 | 34912 / 114272 | training loss: 0.10699918866157532\n",
      "epoch: 3 | 34944 / 114272 | training loss: 0.03143394738435745\n",
      "epoch: 3 | 34976 / 114272 | training loss: 0.09392260760068893\n",
      "epoch: 3 | 35008 / 114272 | training loss: 0.0819087028503418\n",
      "epoch: 3 | 35040 / 114272 | training loss: 0.32532545924186707\n",
      "epoch: 3 | 35072 / 114272 | training loss: 0.0044045536778867245\n",
      "epoch: 3 | 35104 / 114272 | training loss: 0.004760656505823135\n",
      "epoch: 3 | 35136 / 114272 | training loss: 0.008342387154698372\n",
      "epoch: 3 | 35168 / 114272 | training loss: 0.004870778415352106\n",
      "epoch: 3 | 35200 / 114272 | training loss: 0.011870045214891434\n",
      "epoch: 3 | 35232 / 114272 | training loss: 0.24218933284282684\n",
      "epoch: 3 | 35264 / 114272 | training loss: 0.19730931520462036\n",
      "epoch: 3 | 35296 / 114272 | training loss: 0.02324003353714943\n",
      "epoch: 3 | 35328 / 114272 | training loss: 0.0037092871498316526\n",
      "epoch: 3 | 35360 / 114272 | training loss: 0.04686050862073898\n",
      "epoch: 3 | 35392 / 114272 | training loss: 0.004844791255891323\n",
      "epoch: 3 | 35424 / 114272 | training loss: 0.30067673325538635\n",
      "epoch: 3 | 35456 / 114272 | training loss: 0.015803102403879166\n",
      "epoch: 3 | 35488 / 114272 | training loss: 0.0060913413763046265\n",
      "epoch: 3 | 35520 / 114272 | training loss: 0.12060045450925827\n",
      "epoch: 3 | 35552 / 114272 | training loss: 0.031077299267053604\n",
      "epoch: 3 | 35584 / 114272 | training loss: 0.004632660187780857\n",
      "epoch: 3 | 35616 / 114272 | training loss: 0.1725016087293625\n",
      "epoch: 3 | 35648 / 114272 | training loss: 0.004087521228939295\n",
      "epoch: 3 | 35680 / 114272 | training loss: 0.28964805603027344\n",
      "epoch: 3 | 35712 / 114272 | training loss: 0.0029522774275392294\n",
      "epoch: 3 | 35744 / 114272 | training loss: 0.002574765821918845\n",
      "epoch: 3 | 35776 / 114272 | training loss: 0.13404861092567444\n",
      "epoch: 3 | 35808 / 114272 | training loss: 0.006415019743144512\n",
      "epoch: 3 | 35840 / 114272 | training loss: 0.02209649048745632\n",
      "epoch: 3 | 35872 / 114272 | training loss: 0.10725158452987671\n",
      "epoch: 3 | 35904 / 114272 | training loss: 0.0032642653677612543\n",
      "epoch: 3 | 35936 / 114272 | training loss: 0.00661432696506381\n",
      "epoch: 3 | 35968 / 114272 | training loss: 0.11002004146575928\n",
      "epoch: 3 | 36000 / 114272 | training loss: 0.0069126891903579235\n",
      "epoch: 3 | 36032 / 114272 | training loss: 0.019425926730036736\n",
      "epoch: 3 | 36064 / 114272 | training loss: 0.08649385720491409\n",
      "epoch: 3 | 36096 / 114272 | training loss: 0.06639613211154938\n",
      "epoch: 3 | 36128 / 114272 | training loss: 0.17127612233161926\n",
      "epoch: 3 | 36160 / 114272 | training loss: 0.06936971098184586\n",
      "epoch: 3 | 36192 / 114272 | training loss: 0.004458822309970856\n",
      "epoch: 3 | 36224 / 114272 | training loss: 0.0036217202432453632\n",
      "epoch: 3 | 36256 / 114272 | training loss: 0.05284569039940834\n",
      "epoch: 3 | 36288 / 114272 | training loss: 0.05581165477633476\n",
      "epoch: 3 | 36320 / 114272 | training loss: 0.08619210869073868\n",
      "epoch: 3 | 36352 / 114272 | training loss: 0.06436016410589218\n",
      "epoch: 3 | 36384 / 114272 | training loss: 0.00860434491187334\n",
      "epoch: 3 | 36416 / 114272 | training loss: 0.051593538373708725\n",
      "epoch: 3 | 36448 / 114272 | training loss: 0.10905081778764725\n",
      "epoch: 3 | 36480 / 114272 | training loss: 0.011220559477806091\n",
      "epoch: 3 | 36512 / 114272 | training loss: 0.007215023506432772\n",
      "epoch: 3 | 36544 / 114272 | training loss: 0.0026296735741198063\n",
      "epoch: 3 | 36576 / 114272 | training loss: 0.007025272585451603\n",
      "epoch: 3 | 36608 / 114272 | training loss: 0.20098884403705597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 36640 / 114272 | training loss: 0.00931561179459095\n",
      "epoch: 3 | 36672 / 114272 | training loss: 0.11969995498657227\n",
      "epoch: 3 | 36704 / 114272 | training loss: 0.15436424314975739\n",
      "epoch: 3 | 36736 / 114272 | training loss: 0.15247634053230286\n",
      "epoch: 3 | 36768 / 114272 | training loss: 0.0022504543885588646\n",
      "epoch: 3 | 36800 / 114272 | training loss: 0.107148177921772\n",
      "epoch: 3 | 36832 / 114272 | training loss: 0.004371257033199072\n",
      "epoch: 3 | 36864 / 114272 | training loss: 0.0033033748622983694\n",
      "epoch: 3 | 36896 / 114272 | training loss: 0.25825560092926025\n",
      "epoch: 3 | 36928 / 114272 | training loss: 0.38762614130973816\n",
      "epoch: 3 | 36960 / 114272 | training loss: 0.07229702919721603\n",
      "epoch: 3 | 36992 / 114272 | training loss: 0.010451042093336582\n",
      "epoch: 3 | 37024 / 114272 | training loss: 0.01083611510694027\n",
      "epoch: 3 | 37056 / 114272 | training loss: 0.012749828398227692\n",
      "epoch: 3 | 37088 / 114272 | training loss: 0.0019472351996228099\n",
      "epoch: 3 | 37120 / 114272 | training loss: 0.0031485585495829582\n",
      "epoch: 3 | 37152 / 114272 | training loss: 0.15661443769931793\n",
      "epoch: 3 | 37184 / 114272 | training loss: 0.0023279236629605293\n",
      "epoch: 3 | 37216 / 114272 | training loss: 0.06934888660907745\n",
      "epoch: 3 | 37248 / 114272 | training loss: 0.1882149875164032\n",
      "epoch: 3 | 37280 / 114272 | training loss: 0.1159774661064148\n",
      "epoch: 3 | 37312 / 114272 | training loss: 0.0038175310473889112\n",
      "epoch: 3 | 37344 / 114272 | training loss: 0.06789784133434296\n",
      "epoch: 3 | 37376 / 114272 | training loss: 0.024945078417658806\n",
      "epoch: 3 | 37408 / 114272 | training loss: 0.07568947225809097\n",
      "epoch: 3 | 37440 / 114272 | training loss: 0.005787417758256197\n",
      "epoch: 3 | 37472 / 114272 | training loss: 0.004784919787198305\n",
      "epoch: 3 | 37504 / 114272 | training loss: 0.007868241518735886\n",
      "epoch: 3 | 37536 / 114272 | training loss: 0.16054409742355347\n",
      "epoch: 3 | 37568 / 114272 | training loss: 0.003769845003262162\n",
      "epoch: 3 | 37600 / 114272 | training loss: 0.005993254017084837\n",
      "epoch: 3 | 37632 / 114272 | training loss: 0.023478955030441284\n",
      "epoch: 3 | 37664 / 114272 | training loss: 0.015044541098177433\n",
      "epoch: 3 | 37696 / 114272 | training loss: 0.0027474132366478443\n",
      "epoch: 3 | 37728 / 114272 | training loss: 0.0026357586029917\n",
      "epoch: 3 | 37760 / 114272 | training loss: 0.1626036912202835\n",
      "epoch: 3 | 37792 / 114272 | training loss: 0.1404513120651245\n",
      "epoch: 3 | 37824 / 114272 | training loss: 0.3281020224094391\n",
      "epoch: 3 | 37856 / 114272 | training loss: 0.009921476244926453\n",
      "epoch: 3 | 37888 / 114272 | training loss: 0.006273954175412655\n",
      "epoch: 3 | 37920 / 114272 | training loss: 0.008711558766663074\n",
      "epoch: 3 | 37952 / 114272 | training loss: 0.003424027469009161\n",
      "epoch: 3 | 37984 / 114272 | training loss: 0.26646289229393005\n",
      "epoch: 3 | 38016 / 114272 | training loss: 0.00433871615678072\n",
      "epoch: 3 | 38048 / 114272 | training loss: 0.0400879941880703\n",
      "epoch: 3 | 38080 / 114272 | training loss: 0.02207954041659832\n",
      "epoch: 3 | 38112 / 114272 | training loss: 0.09204988181591034\n",
      "epoch: 3 | 38144 / 114272 | training loss: 0.006017455831170082\n",
      "epoch: 3 | 38176 / 114272 | training loss: 0.0463656485080719\n",
      "epoch: 3 | 38208 / 114272 | training loss: 0.027070563286542892\n",
      "epoch: 3 | 38240 / 114272 | training loss: 0.03016119822859764\n",
      "epoch: 3 | 38272 / 114272 | training loss: 0.0028427939396351576\n",
      "epoch: 3 | 38304 / 114272 | training loss: 0.061644550412893295\n",
      "epoch: 3 | 38336 / 114272 | training loss: 0.004958752542734146\n",
      "epoch: 3 | 38368 / 114272 | training loss: 0.0045914663933217525\n",
      "epoch: 3 | 38400 / 114272 | training loss: 0.09360796213150024\n",
      "epoch: 3 | 38432 / 114272 | training loss: 0.15436497330665588\n",
      "epoch: 3 | 38464 / 114272 | training loss: 0.15680280327796936\n",
      "epoch: 3 | 38496 / 114272 | training loss: 0.1495005488395691\n",
      "epoch: 3 | 38528 / 114272 | training loss: 0.10534125566482544\n",
      "epoch: 3 | 38560 / 114272 | training loss: 0.016333628445863724\n",
      "epoch: 3 | 38592 / 114272 | training loss: 0.007457324303686619\n",
      "epoch: 3 | 38624 / 114272 | training loss: 0.06864629685878754\n",
      "epoch: 3 | 38656 / 114272 | training loss: 0.23404327034950256\n",
      "epoch: 3 | 38688 / 114272 | training loss: 0.005863618571311235\n",
      "epoch: 3 | 38720 / 114272 | training loss: 0.06186739727854729\n",
      "epoch: 3 | 38752 / 114272 | training loss: 0.06613269448280334\n",
      "epoch: 3 | 38784 / 114272 | training loss: 0.10918718576431274\n",
      "epoch: 3 | 38816 / 114272 | training loss: 0.002657479140907526\n",
      "epoch: 3 | 38848 / 114272 | training loss: 0.007789624854922295\n",
      "epoch: 3 | 38880 / 114272 | training loss: 0.07579074054956436\n",
      "epoch: 3 | 38912 / 114272 | training loss: 0.1745743751525879\n",
      "epoch: 3 | 38944 / 114272 | training loss: 0.0032234934624284506\n",
      "epoch: 3 | 38976 / 114272 | training loss: 0.006668704561889172\n",
      "epoch: 3 | 39008 / 114272 | training loss: 0.008274169638752937\n",
      "epoch: 3 | 39040 / 114272 | training loss: 0.09239648282527924\n",
      "epoch: 3 | 39072 / 114272 | training loss: 0.11045392602682114\n",
      "epoch: 3 | 39104 / 114272 | training loss: 0.06757131218910217\n",
      "epoch: 3 | 39136 / 114272 | training loss: 0.005981807596981525\n",
      "epoch: 3 | 39168 / 114272 | training loss: 0.009337044321000576\n",
      "epoch: 3 | 39200 / 114272 | training loss: 0.14236687123775482\n",
      "epoch: 3 | 39232 / 114272 | training loss: 0.006598570849746466\n",
      "epoch: 3 | 39264 / 114272 | training loss: 0.006280142348259687\n",
      "epoch: 3 | 39296 / 114272 | training loss: 0.017328444868326187\n",
      "epoch: 3 | 39328 / 114272 | training loss: 0.003753039287403226\n",
      "epoch: 3 | 39360 / 114272 | training loss: 0.009129516780376434\n",
      "epoch: 3 | 39392 / 114272 | training loss: 0.014059625566005707\n",
      "epoch: 3 | 39424 / 114272 | training loss: 0.025084204971790314\n",
      "epoch: 3 | 39456 / 114272 | training loss: 0.222238689661026\n",
      "epoch: 3 | 39488 / 114272 | training loss: 0.004651762079447508\n",
      "epoch: 3 | 39520 / 114272 | training loss: 0.003277020063251257\n",
      "epoch: 3 | 39552 / 114272 | training loss: 0.005773790180683136\n",
      "epoch: 3 | 39584 / 114272 | training loss: 0.22228631377220154\n",
      "epoch: 3 | 39616 / 114272 | training loss: 0.13390707969665527\n",
      "epoch: 3 | 39648 / 114272 | training loss: 0.13449034094810486\n",
      "epoch: 3 | 39680 / 114272 | training loss: 0.009297188371419907\n",
      "epoch: 3 | 39712 / 114272 | training loss: 0.09315089136362076\n",
      "epoch: 3 | 39744 / 114272 | training loss: 0.0046972306445240974\n",
      "epoch: 3 | 39776 / 114272 | training loss: 0.0027916922699660063\n",
      "epoch: 3 | 39808 / 114272 | training loss: 0.059302330017089844\n",
      "epoch: 3 | 39840 / 114272 | training loss: 0.003879607655107975\n",
      "epoch: 3 | 39872 / 114272 | training loss: 0.05048779770731926\n",
      "epoch: 3 | 39904 / 114272 | training loss: 0.005414717365056276\n",
      "epoch: 3 | 39936 / 114272 | training loss: 0.0050483159720897675\n",
      "epoch: 3 | 39968 / 114272 | training loss: 0.002314426936209202\n",
      "epoch: 3 | 40000 / 114272 | training loss: 0.00591643201187253\n",
      "epoch: 3 | 40032 / 114272 | training loss: 0.0034408706706017256\n",
      "epoch: 3 | 40064 / 114272 | training loss: 0.16910706460475922\n",
      "epoch: 3 | 40096 / 114272 | training loss: 0.01246474776417017\n",
      "epoch: 3 | 40128 / 114272 | training loss: 0.05355392396450043\n",
      "epoch: 3 | 40160 / 114272 | training loss: 0.2992557883262634\n",
      "epoch: 3 | 40192 / 114272 | training loss: 0.2505897581577301\n",
      "epoch: 3 | 40224 / 114272 | training loss: 0.008219613693654537\n",
      "epoch: 3 | 40256 / 114272 | training loss: 0.22269947826862335\n",
      "epoch: 3 | 40288 / 114272 | training loss: 0.003820795565843582\n",
      "epoch: 3 | 40320 / 114272 | training loss: 0.0031809110660105944\n",
      "epoch: 3 | 40352 / 114272 | training loss: 0.0787026584148407\n",
      "epoch: 3 | 40384 / 114272 | training loss: 0.006127789616584778\n",
      "epoch: 3 | 40416 / 114272 | training loss: 0.1398959457874298\n",
      "epoch: 3 | 40448 / 114272 | training loss: 0.010283389128744602\n",
      "epoch: 3 | 40480 / 114272 | training loss: 0.3656420111656189\n",
      "epoch: 3 | 40512 / 114272 | training loss: 0.012445389293134212\n",
      "epoch: 3 | 40544 / 114272 | training loss: 0.0029876339249312878\n",
      "epoch: 3 | 40576 / 114272 | training loss: 0.006937835365533829\n",
      "epoch: 3 | 40608 / 114272 | training loss: 0.009692923165857792\n",
      "epoch: 3 | 40640 / 114272 | training loss: 0.003968072589486837\n",
      "epoch: 3 | 40672 / 114272 | training loss: 0.012663382105529308\n",
      "epoch: 3 | 40704 / 114272 | training loss: 0.010183569975197315\n",
      "epoch: 3 | 40736 / 114272 | training loss: 0.13016587495803833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 40768 / 114272 | training loss: 0.10120026767253876\n",
      "epoch: 3 | 40800 / 114272 | training loss: 0.00450010085478425\n",
      "epoch: 3 | 40832 / 114272 | training loss: 0.021358488127589226\n",
      "epoch: 3 | 40864 / 114272 | training loss: 0.002474733628332615\n",
      "epoch: 3 | 40896 / 114272 | training loss: 0.17165258526802063\n",
      "epoch: 3 | 40928 / 114272 | training loss: 0.1149531826376915\n",
      "epoch: 3 | 40960 / 114272 | training loss: 0.009987516328692436\n",
      "epoch: 3 | 40992 / 114272 | training loss: 0.007507192902266979\n",
      "epoch: 3 | 41024 / 114272 | training loss: 0.10844241827726364\n",
      "epoch: 3 | 41056 / 114272 | training loss: 0.00582460779696703\n",
      "epoch: 3 | 41088 / 114272 | training loss: 0.0067337192595005035\n",
      "epoch: 3 | 41120 / 114272 | training loss: 0.028340864926576614\n",
      "epoch: 3 | 41152 / 114272 | training loss: 0.23623061180114746\n",
      "epoch: 3 | 41184 / 114272 | training loss: 0.07213220745325089\n",
      "epoch: 3 | 41216 / 114272 | training loss: 0.006598548498004675\n",
      "epoch: 3 | 41248 / 114272 | training loss: 0.004651231225579977\n",
      "epoch: 3 | 41280 / 114272 | training loss: 0.005174632649868727\n",
      "epoch: 3 | 41312 / 114272 | training loss: 0.3010460138320923\n",
      "epoch: 3 | 41344 / 114272 | training loss: 0.15515483915805817\n",
      "epoch: 3 | 41376 / 114272 | training loss: 0.07922685891389847\n",
      "epoch: 3 | 41408 / 114272 | training loss: 0.009214556775987148\n",
      "epoch: 3 | 41440 / 114272 | training loss: 0.06618821620941162\n",
      "epoch: 3 | 41472 / 114272 | training loss: 0.002520625712350011\n",
      "epoch: 3 | 41504 / 114272 | training loss: 0.005821339786052704\n",
      "epoch: 3 | 41536 / 114272 | training loss: 0.10972936451435089\n",
      "epoch: 3 | 41568 / 114272 | training loss: 0.0037561168428510427\n",
      "epoch: 3 | 41600 / 114272 | training loss: 0.019828971475362778\n",
      "epoch: 3 | 41632 / 114272 | training loss: 0.016040422022342682\n",
      "epoch: 3 | 41664 / 114272 | training loss: 0.0035297058057039976\n",
      "epoch: 3 | 41696 / 114272 | training loss: 0.15113283693790436\n",
      "epoch: 3 | 41728 / 114272 | training loss: 0.007119308691471815\n",
      "epoch: 3 | 41760 / 114272 | training loss: 0.12672658264636993\n",
      "epoch: 3 | 41792 / 114272 | training loss: 0.1723586618900299\n",
      "epoch: 3 | 41824 / 114272 | training loss: 0.16566625237464905\n",
      "epoch: 3 | 41856 / 114272 | training loss: 0.14192979037761688\n",
      "epoch: 3 | 41888 / 114272 | training loss: 0.007240352686494589\n",
      "epoch: 3 | 41920 / 114272 | training loss: 0.01309241820126772\n",
      "epoch: 3 | 41952 / 114272 | training loss: 0.0091319028288126\n",
      "epoch: 3 | 41984 / 114272 | training loss: 0.0055644395761191845\n",
      "epoch: 3 | 42016 / 114272 | training loss: 0.0031107733957469463\n",
      "epoch: 3 | 42048 / 114272 | training loss: 0.013877009972929955\n",
      "epoch: 3 | 42080 / 114272 | training loss: 0.005082164891064167\n",
      "epoch: 3 | 42112 / 114272 | training loss: 0.04891331121325493\n",
      "epoch: 3 | 42144 / 114272 | training loss: 0.018612848594784737\n",
      "epoch: 3 | 42176 / 114272 | training loss: 0.13844048976898193\n",
      "epoch: 3 | 42208 / 114272 | training loss: 0.005871283356100321\n",
      "epoch: 3 | 42240 / 114272 | training loss: 0.0998053327202797\n",
      "epoch: 3 | 42272 / 114272 | training loss: 0.012417729943990707\n",
      "epoch: 3 | 42304 / 114272 | training loss: 0.003532696980983019\n",
      "epoch: 3 | 42336 / 114272 | training loss: 0.0033127935603260994\n",
      "epoch: 3 | 42368 / 114272 | training loss: 0.00361553649418056\n",
      "epoch: 3 | 42400 / 114272 | training loss: 0.009165102615952492\n",
      "epoch: 3 | 42432 / 114272 | training loss: 0.002000825246796012\n",
      "epoch: 3 | 42464 / 114272 | training loss: 0.0026141072157770395\n",
      "epoch: 3 | 42496 / 114272 | training loss: 0.6686521768569946\n",
      "epoch: 3 | 42528 / 114272 | training loss: 0.0016965721733868122\n",
      "epoch: 3 | 42560 / 114272 | training loss: 0.003542286343872547\n",
      "epoch: 3 | 42592 / 114272 | training loss: 0.05433877557516098\n",
      "epoch: 3 | 42624 / 114272 | training loss: 0.004166876431554556\n",
      "epoch: 3 | 42656 / 114272 | training loss: 0.10724442452192307\n",
      "epoch: 3 | 42688 / 114272 | training loss: 0.016050251200795174\n",
      "epoch: 3 | 42720 / 114272 | training loss: 0.13843737542629242\n",
      "epoch: 3 | 42752 / 114272 | training loss: 0.05490123853087425\n",
      "epoch: 3 | 42784 / 114272 | training loss: 0.004901232197880745\n",
      "epoch: 3 | 42816 / 114272 | training loss: 0.15059074759483337\n",
      "epoch: 3 | 42848 / 114272 | training loss: 0.004568480886518955\n",
      "epoch: 3 | 42880 / 114272 | training loss: 0.004264326300472021\n",
      "epoch: 3 | 42912 / 114272 | training loss: 0.0021687750704586506\n",
      "epoch: 3 | 42944 / 114272 | training loss: 0.06231088936328888\n",
      "epoch: 3 | 42976 / 114272 | training loss: 0.001707856310531497\n",
      "epoch: 3 | 43008 / 114272 | training loss: 0.0010994310723617673\n",
      "epoch: 3 | 43040 / 114272 | training loss: 0.15920616686344147\n",
      "epoch: 3 | 43072 / 114272 | training loss: 0.12958025932312012\n",
      "epoch: 3 | 43104 / 114272 | training loss: 0.06923771649599075\n",
      "epoch: 3 | 43136 / 114272 | training loss: 0.09412716329097748\n",
      "epoch: 3 | 43168 / 114272 | training loss: 0.0018437207909300923\n",
      "epoch: 3 | 43200 / 114272 | training loss: 0.005210271570831537\n",
      "epoch: 3 | 43232 / 114272 | training loss: 0.0795845165848732\n",
      "epoch: 3 | 43264 / 114272 | training loss: 0.0019382624886929989\n",
      "epoch: 3 | 43296 / 114272 | training loss: 0.00976792722940445\n",
      "epoch: 3 | 43328 / 114272 | training loss: 0.013863454572856426\n",
      "epoch: 3 | 43360 / 114272 | training loss: 0.006596013903617859\n",
      "epoch: 3 | 43392 / 114272 | training loss: 0.004336261656135321\n",
      "epoch: 3 | 43424 / 114272 | training loss: 0.0016644347924739122\n",
      "epoch: 3 | 43456 / 114272 | training loss: 0.003017258830368519\n",
      "epoch: 3 | 43488 / 114272 | training loss: 0.009973550215363503\n",
      "epoch: 3 | 43520 / 114272 | training loss: 0.0033606530632823706\n",
      "epoch: 3 | 43552 / 114272 | training loss: 0.09456799924373627\n",
      "epoch: 3 | 43584 / 114272 | training loss: 0.30518943071365356\n",
      "epoch: 3 | 43616 / 114272 | training loss: 0.002075315685942769\n",
      "epoch: 3 | 43648 / 114272 | training loss: 0.0013095700414851308\n",
      "epoch: 3 | 43680 / 114272 | training loss: 0.15633879601955414\n",
      "epoch: 3 | 43712 / 114272 | training loss: 0.001630290411412716\n",
      "epoch: 3 | 43744 / 114272 | training loss: 0.09970509260892868\n",
      "epoch: 3 | 43776 / 114272 | training loss: 0.005888786166906357\n",
      "epoch: 3 | 43808 / 114272 | training loss: 0.11570083349943161\n",
      "epoch: 3 | 43840 / 114272 | training loss: 0.0030977395363152027\n",
      "epoch: 3 | 43872 / 114272 | training loss: 0.16499651968479156\n",
      "epoch: 3 | 43904 / 114272 | training loss: 0.0025990367867052555\n",
      "epoch: 3 | 43936 / 114272 | training loss: 0.007588966749608517\n",
      "epoch: 3 | 43968 / 114272 | training loss: 0.07283041626214981\n",
      "epoch: 3 | 44000 / 114272 | training loss: 0.0018335779896005988\n",
      "epoch: 3 | 44032 / 114272 | training loss: 0.07480818033218384\n",
      "epoch: 3 | 44064 / 114272 | training loss: 0.19686508178710938\n",
      "epoch: 3 | 44096 / 114272 | training loss: 0.00625521270558238\n",
      "epoch: 3 | 44128 / 114272 | training loss: 0.0028260021936148405\n",
      "epoch: 3 | 44160 / 114272 | training loss: 0.0014596369583159685\n",
      "epoch: 3 | 44192 / 114272 | training loss: 0.001156297861598432\n",
      "epoch: 3 | 44224 / 114272 | training loss: 0.002047167858108878\n",
      "epoch: 3 | 44256 / 114272 | training loss: 0.0013973992317914963\n",
      "epoch: 3 | 44288 / 114272 | training loss: 0.06514903903007507\n",
      "epoch: 3 | 44320 / 114272 | training loss: 0.03052332252264023\n",
      "epoch: 3 | 44352 / 114272 | training loss: 0.21143075823783875\n",
      "epoch: 3 | 44384 / 114272 | training loss: 0.004751558881253004\n",
      "epoch: 3 | 44416 / 114272 | training loss: 0.002804508898407221\n",
      "epoch: 3 | 44448 / 114272 | training loss: 0.22052399814128876\n",
      "epoch: 3 | 44480 / 114272 | training loss: 0.0018334098858758807\n",
      "epoch: 3 | 44512 / 114272 | training loss: 0.0852595791220665\n",
      "epoch: 3 | 44544 / 114272 | training loss: 0.004266045056283474\n",
      "epoch: 3 | 44576 / 114272 | training loss: 0.0017386365216225386\n",
      "epoch: 3 | 44608 / 114272 | training loss: 0.11247031390666962\n",
      "epoch: 3 | 44640 / 114272 | training loss: 0.0008264075731858611\n",
      "epoch: 3 | 44672 / 114272 | training loss: 0.005220480728894472\n",
      "epoch: 3 | 44704 / 114272 | training loss: 0.0027352336328476667\n",
      "epoch: 3 | 44736 / 114272 | training loss: 0.0026856365147978067\n",
      "epoch: 3 | 44768 / 114272 | training loss: 0.002131494926288724\n",
      "epoch: 3 | 44800 / 114272 | training loss: 0.00098606594838202\n",
      "epoch: 3 | 44832 / 114272 | training loss: 0.05581950023770332\n",
      "epoch: 3 | 44864 / 114272 | training loss: 0.001742291497066617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 44896 / 114272 | training loss: 0.23319508135318756\n",
      "epoch: 3 | 44928 / 114272 | training loss: 0.003272036323323846\n",
      "epoch: 3 | 44960 / 114272 | training loss: 0.0018129100790247321\n",
      "epoch: 3 | 44992 / 114272 | training loss: 0.13029427826404572\n",
      "epoch: 3 | 45024 / 114272 | training loss: 0.005096548702567816\n",
      "epoch: 3 | 45056 / 114272 | training loss: 0.061457350850105286\n",
      "epoch: 3 | 45088 / 114272 | training loss: 0.01888084225356579\n",
      "epoch: 3 | 45120 / 114272 | training loss: 0.003786672605201602\n",
      "epoch: 3 | 45152 / 114272 | training loss: 0.025258395820856094\n",
      "epoch: 3 | 45184 / 114272 | training loss: 0.004651078954339027\n",
      "epoch: 3 | 45216 / 114272 | training loss: 0.09435322880744934\n",
      "epoch: 3 | 45248 / 114272 | training loss: 0.0017141440184786916\n",
      "epoch: 3 | 45280 / 114272 | training loss: 0.11035051941871643\n",
      "epoch: 3 | 45312 / 114272 | training loss: 0.08321744203567505\n",
      "epoch: 3 | 45344 / 114272 | training loss: 0.05033225566148758\n",
      "epoch: 3 | 45376 / 114272 | training loss: 0.0016423769993707538\n",
      "epoch: 3 | 45408 / 114272 | training loss: 0.004434784408658743\n",
      "epoch: 3 | 45440 / 114272 | training loss: 0.0012173427967354655\n",
      "epoch: 3 | 45472 / 114272 | training loss: 0.14842139184474945\n",
      "epoch: 3 | 45504 / 114272 | training loss: 0.0037975164595991373\n",
      "epoch: 3 | 45536 / 114272 | training loss: 0.0009619133779779077\n",
      "epoch: 3 | 45568 / 114272 | training loss: 0.20748305320739746\n",
      "epoch: 3 | 45600 / 114272 | training loss: 0.003129920456558466\n",
      "epoch: 3 | 45632 / 114272 | training loss: 0.034200143069028854\n",
      "epoch: 3 | 45664 / 114272 | training loss: 0.0014676086138933897\n",
      "epoch: 3 | 45696 / 114272 | training loss: 0.13545088469982147\n",
      "epoch: 3 | 45728 / 114272 | training loss: 0.0030162788461893797\n",
      "epoch: 3 | 45760 / 114272 | training loss: 0.012921829707920551\n",
      "epoch: 3 | 45792 / 114272 | training loss: 0.10749555379152298\n",
      "epoch: 3 | 45824 / 114272 | training loss: 0.002112909220159054\n",
      "epoch: 3 | 45856 / 114272 | training loss: 0.0014546868624165654\n",
      "epoch: 3 | 45888 / 114272 | training loss: 0.002016156679019332\n",
      "epoch: 3 | 45920 / 114272 | training loss: 0.003274705959483981\n",
      "epoch: 3 | 45952 / 114272 | training loss: 0.010004404932260513\n",
      "epoch: 3 | 45984 / 114272 | training loss: 0.0030163226183503866\n",
      "epoch: 3 | 46016 / 114272 | training loss: 0.0019680240657180548\n",
      "epoch: 3 | 46048 / 114272 | training loss: 0.2514781057834625\n",
      "epoch: 3 | 46080 / 114272 | training loss: 0.004948870744556189\n",
      "epoch: 3 | 46112 / 114272 | training loss: 0.0031753387302160263\n",
      "epoch: 3 | 46144 / 114272 | training loss: 0.0013399181189015508\n",
      "epoch: 3 | 46176 / 114272 | training loss: 0.08289727568626404\n",
      "epoch: 3 | 46208 / 114272 | training loss: 0.0620768703520298\n",
      "epoch: 3 | 46240 / 114272 | training loss: 0.0012031769147142768\n",
      "epoch: 3 | 46272 / 114272 | training loss: 0.005355654284358025\n",
      "epoch: 3 | 46304 / 114272 | training loss: 0.0032984083518385887\n",
      "epoch: 3 | 46336 / 114272 | training loss: 0.002171336906030774\n",
      "epoch: 3 | 46368 / 114272 | training loss: 0.20036593079566956\n",
      "epoch: 3 | 46400 / 114272 | training loss: 0.0015878600534051657\n",
      "epoch: 3 | 46432 / 114272 | training loss: 0.0016231999034062028\n",
      "epoch: 3 | 46464 / 114272 | training loss: 0.004461237229406834\n",
      "epoch: 3 | 46496 / 114272 | training loss: 0.020186249166727066\n",
      "epoch: 3 | 46528 / 114272 | training loss: 0.0008155449177138507\n",
      "epoch: 3 | 46560 / 114272 | training loss: 0.0012476092670112848\n",
      "epoch: 3 | 46592 / 114272 | training loss: 0.0014513502828776836\n",
      "epoch: 3 | 46624 / 114272 | training loss: 0.0019024417269974947\n",
      "epoch: 3 | 46656 / 114272 | training loss: 0.001836635172367096\n",
      "epoch: 3 | 46688 / 114272 | training loss: 0.0009139787871390581\n",
      "epoch: 3 | 46720 / 114272 | training loss: 0.009475812315940857\n",
      "epoch: 3 | 46752 / 114272 | training loss: 0.00220822193659842\n",
      "epoch: 3 | 46784 / 114272 | training loss: 0.018954697996377945\n",
      "epoch: 3 | 46816 / 114272 | training loss: 0.005017914809286594\n",
      "epoch: 3 | 46848 / 114272 | training loss: 0.12785713374614716\n",
      "epoch: 3 | 46880 / 114272 | training loss: 0.006033995188772678\n",
      "epoch: 3 | 46912 / 114272 | training loss: 0.002117577940225601\n",
      "epoch: 3 | 46944 / 114272 | training loss: 0.11587530374526978\n",
      "epoch: 3 | 46976 / 114272 | training loss: 0.19333337247371674\n",
      "epoch: 3 | 47008 / 114272 | training loss: 0.002064982196316123\n",
      "epoch: 3 | 47040 / 114272 | training loss: 0.042609620839357376\n",
      "epoch: 3 | 47072 / 114272 | training loss: 0.002182715805247426\n",
      "epoch: 3 | 47104 / 114272 | training loss: 0.1762932389974594\n",
      "epoch: 3 | 47136 / 114272 | training loss: 0.1323600560426712\n",
      "epoch: 3 | 47168 / 114272 | training loss: 0.16579747200012207\n",
      "epoch: 3 | 47200 / 114272 | training loss: 0.14315757155418396\n",
      "epoch: 3 | 47232 / 114272 | training loss: 0.271788090467453\n",
      "epoch: 3 | 47264 / 114272 | training loss: 0.004492116626352072\n",
      "epoch: 3 | 47296 / 114272 | training loss: 0.015265868976712227\n",
      "epoch: 3 | 47328 / 114272 | training loss: 0.17597854137420654\n",
      "epoch: 3 | 47360 / 114272 | training loss: 0.0012981512118130922\n",
      "epoch: 3 | 47392 / 114272 | training loss: 0.007031661923974752\n",
      "epoch: 3 | 47424 / 114272 | training loss: 0.07638099789619446\n",
      "epoch: 3 | 47456 / 114272 | training loss: 0.17794635891914368\n",
      "epoch: 3 | 47488 / 114272 | training loss: 0.0035625386517494917\n",
      "epoch: 3 | 47520 / 114272 | training loss: 0.012260531075298786\n",
      "epoch: 3 | 47552 / 114272 | training loss: 0.003553236834704876\n",
      "epoch: 3 | 47584 / 114272 | training loss: 0.006824231706559658\n",
      "epoch: 3 | 47616 / 114272 | training loss: 0.00483864126726985\n",
      "epoch: 3 | 47648 / 114272 | training loss: 0.008887897245585918\n",
      "epoch: 3 | 47680 / 114272 | training loss: 0.0019173482432961464\n",
      "epoch: 3 | 47712 / 114272 | training loss: 0.0036057326942682266\n",
      "epoch: 3 | 47744 / 114272 | training loss: 0.004032775294035673\n",
      "epoch: 3 | 47776 / 114272 | training loss: 0.0024277549237012863\n",
      "epoch: 3 | 47808 / 114272 | training loss: 0.026030266657471657\n",
      "epoch: 3 | 47840 / 114272 | training loss: 0.01343581359833479\n",
      "epoch: 3 | 47872 / 114272 | training loss: 0.10450611263513565\n",
      "epoch: 3 | 47904 / 114272 | training loss: 0.2564071714878082\n",
      "epoch: 3 | 47936 / 114272 | training loss: 0.006759276147931814\n",
      "epoch: 3 | 47968 / 114272 | training loss: 0.004834501072764397\n",
      "epoch: 3 | 48000 / 114272 | training loss: 0.004679669160395861\n",
      "epoch: 3 | 48032 / 114272 | training loss: 0.33177441358566284\n",
      "epoch: 3 | 48064 / 114272 | training loss: 0.006670037284493446\n",
      "epoch: 3 | 48096 / 114272 | training loss: 0.0031554666347801685\n",
      "epoch: 3 | 48128 / 114272 | training loss: 0.0059401970356702805\n",
      "epoch: 3 | 48160 / 114272 | training loss: 0.0032441173680126667\n",
      "epoch: 3 | 48192 / 114272 | training loss: 0.010310706682503223\n",
      "epoch: 3 | 48224 / 114272 | training loss: 0.0019694222137331963\n",
      "epoch: 3 | 48256 / 114272 | training loss: 0.0027383454144001007\n",
      "epoch: 3 | 48288 / 114272 | training loss: 0.15911626815795898\n",
      "epoch: 3 | 48320 / 114272 | training loss: 0.005475421901792288\n",
      "epoch: 3 | 48352 / 114272 | training loss: 0.0024896394461393356\n",
      "epoch: 3 | 48384 / 114272 | training loss: 0.06742943823337555\n",
      "epoch: 3 | 48416 / 114272 | training loss: 0.013233001343905926\n",
      "epoch: 3 | 48448 / 114272 | training loss: 0.19174619019031525\n",
      "epoch: 3 | 48480 / 114272 | training loss: 0.003588996594771743\n",
      "epoch: 3 | 48512 / 114272 | training loss: 0.04562388360500336\n",
      "epoch: 3 | 48544 / 114272 | training loss: 0.0016188209410756826\n",
      "epoch: 3 | 48576 / 114272 | training loss: 0.015978503972291946\n",
      "epoch: 3 | 48608 / 114272 | training loss: 0.0019103470258414745\n",
      "epoch: 3 | 48640 / 114272 | training loss: 0.04186833277344704\n",
      "epoch: 3 | 48672 / 114272 | training loss: 0.0015852970536798239\n",
      "epoch: 3 | 48704 / 114272 | training loss: 0.16967971622943878\n",
      "epoch: 3 | 48736 / 114272 | training loss: 0.13878974318504333\n",
      "epoch: 3 | 48768 / 114272 | training loss: 0.0018909038044512272\n",
      "epoch: 3 | 48800 / 114272 | training loss: 0.05844978243112564\n",
      "epoch: 3 | 48832 / 114272 | training loss: 0.0023256924469023943\n",
      "epoch: 3 | 48864 / 114272 | training loss: 0.00700718816369772\n",
      "epoch: 3 | 48896 / 114272 | training loss: 0.32120946049690247\n",
      "epoch: 3 | 48928 / 114272 | training loss: 0.005303496960550547\n",
      "epoch: 3 | 48960 / 114272 | training loss: 0.19820569455623627\n",
      "epoch: 3 | 48992 / 114272 | training loss: 0.0034583050291985273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 49024 / 114272 | training loss: 0.4102843403816223\n",
      "epoch: 3 | 49056 / 114272 | training loss: 0.001952838385477662\n",
      "epoch: 3 | 49088 / 114272 | training loss: 0.002427438274025917\n",
      "epoch: 3 | 49120 / 114272 | training loss: 0.004597722552716732\n",
      "epoch: 3 | 49152 / 114272 | training loss: 0.010759894736111164\n",
      "epoch: 3 | 49184 / 114272 | training loss: 0.10103107243776321\n",
      "epoch: 3 | 49216 / 114272 | training loss: 0.0024346269201487303\n",
      "epoch: 3 | 49248 / 114272 | training loss: 0.14813180267810822\n",
      "epoch: 3 | 49280 / 114272 | training loss: 0.0053388443775475025\n",
      "epoch: 3 | 49312 / 114272 | training loss: 0.002832112368196249\n",
      "epoch: 3 | 49344 / 114272 | training loss: 0.0036085816100239754\n",
      "epoch: 3 | 49376 / 114272 | training loss: 0.0026392005383968353\n",
      "epoch: 3 | 49408 / 114272 | training loss: 0.01999630220234394\n",
      "epoch: 3 | 49440 / 114272 | training loss: 0.0050399769097566605\n",
      "epoch: 3 | 49472 / 114272 | training loss: 0.004213245585560799\n",
      "epoch: 3 | 49504 / 114272 | training loss: 0.13045363128185272\n",
      "epoch: 3 | 49536 / 114272 | training loss: 0.00243968958966434\n",
      "epoch: 3 | 49568 / 114272 | training loss: 0.0020486456342041492\n",
      "epoch: 3 | 49600 / 114272 | training loss: 0.010727044194936752\n",
      "epoch: 3 | 49632 / 114272 | training loss: 0.1301327347755432\n",
      "epoch: 3 | 49664 / 114272 | training loss: 0.006631410680711269\n",
      "epoch: 3 | 49696 / 114272 | training loss: 0.010977807454764843\n",
      "epoch: 3 | 49728 / 114272 | training loss: 0.007181236054748297\n",
      "epoch: 3 | 49760 / 114272 | training loss: 0.012688269838690758\n",
      "epoch: 3 | 49792 / 114272 | training loss: 0.002022187225520611\n",
      "epoch: 3 | 49824 / 114272 | training loss: 0.0022732317447662354\n",
      "epoch: 3 | 49856 / 114272 | training loss: 0.002345280721783638\n",
      "epoch: 3 | 49888 / 114272 | training loss: 0.0020465513225644827\n",
      "epoch: 3 | 49920 / 114272 | training loss: 0.013927017338573933\n",
      "epoch: 3 | 49952 / 114272 | training loss: 0.0031970657873898745\n",
      "epoch: 3 | 49984 / 114272 | training loss: 0.009713060222566128\n",
      "epoch: 3 | 50016 / 114272 | training loss: 0.5809011459350586\n",
      "epoch: 3 | 50048 / 114272 | training loss: 0.22323265671730042\n",
      "epoch: 3 | 50080 / 114272 | training loss: 0.12607720494270325\n",
      "epoch: 3 | 50112 / 114272 | training loss: 0.005457357503473759\n",
      "epoch: 3 | 50144 / 114272 | training loss: 0.002032998949289322\n",
      "epoch: 3 | 50176 / 114272 | training loss: 0.24075810611248016\n",
      "epoch: 3 | 50208 / 114272 | training loss: 0.07520754635334015\n",
      "epoch: 3 | 50240 / 114272 | training loss: 0.002163687953725457\n",
      "epoch: 3 | 50272 / 114272 | training loss: 0.10972579568624496\n",
      "epoch: 3 | 50304 / 114272 | training loss: 0.13455425202846527\n",
      "epoch: 3 | 50336 / 114272 | training loss: 0.002151070162653923\n",
      "epoch: 3 | 50368 / 114272 | training loss: 0.12472279369831085\n",
      "epoch: 3 | 50400 / 114272 | training loss: 0.09244804829359055\n",
      "epoch: 3 | 50432 / 114272 | training loss: 0.007785845547914505\n",
      "epoch: 3 | 50464 / 114272 | training loss: 0.0026817498728632927\n",
      "epoch: 3 | 50496 / 114272 | training loss: 0.20089875161647797\n",
      "epoch: 3 | 50528 / 114272 | training loss: 0.12426286935806274\n",
      "epoch: 3 | 50560 / 114272 | training loss: 0.0013343134196475148\n",
      "epoch: 3 | 50592 / 114272 | training loss: 0.08764313906431198\n",
      "epoch: 3 | 50624 / 114272 | training loss: 0.0029624265152961016\n",
      "epoch: 3 | 50656 / 114272 | training loss: 0.21201679110527039\n",
      "epoch: 3 | 50688 / 114272 | training loss: 0.0030165547505021095\n",
      "epoch: 3 | 50720 / 114272 | training loss: 0.11489848792552948\n",
      "epoch: 3 | 50752 / 114272 | training loss: 0.036955639719963074\n",
      "epoch: 3 | 50784 / 114272 | training loss: 0.002788207260891795\n",
      "epoch: 3 | 50816 / 114272 | training loss: 0.008160977624356747\n",
      "epoch: 3 | 50848 / 114272 | training loss: 0.001934264088049531\n",
      "epoch: 3 | 50880 / 114272 | training loss: 0.008354822173714638\n",
      "epoch: 3 | 50912 / 114272 | training loss: 0.0053171562030911446\n",
      "epoch: 3 | 50944 / 114272 | training loss: 0.005951524246484041\n",
      "epoch: 3 | 50976 / 114272 | training loss: 0.00335947098210454\n",
      "epoch: 3 | 51008 / 114272 | training loss: 0.0028489253018051386\n",
      "epoch: 3 | 51040 / 114272 | training loss: 0.004710598383098841\n",
      "epoch: 3 | 51072 / 114272 | training loss: 0.16006910800933838\n",
      "epoch: 3 | 51104 / 114272 | training loss: 0.0016503555234521627\n",
      "epoch: 3 | 51136 / 114272 | training loss: 0.03721509501338005\n",
      "epoch: 3 | 51168 / 114272 | training loss: 0.1046764925122261\n",
      "epoch: 3 | 51200 / 114272 | training loss: 0.004199173767119646\n",
      "epoch: 3 | 51232 / 114272 | training loss: 0.008062558248639107\n",
      "epoch: 3 | 51264 / 114272 | training loss: 0.12295016646385193\n",
      "epoch: 3 | 51296 / 114272 | training loss: 0.180197611451149\n",
      "epoch: 3 | 51328 / 114272 | training loss: 0.002388439839705825\n",
      "epoch: 3 | 51360 / 114272 | training loss: 0.007894793525338173\n",
      "epoch: 3 | 51392 / 114272 | training loss: 0.25283607840538025\n",
      "epoch: 3 | 51424 / 114272 | training loss: 0.0064530931413173676\n",
      "epoch: 3 | 51456 / 114272 | training loss: 0.09441894292831421\n",
      "epoch: 3 | 51488 / 114272 | training loss: 0.004717744886875153\n",
      "epoch: 3 | 51520 / 114272 | training loss: 0.0017285357462242246\n",
      "epoch: 3 | 51552 / 114272 | training loss: 0.0019700126722455025\n",
      "epoch: 3 | 51584 / 114272 | training loss: 0.001863665645942092\n",
      "epoch: 3 | 51616 / 114272 | training loss: 0.0076169706881046295\n",
      "epoch: 3 | 51648 / 114272 | training loss: 0.0023563497234135866\n",
      "epoch: 3 | 51680 / 114272 | training loss: 0.00281161000020802\n",
      "epoch: 3 | 51712 / 114272 | training loss: 0.19838103652000427\n",
      "epoch: 3 | 51744 / 114272 | training loss: 0.006122577004134655\n",
      "epoch: 3 | 51776 / 114272 | training loss: 0.23311680555343628\n",
      "epoch: 3 | 51808 / 114272 | training loss: 0.0052559575997292995\n",
      "epoch: 3 | 51840 / 114272 | training loss: 0.002742448356002569\n",
      "epoch: 3 | 51872 / 114272 | training loss: 0.0016545476391911507\n",
      "epoch: 3 | 51904 / 114272 | training loss: 0.012233961373567581\n",
      "epoch: 3 | 51936 / 114272 | training loss: 0.26935875415802\n",
      "epoch: 3 | 51968 / 114272 | training loss: 0.005641507916152477\n",
      "epoch: 3 | 52000 / 114272 | training loss: 0.0659305602312088\n",
      "epoch: 3 | 52032 / 114272 | training loss: 0.003399824956431985\n",
      "epoch: 3 | 52064 / 114272 | training loss: 0.07453309744596481\n",
      "epoch: 3 | 52096 / 114272 | training loss: 0.059046078473329544\n",
      "epoch: 3 | 52128 / 114272 | training loss: 0.13385377824306488\n",
      "epoch: 3 | 52160 / 114272 | training loss: 0.003556664567440748\n",
      "epoch: 3 | 52192 / 114272 | training loss: 0.004020603373646736\n",
      "epoch: 3 | 52224 / 114272 | training loss: 0.010447707958519459\n",
      "epoch: 3 | 52256 / 114272 | training loss: 0.0014519506366923451\n",
      "epoch: 3 | 52288 / 114272 | training loss: 0.007146844640374184\n",
      "epoch: 3 | 52320 / 114272 | training loss: 0.07846459746360779\n",
      "epoch: 3 | 52352 / 114272 | training loss: 0.010476543568074703\n",
      "epoch: 3 | 52384 / 114272 | training loss: 0.005445330403745174\n",
      "epoch: 3 | 52416 / 114272 | training loss: 0.1381777673959732\n",
      "epoch: 3 | 52448 / 114272 | training loss: 0.0863431990146637\n",
      "epoch: 3 | 52480 / 114272 | training loss: 0.005053595174103975\n",
      "epoch: 3 | 52512 / 114272 | training loss: 0.10426849126815796\n",
      "epoch: 3 | 52544 / 114272 | training loss: 0.230713352560997\n",
      "epoch: 3 | 52576 / 114272 | training loss: 0.0018099005101248622\n",
      "epoch: 3 | 52608 / 114272 | training loss: 0.028054330497980118\n",
      "epoch: 3 | 52640 / 114272 | training loss: 0.006307373289018869\n",
      "epoch: 3 | 52672 / 114272 | training loss: 0.007371308747678995\n",
      "epoch: 3 | 52704 / 114272 | training loss: 0.006054831203073263\n",
      "epoch: 3 | 52736 / 114272 | training loss: 0.10188878327608109\n",
      "epoch: 3 | 52768 / 114272 | training loss: 0.00783402007073164\n",
      "epoch: 3 | 52800 / 114272 | training loss: 0.04117720574140549\n",
      "epoch: 3 | 52832 / 114272 | training loss: 0.0016340416623279452\n",
      "epoch: 3 | 52864 / 114272 | training loss: 0.1642342060804367\n",
      "epoch: 3 | 52896 / 114272 | training loss: 0.003856149036437273\n",
      "epoch: 3 | 52928 / 114272 | training loss: 0.009048178791999817\n",
      "epoch: 3 | 52960 / 114272 | training loss: 0.16489353775978088\n",
      "epoch: 3 | 52992 / 114272 | training loss: 0.3233656883239746\n",
      "epoch: 3 | 53024 / 114272 | training loss: 0.0039770700968801975\n",
      "epoch: 3 | 53056 / 114272 | training loss: 0.0014745919033885002\n",
      "epoch: 3 | 53088 / 114272 | training loss: 0.28324034810066223\n",
      "epoch: 3 | 53120 / 114272 | training loss: 0.0061594583094120026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 53152 / 114272 | training loss: 0.0021086379420012236\n",
      "epoch: 3 | 53184 / 114272 | training loss: 0.16893328726291656\n",
      "epoch: 3 | 53216 / 114272 | training loss: 0.11123812198638916\n",
      "epoch: 3 | 53248 / 114272 | training loss: 0.021800648421049118\n",
      "epoch: 3 | 53280 / 114272 | training loss: 0.20856575667858124\n",
      "epoch: 3 | 53312 / 114272 | training loss: 0.11744372546672821\n",
      "epoch: 3 | 53344 / 114272 | training loss: 0.08112960308790207\n",
      "epoch: 3 | 53376 / 114272 | training loss: 0.14397524297237396\n",
      "epoch: 3 | 53408 / 114272 | training loss: 0.11313677579164505\n",
      "epoch: 3 | 53440 / 114272 | training loss: 0.0048868609592318535\n",
      "epoch: 3 | 53472 / 114272 | training loss: 0.2158350646495819\n",
      "epoch: 3 | 53504 / 114272 | training loss: 0.025877704843878746\n",
      "epoch: 3 | 53536 / 114272 | training loss: 0.005229894071817398\n",
      "epoch: 3 | 53568 / 114272 | training loss: 0.0762677937746048\n",
      "epoch: 3 | 53600 / 114272 | training loss: 0.21129174530506134\n",
      "epoch: 3 | 53632 / 114272 | training loss: 0.06917423009872437\n",
      "epoch: 3 | 53664 / 114272 | training loss: 0.0037319851107895374\n",
      "epoch: 3 | 53696 / 114272 | training loss: 0.08654318749904633\n",
      "epoch: 3 | 53728 / 114272 | training loss: 0.008268877863883972\n",
      "epoch: 3 | 53760 / 114272 | training loss: 0.04279038682579994\n",
      "epoch: 3 | 53792 / 114272 | training loss: 0.007359520066529512\n",
      "epoch: 3 | 53824 / 114272 | training loss: 0.22448240220546722\n",
      "epoch: 3 | 53856 / 114272 | training loss: 0.09189449995756149\n",
      "epoch: 3 | 53888 / 114272 | training loss: 0.01058770902454853\n",
      "epoch: 3 | 53920 / 114272 | training loss: 0.08853524178266525\n",
      "epoch: 3 | 53952 / 114272 | training loss: 0.008158124051988125\n",
      "epoch: 3 | 53984 / 114272 | training loss: 0.007143773604184389\n",
      "epoch: 3 | 54016 / 114272 | training loss: 0.040326572954654694\n",
      "epoch: 3 | 54048 / 114272 | training loss: 0.0047281477600336075\n",
      "epoch: 3 | 54080 / 114272 | training loss: 0.00884807389229536\n",
      "epoch: 3 | 54112 / 114272 | training loss: 0.053109146654605865\n",
      "epoch: 3 | 54144 / 114272 | training loss: 0.004491837695240974\n",
      "epoch: 3 | 54176 / 114272 | training loss: 0.049294859170913696\n",
      "epoch: 3 | 54208 / 114272 | training loss: 0.1682712584733963\n",
      "epoch: 3 | 54240 / 114272 | training loss: 0.01625300943851471\n",
      "epoch: 3 | 54272 / 114272 | training loss: 0.14132815599441528\n",
      "epoch: 3 | 54304 / 114272 | training loss: 0.006501972675323486\n",
      "epoch: 3 | 54336 / 114272 | training loss: 0.5092030763626099\n",
      "epoch: 3 | 54368 / 114272 | training loss: 0.0009485830087214708\n",
      "epoch: 3 | 54400 / 114272 | training loss: 0.0014926277799531817\n",
      "epoch: 3 | 54432 / 114272 | training loss: 0.40944382548332214\n",
      "epoch: 3 | 54464 / 114272 | training loss: 0.01584470272064209\n",
      "epoch: 3 | 54496 / 114272 | training loss: 0.018300721421837807\n",
      "epoch: 3 | 54528 / 114272 | training loss: 0.19353510439395905\n",
      "epoch: 3 | 54560 / 114272 | training loss: 0.006578154396265745\n",
      "epoch: 3 | 54592 / 114272 | training loss: 0.12843665480613708\n",
      "epoch: 3 | 54624 / 114272 | training loss: 0.051760271191596985\n",
      "epoch: 3 | 54656 / 114272 | training loss: 0.0009945553028956056\n",
      "epoch: 3 | 54688 / 114272 | training loss: 0.010292617604136467\n",
      "epoch: 3 | 54720 / 114272 | training loss: 0.09929334372282028\n",
      "epoch: 3 | 54752 / 114272 | training loss: 0.004474741872400045\n",
      "epoch: 3 | 54784 / 114272 | training loss: 0.05240391939878464\n",
      "epoch: 3 | 54816 / 114272 | training loss: 0.003292968962341547\n",
      "epoch: 3 | 54848 / 114272 | training loss: 0.1671280562877655\n",
      "epoch: 3 | 54880 / 114272 | training loss: 0.0025280825793743134\n",
      "epoch: 3 | 54912 / 114272 | training loss: 0.001905000419355929\n",
      "epoch: 3 | 54944 / 114272 | training loss: 0.00396066764369607\n",
      "epoch: 3 | 54976 / 114272 | training loss: 0.25169995427131653\n",
      "epoch: 3 | 55008 / 114272 | training loss: 0.08101320266723633\n",
      "epoch: 3 | 55040 / 114272 | training loss: 0.011690601706504822\n",
      "epoch: 3 | 55072 / 114272 | training loss: 0.11280845105648041\n",
      "epoch: 3 | 55104 / 114272 | training loss: 0.0283427182585001\n",
      "epoch: 3 | 55136 / 114272 | training loss: 0.025323670357465744\n",
      "epoch: 3 | 55168 / 114272 | training loss: 0.004579946864396334\n",
      "epoch: 3 | 55200 / 114272 | training loss: 0.25500813126564026\n",
      "epoch: 3 | 55232 / 114272 | training loss: 0.0037761265411973\n",
      "epoch: 3 | 55264 / 114272 | training loss: 0.10789438337087631\n",
      "epoch: 3 | 55296 / 114272 | training loss: 0.006726713851094246\n",
      "epoch: 3 | 55328 / 114272 | training loss: 0.0958486869931221\n",
      "epoch: 3 | 55360 / 114272 | training loss: 0.015498221851885319\n",
      "epoch: 3 | 55392 / 114272 | training loss: 0.3099679946899414\n",
      "epoch: 3 | 55424 / 114272 | training loss: 0.008119935169816017\n",
      "epoch: 3 | 55456 / 114272 | training loss: 0.07586520910263062\n",
      "epoch: 3 | 55488 / 114272 | training loss: 0.003477924969047308\n",
      "epoch: 3 | 55520 / 114272 | training loss: 0.0011140339775010943\n",
      "epoch: 3 | 55552 / 114272 | training loss: 0.0033418270759284496\n",
      "epoch: 3 | 55584 / 114272 | training loss: 0.003149584401398897\n",
      "epoch: 3 | 55616 / 114272 | training loss: 0.009036755189299583\n",
      "epoch: 3 | 55648 / 114272 | training loss: 0.003496347926557064\n",
      "epoch: 3 | 55680 / 114272 | training loss: 0.010867455042898655\n",
      "epoch: 3 | 55712 / 114272 | training loss: 0.2139052152633667\n",
      "epoch: 3 | 55744 / 114272 | training loss: 0.069620281457901\n",
      "epoch: 3 | 55776 / 114272 | training loss: 0.22678489983081818\n",
      "epoch: 3 | 55808 / 114272 | training loss: 0.003581306664273143\n",
      "epoch: 3 | 55840 / 114272 | training loss: 0.18155598640441895\n",
      "epoch: 3 | 55872 / 114272 | training loss: 0.050996534526348114\n",
      "epoch: 3 | 55904 / 114272 | training loss: 0.3638124465942383\n",
      "epoch: 3 | 55936 / 114272 | training loss: 0.004383201245218515\n",
      "epoch: 3 | 55968 / 114272 | training loss: 0.00713725620880723\n",
      "epoch: 3 | 56000 / 114272 | training loss: 0.040123388171195984\n",
      "epoch: 3 | 56032 / 114272 | training loss: 0.19490279257297516\n",
      "epoch: 3 | 56064 / 114272 | training loss: 0.008218416012823582\n",
      "epoch: 3 | 56096 / 114272 | training loss: 0.11928681284189224\n",
      "epoch: 3 | 56128 / 114272 | training loss: 0.0031145084649324417\n",
      "epoch: 3 | 56160 / 114272 | training loss: 0.005588494706898928\n",
      "epoch: 3 | 56192 / 114272 | training loss: 0.3658048212528229\n",
      "epoch: 3 | 56224 / 114272 | training loss: 0.012024828232824802\n",
      "epoch: 3 | 56256 / 114272 | training loss: 0.054553475230932236\n",
      "epoch: 3 | 56288 / 114272 | training loss: 0.006374932825565338\n",
      "epoch: 3 | 56320 / 114272 | training loss: 0.0035351139958947897\n",
      "epoch: 3 | 56352 / 114272 | training loss: 0.20663630962371826\n",
      "epoch: 3 | 56384 / 114272 | training loss: 0.0611865408718586\n",
      "epoch: 3 | 56416 / 114272 | training loss: 0.06152098625898361\n",
      "epoch: 3 | 56448 / 114272 | training loss: 0.007909034378826618\n",
      "epoch: 3 | 56480 / 114272 | training loss: 0.00834075640887022\n",
      "epoch: 3 | 56512 / 114272 | training loss: 0.09401087462902069\n",
      "epoch: 3 | 56544 / 114272 | training loss: 0.0040581426583230495\n",
      "epoch: 3 | 56576 / 114272 | training loss: 0.13225224614143372\n",
      "epoch: 3 | 56608 / 114272 | training loss: 0.017911257222294807\n",
      "epoch: 3 | 56640 / 114272 | training loss: 0.004524890333414078\n",
      "epoch: 3 | 56672 / 114272 | training loss: 0.0033504546154290438\n",
      "epoch: 3 | 56704 / 114272 | training loss: 0.14597588777542114\n",
      "epoch: 3 | 56736 / 114272 | training loss: 0.00609463918954134\n",
      "epoch: 3 | 56768 / 114272 | training loss: 0.007606533356010914\n",
      "epoch: 3 | 56800 / 114272 | training loss: 0.01406629104167223\n",
      "epoch: 3 | 56832 / 114272 | training loss: 0.09002567082643509\n",
      "epoch: 3 | 56864 / 114272 | training loss: 0.0040090493857860565\n",
      "epoch: 3 | 56896 / 114272 | training loss: 0.012663018889725208\n",
      "epoch: 3 | 56928 / 114272 | training loss: 0.3947486877441406\n",
      "epoch: 3 | 56960 / 114272 | training loss: 0.003121842397376895\n",
      "epoch: 3 | 56992 / 114272 | training loss: 0.004537641070783138\n",
      "epoch: 3 | 57024 / 114272 | training loss: 0.005400330759584904\n",
      "epoch: 3 | 57056 / 114272 | training loss: 0.020037343725562096\n",
      "epoch: 3 | 57088 / 114272 | training loss: 0.0075533161871135235\n",
      "epoch: 3 | 57120 / 114272 | training loss: 0.00812037754803896\n",
      "epoch: 3 | 57152 / 114272 | training loss: 0.0037250251043587923\n",
      "epoch: 3 | 57184 / 114272 | training loss: 0.23768183588981628\n",
      "epoch: 3 | 57216 / 114272 | training loss: 0.16946253180503845\n",
      "epoch: 3 | 57248 / 114272 | training loss: 0.2582307457923889\n",
      "epoch: 3 | 57280 / 114272 | training loss: 0.04293116554617882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 57312 / 114272 | training loss: 0.004902217537164688\n",
      "epoch: 3 | 57344 / 114272 | training loss: 0.0055266390554606915\n",
      "epoch: 3 | 57376 / 114272 | training loss: 0.0028774773236364126\n",
      "epoch: 3 | 57408 / 114272 | training loss: 0.02130683697760105\n",
      "epoch: 3 | 57440 / 114272 | training loss: 0.14392715692520142\n",
      "epoch: 3 | 57472 / 114272 | training loss: 0.0034521932248026133\n",
      "epoch: 3 | 57504 / 114272 | training loss: 0.0028480023611336946\n",
      "epoch: 3 | 57536 / 114272 | training loss: 0.002274511381983757\n",
      "epoch: 3 | 57568 / 114272 | training loss: 0.05594038590788841\n",
      "epoch: 3 | 57600 / 114272 | training loss: 0.00571295665577054\n",
      "epoch: 3 | 57632 / 114272 | training loss: 0.005636273883283138\n",
      "epoch: 3 | 57664 / 114272 | training loss: 0.13433456420898438\n",
      "epoch: 3 | 57696 / 114272 | training loss: 0.00708179222419858\n",
      "epoch: 3 | 57728 / 114272 | training loss: 0.10777489840984344\n",
      "epoch: 3 | 57760 / 114272 | training loss: 0.3207632303237915\n",
      "epoch: 3 | 57792 / 114272 | training loss: 0.09609575569629669\n",
      "epoch: 3 | 57824 / 114272 | training loss: 0.0028963852673768997\n",
      "epoch: 3 | 57856 / 114272 | training loss: 0.0020456742495298386\n",
      "epoch: 3 | 57888 / 114272 | training loss: 0.16696377098560333\n",
      "epoch: 3 | 57920 / 114272 | training loss: 0.005669664591550827\n",
      "epoch: 3 | 57952 / 114272 | training loss: 0.24462413787841797\n",
      "epoch: 3 | 57984 / 114272 | training loss: 0.020442210137844086\n",
      "epoch: 3 | 58016 / 114272 | training loss: 0.21917429566383362\n",
      "epoch: 3 | 58048 / 114272 | training loss: 0.05022513121366501\n",
      "epoch: 3 | 58080 / 114272 | training loss: 0.005214438308030367\n",
      "epoch: 3 | 58112 / 114272 | training loss: 0.043504878878593445\n",
      "epoch: 3 | 58144 / 114272 | training loss: 0.00983638409525156\n",
      "epoch: 3 | 58176 / 114272 | training loss: 0.012413758784532547\n",
      "epoch: 3 | 58208 / 114272 | training loss: 0.013317156583070755\n",
      "epoch: 3 | 58240 / 114272 | training loss: 0.21265439689159393\n",
      "epoch: 3 | 58272 / 114272 | training loss: 0.007593746297061443\n",
      "epoch: 3 | 58304 / 114272 | training loss: 0.0022049793042242527\n",
      "epoch: 3 | 58336 / 114272 | training loss: 0.014014380052685738\n",
      "epoch: 3 | 58368 / 114272 | training loss: 0.006193580571562052\n",
      "epoch: 3 | 58400 / 114272 | training loss: 0.07960541546344757\n",
      "epoch: 3 | 58432 / 114272 | training loss: 0.07999064028263092\n",
      "epoch: 3 | 58464 / 114272 | training loss: 0.11228890717029572\n",
      "epoch: 3 | 58496 / 114272 | training loss: 0.02250334434211254\n",
      "epoch: 3 | 58528 / 114272 | training loss: 0.08481243997812271\n",
      "epoch: 3 | 58560 / 114272 | training loss: 0.004628400318324566\n",
      "epoch: 3 | 58592 / 114272 | training loss: 0.008559479378163815\n",
      "epoch: 3 | 58624 / 114272 | training loss: 0.14652661979198456\n",
      "epoch: 3 | 58656 / 114272 | training loss: 0.007329929620027542\n",
      "epoch: 3 | 58688 / 114272 | training loss: 0.06103293597698212\n",
      "epoch: 3 | 58720 / 114272 | training loss: 0.006193504668772221\n",
      "epoch: 3 | 58752 / 114272 | training loss: 0.09084141999483109\n",
      "epoch: 3 | 58784 / 114272 | training loss: 0.002357310149818659\n",
      "epoch: 3 | 58816 / 114272 | training loss: 0.0041772834956645966\n",
      "epoch: 3 | 58848 / 114272 | training loss: 0.1291508972644806\n",
      "epoch: 3 | 58880 / 114272 | training loss: 0.0027327181305736303\n",
      "epoch: 3 | 58912 / 114272 | training loss: 0.0035758516751229763\n",
      "epoch: 3 | 58944 / 114272 | training loss: 0.16317839920520782\n",
      "epoch: 3 | 58976 / 114272 | training loss: 0.35568514466285706\n",
      "epoch: 3 | 59008 / 114272 | training loss: 0.007603579666465521\n",
      "epoch: 3 | 59040 / 114272 | training loss: 0.09974261373281479\n",
      "epoch: 3 | 59072 / 114272 | training loss: 0.001533130882307887\n",
      "epoch: 3 | 59104 / 114272 | training loss: 0.06927599757909775\n",
      "epoch: 3 | 59136 / 114272 | training loss: 0.004662560299038887\n",
      "epoch: 3 | 59168 / 114272 | training loss: 0.07791117578744888\n",
      "epoch: 3 | 59200 / 114272 | training loss: 0.10950368642807007\n",
      "epoch: 3 | 59232 / 114272 | training loss: 0.0038609548937529325\n",
      "epoch: 3 | 59264 / 114272 | training loss: 0.1693965047597885\n",
      "epoch: 3 | 59296 / 114272 | training loss: 0.0018615511944517493\n",
      "epoch: 3 | 59328 / 114272 | training loss: 0.0019786979537457228\n",
      "epoch: 3 | 59360 / 114272 | training loss: 0.004621043801307678\n",
      "epoch: 3 | 59392 / 114272 | training loss: 0.2557032108306885\n",
      "epoch: 3 | 59424 / 114272 | training loss: 0.073222815990448\n",
      "epoch: 3 | 59456 / 114272 | training loss: 0.2204505056142807\n",
      "epoch: 3 | 59488 / 114272 | training loss: 0.002404649741947651\n",
      "epoch: 3 | 59520 / 114272 | training loss: 0.003972583916038275\n",
      "epoch: 3 | 59552 / 114272 | training loss: 0.05590849369764328\n",
      "epoch: 3 | 59584 / 114272 | training loss: 0.062034934759140015\n",
      "epoch: 3 | 59616 / 114272 | training loss: 0.005516387522220612\n",
      "epoch: 3 | 59648 / 114272 | training loss: 0.0828980803489685\n",
      "epoch: 3 | 59680 / 114272 | training loss: 0.0034557541366666555\n",
      "epoch: 3 | 59712 / 114272 | training loss: 0.006112436763942242\n",
      "epoch: 3 | 59744 / 114272 | training loss: 0.006615038961172104\n",
      "epoch: 3 | 59776 / 114272 | training loss: 0.0039049198385328054\n",
      "epoch: 3 | 59808 / 114272 | training loss: 0.076145239174366\n",
      "epoch: 3 | 59840 / 114272 | training loss: 0.0037904037162661552\n",
      "epoch: 3 | 59872 / 114272 | training loss: 0.11114475876092911\n",
      "epoch: 3 | 59904 / 114272 | training loss: 0.005896991118788719\n",
      "epoch: 3 | 59936 / 114272 | training loss: 0.01360291801393032\n",
      "epoch: 3 | 59968 / 114272 | training loss: 0.00898297131061554\n",
      "epoch: 3 | 60000 / 114272 | training loss: 0.003641431452706456\n",
      "epoch: 3 | 60032 / 114272 | training loss: 0.01711069606244564\n",
      "epoch: 3 | 60064 / 114272 | training loss: 0.027061933651566505\n",
      "epoch: 3 | 60096 / 114272 | training loss: 0.012912224046885967\n",
      "epoch: 3 | 60128 / 114272 | training loss: 0.008316014893352985\n",
      "epoch: 3 | 60160 / 114272 | training loss: 0.09394850581884384\n",
      "epoch: 3 | 60192 / 114272 | training loss: 0.006029630545526743\n",
      "epoch: 3 | 60224 / 114272 | training loss: 0.003511944320052862\n",
      "epoch: 3 | 60256 / 114272 | training loss: 0.05677780508995056\n",
      "epoch: 3 | 60288 / 114272 | training loss: 0.006819162983447313\n",
      "epoch: 3 | 60320 / 114272 | training loss: 0.002502061426639557\n",
      "epoch: 3 | 60352 / 114272 | training loss: 0.004317171406000853\n",
      "epoch: 3 | 60384 / 114272 | training loss: 0.07175955921411514\n",
      "epoch: 3 | 60416 / 114272 | training loss: 0.0014138052938506007\n",
      "epoch: 3 | 60448 / 114272 | training loss: 0.031328655779361725\n",
      "epoch: 3 | 60480 / 114272 | training loss: 0.0041703954339027405\n",
      "epoch: 3 | 60512 / 114272 | training loss: 0.18115997314453125\n",
      "epoch: 3 | 60544 / 114272 | training loss: 0.019440585747361183\n",
      "epoch: 3 | 60576 / 114272 | training loss: 0.07585282623767853\n",
      "epoch: 3 | 60608 / 114272 | training loss: 0.008920557796955109\n",
      "epoch: 3 | 60640 / 114272 | training loss: 0.0048589035868644714\n",
      "epoch: 3 | 60672 / 114272 | training loss: 0.12163592129945755\n",
      "epoch: 3 | 60704 / 114272 | training loss: 0.0032662898302078247\n",
      "epoch: 3 | 60736 / 114272 | training loss: 0.023138398304581642\n",
      "epoch: 3 | 60768 / 114272 | training loss: 0.3988252580165863\n",
      "epoch: 3 | 60800 / 114272 | training loss: 0.004857882857322693\n",
      "epoch: 3 | 60832 / 114272 | training loss: 0.07710463553667068\n",
      "epoch: 3 | 60864 / 114272 | training loss: 0.28073450922966003\n",
      "epoch: 3 | 60896 / 114272 | training loss: 0.003052263054996729\n",
      "epoch: 3 | 60928 / 114272 | training loss: 0.06689047068357468\n",
      "epoch: 3 | 60960 / 114272 | training loss: 0.00961342267692089\n",
      "epoch: 3 | 60992 / 114272 | training loss: 0.006767968647181988\n",
      "epoch: 3 | 61024 / 114272 | training loss: 0.06840897351503372\n",
      "epoch: 3 | 61056 / 114272 | training loss: 0.21086956560611725\n",
      "epoch: 3 | 61088 / 114272 | training loss: 0.007462623529136181\n",
      "epoch: 3 | 61120 / 114272 | training loss: 0.006450059358030558\n",
      "epoch: 3 | 61152 / 114272 | training loss: 0.08123625814914703\n",
      "epoch: 3 | 61184 / 114272 | training loss: 0.02998788096010685\n",
      "epoch: 3 | 61216 / 114272 | training loss: 0.09232994168996811\n",
      "epoch: 3 | 61248 / 114272 | training loss: 0.0017669175285845995\n",
      "epoch: 3 | 61280 / 114272 | training loss: 0.10567614436149597\n",
      "epoch: 3 | 61312 / 114272 | training loss: 0.07027915120124817\n",
      "epoch: 3 | 61344 / 114272 | training loss: 0.0019771382212638855\n",
      "epoch: 3 | 61376 / 114272 | training loss: 0.15885233879089355\n",
      "epoch: 3 | 61408 / 114272 | training loss: 0.0014186663320288062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 61440 / 114272 | training loss: 0.04278277978301048\n",
      "epoch: 3 | 61472 / 114272 | training loss: 0.002927390392869711\n",
      "epoch: 3 | 61504 / 114272 | training loss: 0.005974253173917532\n",
      "epoch: 3 | 61536 / 114272 | training loss: 0.008169875480234623\n",
      "epoch: 3 | 61568 / 114272 | training loss: 0.004116842523217201\n",
      "epoch: 3 | 61600 / 114272 | training loss: 0.0022887010127305984\n",
      "epoch: 3 | 61632 / 114272 | training loss: 0.0498720146715641\n",
      "epoch: 3 | 61664 / 114272 | training loss: 0.01343550905585289\n",
      "epoch: 3 | 61696 / 114272 | training loss: 0.013061970472335815\n",
      "epoch: 3 | 61728 / 114272 | training loss: 0.08180420100688934\n",
      "epoch: 3 | 61760 / 114272 | training loss: 0.14632579684257507\n",
      "epoch: 3 | 61792 / 114272 | training loss: 0.004369367845356464\n",
      "epoch: 3 | 61824 / 114272 | training loss: 0.007472872734069824\n",
      "epoch: 3 | 61856 / 114272 | training loss: 0.2519378364086151\n",
      "epoch: 3 | 61888 / 114272 | training loss: 0.06275921314954758\n",
      "epoch: 3 | 61920 / 114272 | training loss: 0.007131085265427828\n",
      "epoch: 3 | 61952 / 114272 | training loss: 0.004648780450224876\n",
      "epoch: 3 | 61984 / 114272 | training loss: 0.09822555631399155\n",
      "epoch: 3 | 62016 / 114272 | training loss: 0.012656972743570805\n",
      "epoch: 3 | 62048 / 114272 | training loss: 0.0042901672422885895\n",
      "epoch: 3 | 62080 / 114272 | training loss: 0.007815645076334476\n",
      "epoch: 3 | 62112 / 114272 | training loss: 0.19080328941345215\n",
      "epoch: 3 | 62144 / 114272 | training loss: 0.0023157752584666014\n",
      "epoch: 3 | 62176 / 114272 | training loss: 0.002364265499636531\n",
      "epoch: 3 | 62208 / 114272 | training loss: 0.1751001626253128\n",
      "epoch: 3 | 62240 / 114272 | training loss: 0.01153866108506918\n",
      "epoch: 3 | 62272 / 114272 | training loss: 0.01732618734240532\n",
      "epoch: 3 | 62304 / 114272 | training loss: 0.006320510059595108\n",
      "epoch: 3 | 62336 / 114272 | training loss: 0.006332136690616608\n",
      "epoch: 3 | 62368 / 114272 | training loss: 0.35442376136779785\n",
      "epoch: 3 | 62400 / 114272 | training loss: 0.007740847300738096\n",
      "epoch: 3 | 62432 / 114272 | training loss: 0.012329086661338806\n",
      "epoch: 3 | 62464 / 114272 | training loss: 0.021167360246181488\n",
      "epoch: 3 | 62496 / 114272 | training loss: 0.2452777773141861\n",
      "epoch: 3 | 62528 / 114272 | training loss: 0.003939587157219648\n",
      "epoch: 3 | 62560 / 114272 | training loss: 0.0030798171646893024\n",
      "epoch: 3 | 62592 / 114272 | training loss: 0.17241694033145905\n",
      "epoch: 3 | 62624 / 114272 | training loss: 0.18568380177021027\n",
      "epoch: 3 | 62656 / 114272 | training loss: 0.006129034794867039\n",
      "epoch: 3 | 62688 / 114272 | training loss: 0.2309092879295349\n",
      "epoch: 3 | 62720 / 114272 | training loss: 0.10721921920776367\n",
      "epoch: 3 | 62752 / 114272 | training loss: 0.005346944555640221\n",
      "epoch: 3 | 62784 / 114272 | training loss: 0.06654826551675797\n",
      "epoch: 3 | 62816 / 114272 | training loss: 0.010934820398688316\n",
      "epoch: 3 | 62848 / 114272 | training loss: 0.1389419436454773\n",
      "epoch: 3 | 62880 / 114272 | training loss: 0.20797717571258545\n",
      "epoch: 3 | 62912 / 114272 | training loss: 0.009670361876487732\n",
      "epoch: 3 | 62944 / 114272 | training loss: 0.037096310406923294\n",
      "epoch: 3 | 62976 / 114272 | training loss: 0.14641079306602478\n",
      "epoch: 3 | 63008 / 114272 | training loss: 0.0031705161090940237\n",
      "epoch: 3 | 63040 / 114272 | training loss: 0.004829654935747385\n",
      "epoch: 3 | 63072 / 114272 | training loss: 0.06802444159984589\n",
      "epoch: 3 | 63104 / 114272 | training loss: 0.01314943190664053\n",
      "epoch: 3 | 63136 / 114272 | training loss: 0.007257990073412657\n",
      "epoch: 3 | 63168 / 114272 | training loss: 0.004414644557982683\n",
      "epoch: 3 | 63200 / 114272 | training loss: 0.003181334352120757\n",
      "epoch: 3 | 63232 / 114272 | training loss: 0.4757043123245239\n",
      "epoch: 3 | 63264 / 114272 | training loss: 0.15398889780044556\n",
      "epoch: 3 | 63296 / 114272 | training loss: 0.010824074037373066\n",
      "epoch: 3 | 63328 / 114272 | training loss: 0.020214157178997993\n",
      "epoch: 3 | 63360 / 114272 | training loss: 0.0946098268032074\n",
      "epoch: 3 | 63392 / 114272 | training loss: 0.004282876383513212\n",
      "epoch: 3 | 63424 / 114272 | training loss: 0.004812816623598337\n",
      "epoch: 3 | 63456 / 114272 | training loss: 0.0218318123370409\n",
      "epoch: 3 | 63488 / 114272 | training loss: 0.012848787009716034\n",
      "epoch: 3 | 63520 / 114272 | training loss: 0.0038437468465417624\n",
      "epoch: 3 | 63552 / 114272 | training loss: 0.10967214405536652\n",
      "epoch: 3 | 63584 / 114272 | training loss: 0.0069610802456736565\n",
      "epoch: 3 | 63616 / 114272 | training loss: 0.20836061239242554\n",
      "epoch: 3 | 63648 / 114272 | training loss: 0.2864687144756317\n",
      "epoch: 3 | 63680 / 114272 | training loss: 0.1434548944234848\n",
      "epoch: 3 | 63712 / 114272 | training loss: 0.1147141084074974\n",
      "epoch: 3 | 63744 / 114272 | training loss: 0.1311177909374237\n",
      "epoch: 3 | 63776 / 114272 | training loss: 0.006144633516669273\n",
      "epoch: 3 | 63808 / 114272 | training loss: 0.031796809285879135\n",
      "epoch: 3 | 63840 / 114272 | training loss: 0.0032895745243877172\n",
      "epoch: 3 | 63872 / 114272 | training loss: 0.007801981642842293\n",
      "epoch: 3 | 63904 / 114272 | training loss: 0.011148668825626373\n",
      "epoch: 3 | 63936 / 114272 | training loss: 0.00829625129699707\n",
      "epoch: 3 | 63968 / 114272 | training loss: 0.03547472134232521\n",
      "epoch: 3 | 64000 / 114272 | training loss: 0.011216345243155956\n",
      "epoch: 3 | 64032 / 114272 | training loss: 0.006436711177229881\n",
      "epoch: 3 | 64064 / 114272 | training loss: 0.008608600124716759\n",
      "epoch: 3 | 64096 / 114272 | training loss: 0.003238242119550705\n",
      "epoch: 3 | 64128 / 114272 | training loss: 0.006269118748605251\n",
      "epoch: 3 | 64160 / 114272 | training loss: 0.10930036008358002\n",
      "epoch: 3 | 64192 / 114272 | training loss: 0.002753159264102578\n",
      "epoch: 3 | 64224 / 114272 | training loss: 0.11803330481052399\n",
      "epoch: 3 | 64256 / 114272 | training loss: 0.00854534562677145\n",
      "epoch: 3 | 64288 / 114272 | training loss: 0.11914114654064178\n",
      "epoch: 3 | 64320 / 114272 | training loss: 0.00810686033219099\n",
      "epoch: 3 | 64352 / 114272 | training loss: 0.13865390419960022\n",
      "epoch: 3 | 64384 / 114272 | training loss: 0.007111354731023312\n",
      "epoch: 3 | 64416 / 114272 | training loss: 0.10726413875818253\n",
      "epoch: 3 | 64448 / 114272 | training loss: 0.005464633926749229\n",
      "epoch: 3 | 64480 / 114272 | training loss: 0.013663213700056076\n",
      "epoch: 3 | 64512 / 114272 | training loss: 0.022143471986055374\n",
      "epoch: 3 | 64544 / 114272 | training loss: 0.002874450758099556\n",
      "epoch: 3 | 64576 / 114272 | training loss: 0.005735363811254501\n",
      "epoch: 3 | 64608 / 114272 | training loss: 0.07878537476062775\n",
      "epoch: 3 | 64640 / 114272 | training loss: 0.0032495595514774323\n",
      "epoch: 3 | 64672 / 114272 | training loss: 0.10428545624017715\n",
      "epoch: 3 | 64704 / 114272 | training loss: 0.005562187638133764\n",
      "epoch: 3 | 64736 / 114272 | training loss: 0.07834863662719727\n",
      "epoch: 3 | 64768 / 114272 | training loss: 0.006570414640009403\n",
      "epoch: 3 | 64800 / 114272 | training loss: 0.21256910264492035\n",
      "epoch: 3 | 64832 / 114272 | training loss: 0.011984771117568016\n",
      "epoch: 3 | 64864 / 114272 | training loss: 0.11008723825216293\n",
      "epoch: 3 | 64896 / 114272 | training loss: 0.1517341434955597\n",
      "epoch: 3 | 64928 / 114272 | training loss: 0.21879823505878448\n",
      "epoch: 3 | 64960 / 114272 | training loss: 0.008677047677338123\n",
      "epoch: 3 | 64992 / 114272 | training loss: 0.088274747133255\n",
      "epoch: 3 | 65024 / 114272 | training loss: 0.13640235364437103\n",
      "epoch: 3 | 65056 / 114272 | training loss: 0.27210691571235657\n",
      "epoch: 3 | 65088 / 114272 | training loss: 0.007918503135442734\n",
      "epoch: 3 | 65120 / 114272 | training loss: 0.1478874385356903\n",
      "epoch: 3 | 65152 / 114272 | training loss: 0.0028871556278318167\n",
      "epoch: 3 | 65184 / 114272 | training loss: 0.00797833688557148\n",
      "epoch: 3 | 65216 / 114272 | training loss: 0.018987976014614105\n",
      "epoch: 3 | 65248 / 114272 | training loss: 0.006220968905836344\n",
      "epoch: 3 | 65280 / 114272 | training loss: 0.004442603327333927\n",
      "epoch: 3 | 65312 / 114272 | training loss: 0.0035701319575309753\n",
      "epoch: 3 | 65344 / 114272 | training loss: 0.009747061878442764\n",
      "epoch: 3 | 65376 / 114272 | training loss: 0.03005835972726345\n",
      "epoch: 3 | 65408 / 114272 | training loss: 0.010282370261847973\n",
      "epoch: 3 | 65440 / 114272 | training loss: 0.003731941804289818\n",
      "epoch: 3 | 65472 / 114272 | training loss: 0.01923901028931141\n",
      "epoch: 3 | 65504 / 114272 | training loss: 0.002750155283138156\n",
      "epoch: 3 | 65536 / 114272 | training loss: 0.008519387803971767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 65568 / 114272 | training loss: 0.2902058959007263\n",
      "epoch: 3 | 65600 / 114272 | training loss: 0.011600682511925697\n",
      "epoch: 3 | 65632 / 114272 | training loss: 0.19375483691692352\n",
      "epoch: 3 | 65664 / 114272 | training loss: 0.0065801870077848434\n",
      "epoch: 3 | 65696 / 114272 | training loss: 0.004607903305441141\n",
      "epoch: 3 | 65728 / 114272 | training loss: 0.1409039944410324\n",
      "epoch: 3 | 65760 / 114272 | training loss: 0.03586406633257866\n",
      "epoch: 3 | 65792 / 114272 | training loss: 0.12662513554096222\n",
      "epoch: 3 | 65824 / 114272 | training loss: 0.012126627378165722\n",
      "epoch: 3 | 65856 / 114272 | training loss: 0.025145750492811203\n",
      "epoch: 3 | 65888 / 114272 | training loss: 0.1637909859418869\n",
      "epoch: 3 | 65920 / 114272 | training loss: 0.009429977275431156\n",
      "epoch: 3 | 65952 / 114272 | training loss: 0.002627916634082794\n",
      "epoch: 3 | 65984 / 114272 | training loss: 0.0037709642201662064\n",
      "epoch: 3 | 66016 / 114272 | training loss: 0.05872691050171852\n",
      "epoch: 3 | 66048 / 114272 | training loss: 0.11553605645895004\n",
      "epoch: 3 | 66080 / 114272 | training loss: 0.007181864697486162\n",
      "epoch: 3 | 66112 / 114272 | training loss: 0.0104975001886487\n",
      "epoch: 3 | 66144 / 114272 | training loss: 0.025682443752884865\n",
      "epoch: 3 | 66176 / 114272 | training loss: 0.0017289400566369295\n",
      "epoch: 3 | 66208 / 114272 | training loss: 0.0032670460641384125\n",
      "epoch: 3 | 66240 / 114272 | training loss: 0.016744956374168396\n",
      "epoch: 3 | 66272 / 114272 | training loss: 0.004131146240979433\n",
      "epoch: 3 | 66304 / 114272 | training loss: 0.0021605517249554396\n",
      "epoch: 3 | 66336 / 114272 | training loss: 0.013255581259727478\n",
      "epoch: 3 | 66368 / 114272 | training loss: 0.3744184672832489\n",
      "epoch: 3 | 66400 / 114272 | training loss: 0.0040737236849963665\n",
      "epoch: 3 | 66432 / 114272 | training loss: 0.26109591126441956\n",
      "epoch: 3 | 66464 / 114272 | training loss: 0.03295106440782547\n",
      "epoch: 3 | 66496 / 114272 | training loss: 0.04333534091711044\n",
      "epoch: 3 | 66528 / 114272 | training loss: 0.11370953172445297\n",
      "epoch: 3 | 66560 / 114272 | training loss: 0.002175986533984542\n",
      "epoch: 3 | 66592 / 114272 | training loss: 0.004914524499326944\n",
      "epoch: 3 | 66624 / 114272 | training loss: 0.1257210224866867\n",
      "epoch: 3 | 66656 / 114272 | training loss: 0.13104648888111115\n",
      "epoch: 3 | 66688 / 114272 | training loss: 0.006882792338728905\n",
      "epoch: 3 | 66720 / 114272 | training loss: 0.06066811829805374\n",
      "epoch: 3 | 66752 / 114272 | training loss: 0.015986984595656395\n",
      "epoch: 3 | 66784 / 114272 | training loss: 0.0039659771136939526\n",
      "epoch: 3 | 66816 / 114272 | training loss: 0.005899849347770214\n",
      "epoch: 3 | 66848 / 114272 | training loss: 0.003812599927186966\n",
      "epoch: 3 | 66880 / 114272 | training loss: 0.004311573226004839\n",
      "epoch: 3 | 66912 / 114272 | training loss: 0.008289220742881298\n",
      "epoch: 3 | 66944 / 114272 | training loss: 0.0015126289799809456\n",
      "epoch: 3 | 66976 / 114272 | training loss: 0.036006465554237366\n",
      "epoch: 3 | 67008 / 114272 | training loss: 0.11336959898471832\n",
      "epoch: 3 | 67040 / 114272 | training loss: 0.13300186395645142\n",
      "epoch: 3 | 67072 / 114272 | training loss: 0.005024155601859093\n",
      "epoch: 3 | 67104 / 114272 | training loss: 0.004697364754974842\n",
      "epoch: 3 | 67136 / 114272 | training loss: 0.0653621032834053\n",
      "epoch: 3 | 67168 / 114272 | training loss: 0.001619559247046709\n",
      "epoch: 3 | 67200 / 114272 | training loss: 0.08022844791412354\n",
      "epoch: 3 | 67232 / 114272 | training loss: 0.002513997256755829\n",
      "epoch: 3 | 67264 / 114272 | training loss: 0.004253644496202469\n",
      "epoch: 3 | 67296 / 114272 | training loss: 0.09782365709543228\n",
      "epoch: 3 | 67328 / 114272 | training loss: 0.11907001584768295\n",
      "epoch: 3 | 67360 / 114272 | training loss: 0.0007498818449676037\n",
      "epoch: 3 | 67392 / 114272 | training loss: 0.09415701031684875\n",
      "epoch: 3 | 67424 / 114272 | training loss: 0.19497403502464294\n",
      "epoch: 3 | 67456 / 114272 | training loss: 0.00300588458776474\n",
      "epoch: 3 | 67488 / 114272 | training loss: 0.14595220983028412\n",
      "epoch: 3 | 67520 / 114272 | training loss: 0.009591455571353436\n",
      "epoch: 3 | 67552 / 114272 | training loss: 0.0029527617152780294\n",
      "epoch: 3 | 67584 / 114272 | training loss: 0.016020502895116806\n",
      "epoch: 3 | 67616 / 114272 | training loss: 0.010258227586746216\n",
      "epoch: 3 | 67648 / 114272 | training loss: 0.003477643011137843\n",
      "epoch: 3 | 67680 / 114272 | training loss: 0.08363471925258636\n",
      "epoch: 3 | 67712 / 114272 | training loss: 0.22111716866493225\n",
      "epoch: 3 | 67744 / 114272 | training loss: 0.00461973063647747\n",
      "epoch: 3 | 67776 / 114272 | training loss: 0.06104166433215141\n",
      "epoch: 3 | 67808 / 114272 | training loss: 0.005334825720638037\n",
      "epoch: 3 | 67840 / 114272 | training loss: 0.011840292252600193\n",
      "epoch: 3 | 67872 / 114272 | training loss: 0.010179954580962658\n",
      "epoch: 3 | 67904 / 114272 | training loss: 0.01472572609782219\n",
      "epoch: 3 | 67936 / 114272 | training loss: 0.1286158710718155\n",
      "epoch: 3 | 67968 / 114272 | training loss: 0.3374079763889313\n",
      "epoch: 3 | 68000 / 114272 | training loss: 0.00324664986692369\n",
      "epoch: 3 | 68032 / 114272 | training loss: 0.21156874299049377\n",
      "epoch: 3 | 68064 / 114272 | training loss: 0.005312837194651365\n",
      "epoch: 3 | 68096 / 114272 | training loss: 0.00789177231490612\n",
      "epoch: 3 | 68128 / 114272 | training loss: 0.19719333946704865\n",
      "epoch: 3 | 68160 / 114272 | training loss: 0.020296147093176842\n",
      "epoch: 3 | 68192 / 114272 | training loss: 0.1364048719406128\n",
      "epoch: 3 | 68224 / 114272 | training loss: 0.06152348592877388\n",
      "epoch: 3 | 68256 / 114272 | training loss: 0.11602284759283066\n",
      "epoch: 3 | 68288 / 114272 | training loss: 0.003434437559917569\n",
      "epoch: 3 | 68320 / 114272 | training loss: 0.09395672380924225\n",
      "epoch: 3 | 68352 / 114272 | training loss: 0.09869512915611267\n",
      "epoch: 3 | 68384 / 114272 | training loss: 0.007484158035367727\n",
      "epoch: 3 | 68416 / 114272 | training loss: 0.12094288319349289\n",
      "epoch: 3 | 68448 / 114272 | training loss: 0.017368923872709274\n",
      "epoch: 3 | 68480 / 114272 | training loss: 0.0030868120957165956\n",
      "epoch: 3 | 68512 / 114272 | training loss: 0.07618232816457748\n",
      "epoch: 3 | 68544 / 114272 | training loss: 0.004691946320235729\n",
      "epoch: 3 | 68576 / 114272 | training loss: 0.06467048823833466\n",
      "epoch: 3 | 68608 / 114272 | training loss: 0.12572826445102692\n",
      "epoch: 3 | 68640 / 114272 | training loss: 0.021122241392731667\n",
      "epoch: 3 | 68672 / 114272 | training loss: 0.007892927154898643\n",
      "epoch: 3 | 68704 / 114272 | training loss: 0.0833691880106926\n",
      "epoch: 3 | 68736 / 114272 | training loss: 0.04256714507937431\n",
      "epoch: 3 | 68768 / 114272 | training loss: 0.07090789824724197\n",
      "epoch: 3 | 68800 / 114272 | training loss: 0.004306493792682886\n",
      "epoch: 3 | 68832 / 114272 | training loss: 0.4624253511428833\n",
      "epoch: 3 | 68864 / 114272 | training loss: 0.5030407309532166\n",
      "epoch: 3 | 68896 / 114272 | training loss: 0.007977930828928947\n",
      "epoch: 3 | 68928 / 114272 | training loss: 0.010546835139393806\n",
      "epoch: 3 | 68960 / 114272 | training loss: 0.1904316395521164\n",
      "epoch: 3 | 68992 / 114272 | training loss: 0.013300321064889431\n",
      "epoch: 3 | 69024 / 114272 | training loss: 0.1097390428185463\n",
      "epoch: 3 | 69056 / 114272 | training loss: 0.16467295587062836\n",
      "epoch: 3 | 69088 / 114272 | training loss: 0.10866080969572067\n",
      "epoch: 3 | 69120 / 114272 | training loss: 0.13418209552764893\n",
      "epoch: 3 | 69152 / 114272 | training loss: 0.08969597518444061\n",
      "epoch: 3 | 69184 / 114272 | training loss: 0.053168315440416336\n",
      "epoch: 3 | 69216 / 114272 | training loss: 0.08759868890047073\n",
      "epoch: 3 | 69248 / 114272 | training loss: 0.05997268483042717\n",
      "epoch: 3 | 69280 / 114272 | training loss: 0.009428058750927448\n",
      "epoch: 3 | 69312 / 114272 | training loss: 0.09465503692626953\n",
      "epoch: 3 | 69344 / 114272 | training loss: 0.007416639477014542\n",
      "epoch: 3 | 69376 / 114272 | training loss: 0.006077350117266178\n",
      "epoch: 3 | 69408 / 114272 | training loss: 0.0613015815615654\n",
      "epoch: 3 | 69440 / 114272 | training loss: 0.069867342710495\n",
      "epoch: 3 | 69472 / 114272 | training loss: 0.14390277862548828\n",
      "epoch: 3 | 69504 / 114272 | training loss: 0.060328159481287\n",
      "epoch: 3 | 69536 / 114272 | training loss: 0.0029623019509017467\n",
      "epoch: 3 | 69568 / 114272 | training loss: 0.00788259506225586\n",
      "epoch: 3 | 69600 / 114272 | training loss: 0.007601197808980942\n",
      "epoch: 3 | 69632 / 114272 | training loss: 0.01793370582163334\n",
      "epoch: 3 | 69664 / 114272 | training loss: 0.06222976744174957\n",
      "epoch: 3 | 69696 / 114272 | training loss: 0.005125933326780796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 69728 / 114272 | training loss: 0.038450248539447784\n",
      "epoch: 3 | 69760 / 114272 | training loss: 0.034792572259902954\n",
      "epoch: 3 | 69792 / 114272 | training loss: 0.05051853135228157\n",
      "epoch: 3 | 69824 / 114272 | training loss: 0.12222745269536972\n",
      "epoch: 3 | 69856 / 114272 | training loss: 0.048618149012327194\n",
      "epoch: 3 | 69888 / 114272 | training loss: 0.11880312860012054\n",
      "epoch: 3 | 69920 / 114272 | training loss: 0.013649556785821915\n",
      "epoch: 3 | 69952 / 114272 | training loss: 0.04510705545544624\n",
      "epoch: 3 | 69984 / 114272 | training loss: 0.011585633270442486\n",
      "epoch: 3 | 70016 / 114272 | training loss: 0.006386264692991972\n",
      "epoch: 3 | 70048 / 114272 | training loss: 0.06048902869224548\n",
      "epoch: 3 | 70080 / 114272 | training loss: 0.012295348569750786\n",
      "epoch: 3 | 70112 / 114272 | training loss: 0.07475947588682175\n",
      "epoch: 3 | 70144 / 114272 | training loss: 0.08409576117992401\n",
      "epoch: 3 | 70176 / 114272 | training loss: 0.008180659264326096\n",
      "epoch: 3 | 70208 / 114272 | training loss: 0.006669051945209503\n",
      "epoch: 3 | 70240 / 114272 | training loss: 0.003039636416360736\n",
      "epoch: 3 | 70272 / 114272 | training loss: 0.2810342609882355\n",
      "epoch: 3 | 70304 / 114272 | training loss: 0.0051409960724413395\n",
      "epoch: 3 | 70336 / 114272 | training loss: 0.004102922510355711\n",
      "epoch: 3 | 70368 / 114272 | training loss: 0.012946155853569508\n",
      "epoch: 3 | 70400 / 114272 | training loss: 0.09325562417507172\n",
      "epoch: 3 | 70432 / 114272 | training loss: 0.07254508137702942\n",
      "epoch: 3 | 70464 / 114272 | training loss: 0.00934863742440939\n",
      "epoch: 3 | 70496 / 114272 | training loss: 0.08169116824865341\n",
      "epoch: 3 | 70528 / 114272 | training loss: 0.16546230018138885\n",
      "epoch: 3 | 70560 / 114272 | training loss: 0.0840831845998764\n",
      "epoch: 3 | 70592 / 114272 | training loss: 0.01633518934249878\n",
      "epoch: 3 | 70624 / 114272 | training loss: 0.003368290374055505\n",
      "epoch: 3 | 70656 / 114272 | training loss: 0.049806393682956696\n",
      "epoch: 3 | 70688 / 114272 | training loss: 0.15544718503952026\n",
      "epoch: 3 | 70720 / 114272 | training loss: 0.22347210347652435\n",
      "epoch: 3 | 70752 / 114272 | training loss: 0.06125288084149361\n",
      "epoch: 3 | 70784 / 114272 | training loss: 0.03435956686735153\n",
      "epoch: 3 | 70816 / 114272 | training loss: 0.0028014297131448984\n",
      "epoch: 3 | 70848 / 114272 | training loss: 0.0016720524290576577\n",
      "epoch: 3 | 70880 / 114272 | training loss: 0.18925917148590088\n",
      "epoch: 3 | 70912 / 114272 | training loss: 0.007457333151251078\n",
      "epoch: 3 | 70944 / 114272 | training loss: 0.08976400643587112\n",
      "epoch: 3 | 70976 / 114272 | training loss: 0.005364171229302883\n",
      "epoch: 3 | 71008 / 114272 | training loss: 0.004240746144205332\n",
      "epoch: 3 | 71040 / 114272 | training loss: 0.005245980806648731\n",
      "epoch: 3 | 71072 / 114272 | training loss: 0.22950604557991028\n",
      "epoch: 3 | 71104 / 114272 | training loss: 0.015198703855276108\n",
      "epoch: 3 | 71136 / 114272 | training loss: 0.09612654149532318\n",
      "epoch: 3 | 71168 / 114272 | training loss: 0.03635339438915253\n",
      "epoch: 3 | 71200 / 114272 | training loss: 0.1661214530467987\n",
      "epoch: 3 | 71232 / 114272 | training loss: 0.176398366689682\n",
      "epoch: 3 | 71264 / 114272 | training loss: 0.16883434355258942\n",
      "epoch: 3 | 71296 / 114272 | training loss: 0.0026445689145475626\n",
      "epoch: 3 | 71328 / 114272 | training loss: 0.028696849942207336\n",
      "epoch: 3 | 71360 / 114272 | training loss: 0.008144691586494446\n",
      "epoch: 3 | 71392 / 114272 | training loss: 0.0104755200445652\n",
      "epoch: 3 | 71424 / 114272 | training loss: 0.014928670600056648\n",
      "epoch: 3 | 71456 / 114272 | training loss: 0.013602277263998985\n",
      "epoch: 3 | 71488 / 114272 | training loss: 0.11079724878072739\n",
      "epoch: 3 | 71520 / 114272 | training loss: 0.036132216453552246\n",
      "epoch: 3 | 71552 / 114272 | training loss: 0.39575183391571045\n",
      "epoch: 3 | 71584 / 114272 | training loss: 0.0484921857714653\n",
      "epoch: 3 | 71616 / 114272 | training loss: 0.004908360540866852\n",
      "epoch: 3 | 71648 / 114272 | training loss: 0.011199476197361946\n",
      "epoch: 3 | 71680 / 114272 | training loss: 0.07183858007192612\n",
      "epoch: 3 | 71712 / 114272 | training loss: 0.022211842238903046\n",
      "epoch: 3 | 71744 / 114272 | training loss: 0.13309180736541748\n",
      "epoch: 3 | 71776 / 114272 | training loss: 0.0031430956441909075\n",
      "epoch: 3 | 71808 / 114272 | training loss: 0.0050102039240300655\n",
      "epoch: 3 | 71840 / 114272 | training loss: 0.30158913135528564\n",
      "epoch: 3 | 71872 / 114272 | training loss: 0.13255378603935242\n",
      "epoch: 3 | 71904 / 114272 | training loss: 0.0748811662197113\n",
      "epoch: 3 | 71936 / 114272 | training loss: 0.07511863112449646\n",
      "epoch: 3 | 71968 / 114272 | training loss: 0.002877551829442382\n",
      "epoch: 3 | 72000 / 114272 | training loss: 0.008512006141245365\n",
      "epoch: 3 | 72032 / 114272 | training loss: 0.12043752521276474\n",
      "epoch: 3 | 72064 / 114272 | training loss: 0.0015404163859784603\n",
      "epoch: 3 | 72096 / 114272 | training loss: 0.060755617916584015\n",
      "epoch: 3 | 72128 / 114272 | training loss: 0.006567625794559717\n",
      "epoch: 3 | 72160 / 114272 | training loss: 0.009284497238695621\n",
      "epoch: 3 | 72192 / 114272 | training loss: 0.12853269279003143\n",
      "epoch: 3 | 72224 / 114272 | training loss: 0.002027220791205764\n",
      "epoch: 3 | 72256 / 114272 | training loss: 0.12533049285411835\n",
      "epoch: 3 | 72288 / 114272 | training loss: 0.08602582663297653\n",
      "epoch: 3 | 72320 / 114272 | training loss: 0.05649348720908165\n",
      "epoch: 3 | 72352 / 114272 | training loss: 0.009978501126170158\n",
      "epoch: 3 | 72384 / 114272 | training loss: 0.10718792676925659\n",
      "epoch: 3 | 72416 / 114272 | training loss: 0.11054319888353348\n",
      "epoch: 3 | 72448 / 114272 | training loss: 0.14292661845684052\n",
      "epoch: 3 | 72480 / 114272 | training loss: 0.0325244776904583\n",
      "epoch: 3 | 72512 / 114272 | training loss: 0.004826661664992571\n",
      "epoch: 3 | 72544 / 114272 | training loss: 0.02369876764714718\n",
      "epoch: 3 | 72576 / 114272 | training loss: 0.03143160417675972\n",
      "epoch: 3 | 72608 / 114272 | training loss: 0.05052655190229416\n",
      "epoch: 3 | 72640 / 114272 | training loss: 0.012727558612823486\n",
      "epoch: 3 | 72672 / 114272 | training loss: 0.0282356608659029\n",
      "epoch: 3 | 72704 / 114272 | training loss: 0.14763414859771729\n",
      "epoch: 3 | 72736 / 114272 | training loss: 0.04623539000749588\n",
      "epoch: 3 | 72768 / 114272 | training loss: 0.13738347589969635\n",
      "epoch: 3 | 72800 / 114272 | training loss: 0.11634397506713867\n",
      "epoch: 3 | 72832 / 114272 | training loss: 0.06901781260967255\n",
      "epoch: 3 | 72864 / 114272 | training loss: 0.012655552476644516\n",
      "epoch: 3 | 72896 / 114272 | training loss: 0.03338714689016342\n",
      "epoch: 3 | 72928 / 114272 | training loss: 0.00921403244137764\n",
      "epoch: 3 | 72960 / 114272 | training loss: 0.12384053319692612\n",
      "epoch: 3 | 72992 / 114272 | training loss: 0.012915506027638912\n",
      "epoch: 3 | 73024 / 114272 | training loss: 0.008424830622971058\n",
      "epoch: 3 | 73056 / 114272 | training loss: 0.013648195192217827\n",
      "epoch: 3 | 73088 / 114272 | training loss: 0.004934131167829037\n",
      "epoch: 3 | 73120 / 114272 | training loss: 0.24409052729606628\n",
      "epoch: 3 | 73152 / 114272 | training loss: 0.13092128932476044\n",
      "epoch: 3 | 73184 / 114272 | training loss: 0.02419072762131691\n",
      "epoch: 3 | 73216 / 114272 | training loss: 0.22805869579315186\n",
      "epoch: 3 | 73248 / 114272 | training loss: 0.06918530911207199\n",
      "epoch: 3 | 73280 / 114272 | training loss: 0.011329005472362041\n",
      "epoch: 3 | 73312 / 114272 | training loss: 0.21615958213806152\n",
      "epoch: 3 | 73344 / 114272 | training loss: 0.007305616978555918\n",
      "epoch: 3 | 73376 / 114272 | training loss: 0.015688931569457054\n",
      "epoch: 3 | 73408 / 114272 | training loss: 0.21101053059101105\n",
      "epoch: 3 | 73440 / 114272 | training loss: 0.006700460333377123\n",
      "epoch: 3 | 73472 / 114272 | training loss: 0.13272416591644287\n",
      "epoch: 3 | 73504 / 114272 | training loss: 0.002311247168108821\n",
      "epoch: 3 | 73536 / 114272 | training loss: 0.24637222290039062\n",
      "epoch: 3 | 73568 / 114272 | training loss: 0.0056605213321745396\n",
      "epoch: 3 | 73600 / 114272 | training loss: 0.15918035805225372\n",
      "epoch: 3 | 73632 / 114272 | training loss: 0.04104193300008774\n",
      "epoch: 3 | 73664 / 114272 | training loss: 0.00527452165260911\n",
      "epoch: 3 | 73696 / 114272 | training loss: 0.18866518139839172\n",
      "epoch: 3 | 73728 / 114272 | training loss: 0.0021945710759609938\n",
      "epoch: 3 | 73760 / 114272 | training loss: 0.08689076453447342\n",
      "epoch: 3 | 73792 / 114272 | training loss: 0.06045887991786003\n",
      "epoch: 3 | 73824 / 114272 | training loss: 0.2671157121658325\n",
      "epoch: 3 | 73856 / 114272 | training loss: 0.06952587515115738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 73888 / 114272 | training loss: 0.08967792987823486\n",
      "epoch: 3 | 73920 / 114272 | training loss: 0.06310168653726578\n",
      "epoch: 3 | 73952 / 114272 | training loss: 0.02356741577386856\n",
      "epoch: 3 | 73984 / 114272 | training loss: 0.012177181430161\n",
      "epoch: 3 | 74016 / 114272 | training loss: 0.09929708391427994\n",
      "epoch: 3 | 74048 / 114272 | training loss: 0.00507090101018548\n",
      "epoch: 3 | 74080 / 114272 | training loss: 0.009507940150797367\n",
      "epoch: 3 | 74112 / 114272 | training loss: 0.24198850989341736\n",
      "epoch: 3 | 74144 / 114272 | training loss: 0.11867508292198181\n",
      "epoch: 3 | 74176 / 114272 | training loss: 0.02472354844212532\n",
      "epoch: 3 | 74208 / 114272 | training loss: 0.17102006077766418\n",
      "epoch: 3 | 74240 / 114272 | training loss: 0.04339787736535072\n",
      "epoch: 3 | 74272 / 114272 | training loss: 0.005963821895420551\n",
      "epoch: 3 | 74304 / 114272 | training loss: 0.01311138179153204\n",
      "epoch: 3 | 74336 / 114272 | training loss: 0.007335233967751265\n",
      "epoch: 3 | 74368 / 114272 | training loss: 0.023641712963581085\n",
      "epoch: 3 | 74400 / 114272 | training loss: 0.049426328390836716\n",
      "epoch: 3 | 74432 / 114272 | training loss: 0.072701595723629\n",
      "epoch: 3 | 74464 / 114272 | training loss: 0.004788440186530352\n",
      "epoch: 3 | 74496 / 114272 | training loss: 0.07564686983823776\n",
      "epoch: 3 | 74528 / 114272 | training loss: 0.25424426794052124\n",
      "epoch: 3 | 74560 / 114272 | training loss: 0.03735615685582161\n",
      "epoch: 3 | 74592 / 114272 | training loss: 0.006956585682928562\n",
      "epoch: 3 | 74624 / 114272 | training loss: 0.004570608492940664\n",
      "epoch: 3 | 74656 / 114272 | training loss: 0.17730852961540222\n",
      "epoch: 3 | 74688 / 114272 | training loss: 0.19686733186244965\n",
      "epoch: 3 | 74720 / 114272 | training loss: 0.12801571190357208\n",
      "epoch: 3 | 74752 / 114272 | training loss: 0.003620804287493229\n",
      "epoch: 3 | 74784 / 114272 | training loss: 0.0048685926012694836\n",
      "epoch: 3 | 74816 / 114272 | training loss: 0.004292694851756096\n",
      "epoch: 3 | 74848 / 114272 | training loss: 0.01922725699841976\n",
      "epoch: 3 | 74880 / 114272 | training loss: 0.002379463752731681\n",
      "epoch: 3 | 74912 / 114272 | training loss: 0.0939795970916748\n",
      "epoch: 3 | 74944 / 114272 | training loss: 0.11651911586523056\n",
      "epoch: 3 | 74976 / 114272 | training loss: 0.08765724301338196\n",
      "epoch: 3 | 75008 / 114272 | training loss: 0.009027036838233471\n",
      "epoch: 3 | 75040 / 114272 | training loss: 0.003456090111285448\n",
      "epoch: 3 | 75072 / 114272 | training loss: 0.006588908843696117\n",
      "epoch: 3 | 75104 / 114272 | training loss: 0.00596786942332983\n",
      "epoch: 3 | 75136 / 114272 | training loss: 0.13791121542453766\n",
      "epoch: 3 | 75168 / 114272 | training loss: 0.012028559111058712\n",
      "epoch: 3 | 75200 / 114272 | training loss: 0.027841635048389435\n",
      "epoch: 3 | 75232 / 114272 | training loss: 0.013671711087226868\n",
      "epoch: 3 | 75264 / 114272 | training loss: 0.11841347813606262\n",
      "epoch: 3 | 75296 / 114272 | training loss: 0.27171143889427185\n",
      "epoch: 3 | 75328 / 114272 | training loss: 0.003400271525606513\n",
      "epoch: 3 | 75360 / 114272 | training loss: 0.013379755429923534\n",
      "epoch: 3 | 75392 / 114272 | training loss: 0.009371108375489712\n",
      "epoch: 3 | 75424 / 114272 | training loss: 0.0027208158280700445\n",
      "epoch: 3 | 75456 / 114272 | training loss: 0.0025176957715302706\n",
      "epoch: 3 | 75488 / 114272 | training loss: 0.11439843475818634\n",
      "epoch: 3 | 75520 / 114272 | training loss: 0.004030630923807621\n",
      "epoch: 3 | 75552 / 114272 | training loss: 0.15303552150726318\n",
      "epoch: 3 | 75584 / 114272 | training loss: 0.10290418565273285\n",
      "epoch: 3 | 75616 / 114272 | training loss: 0.0048553613014519215\n",
      "epoch: 3 | 75648 / 114272 | training loss: 0.003334289649501443\n",
      "epoch: 3 | 75680 / 114272 | training loss: 0.214344322681427\n",
      "epoch: 3 | 75712 / 114272 | training loss: 0.0016256013186648488\n",
      "epoch: 3 | 75744 / 114272 | training loss: 0.011940940283238888\n",
      "epoch: 3 | 75776 / 114272 | training loss: 0.004544511903077364\n",
      "epoch: 3 | 75808 / 114272 | training loss: 0.11880560219287872\n",
      "epoch: 3 | 75840 / 114272 | training loss: 0.012033948674798012\n",
      "epoch: 3 | 75872 / 114272 | training loss: 0.049720991402864456\n",
      "epoch: 3 | 75904 / 114272 | training loss: 0.40307748317718506\n",
      "epoch: 3 | 75936 / 114272 | training loss: 0.23408310115337372\n",
      "epoch: 3 | 75968 / 114272 | training loss: 0.1101706400513649\n",
      "epoch: 3 | 76000 / 114272 | training loss: 0.2598716616630554\n",
      "epoch: 3 | 76032 / 114272 | training loss: 0.11362278461456299\n",
      "epoch: 3 | 76064 / 114272 | training loss: 0.0016447086818516254\n",
      "epoch: 3 | 76096 / 114272 | training loss: 0.012346136383712292\n",
      "epoch: 3 | 76128 / 114272 | training loss: 0.07308463007211685\n",
      "epoch: 3 | 76160 / 114272 | training loss: 0.04217304661870003\n",
      "epoch: 3 | 76192 / 114272 | training loss: 0.006055787671357393\n",
      "epoch: 3 | 76224 / 114272 | training loss: 0.11233960837125778\n",
      "epoch: 3 | 76256 / 114272 | training loss: 0.005491528194397688\n",
      "epoch: 3 | 76288 / 114272 | training loss: 0.023488035425543785\n",
      "epoch: 3 | 76320 / 114272 | training loss: 0.0066206869669258595\n",
      "epoch: 3 | 76352 / 114272 | training loss: 0.0064262147061526775\n",
      "epoch: 3 | 76384 / 114272 | training loss: 0.004151654429733753\n",
      "epoch: 3 | 76416 / 114272 | training loss: 0.16981466114521027\n",
      "epoch: 3 | 76448 / 114272 | training loss: 0.1603243499994278\n",
      "epoch: 3 | 76480 / 114272 | training loss: 0.013926167972385883\n",
      "epoch: 3 | 76512 / 114272 | training loss: 0.07849050313234329\n",
      "epoch: 3 | 76544 / 114272 | training loss: 0.0026038605719804764\n",
      "epoch: 3 | 76576 / 114272 | training loss: 0.006702741142362356\n",
      "epoch: 3 | 76608 / 114272 | training loss: 0.01533612422645092\n",
      "epoch: 3 | 76640 / 114272 | training loss: 0.11801989376544952\n",
      "epoch: 3 | 76672 / 114272 | training loss: 0.027621755376458168\n",
      "epoch: 3 | 76704 / 114272 | training loss: 0.022500429302453995\n",
      "epoch: 3 | 76736 / 114272 | training loss: 0.09927240759134293\n",
      "epoch: 3 | 76768 / 114272 | training loss: 0.01146400161087513\n",
      "epoch: 3 | 76800 / 114272 | training loss: 0.00475526275113225\n",
      "epoch: 3 | 76832 / 114272 | training loss: 0.003500596387311816\n",
      "epoch: 3 | 76864 / 114272 | training loss: 0.28034308552742004\n",
      "epoch: 3 | 76896 / 114272 | training loss: 0.023762986063957214\n",
      "epoch: 3 | 76928 / 114272 | training loss: 0.11358018219470978\n",
      "epoch: 3 | 76960 / 114272 | training loss: 0.20798593759536743\n",
      "epoch: 3 | 76992 / 114272 | training loss: 0.23234498500823975\n",
      "epoch: 3 | 77024 / 114272 | training loss: 0.008703604340553284\n",
      "epoch: 3 | 77056 / 114272 | training loss: 0.009332499466836452\n",
      "epoch: 3 | 77088 / 114272 | training loss: 0.04224478825926781\n",
      "epoch: 3 | 77120 / 114272 | training loss: 0.004851075354963541\n",
      "epoch: 3 | 77152 / 114272 | training loss: 0.0020528994500637054\n",
      "epoch: 3 | 77184 / 114272 | training loss: 0.0032028958667069674\n",
      "epoch: 3 | 77216 / 114272 | training loss: 0.27500179409980774\n",
      "epoch: 3 | 77248 / 114272 | training loss: 0.1581130027770996\n",
      "epoch: 3 | 77280 / 114272 | training loss: 0.0016557100461795926\n",
      "epoch: 3 | 77312 / 114272 | training loss: 0.10340547561645508\n",
      "epoch: 3 | 77344 / 114272 | training loss: 0.06638345867395401\n",
      "epoch: 3 | 77376 / 114272 | training loss: 0.002968233311548829\n",
      "epoch: 3 | 77408 / 114272 | training loss: 0.030169755220413208\n",
      "epoch: 3 | 77440 / 114272 | training loss: 0.06096272170543671\n",
      "epoch: 3 | 77472 / 114272 | training loss: 0.0031448076479136944\n",
      "epoch: 3 | 77504 / 114272 | training loss: 0.12028804421424866\n",
      "epoch: 3 | 77536 / 114272 | training loss: 0.009681819006800652\n",
      "epoch: 3 | 77568 / 114272 | training loss: 0.0033061550930142403\n",
      "epoch: 3 | 77600 / 114272 | training loss: 0.006035082042217255\n",
      "epoch: 3 | 77632 / 114272 | training loss: 0.0054582394659519196\n",
      "epoch: 3 | 77664 / 114272 | training loss: 0.05388430878520012\n",
      "epoch: 3 | 77696 / 114272 | training loss: 0.030239729210734367\n",
      "epoch: 3 | 77728 / 114272 | training loss: 0.009188373573124409\n",
      "epoch: 3 | 77760 / 114272 | training loss: 0.029683513566851616\n",
      "epoch: 3 | 77792 / 114272 | training loss: 0.27896738052368164\n",
      "epoch: 3 | 77824 / 114272 | training loss: 0.037792231887578964\n",
      "epoch: 3 | 77856 / 114272 | training loss: 0.14071103930473328\n",
      "epoch: 3 | 77888 / 114272 | training loss: 0.02011989615857601\n",
      "epoch: 3 | 77920 / 114272 | training loss: 0.07524148374795914\n",
      "epoch: 3 | 77952 / 114272 | training loss: 0.0047408598475158215\n",
      "epoch: 3 | 77984 / 114272 | training loss: 0.003519560443237424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 78016 / 114272 | training loss: 0.15014591813087463\n",
      "epoch: 3 | 78048 / 114272 | training loss: 0.009031644091010094\n",
      "epoch: 3 | 78080 / 114272 | training loss: 0.0020083640702068806\n",
      "epoch: 3 | 78112 / 114272 | training loss: 0.00204499252140522\n",
      "epoch: 3 | 78144 / 114272 | training loss: 0.003147150855511427\n",
      "epoch: 3 | 78176 / 114272 | training loss: 0.053529273718595505\n",
      "epoch: 3 | 78208 / 114272 | training loss: 0.008042942732572556\n",
      "epoch: 3 | 78240 / 114272 | training loss: 0.19751307368278503\n",
      "epoch: 3 | 78272 / 114272 | training loss: 0.08736488223075867\n",
      "epoch: 3 | 78304 / 114272 | training loss: 0.16999384760856628\n",
      "epoch: 3 | 78336 / 114272 | training loss: 0.10439890623092651\n",
      "epoch: 3 | 78368 / 114272 | training loss: 0.004349879454821348\n",
      "epoch: 3 | 78400 / 114272 | training loss: 0.0053147217258811\n",
      "epoch: 3 | 78432 / 114272 | training loss: 0.2509642541408539\n",
      "epoch: 3 | 78464 / 114272 | training loss: 0.0815422534942627\n",
      "epoch: 3 | 78496 / 114272 | training loss: 0.11341526359319687\n",
      "epoch: 3 | 78528 / 114272 | training loss: 0.16876056790351868\n",
      "epoch: 3 | 78560 / 114272 | training loss: 0.006094633135944605\n",
      "epoch: 3 | 78592 / 114272 | training loss: 0.1252264380455017\n",
      "epoch: 3 | 78624 / 114272 | training loss: 0.06373502314090729\n",
      "epoch: 3 | 78656 / 114272 | training loss: 0.004937841556966305\n",
      "epoch: 3 | 78688 / 114272 | training loss: 0.20661050081253052\n",
      "epoch: 3 | 78720 / 114272 | training loss: 0.015416796319186687\n",
      "epoch: 3 | 78752 / 114272 | training loss: 0.05924296751618385\n",
      "epoch: 3 | 78784 / 114272 | training loss: 0.0857221931219101\n",
      "epoch: 3 | 78816 / 114272 | training loss: 0.008174347691237926\n",
      "epoch: 3 | 78848 / 114272 | training loss: 0.015527722425758839\n",
      "epoch: 3 | 78880 / 114272 | training loss: 0.07041002064943314\n",
      "epoch: 3 | 78912 / 114272 | training loss: 0.11945640295743942\n",
      "epoch: 3 | 78944 / 114272 | training loss: 0.11450231075286865\n",
      "epoch: 3 | 78976 / 114272 | training loss: 0.015077995136380196\n",
      "epoch: 3 | 79008 / 114272 | training loss: 0.23548202216625214\n",
      "epoch: 3 | 79040 / 114272 | training loss: 0.11199726164340973\n",
      "epoch: 3 | 79072 / 114272 | training loss: 0.13881705701351166\n",
      "epoch: 3 | 79104 / 114272 | training loss: 0.007033406291157007\n",
      "epoch: 3 | 79136 / 114272 | training loss: 0.12434075772762299\n",
      "epoch: 3 | 79168 / 114272 | training loss: 0.008532645180821419\n",
      "epoch: 3 | 79200 / 114272 | training loss: 0.008096466772258282\n",
      "epoch: 3 | 79232 / 114272 | training loss: 0.009082356467843056\n",
      "epoch: 3 | 79264 / 114272 | training loss: 0.006271871738135815\n",
      "epoch: 3 | 79296 / 114272 | training loss: 0.009410131722688675\n",
      "epoch: 3 | 79328 / 114272 | training loss: 0.09568335115909576\n",
      "epoch: 3 | 79360 / 114272 | training loss: 0.11000385880470276\n",
      "epoch: 3 | 79392 / 114272 | training loss: 0.009702490642666817\n",
      "epoch: 3 | 79424 / 114272 | training loss: 0.09547702968120575\n",
      "epoch: 3 | 79456 / 114272 | training loss: 0.003926812671124935\n",
      "epoch: 3 | 79488 / 114272 | training loss: 0.008287884294986725\n",
      "epoch: 3 | 79520 / 114272 | training loss: 0.08746501058340073\n",
      "epoch: 3 | 79552 / 114272 | training loss: 0.02156698703765869\n",
      "epoch: 3 | 79584 / 114272 | training loss: 0.07376699149608612\n",
      "epoch: 3 | 79616 / 114272 | training loss: 0.0681845024228096\n",
      "epoch: 3 | 79648 / 114272 | training loss: 0.0053529017604887486\n",
      "epoch: 3 | 79680 / 114272 | training loss: 0.021114764735102654\n",
      "epoch: 3 | 79712 / 114272 | training loss: 0.004889439791440964\n",
      "epoch: 3 | 79744 / 114272 | training loss: 0.022909153252840042\n",
      "epoch: 3 | 79776 / 114272 | training loss: 0.013532990589737892\n",
      "epoch: 3 | 79808 / 114272 | training loss: 0.010000203736126423\n",
      "epoch: 3 | 79840 / 114272 | training loss: 0.033544085919857025\n",
      "epoch: 3 | 79872 / 114272 | training loss: 0.01880844123661518\n",
      "epoch: 3 | 79904 / 114272 | training loss: 0.13667032122612\n",
      "epoch: 3 | 79936 / 114272 | training loss: 0.2867611348628998\n",
      "epoch: 3 | 79968 / 114272 | training loss: 0.015957869589328766\n",
      "epoch: 3 | 80000 / 114272 | training loss: 0.11507613211870193\n",
      "epoch: 3 | 80032 / 114272 | training loss: 0.06165121868252754\n",
      "epoch: 3 | 80064 / 114272 | training loss: 0.0439668744802475\n",
      "epoch: 3 | 80096 / 114272 | training loss: 0.01035286858677864\n",
      "epoch: 3 | 80128 / 114272 | training loss: 0.0032478473149240017\n",
      "epoch: 3 | 80160 / 114272 | training loss: 0.006675897631794214\n",
      "epoch: 3 | 80192 / 114272 | training loss: 0.10236988961696625\n",
      "epoch: 3 | 80224 / 114272 | training loss: 0.003469888586550951\n",
      "epoch: 3 | 80256 / 114272 | training loss: 0.10225872695446014\n",
      "epoch: 3 | 80288 / 114272 | training loss: 0.2653310298919678\n",
      "epoch: 3 | 80320 / 114272 | training loss: 0.21860414743423462\n",
      "epoch: 3 | 80352 / 114272 | training loss: 0.009723816998302937\n",
      "epoch: 3 | 80384 / 114272 | training loss: 0.0009676460758782923\n",
      "epoch: 3 | 80416 / 114272 | training loss: 0.32415562868118286\n",
      "epoch: 3 | 80448 / 114272 | training loss: 0.0013268005568534136\n",
      "epoch: 3 | 80480 / 114272 | training loss: 0.10475104302167892\n",
      "epoch: 3 | 80512 / 114272 | training loss: 0.16734999418258667\n",
      "epoch: 3 | 80544 / 114272 | training loss: 0.033455487340688705\n",
      "epoch: 3 | 80576 / 114272 | training loss: 0.13252213597297668\n",
      "epoch: 3 | 80608 / 114272 | training loss: 0.010580575093626976\n",
      "epoch: 3 | 80640 / 114272 | training loss: 0.058410920202732086\n",
      "epoch: 3 | 80672 / 114272 | training loss: 0.07301568239927292\n",
      "epoch: 3 | 80704 / 114272 | training loss: 0.21814310550689697\n",
      "epoch: 3 | 80736 / 114272 | training loss: 0.01652352511882782\n",
      "epoch: 3 | 80768 / 114272 | training loss: 0.0038992934860289097\n",
      "epoch: 3 | 80800 / 114272 | training loss: 0.016807954758405685\n",
      "epoch: 3 | 80832 / 114272 | training loss: 0.016957880929112434\n",
      "epoch: 3 | 80864 / 114272 | training loss: 0.005902459379285574\n",
      "epoch: 3 | 80896 / 114272 | training loss: 0.002818180015310645\n",
      "epoch: 3 | 80928 / 114272 | training loss: 0.001057385583408177\n",
      "epoch: 3 | 80960 / 114272 | training loss: 0.0029850404243916273\n",
      "epoch: 3 | 80992 / 114272 | training loss: 0.009604295715689659\n",
      "epoch: 3 | 81024 / 114272 | training loss: 0.0032983613200485706\n",
      "epoch: 3 | 81056 / 114272 | training loss: 0.054210416972637177\n",
      "epoch: 3 | 81088 / 114272 | training loss: 0.2666087746620178\n",
      "epoch: 3 | 81120 / 114272 | training loss: 0.003038566093891859\n",
      "epoch: 3 | 81152 / 114272 | training loss: 0.2591691315174103\n",
      "epoch: 3 | 81184 / 114272 | training loss: 0.0016082918737083673\n",
      "epoch: 3 | 81216 / 114272 | training loss: 0.0037005122285336256\n",
      "epoch: 3 | 81248 / 114272 | training loss: 0.3858753442764282\n",
      "epoch: 3 | 81280 / 114272 | training loss: 0.1418890804052353\n",
      "epoch: 3 | 81312 / 114272 | training loss: 0.04131823405623436\n",
      "epoch: 3 | 81344 / 114272 | training loss: 0.0012719997903332114\n",
      "epoch: 3 | 81376 / 114272 | training loss: 0.0017601356375962496\n",
      "epoch: 3 | 81408 / 114272 | training loss: 0.007645753212273121\n",
      "epoch: 3 | 81440 / 114272 | training loss: 0.004861308261752129\n",
      "epoch: 3 | 81472 / 114272 | training loss: 0.025202671065926552\n",
      "epoch: 3 | 81504 / 114272 | training loss: 0.08328788727521896\n",
      "epoch: 3 | 81536 / 114272 | training loss: 0.056274715811014175\n",
      "epoch: 3 | 81568 / 114272 | training loss: 0.006103757303208113\n",
      "epoch: 3 | 81600 / 114272 | training loss: 0.010993082076311111\n",
      "epoch: 3 | 81632 / 114272 | training loss: 0.06766172498464584\n",
      "epoch: 3 | 81664 / 114272 | training loss: 0.0061810812912881374\n",
      "epoch: 3 | 81696 / 114272 | training loss: 0.003152119228616357\n",
      "epoch: 3 | 81728 / 114272 | training loss: 0.02793727070093155\n",
      "epoch: 3 | 81760 / 114272 | training loss: 0.10683183372020721\n",
      "epoch: 3 | 81792 / 114272 | training loss: 0.01834624633193016\n",
      "epoch: 3 | 81824 / 114272 | training loss: 0.006427754182368517\n",
      "epoch: 3 | 81856 / 114272 | training loss: 0.0040799397975206375\n",
      "epoch: 3 | 81888 / 114272 | training loss: 0.011259539052844048\n",
      "epoch: 3 | 81920 / 114272 | training loss: 0.09299766272306442\n",
      "epoch: 3 | 81952 / 114272 | training loss: 0.004593321587890387\n",
      "epoch: 3 | 81984 / 114272 | training loss: 0.04177071899175644\n",
      "epoch: 3 | 82016 / 114272 | training loss: 0.2835531234741211\n",
      "epoch: 3 | 82048 / 114272 | training loss: 0.01725531555712223\n",
      "epoch: 3 | 82080 / 114272 | training loss: 0.0064699118956923485\n",
      "epoch: 3 | 82112 / 114272 | training loss: 0.0062515707686543465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 82144 / 114272 | training loss: 0.014614744111895561\n",
      "epoch: 3 | 82176 / 114272 | training loss: 0.0011084795696660876\n",
      "epoch: 3 | 82208 / 114272 | training loss: 0.0039629810489714146\n",
      "epoch: 3 | 82240 / 114272 | training loss: 0.03607498109340668\n",
      "epoch: 3 | 82272 / 114272 | training loss: 0.015942245721817017\n",
      "epoch: 3 | 82304 / 114272 | training loss: 0.001142586232163012\n",
      "epoch: 3 | 82336 / 114272 | training loss: 0.01838461123406887\n",
      "epoch: 3 | 82368 / 114272 | training loss: 0.002535024890676141\n",
      "epoch: 3 | 82400 / 114272 | training loss: 0.02579018659889698\n",
      "epoch: 3 | 82432 / 114272 | training loss: 0.016209764406085014\n",
      "epoch: 3 | 82464 / 114272 | training loss: 0.14465771615505219\n",
      "epoch: 3 | 82496 / 114272 | training loss: 0.003970553632825613\n",
      "epoch: 3 | 82528 / 114272 | training loss: 0.006246966775506735\n",
      "epoch: 3 | 82560 / 114272 | training loss: 0.30403149127960205\n",
      "epoch: 3 | 82592 / 114272 | training loss: 0.004522187635302544\n",
      "epoch: 3 | 82624 / 114272 | training loss: 0.292994886636734\n",
      "epoch: 3 | 82656 / 114272 | training loss: 0.11082135885953903\n",
      "epoch: 3 | 82688 / 114272 | training loss: 0.00120776635594666\n",
      "epoch: 3 | 82720 / 114272 | training loss: 0.0029915980994701385\n",
      "epoch: 3 | 82752 / 114272 | training loss: 0.004537810105830431\n",
      "epoch: 3 | 82784 / 114272 | training loss: 0.007185955997556448\n",
      "epoch: 3 | 82816 / 114272 | training loss: 0.0010572748724371195\n",
      "epoch: 3 | 82848 / 114272 | training loss: 0.001599046285264194\n",
      "epoch: 3 | 82880 / 114272 | training loss: 0.00214773858897388\n",
      "epoch: 3 | 82912 / 114272 | training loss: 0.010458532720804214\n",
      "epoch: 3 | 82944 / 114272 | training loss: 0.0041767326183617115\n",
      "epoch: 3 | 82976 / 114272 | training loss: 0.008944785222411156\n",
      "epoch: 3 | 83008 / 114272 | training loss: 0.014919332228600979\n",
      "epoch: 3 | 83040 / 114272 | training loss: 0.002280648099258542\n",
      "epoch: 3 | 83072 / 114272 | training loss: 0.08108425885438919\n",
      "epoch: 3 | 83104 / 114272 | training loss: 0.0033713846933096647\n",
      "epoch: 3 | 83136 / 114272 | training loss: 0.002322692424058914\n",
      "epoch: 3 | 83168 / 114272 | training loss: 0.003576205112040043\n",
      "epoch: 3 | 83200 / 114272 | training loss: 0.20042450726032257\n",
      "epoch: 3 | 83232 / 114272 | training loss: 0.0018466069595888257\n",
      "epoch: 3 | 83264 / 114272 | training loss: 0.18650312721729279\n",
      "epoch: 3 | 83296 / 114272 | training loss: 0.0037174588069319725\n",
      "epoch: 3 | 83328 / 114272 | training loss: 0.0029836082831025124\n",
      "epoch: 3 | 83360 / 114272 | training loss: 0.09665268659591675\n",
      "epoch: 3 | 83392 / 114272 | training loss: 0.009517546743154526\n",
      "epoch: 3 | 83424 / 114272 | training loss: 0.1602494865655899\n",
      "epoch: 3 | 83456 / 114272 | training loss: 0.2652048170566559\n",
      "epoch: 3 | 83488 / 114272 | training loss: 0.04045717790722847\n",
      "epoch: 3 | 83520 / 114272 | training loss: 0.10453062504529953\n",
      "epoch: 3 | 83552 / 114272 | training loss: 0.005634136497974396\n",
      "epoch: 3 | 83584 / 114272 | training loss: 0.010345309972763062\n",
      "epoch: 3 | 83616 / 114272 | training loss: 0.0035672690719366074\n",
      "epoch: 3 | 83648 / 114272 | training loss: 0.05652949586510658\n",
      "epoch: 3 | 83680 / 114272 | training loss: 0.0014300281181931496\n",
      "epoch: 3 | 83712 / 114272 | training loss: 0.25414812564849854\n",
      "epoch: 3 | 83744 / 114272 | training loss: 0.06615854054689407\n",
      "epoch: 3 | 83776 / 114272 | training loss: 0.002429142827168107\n",
      "epoch: 3 | 83808 / 114272 | training loss: 0.0034129787236452103\n",
      "epoch: 3 | 83840 / 114272 | training loss: 0.0019450801191851497\n",
      "epoch: 3 | 83872 / 114272 | training loss: 0.1409253031015396\n",
      "epoch: 3 | 83904 / 114272 | training loss: 0.014047224074602127\n",
      "epoch: 3 | 83936 / 114272 | training loss: 0.2008005827665329\n",
      "epoch: 3 | 83968 / 114272 | training loss: 0.013490400277078152\n",
      "epoch: 3 | 84000 / 114272 | training loss: 0.024224013090133667\n",
      "epoch: 3 | 84032 / 114272 | training loss: 0.008204332552850246\n",
      "epoch: 3 | 84064 / 114272 | training loss: 0.0009550763061270118\n",
      "epoch: 3 | 84096 / 114272 | training loss: 0.082992322742939\n",
      "epoch: 3 | 84128 / 114272 | training loss: 0.005995878484100103\n",
      "epoch: 3 | 84160 / 114272 | training loss: 0.006541944574564695\n",
      "epoch: 3 | 84192 / 114272 | training loss: 0.18631038069725037\n",
      "epoch: 3 | 84224 / 114272 | training loss: 0.04248878359794617\n",
      "epoch: 3 | 84256 / 114272 | training loss: 0.0037208658177405596\n",
      "epoch: 3 | 84288 / 114272 | training loss: 0.34776464104652405\n",
      "epoch: 3 | 84320 / 114272 | training loss: 0.01721220090985298\n",
      "epoch: 3 | 84352 / 114272 | training loss: 0.002954788040369749\n",
      "epoch: 3 | 84384 / 114272 | training loss: 0.01858438178896904\n",
      "epoch: 3 | 84416 / 114272 | training loss: 0.0040809642523527145\n",
      "epoch: 3 | 84448 / 114272 | training loss: 0.013133996166288853\n",
      "epoch: 3 | 84480 / 114272 | training loss: 0.001871504238806665\n",
      "epoch: 3 | 84512 / 114272 | training loss: 0.035114601254463196\n",
      "epoch: 3 | 84544 / 114272 | training loss: 0.002479270566254854\n",
      "epoch: 3 | 84576 / 114272 | training loss: 0.003645283402875066\n",
      "epoch: 3 | 84608 / 114272 | training loss: 0.009669523686170578\n",
      "epoch: 3 | 84640 / 114272 | training loss: 0.004734305664896965\n",
      "epoch: 3 | 84672 / 114272 | training loss: 0.0023792851716279984\n",
      "epoch: 3 | 84704 / 114272 | training loss: 0.002924970118328929\n",
      "epoch: 3 | 84736 / 114272 | training loss: 0.0118594691157341\n",
      "epoch: 3 | 84768 / 114272 | training loss: 0.0038338510785251856\n",
      "epoch: 3 | 84800 / 114272 | training loss: 0.0027109687216579914\n",
      "epoch: 3 | 84832 / 114272 | training loss: 0.0033026321325451136\n",
      "epoch: 3 | 84864 / 114272 | training loss: 0.0029200122226029634\n",
      "epoch: 3 | 84896 / 114272 | training loss: 0.0031342755537480116\n",
      "epoch: 3 | 84928 / 114272 | training loss: 0.001037027919664979\n",
      "epoch: 3 | 84960 / 114272 | training loss: 0.001666271360591054\n",
      "epoch: 3 | 84992 / 114272 | training loss: 0.007913406006991863\n",
      "epoch: 3 | 85024 / 114272 | training loss: 0.0007958679925650358\n",
      "epoch: 3 | 85056 / 114272 | training loss: 0.21240314841270447\n",
      "epoch: 3 | 85088 / 114272 | training loss: 0.1392359584569931\n",
      "epoch: 3 | 85120 / 114272 | training loss: 0.0017878598300740123\n",
      "epoch: 3 | 85152 / 114272 | training loss: 0.29309970140457153\n",
      "epoch: 3 | 85184 / 114272 | training loss: 0.06237710267305374\n",
      "epoch: 3 | 85216 / 114272 | training loss: 0.001213797484524548\n",
      "epoch: 3 | 85248 / 114272 | training loss: 0.002208036370575428\n",
      "epoch: 3 | 85280 / 114272 | training loss: 0.0031275469809770584\n",
      "epoch: 3 | 85312 / 114272 | training loss: 0.0034777589607983828\n",
      "epoch: 3 | 85344 / 114272 | training loss: 0.00602013198658824\n",
      "epoch: 3 | 85376 / 114272 | training loss: 0.002117504831403494\n",
      "epoch: 3 | 85408 / 114272 | training loss: 0.0016722765285521746\n",
      "epoch: 3 | 85440 / 114272 | training loss: 0.1768803596496582\n",
      "epoch: 3 | 85472 / 114272 | training loss: 0.0010642345296218991\n",
      "epoch: 3 | 85504 / 114272 | training loss: 0.0038425240200012922\n",
      "epoch: 3 | 85536 / 114272 | training loss: 0.09641385078430176\n",
      "epoch: 3 | 85568 / 114272 | training loss: 0.1019960343837738\n",
      "epoch: 3 | 85600 / 114272 | training loss: 0.21757672727108002\n",
      "epoch: 3 | 85632 / 114272 | training loss: 0.00349431112408638\n",
      "epoch: 3 | 85664 / 114272 | training loss: 0.002607626374810934\n",
      "epoch: 3 | 85696 / 114272 | training loss: 0.016185417771339417\n",
      "epoch: 3 | 85728 / 114272 | training loss: 0.005472326185554266\n",
      "epoch: 3 | 85760 / 114272 | training loss: 0.002213421044871211\n",
      "epoch: 3 | 85792 / 114272 | training loss: 0.0008978380938060582\n",
      "epoch: 3 | 85824 / 114272 | training loss: 0.11610729247331619\n",
      "epoch: 3 | 85856 / 114272 | training loss: 0.0009632525034248829\n",
      "epoch: 3 | 85888 / 114272 | training loss: 0.0010902760550379753\n",
      "epoch: 3 | 85920 / 114272 | training loss: 0.0008670032839290798\n",
      "epoch: 3 | 85952 / 114272 | training loss: 0.0011838494101539254\n",
      "epoch: 3 | 85984 / 114272 | training loss: 0.002197019988670945\n",
      "epoch: 3 | 86016 / 114272 | training loss: 0.03594864159822464\n",
      "epoch: 3 | 86048 / 114272 | training loss: 0.11915314942598343\n",
      "epoch: 3 | 86080 / 114272 | training loss: 0.25118279457092285\n",
      "epoch: 3 | 86112 / 114272 | training loss: 0.029823506250977516\n",
      "epoch: 3 | 86144 / 114272 | training loss: 0.13413459062576294\n",
      "epoch: 3 | 86176 / 114272 | training loss: 0.002110118744894862\n",
      "epoch: 3 | 86208 / 114272 | training loss: 0.16110743582248688\n",
      "epoch: 3 | 86240 / 114272 | training loss: 0.0010290741920471191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 86272 / 114272 | training loss: 0.0010589641751721501\n",
      "epoch: 3 | 86304 / 114272 | training loss: 0.022161580622196198\n",
      "epoch: 3 | 86336 / 114272 | training loss: 0.2265922874212265\n",
      "epoch: 3 | 86368 / 114272 | training loss: 0.02076907269656658\n",
      "epoch: 3 | 86400 / 114272 | training loss: 0.15263596177101135\n",
      "epoch: 3 | 86432 / 114272 | training loss: 0.0767325833439827\n",
      "epoch: 3 | 86464 / 114272 | training loss: 0.4234659969806671\n",
      "epoch: 3 | 86496 / 114272 | training loss: 0.0023940016981214285\n",
      "epoch: 3 | 86528 / 114272 | training loss: 0.011758352629840374\n",
      "epoch: 3 | 86560 / 114272 | training loss: 0.0020643938332796097\n",
      "epoch: 3 | 86592 / 114272 | training loss: 0.23002146184444427\n",
      "epoch: 3 | 86624 / 114272 | training loss: 0.25260525941848755\n",
      "epoch: 3 | 86656 / 114272 | training loss: 0.20261383056640625\n",
      "epoch: 3 | 86688 / 114272 | training loss: 0.2025453746318817\n",
      "epoch: 3 | 86720 / 114272 | training loss: 0.004669204354286194\n",
      "epoch: 3 | 86752 / 114272 | training loss: 0.0962841585278511\n",
      "epoch: 3 | 86784 / 114272 | training loss: 0.0014278854941949248\n",
      "epoch: 3 | 86816 / 114272 | training loss: 0.014477070420980453\n",
      "epoch: 3 | 86848 / 114272 | training loss: 0.32186606526374817\n",
      "epoch: 3 | 86880 / 114272 | training loss: 0.004351362120360136\n",
      "epoch: 3 | 86912 / 114272 | training loss: 0.003045546356588602\n",
      "epoch: 3 | 86944 / 114272 | training loss: 0.001759817241691053\n",
      "epoch: 3 | 86976 / 114272 | training loss: 0.004793365020304918\n",
      "epoch: 3 | 87008 / 114272 | training loss: 0.003489401889964938\n",
      "epoch: 3 | 87040 / 114272 | training loss: 0.21313410997390747\n",
      "epoch: 3 | 87072 / 114272 | training loss: 0.0912109836935997\n",
      "epoch: 3 | 87104 / 114272 | training loss: 0.007739117834717035\n",
      "epoch: 3 | 87136 / 114272 | training loss: 0.0024610371328890324\n",
      "epoch: 3 | 87168 / 114272 | training loss: 0.008150623179972172\n",
      "epoch: 3 | 87200 / 114272 | training loss: 0.0038437217008322477\n",
      "epoch: 3 | 87232 / 114272 | training loss: 0.0018297311617061496\n",
      "epoch: 3 | 87264 / 114272 | training loss: 0.3478858172893524\n",
      "epoch: 3 | 87296 / 114272 | training loss: 0.0031656473875045776\n",
      "epoch: 3 | 87328 / 114272 | training loss: 0.0057000163942575455\n",
      "epoch: 3 | 87360 / 114272 | training loss: 0.002062949351966381\n",
      "epoch: 3 | 87392 / 114272 | training loss: 0.003725622780621052\n",
      "epoch: 3 | 87424 / 114272 | training loss: 0.008054148405790329\n",
      "epoch: 3 | 87456 / 114272 | training loss: 0.18995723128318787\n",
      "epoch: 3 | 87488 / 114272 | training loss: 0.13916455209255219\n",
      "epoch: 3 | 87520 / 114272 | training loss: 0.07906904071569443\n",
      "epoch: 3 | 87552 / 114272 | training loss: 0.0029023305978626013\n",
      "epoch: 3 | 87584 / 114272 | training loss: 0.25153958797454834\n",
      "epoch: 3 | 87616 / 114272 | training loss: 0.1472116857767105\n",
      "epoch: 3 | 87648 / 114272 | training loss: 0.1671198159456253\n",
      "epoch: 3 | 87680 / 114272 | training loss: 0.0044899689964950085\n",
      "epoch: 3 | 87712 / 114272 | training loss: 0.28731849789619446\n",
      "epoch: 3 | 87744 / 114272 | training loss: 0.02342582307755947\n",
      "epoch: 3 | 87776 / 114272 | training loss: 0.003382768016308546\n",
      "epoch: 3 | 87808 / 114272 | training loss: 0.0016791779780760407\n",
      "epoch: 3 | 87840 / 114272 | training loss: 0.1860724240541458\n",
      "epoch: 3 | 87872 / 114272 | training loss: 0.11433161050081253\n",
      "epoch: 3 | 87904 / 114272 | training loss: 0.005129179917275906\n",
      "epoch: 3 | 87936 / 114272 | training loss: 0.011364784091711044\n",
      "epoch: 3 | 87968 / 114272 | training loss: 0.0016471685376018286\n",
      "epoch: 3 | 88000 / 114272 | training loss: 0.004129808861762285\n",
      "epoch: 3 | 88032 / 114272 | training loss: 0.1944998949766159\n",
      "epoch: 3 | 88064 / 114272 | training loss: 0.11627808958292007\n",
      "epoch: 3 | 88096 / 114272 | training loss: 0.003592738648876548\n",
      "epoch: 3 | 88128 / 114272 | training loss: 0.133297398686409\n",
      "epoch: 3 | 88160 / 114272 | training loss: 0.0521344393491745\n",
      "epoch: 3 | 88192 / 114272 | training loss: 0.155289888381958\n",
      "epoch: 3 | 88224 / 114272 | training loss: 0.008488854393362999\n",
      "epoch: 3 | 88256 / 114272 | training loss: 0.10422690957784653\n",
      "epoch: 3 | 88288 / 114272 | training loss: 0.004058903083205223\n",
      "epoch: 3 | 88320 / 114272 | training loss: 0.1099957823753357\n",
      "epoch: 3 | 88352 / 114272 | training loss: 0.006394347175955772\n",
      "epoch: 3 | 88384 / 114272 | training loss: 0.0067005455493927\n",
      "epoch: 3 | 88416 / 114272 | training loss: 0.08594205975532532\n",
      "epoch: 3 | 88448 / 114272 | training loss: 0.04002378135919571\n",
      "epoch: 3 | 88480 / 114272 | training loss: 0.0677357017993927\n",
      "epoch: 3 | 88512 / 114272 | training loss: 0.06513983011245728\n",
      "epoch: 3 | 88544 / 114272 | training loss: 0.06897122412919998\n",
      "epoch: 3 | 88576 / 114272 | training loss: 0.011244949884712696\n",
      "epoch: 3 | 88608 / 114272 | training loss: 0.01645379140973091\n",
      "epoch: 3 | 88640 / 114272 | training loss: 0.0016079227207228541\n",
      "epoch: 3 | 88672 / 114272 | training loss: 0.12131036072969437\n",
      "epoch: 3 | 88704 / 114272 | training loss: 0.008537246845662594\n",
      "epoch: 3 | 88736 / 114272 | training loss: 0.005381409078836441\n",
      "epoch: 3 | 88768 / 114272 | training loss: 0.009974845685064793\n",
      "epoch: 3 | 88800 / 114272 | training loss: 0.0015468209749087691\n",
      "epoch: 3 | 88832 / 114272 | training loss: 0.008412383496761322\n",
      "epoch: 3 | 88864 / 114272 | training loss: 0.01688077300786972\n",
      "epoch: 3 | 88896 / 114272 | training loss: 0.007985616102814674\n",
      "epoch: 3 | 88928 / 114272 | training loss: 0.11080128699541092\n",
      "epoch: 3 | 88960 / 114272 | training loss: 0.0029234271496534348\n",
      "epoch: 3 | 88992 / 114272 | training loss: 0.09221630543470383\n",
      "epoch: 3 | 89024 / 114272 | training loss: 0.016600901260972023\n",
      "epoch: 3 | 89056 / 114272 | training loss: 0.22695229947566986\n",
      "epoch: 3 | 89088 / 114272 | training loss: 0.2337256222963333\n",
      "epoch: 3 | 89120 / 114272 | training loss: 0.0037217314820736647\n",
      "epoch: 3 | 89152 / 114272 | training loss: 0.04658235237002373\n",
      "epoch: 3 | 89184 / 114272 | training loss: 0.010338764637708664\n",
      "epoch: 3 | 89216 / 114272 | training loss: 0.006195280235260725\n",
      "epoch: 3 | 89248 / 114272 | training loss: 0.01913747563958168\n",
      "epoch: 3 | 89280 / 114272 | training loss: 0.07784572243690491\n",
      "epoch: 3 | 89312 / 114272 | training loss: 0.18266400694847107\n",
      "epoch: 3 | 89344 / 114272 | training loss: 0.003978040535002947\n",
      "epoch: 3 | 89376 / 114272 | training loss: 0.0013079340569674969\n",
      "epoch: 3 | 89408 / 114272 | training loss: 0.004233857151120901\n",
      "epoch: 3 | 89440 / 114272 | training loss: 0.07161600142717361\n",
      "epoch: 3 | 89472 / 114272 | training loss: 0.006622242275625467\n",
      "epoch: 3 | 89504 / 114272 | training loss: 0.1789977103471756\n",
      "epoch: 3 | 89536 / 114272 | training loss: 0.008328543044626713\n",
      "epoch: 3 | 89568 / 114272 | training loss: 0.1179681047797203\n",
      "epoch: 3 | 89600 / 114272 | training loss: 0.004462915007025003\n",
      "epoch: 3 | 89632 / 114272 | training loss: 0.20503729581832886\n",
      "epoch: 3 | 89664 / 114272 | training loss: 0.013813481666147709\n",
      "epoch: 3 | 89696 / 114272 | training loss: 0.0032086726278066635\n",
      "epoch: 3 | 89728 / 114272 | training loss: 0.0041925255209207535\n",
      "epoch: 3 | 89760 / 114272 | training loss: 0.0019438073504716158\n",
      "epoch: 3 | 89792 / 114272 | training loss: 0.003732535056769848\n",
      "epoch: 3 | 89824 / 114272 | training loss: 0.004289289936423302\n",
      "epoch: 3 | 89856 / 114272 | training loss: 0.004622912034392357\n",
      "epoch: 3 | 89888 / 114272 | training loss: 0.006870925426483154\n",
      "epoch: 3 | 89920 / 114272 | training loss: 0.0068292333744466305\n",
      "epoch: 3 | 89952 / 114272 | training loss: 0.10612499713897705\n",
      "epoch: 3 | 89984 / 114272 | training loss: 0.05493232607841492\n",
      "epoch: 3 | 90016 / 114272 | training loss: 0.03632668778300285\n",
      "epoch: 3 | 90048 / 114272 | training loss: 0.30746084451675415\n",
      "epoch: 3 | 90080 / 114272 | training loss: 0.009011938236653805\n",
      "epoch: 3 | 90112 / 114272 | training loss: 0.0027857855893671513\n",
      "epoch: 3 | 90144 / 114272 | training loss: 0.12013670802116394\n",
      "epoch: 3 | 90176 / 114272 | training loss: 0.0644889548420906\n",
      "epoch: 3 | 90208 / 114272 | training loss: 0.08129339665174484\n",
      "epoch: 3 | 90240 / 114272 | training loss: 0.0020194253884255886\n",
      "epoch: 3 | 90272 / 114272 | training loss: 0.11577291786670685\n",
      "epoch: 3 | 90304 / 114272 | training loss: 0.008041457273066044\n",
      "epoch: 3 | 90336 / 114272 | training loss: 0.002758821938186884\n",
      "epoch: 3 | 90368 / 114272 | training loss: 0.0029972016345709562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 90400 / 114272 | training loss: 0.003863423829898238\n",
      "epoch: 3 | 90432 / 114272 | training loss: 0.007620017509907484\n",
      "epoch: 3 | 90464 / 114272 | training loss: 0.002275672508403659\n",
      "epoch: 3 | 90496 / 114272 | training loss: 0.05669102445244789\n",
      "epoch: 3 | 90528 / 114272 | training loss: 0.12643688917160034\n",
      "epoch: 3 | 90560 / 114272 | training loss: 0.0025686693843454123\n",
      "epoch: 3 | 90592 / 114272 | training loss: 0.01639673300087452\n",
      "epoch: 3 | 90624 / 114272 | training loss: 0.011387759819626808\n",
      "epoch: 3 | 90656 / 114272 | training loss: 0.004497427958995104\n",
      "epoch: 3 | 90688 / 114272 | training loss: 0.0024383950512856245\n",
      "epoch: 3 | 90720 / 114272 | training loss: 0.006262929178774357\n",
      "epoch: 3 | 90752 / 114272 | training loss: 0.008839665912091732\n",
      "epoch: 3 | 90784 / 114272 | training loss: 0.10603654384613037\n",
      "epoch: 3 | 90816 / 114272 | training loss: 0.001993058016523719\n",
      "epoch: 3 | 90848 / 114272 | training loss: 0.007162709254771471\n",
      "epoch: 3 | 90880 / 114272 | training loss: 0.009450000710785389\n",
      "epoch: 3 | 90912 / 114272 | training loss: 0.002235190011560917\n",
      "epoch: 3 | 90944 / 114272 | training loss: 0.11607078462839127\n",
      "epoch: 3 | 90976 / 114272 | training loss: 0.15308018028736115\n",
      "epoch: 3 | 91008 / 114272 | training loss: 0.007659845985472202\n",
      "epoch: 3 | 91040 / 114272 | training loss: 0.1809244304895401\n",
      "epoch: 3 | 91072 / 114272 | training loss: 0.0319555290043354\n",
      "epoch: 3 | 91104 / 114272 | training loss: 0.007029735017567873\n",
      "epoch: 3 | 91136 / 114272 | training loss: 0.00566629646345973\n",
      "epoch: 3 | 91168 / 114272 | training loss: 0.005123896989971399\n",
      "epoch: 3 | 91200 / 114272 | training loss: 0.3455716669559479\n",
      "epoch: 3 | 91232 / 114272 | training loss: 0.01761491410434246\n",
      "epoch: 3 | 91264 / 114272 | training loss: 0.12950682640075684\n",
      "epoch: 3 | 91296 / 114272 | training loss: 0.004505910445004702\n",
      "epoch: 3 | 91328 / 114272 | training loss: 0.058154769241809845\n",
      "epoch: 3 | 91360 / 114272 | training loss: 0.0023031015880405903\n",
      "epoch: 3 | 91392 / 114272 | training loss: 0.06699101626873016\n",
      "epoch: 3 | 91424 / 114272 | training loss: 0.12283924221992493\n",
      "epoch: 3 | 91456 / 114272 | training loss: 0.003771253162994981\n",
      "epoch: 3 | 91488 / 114272 | training loss: 0.003198751015588641\n",
      "epoch: 3 | 91520 / 114272 | training loss: 0.0029742552433162928\n",
      "epoch: 3 | 91552 / 114272 | training loss: 0.03886592388153076\n",
      "epoch: 3 | 91584 / 114272 | training loss: 0.004518806003034115\n",
      "epoch: 3 | 91616 / 114272 | training loss: 0.3057200312614441\n",
      "epoch: 3 | 91648 / 114272 | training loss: 0.005682368762791157\n",
      "epoch: 3 | 91680 / 114272 | training loss: 0.2586263120174408\n",
      "epoch: 3 | 91712 / 114272 | training loss: 0.09982774406671524\n",
      "epoch: 3 | 91744 / 114272 | training loss: 0.005859311670064926\n",
      "epoch: 3 | 91776 / 114272 | training loss: 0.3117735683917999\n",
      "epoch: 3 | 91808 / 114272 | training loss: 0.11962637305259705\n",
      "epoch: 3 | 91840 / 114272 | training loss: 0.008107736706733704\n",
      "epoch: 3 | 91872 / 114272 | training loss: 0.13518251478672028\n",
      "epoch: 3 | 91904 / 114272 | training loss: 0.14379598200321198\n",
      "epoch: 3 | 91936 / 114272 | training loss: 0.08281192183494568\n",
      "epoch: 3 | 91968 / 114272 | training loss: 0.024478038772940636\n",
      "epoch: 3 | 92000 / 114272 | training loss: 0.00578663544729352\n",
      "epoch: 3 | 92032 / 114272 | training loss: 0.00442996621131897\n",
      "epoch: 3 | 92064 / 114272 | training loss: 0.08031956106424332\n",
      "epoch: 3 | 92096 / 114272 | training loss: 0.009571915492415428\n",
      "epoch: 3 | 92128 / 114272 | training loss: 0.008035456761717796\n",
      "epoch: 3 | 92160 / 114272 | training loss: 0.002414040034636855\n",
      "epoch: 3 | 92192 / 114272 | training loss: 0.09906096756458282\n",
      "epoch: 3 | 92224 / 114272 | training loss: 0.007060047704726458\n",
      "epoch: 3 | 92256 / 114272 | training loss: 0.004524086602032185\n",
      "epoch: 3 | 92288 / 114272 | training loss: 0.24175772070884705\n",
      "epoch: 3 | 92320 / 114272 | training loss: 0.06604384630918503\n",
      "epoch: 3 | 92352 / 114272 | training loss: 0.005926445592194796\n",
      "epoch: 3 | 92384 / 114272 | training loss: 0.00900159403681755\n",
      "epoch: 3 | 92416 / 114272 | training loss: 0.006761719472706318\n",
      "epoch: 3 | 92448 / 114272 | training loss: 0.09047374874353409\n",
      "epoch: 3 | 92480 / 114272 | training loss: 0.3560016453266144\n",
      "epoch: 3 | 92512 / 114272 | training loss: 0.05075633525848389\n",
      "epoch: 3 | 92544 / 114272 | training loss: 0.09043873101472855\n",
      "epoch: 3 | 92576 / 114272 | training loss: 0.26815932989120483\n",
      "epoch: 3 | 92608 / 114272 | training loss: 0.12129589915275574\n",
      "epoch: 3 | 92640 / 114272 | training loss: 0.25889772176742554\n",
      "epoch: 3 | 92672 / 114272 | training loss: 0.016558736562728882\n",
      "epoch: 3 | 92704 / 114272 | training loss: 0.010960779152810574\n",
      "epoch: 3 | 92736 / 114272 | training loss: 0.11016536504030228\n",
      "epoch: 3 | 92768 / 114272 | training loss: 0.1251021921634674\n",
      "epoch: 3 | 92800 / 114272 | training loss: 0.009617672301828861\n",
      "epoch: 3 | 92832 / 114272 | training loss: 0.11484361439943314\n",
      "epoch: 3 | 92864 / 114272 | training loss: 0.04843224212527275\n",
      "epoch: 3 | 92896 / 114272 | training loss: 0.00704139843583107\n",
      "epoch: 3 | 92928 / 114272 | training loss: 0.23111429810523987\n",
      "epoch: 3 | 92960 / 114272 | training loss: 0.07910125702619553\n",
      "epoch: 3 | 92992 / 114272 | training loss: 0.014242792502045631\n",
      "epoch: 3 | 93024 / 114272 | training loss: 0.03591218590736389\n",
      "epoch: 3 | 93056 / 114272 | training loss: 0.09946934133768082\n",
      "epoch: 3 | 93088 / 114272 | training loss: 0.007097908295691013\n",
      "epoch: 3 | 93120 / 114272 | training loss: 0.007471010554581881\n",
      "epoch: 3 | 93152 / 114272 | training loss: 0.15048962831497192\n",
      "epoch: 3 | 93184 / 114272 | training loss: 0.006907270289957523\n",
      "epoch: 3 | 93216 / 114272 | training loss: 0.2757236957550049\n",
      "epoch: 3 | 93248 / 114272 | training loss: 0.007842021062970161\n",
      "epoch: 3 | 93280 / 114272 | training loss: 0.0756973996758461\n",
      "epoch: 3 | 93312 / 114272 | training loss: 0.13968497514724731\n",
      "epoch: 3 | 93344 / 114272 | training loss: 0.11031591147184372\n",
      "epoch: 3 | 93376 / 114272 | training loss: 0.024367542937397957\n",
      "epoch: 3 | 93408 / 114272 | training loss: 0.12334490567445755\n",
      "epoch: 3 | 93440 / 114272 | training loss: 0.1141752302646637\n",
      "epoch: 3 | 93472 / 114272 | training loss: 0.07404156774282455\n",
      "epoch: 3 | 93504 / 114272 | training loss: 0.010175459086894989\n",
      "epoch: 3 | 93536 / 114272 | training loss: 0.008628593757748604\n",
      "epoch: 3 | 93568 / 114272 | training loss: 0.20873373746871948\n",
      "epoch: 3 | 93600 / 114272 | training loss: 0.06074872240424156\n",
      "epoch: 3 | 93632 / 114272 | training loss: 0.08148090541362762\n",
      "epoch: 3 | 93664 / 114272 | training loss: 0.008787950500845909\n",
      "epoch: 3 | 93696 / 114272 | training loss: 0.005764171946793795\n",
      "epoch: 3 | 93728 / 114272 | training loss: 0.002658611861988902\n",
      "epoch: 3 | 93760 / 114272 | training loss: 0.08639314770698547\n",
      "epoch: 3 | 93792 / 114272 | training loss: 0.004020378924906254\n",
      "epoch: 3 | 93824 / 114272 | training loss: 0.17316286265850067\n",
      "epoch: 3 | 93856 / 114272 | training loss: 0.18729054927825928\n",
      "epoch: 3 | 93888 / 114272 | training loss: 0.01563008315861225\n",
      "epoch: 3 | 93920 / 114272 | training loss: 0.008071725256741047\n",
      "epoch: 3 | 93952 / 114272 | training loss: 0.00402257451787591\n",
      "epoch: 3 | 93984 / 114272 | training loss: 0.06873022019863129\n",
      "epoch: 3 | 94016 / 114272 | training loss: 0.006747240666300058\n",
      "epoch: 3 | 94048 / 114272 | training loss: 0.023471267893910408\n",
      "epoch: 3 | 94080 / 114272 | training loss: 0.10667934268712997\n",
      "epoch: 3 | 94112 / 114272 | training loss: 0.0074347262270748615\n",
      "epoch: 3 | 94144 / 114272 | training loss: 0.20257416367530823\n",
      "epoch: 3 | 94176 / 114272 | training loss: 0.22614926099777222\n",
      "epoch: 3 | 94208 / 114272 | training loss: 0.2123052477836609\n",
      "epoch: 3 | 94240 / 114272 | training loss: 0.1438964158296585\n",
      "epoch: 3 | 94272 / 114272 | training loss: 0.03449761122465134\n",
      "epoch: 3 | 94304 / 114272 | training loss: 0.050981126725673676\n",
      "epoch: 3 | 94336 / 114272 | training loss: 0.11967337131500244\n",
      "epoch: 3 | 94368 / 114272 | training loss: 0.007140606641769409\n",
      "epoch: 3 | 94400 / 114272 | training loss: 0.006763134617358446\n",
      "epoch: 3 | 94432 / 114272 | training loss: 0.003648120677098632\n",
      "epoch: 3 | 94464 / 114272 | training loss: 0.008200031705200672\n",
      "epoch: 3 | 94496 / 114272 | training loss: 0.09915083646774292\n",
      "epoch: 3 | 94528 / 114272 | training loss: 0.13542795181274414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 94560 / 114272 | training loss: 0.006427769549190998\n",
      "epoch: 3 | 94592 / 114272 | training loss: 0.2873896658420563\n",
      "epoch: 3 | 94624 / 114272 | training loss: 0.1291390359401703\n",
      "epoch: 3 | 94656 / 114272 | training loss: 0.14662429690361023\n",
      "epoch: 3 | 94688 / 114272 | training loss: 0.2220640480518341\n",
      "epoch: 3 | 94720 / 114272 | training loss: 0.045477088540792465\n",
      "epoch: 3 | 94752 / 114272 | training loss: 0.009394420310854912\n",
      "epoch: 3 | 94784 / 114272 | training loss: 0.0022264791186898947\n",
      "epoch: 3 | 94816 / 114272 | training loss: 0.09566374868154526\n",
      "epoch: 3 | 94848 / 114272 | training loss: 0.056534674018621445\n",
      "epoch: 3 | 94880 / 114272 | training loss: 0.1337081789970398\n",
      "epoch: 3 | 94912 / 114272 | training loss: 0.009291652590036392\n",
      "epoch: 3 | 94944 / 114272 | training loss: 0.008833572268486023\n",
      "epoch: 3 | 94976 / 114272 | training loss: 0.10509224236011505\n",
      "epoch: 3 | 95008 / 114272 | training loss: 0.006393396761268377\n",
      "epoch: 3 | 95040 / 114272 | training loss: 0.08588945120573044\n",
      "epoch: 3 | 95072 / 114272 | training loss: 0.018526853993535042\n",
      "epoch: 3 | 95104 / 114272 | training loss: 0.0026123046409338713\n",
      "epoch: 3 | 95136 / 114272 | training loss: 0.11642495542764664\n",
      "epoch: 3 | 95168 / 114272 | training loss: 0.08970706909894943\n",
      "epoch: 3 | 95200 / 114272 | training loss: 0.015501118265092373\n",
      "epoch: 3 | 95232 / 114272 | training loss: 0.0040203421376645565\n",
      "epoch: 3 | 95264 / 114272 | training loss: 0.0035036581102758646\n",
      "epoch: 3 | 95296 / 114272 | training loss: 0.017333894968032837\n",
      "epoch: 3 | 95328 / 114272 | training loss: 0.011005319654941559\n",
      "epoch: 3 | 95360 / 114272 | training loss: 0.06903193891048431\n",
      "epoch: 3 | 95392 / 114272 | training loss: 0.007499898783862591\n",
      "epoch: 3 | 95424 / 114272 | training loss: 0.0033504548482596874\n",
      "epoch: 3 | 95456 / 114272 | training loss: 0.014434680342674255\n",
      "epoch: 3 | 95488 / 114272 | training loss: 0.005444997921586037\n",
      "epoch: 3 | 95520 / 114272 | training loss: 0.14675718545913696\n",
      "epoch: 3 | 95552 / 114272 | training loss: 0.003801608458161354\n",
      "epoch: 3 | 95584 / 114272 | training loss: 0.044945478439331055\n",
      "epoch: 3 | 95616 / 114272 | training loss: 0.001452542608603835\n",
      "epoch: 3 | 95648 / 114272 | training loss: 0.016892392188310623\n",
      "epoch: 3 | 95680 / 114272 | training loss: 0.002994903363287449\n",
      "epoch: 3 | 95712 / 114272 | training loss: 0.14985188841819763\n",
      "epoch: 3 | 95744 / 114272 | training loss: 0.0018402010900899768\n",
      "epoch: 3 | 95776 / 114272 | training loss: 0.16901466250419617\n",
      "epoch: 3 | 95808 / 114272 | training loss: 0.006128861103206873\n",
      "epoch: 3 | 95840 / 114272 | training loss: 0.006531577091664076\n",
      "epoch: 3 | 95872 / 114272 | training loss: 0.2205301970243454\n",
      "epoch: 3 | 95904 / 114272 | training loss: 0.007103316951543093\n",
      "epoch: 3 | 95936 / 114272 | training loss: 0.10286206007003784\n",
      "epoch: 3 | 95968 / 114272 | training loss: 0.1646803468465805\n",
      "epoch: 3 | 96000 / 114272 | training loss: 0.13520726561546326\n",
      "epoch: 3 | 96032 / 114272 | training loss: 0.003881217446178198\n",
      "epoch: 3 | 96064 / 114272 | training loss: 0.09677788615226746\n",
      "epoch: 3 | 96096 / 114272 | training loss: 0.13874451816082\n",
      "epoch: 3 | 96128 / 114272 | training loss: 0.05133068934082985\n",
      "epoch: 3 | 96160 / 114272 | training loss: 0.03459769859910011\n",
      "epoch: 3 | 96192 / 114272 | training loss: 0.0036660318728536367\n",
      "epoch: 3 | 96224 / 114272 | training loss: 0.023753900080919266\n",
      "epoch: 3 | 96256 / 114272 | training loss: 0.006426331587135792\n",
      "epoch: 3 | 96288 / 114272 | training loss: 0.010290645062923431\n",
      "epoch: 3 | 96320 / 114272 | training loss: 0.001954855630174279\n",
      "epoch: 3 | 96352 / 114272 | training loss: 0.005291149485856295\n",
      "epoch: 3 | 96384 / 114272 | training loss: 0.07650547474622726\n",
      "epoch: 3 | 96416 / 114272 | training loss: 0.05681147053837776\n",
      "epoch: 3 | 96448 / 114272 | training loss: 0.29178544878959656\n",
      "epoch: 3 | 96480 / 114272 | training loss: 0.09428749233484268\n",
      "epoch: 3 | 96512 / 114272 | training loss: 0.18507055938243866\n",
      "epoch: 3 | 96544 / 114272 | training loss: 0.08800553530454636\n",
      "epoch: 3 | 96576 / 114272 | training loss: 0.0033217985183000565\n",
      "epoch: 3 | 96608 / 114272 | training loss: 0.07545369863510132\n",
      "epoch: 3 | 96640 / 114272 | training loss: 0.09919385612010956\n",
      "epoch: 3 | 96672 / 114272 | training loss: 0.024784358218312263\n",
      "epoch: 3 | 96704 / 114272 | training loss: 0.01989937014877796\n",
      "epoch: 3 | 96736 / 114272 | training loss: 0.22497889399528503\n",
      "epoch: 3 | 96768 / 114272 | training loss: 0.10248416662216187\n",
      "epoch: 3 | 96800 / 114272 | training loss: 0.015992550179362297\n",
      "epoch: 3 | 96832 / 114272 | training loss: 0.3165324330329895\n",
      "epoch: 3 | 96864 / 114272 | training loss: 0.26606544852256775\n",
      "epoch: 3 | 96896 / 114272 | training loss: 0.008098626509308815\n",
      "epoch: 3 | 96928 / 114272 | training loss: 0.05173619091510773\n",
      "epoch: 3 | 96960 / 114272 | training loss: 0.009076935239136219\n",
      "epoch: 3 | 96992 / 114272 | training loss: 0.0011271846015006304\n",
      "epoch: 3 | 97024 / 114272 | training loss: 0.1074812263250351\n",
      "epoch: 3 | 97056 / 114272 | training loss: 0.026802176609635353\n",
      "epoch: 3 | 97088 / 114272 | training loss: 0.011920148506760597\n",
      "epoch: 3 | 97120 / 114272 | training loss: 0.006292186677455902\n",
      "epoch: 3 | 97152 / 114272 | training loss: 0.10821989923715591\n",
      "epoch: 3 | 97184 / 114272 | training loss: 0.09495040029287338\n",
      "epoch: 3 | 97216 / 114272 | training loss: 0.002241495531052351\n",
      "epoch: 3 | 97248 / 114272 | training loss: 0.020901616662740707\n",
      "epoch: 3 | 97280 / 114272 | training loss: 0.10239315032958984\n",
      "epoch: 3 | 97312 / 114272 | training loss: 0.006565703544765711\n",
      "epoch: 3 | 97344 / 114272 | training loss: 0.013785953633487225\n",
      "epoch: 3 | 97376 / 114272 | training loss: 0.16787342727184296\n",
      "epoch: 3 | 97408 / 114272 | training loss: 0.07274957001209259\n",
      "epoch: 3 | 97440 / 114272 | training loss: 0.0013335797702893615\n",
      "epoch: 3 | 97472 / 114272 | training loss: 0.1748562902212143\n",
      "epoch: 3 | 97504 / 114272 | training loss: 0.005754949524998665\n",
      "epoch: 3 | 97536 / 114272 | training loss: 0.00824449397623539\n",
      "epoch: 3 | 97568 / 114272 | training loss: 0.05078713223338127\n",
      "epoch: 3 | 97600 / 114272 | training loss: 0.007153593003749847\n",
      "epoch: 3 | 97632 / 114272 | training loss: 0.17483189702033997\n",
      "epoch: 3 | 97664 / 114272 | training loss: 0.06622517853975296\n",
      "epoch: 3 | 97696 / 114272 | training loss: 0.21387700736522675\n",
      "epoch: 3 | 97728 / 114272 | training loss: 0.0030577077995985746\n",
      "epoch: 3 | 97760 / 114272 | training loss: 0.004816766828298569\n",
      "epoch: 3 | 97792 / 114272 | training loss: 0.0032476764172315598\n",
      "epoch: 3 | 97824 / 114272 | training loss: 0.10077867656946182\n",
      "epoch: 3 | 97856 / 114272 | training loss: 0.054469138383865356\n",
      "epoch: 3 | 97888 / 114272 | training loss: 0.011523203924298286\n",
      "epoch: 3 | 97920 / 114272 | training loss: 0.13904805481433868\n",
      "epoch: 3 | 97952 / 114272 | training loss: 0.053031519055366516\n",
      "epoch: 3 | 97984 / 114272 | training loss: 0.029240909963846207\n",
      "epoch: 3 | 98016 / 114272 | training loss: 0.0871676504611969\n",
      "epoch: 3 | 98048 / 114272 | training loss: 0.06468947231769562\n",
      "epoch: 3 | 98080 / 114272 | training loss: 0.007911325432360172\n",
      "epoch: 3 | 98112 / 114272 | training loss: 0.003989196382462978\n",
      "epoch: 3 | 98144 / 114272 | training loss: 0.27551084756851196\n",
      "epoch: 3 | 98176 / 114272 | training loss: 0.0028171176090836525\n",
      "epoch: 3 | 98208 / 114272 | training loss: 0.028498506173491478\n",
      "epoch: 3 | 98240 / 114272 | training loss: 0.0579974502325058\n",
      "epoch: 3 | 98272 / 114272 | training loss: 0.007624401245266199\n",
      "epoch: 3 | 98304 / 114272 | training loss: 0.04325459897518158\n",
      "epoch: 3 | 98336 / 114272 | training loss: 0.006385066546499729\n",
      "epoch: 3 | 98368 / 114272 | training loss: 0.09044384211301804\n",
      "epoch: 3 | 98400 / 114272 | training loss: 0.0062669855542480946\n",
      "epoch: 3 | 98432 / 114272 | training loss: 0.013927221298217773\n",
      "epoch: 3 | 98464 / 114272 | training loss: 0.0014286412624642253\n",
      "epoch: 3 | 98496 / 114272 | training loss: 0.15238074958324432\n",
      "epoch: 3 | 98528 / 114272 | training loss: 0.08405093103647232\n",
      "epoch: 3 | 98560 / 114272 | training loss: 0.10577331483364105\n",
      "epoch: 3 | 98592 / 114272 | training loss: 0.0015626945532858372\n",
      "epoch: 3 | 98624 / 114272 | training loss: 0.007582535035908222\n",
      "epoch: 3 | 98656 / 114272 | training loss: 0.004085066262632608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 98688 / 114272 | training loss: 0.0070970854721963406\n",
      "epoch: 3 | 98720 / 114272 | training loss: 0.20700260996818542\n",
      "epoch: 3 | 98752 / 114272 | training loss: 0.007621890865266323\n",
      "epoch: 3 | 98784 / 114272 | training loss: 0.008894694969058037\n",
      "epoch: 3 | 98816 / 114272 | training loss: 0.030977699905633926\n",
      "epoch: 3 | 98848 / 114272 | training loss: 0.006915480364114046\n",
      "epoch: 3 | 98880 / 114272 | training loss: 0.12010736018419266\n",
      "epoch: 3 | 98912 / 114272 | training loss: 0.003944308962672949\n",
      "epoch: 3 | 98944 / 114272 | training loss: 0.1136661097407341\n",
      "epoch: 3 | 98976 / 114272 | training loss: 0.0270552821457386\n",
      "epoch: 3 | 99008 / 114272 | training loss: 0.004007683601230383\n",
      "epoch: 3 | 99040 / 114272 | training loss: 0.14474378526210785\n",
      "epoch: 3 | 99072 / 114272 | training loss: 0.058403391391038895\n",
      "epoch: 3 | 99104 / 114272 | training loss: 0.051556192338466644\n",
      "epoch: 3 | 99136 / 114272 | training loss: 0.19440363347530365\n",
      "epoch: 3 | 99168 / 114272 | training loss: 0.016146522015333176\n",
      "epoch: 3 | 99200 / 114272 | training loss: 0.008990348316729069\n",
      "epoch: 3 | 99232 / 114272 | training loss: 0.015481996349990368\n",
      "epoch: 3 | 99264 / 114272 | training loss: 0.07235162705183029\n",
      "epoch: 3 | 99296 / 114272 | training loss: 0.17890633642673492\n",
      "epoch: 3 | 99328 / 114272 | training loss: 0.2197963297367096\n",
      "epoch: 3 | 99360 / 114272 | training loss: 0.004234717693179846\n",
      "epoch: 3 | 99392 / 114272 | training loss: 0.006019826512783766\n",
      "epoch: 3 | 99424 / 114272 | training loss: 0.11810165643692017\n",
      "epoch: 3 | 99456 / 114272 | training loss: 0.02037917822599411\n",
      "epoch: 3 | 99488 / 114272 | training loss: 0.0042828782461583614\n",
      "epoch: 3 | 99520 / 114272 | training loss: 0.026990702375769615\n",
      "epoch: 3 | 99552 / 114272 | training loss: 0.007442685775458813\n",
      "epoch: 3 | 99584 / 114272 | training loss: 0.008013367652893066\n",
      "epoch: 3 | 99616 / 114272 | training loss: 0.05107228457927704\n",
      "epoch: 3 | 99648 / 114272 | training loss: 0.057319991290569305\n",
      "epoch: 3 | 99680 / 114272 | training loss: 0.11292801797389984\n",
      "epoch: 3 | 99712 / 114272 | training loss: 0.0050866687670350075\n",
      "epoch: 3 | 99744 / 114272 | training loss: 0.03840647265315056\n",
      "epoch: 3 | 99776 / 114272 | training loss: 0.0028204189147800207\n",
      "epoch: 3 | 99808 / 114272 | training loss: 0.010814878158271313\n",
      "epoch: 3 | 99840 / 114272 | training loss: 0.180300772190094\n",
      "epoch: 3 | 99872 / 114272 | training loss: 0.03186829760670662\n",
      "epoch: 3 | 99904 / 114272 | training loss: 0.0067815654911100864\n",
      "epoch: 3 | 99936 / 114272 | training loss: 0.00568779231980443\n",
      "epoch: 3 | 99968 / 114272 | training loss: 0.006026833783835173\n",
      "epoch: 3 | 100000 / 114272 | training loss: 0.05367867276072502\n",
      "epoch: 3 | 100032 / 114272 | training loss: 0.006267503369599581\n",
      "epoch: 3 | 100064 / 114272 | training loss: 0.21847203373908997\n",
      "epoch: 3 | 100096 / 114272 | training loss: 0.17609015107154846\n",
      "epoch: 3 | 100128 / 114272 | training loss: 0.14316418766975403\n",
      "epoch: 3 | 100160 / 114272 | training loss: 0.2522527575492859\n",
      "epoch: 3 | 100192 / 114272 | training loss: 0.1824464052915573\n",
      "epoch: 3 | 100224 / 114272 | training loss: 0.11764460802078247\n",
      "epoch: 3 | 100256 / 114272 | training loss: 0.0038799764588475227\n",
      "epoch: 3 | 100288 / 114272 | training loss: 0.0019691267516463995\n",
      "epoch: 3 | 100320 / 114272 | training loss: 0.004061621148139238\n",
      "epoch: 3 | 100352 / 114272 | training loss: 0.0029252110980451107\n",
      "epoch: 3 | 100384 / 114272 | training loss: 0.006486854050308466\n",
      "epoch: 3 | 100416 / 114272 | training loss: 0.13248690962791443\n",
      "epoch: 3 | 100448 / 114272 | training loss: 0.09699428081512451\n",
      "epoch: 3 | 100480 / 114272 | training loss: 0.0929456502199173\n",
      "epoch: 3 | 100512 / 114272 | training loss: 0.009379728697240353\n",
      "epoch: 3 | 100544 / 114272 | training loss: 0.021228764206171036\n",
      "epoch: 3 | 100576 / 114272 | training loss: 0.00541442958638072\n",
      "epoch: 3 | 100608 / 114272 | training loss: 0.11169259250164032\n",
      "epoch: 3 | 100640 / 114272 | training loss: 0.0032315896824002266\n",
      "epoch: 3 | 100672 / 114272 | training loss: 0.17551954090595245\n",
      "epoch: 3 | 100704 / 114272 | training loss: 0.002196243731305003\n",
      "epoch: 3 | 100736 / 114272 | training loss: 0.2573258876800537\n",
      "epoch: 3 | 100768 / 114272 | training loss: 0.004491907544434071\n",
      "epoch: 3 | 100800 / 114272 | training loss: 0.27049678564071655\n",
      "epoch: 3 | 100832 / 114272 | training loss: 0.06853104382753372\n",
      "epoch: 3 | 100864 / 114272 | training loss: 0.009516180492937565\n",
      "epoch: 3 | 100896 / 114272 | training loss: 0.05818277597427368\n",
      "epoch: 3 | 100928 / 114272 | training loss: 0.031131919473409653\n",
      "epoch: 3 | 100960 / 114272 | training loss: 0.09926757961511612\n",
      "epoch: 3 | 100992 / 114272 | training loss: 0.004248902667313814\n",
      "epoch: 3 | 101024 / 114272 | training loss: 0.15855540335178375\n",
      "epoch: 3 | 101056 / 114272 | training loss: 0.15017551183700562\n",
      "epoch: 3 | 101088 / 114272 | training loss: 0.11597763001918793\n",
      "epoch: 3 | 101120 / 114272 | training loss: 0.185362309217453\n",
      "epoch: 3 | 101152 / 114272 | training loss: 0.1317838579416275\n",
      "epoch: 3 | 101184 / 114272 | training loss: 0.13032495975494385\n",
      "epoch: 3 | 101216 / 114272 | training loss: 0.004429015330970287\n",
      "epoch: 3 | 101248 / 114272 | training loss: 0.0857713595032692\n",
      "epoch: 3 | 101280 / 114272 | training loss: 0.08226588368415833\n",
      "epoch: 3 | 101312 / 114272 | training loss: 0.010108350776135921\n",
      "epoch: 3 | 101344 / 114272 | training loss: 0.1092638149857521\n",
      "epoch: 3 | 101376 / 114272 | training loss: 0.018807057291269302\n",
      "epoch: 3 | 101408 / 114272 | training loss: 0.1808987855911255\n",
      "epoch: 3 | 101440 / 114272 | training loss: 0.009497606195509434\n",
      "epoch: 3 | 101472 / 114272 | training loss: 0.06478491425514221\n",
      "epoch: 3 | 101504 / 114272 | training loss: 0.008848820813000202\n",
      "epoch: 3 | 101536 / 114272 | training loss: 0.04180033504962921\n",
      "epoch: 3 | 101568 / 114272 | training loss: 0.06968298554420471\n",
      "epoch: 3 | 101600 / 114272 | training loss: 0.10890328884124756\n",
      "epoch: 3 | 101632 / 114272 | training loss: 0.21442991495132446\n",
      "epoch: 3 | 101664 / 114272 | training loss: 0.0822690948843956\n",
      "epoch: 3 | 101696 / 114272 | training loss: 0.16370463371276855\n",
      "epoch: 3 | 101728 / 114272 | training loss: 0.17368662357330322\n",
      "epoch: 3 | 101760 / 114272 | training loss: 0.1387663185596466\n",
      "epoch: 3 | 101792 / 114272 | training loss: 0.06470275670289993\n",
      "epoch: 3 | 101824 / 114272 | training loss: 0.01923370733857155\n",
      "epoch: 3 | 101856 / 114272 | training loss: 0.011874211952090263\n",
      "epoch: 3 | 101888 / 114272 | training loss: 0.008342087268829346\n",
      "epoch: 3 | 101920 / 114272 | training loss: 0.02616531401872635\n",
      "epoch: 3 | 101952 / 114272 | training loss: 0.004772980231791735\n",
      "epoch: 3 | 101984 / 114272 | training loss: 0.01366116851568222\n",
      "epoch: 3 | 102016 / 114272 | training loss: 0.010275743901729584\n",
      "epoch: 3 | 102048 / 114272 | training loss: 0.13401108980178833\n",
      "epoch: 3 | 102080 / 114272 | training loss: 0.2551015019416809\n",
      "epoch: 3 | 102112 / 114272 | training loss: 0.026193983852863312\n",
      "epoch: 3 | 102144 / 114272 | training loss: 0.13286171853542328\n",
      "epoch: 3 | 102176 / 114272 | training loss: 0.0036053366493433714\n",
      "epoch: 3 | 102208 / 114272 | training loss: 0.00439824815839529\n",
      "epoch: 3 | 102240 / 114272 | training loss: 0.16694824397563934\n",
      "epoch: 3 | 102272 / 114272 | training loss: 0.010611196048557758\n",
      "epoch: 3 | 102304 / 114272 | training loss: 0.02602420374751091\n",
      "epoch: 3 | 102336 / 114272 | training loss: 0.2303299605846405\n",
      "epoch: 3 | 102368 / 114272 | training loss: 0.16565130650997162\n",
      "epoch: 3 | 102400 / 114272 | training loss: 0.24570268392562866\n",
      "epoch: 3 | 102432 / 114272 | training loss: 0.0029619077686220407\n",
      "epoch: 3 | 102464 / 114272 | training loss: 0.01545203011482954\n",
      "epoch: 3 | 102496 / 114272 | training loss: 0.044173821806907654\n",
      "epoch: 3 | 102528 / 114272 | training loss: 0.010873052291572094\n",
      "epoch: 3 | 102560 / 114272 | training loss: 0.012589584104716778\n",
      "epoch: 3 | 102592 / 114272 | training loss: 0.00416980916634202\n",
      "epoch: 3 | 102624 / 114272 | training loss: 0.009943117387592793\n",
      "epoch: 3 | 102656 / 114272 | training loss: 0.0038609167095273733\n",
      "epoch: 3 | 102688 / 114272 | training loss: 0.12342337518930435\n",
      "epoch: 3 | 102720 / 114272 | training loss: 0.03209784999489784\n",
      "epoch: 3 | 102752 / 114272 | training loss: 0.37368810176849365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 102784 / 114272 | training loss: 0.0015012496151030064\n",
      "epoch: 3 | 102816 / 114272 | training loss: 0.008830440230667591\n",
      "epoch: 3 | 102848 / 114272 | training loss: 0.01165552157908678\n",
      "epoch: 3 | 102880 / 114272 | training loss: 0.21853533387184143\n",
      "epoch: 3 | 102912 / 114272 | training loss: 0.38178759813308716\n",
      "epoch: 3 | 102944 / 114272 | training loss: 0.08565357327461243\n",
      "epoch: 3 | 102976 / 114272 | training loss: 0.17491672933101654\n",
      "epoch: 3 | 103008 / 114272 | training loss: 0.0039250608533620834\n",
      "epoch: 3 | 103040 / 114272 | training loss: 0.29389843344688416\n",
      "epoch: 3 | 103072 / 114272 | training loss: 0.062106918543577194\n",
      "epoch: 3 | 103104 / 114272 | training loss: 0.11237550526857376\n",
      "epoch: 3 | 103136 / 114272 | training loss: 0.0038987884763628244\n",
      "epoch: 3 | 103168 / 114272 | training loss: 0.3869938850402832\n",
      "epoch: 3 | 103200 / 114272 | training loss: 0.004462080076336861\n",
      "epoch: 3 | 103232 / 114272 | training loss: 0.013751264661550522\n",
      "epoch: 3 | 103264 / 114272 | training loss: 0.002780333859845996\n",
      "epoch: 3 | 103296 / 114272 | training loss: 0.002931694034487009\n",
      "epoch: 3 | 103328 / 114272 | training loss: 0.0069577437825500965\n",
      "epoch: 3 | 103360 / 114272 | training loss: 0.008949645794928074\n",
      "epoch: 3 | 103392 / 114272 | training loss: 0.0034505645744502544\n",
      "epoch: 3 | 103424 / 114272 | training loss: 0.013177505694329739\n",
      "epoch: 3 | 103456 / 114272 | training loss: 0.14429916441440582\n",
      "epoch: 3 | 103488 / 114272 | training loss: 0.0013885613298043609\n",
      "epoch: 3 | 103520 / 114272 | training loss: 0.0250235665589571\n",
      "epoch: 3 | 103552 / 114272 | training loss: 0.12566004693508148\n",
      "epoch: 3 | 103584 / 114272 | training loss: 0.005726215895265341\n",
      "epoch: 3 | 103616 / 114272 | training loss: 0.055204086005687714\n",
      "epoch: 3 | 103648 / 114272 | training loss: 0.0037070715334266424\n",
      "epoch: 3 | 103680 / 114272 | training loss: 0.09479758143424988\n",
      "epoch: 3 | 103712 / 114272 | training loss: 0.17185074090957642\n",
      "epoch: 3 | 103744 / 114272 | training loss: 0.0153486467897892\n",
      "epoch: 3 | 103776 / 114272 | training loss: 0.09481249004602432\n",
      "epoch: 3 | 103808 / 114272 | training loss: 0.05398305878043175\n",
      "epoch: 3 | 103840 / 114272 | training loss: 0.0025732212234288454\n",
      "epoch: 3 | 103872 / 114272 | training loss: 0.007563251070678234\n",
      "epoch: 3 | 103904 / 114272 | training loss: 0.0018839009571820498\n",
      "epoch: 3 | 103936 / 114272 | training loss: 0.14957405626773834\n",
      "epoch: 3 | 103968 / 114272 | training loss: 0.003900517476722598\n",
      "epoch: 3 | 104000 / 114272 | training loss: 0.004156733397394419\n",
      "epoch: 3 | 104032 / 114272 | training loss: 0.1535026729106903\n",
      "epoch: 3 | 104064 / 114272 | training loss: 0.006656235083937645\n",
      "epoch: 3 | 104096 / 114272 | training loss: 0.011369077488780022\n",
      "epoch: 3 | 104128 / 114272 | training loss: 0.24589993059635162\n",
      "epoch: 3 | 104160 / 114272 | training loss: 0.2550663948059082\n",
      "epoch: 3 | 104192 / 114272 | training loss: 0.008962295018136501\n",
      "epoch: 3 | 104224 / 114272 | training loss: 0.26933732628822327\n",
      "epoch: 3 | 104256 / 114272 | training loss: 0.045196883380413055\n",
      "epoch: 3 | 104288 / 114272 | training loss: 0.0008105244487524033\n",
      "epoch: 3 | 104320 / 114272 | training loss: 0.009567312896251678\n",
      "epoch: 3 | 104352 / 114272 | training loss: 0.12842616438865662\n",
      "epoch: 3 | 104384 / 114272 | training loss: 0.005813202820718288\n",
      "epoch: 3 | 104416 / 114272 | training loss: 0.022061049938201904\n",
      "epoch: 3 | 104448 / 114272 | training loss: 0.007058018818497658\n",
      "epoch: 3 | 104480 / 114272 | training loss: 0.0029862786177545786\n",
      "epoch: 3 | 104512 / 114272 | training loss: 0.005112683400511742\n",
      "epoch: 3 | 104544 / 114272 | training loss: 0.0062491376884281635\n",
      "epoch: 3 | 104576 / 114272 | training loss: 0.3483239412307739\n",
      "epoch: 3 | 104608 / 114272 | training loss: 0.00406463211402297\n",
      "epoch: 3 | 104640 / 114272 | training loss: 0.15243294835090637\n",
      "epoch: 3 | 104672 / 114272 | training loss: 0.009355267509818077\n",
      "epoch: 3 | 104704 / 114272 | training loss: 0.017240209504961967\n",
      "epoch: 3 | 104736 / 114272 | training loss: 0.1135263442993164\n",
      "epoch: 3 | 104768 / 114272 | training loss: 0.020993810147047043\n",
      "epoch: 3 | 104800 / 114272 | training loss: 0.02684657648205757\n",
      "epoch: 3 | 104832 / 114272 | training loss: 0.298543244600296\n",
      "epoch: 3 | 104864 / 114272 | training loss: 0.07176940888166428\n",
      "epoch: 3 | 104896 / 114272 | training loss: 0.20601396262645721\n",
      "epoch: 3 | 104928 / 114272 | training loss: 0.006229298189282417\n",
      "epoch: 3 | 104960 / 114272 | training loss: 0.24436193704605103\n",
      "epoch: 3 | 104992 / 114272 | training loss: 0.0706695020198822\n",
      "epoch: 3 | 105024 / 114272 | training loss: 0.0064112902618944645\n",
      "epoch: 3 | 105056 / 114272 | training loss: 0.010137834586203098\n",
      "epoch: 3 | 105088 / 114272 | training loss: 0.16945615410804749\n",
      "epoch: 3 | 105120 / 114272 | training loss: 0.034837756305933\n",
      "epoch: 3 | 105152 / 114272 | training loss: 0.1426045149564743\n",
      "epoch: 3 | 105184 / 114272 | training loss: 0.09453262388706207\n",
      "epoch: 3 | 105216 / 114272 | training loss: 0.1972845494747162\n",
      "epoch: 3 | 105248 / 114272 | training loss: 0.29555851221084595\n",
      "epoch: 3 | 105280 / 114272 | training loss: 0.07453466206789017\n",
      "epoch: 3 | 105312 / 114272 | training loss: 0.14866918325424194\n",
      "epoch: 3 | 105344 / 114272 | training loss: 0.007932674139738083\n",
      "epoch: 3 | 105376 / 114272 | training loss: 0.006492367014288902\n",
      "epoch: 3 | 105408 / 114272 | training loss: 0.011060177348554134\n",
      "epoch: 3 | 105440 / 114272 | training loss: 0.04073590785264969\n",
      "epoch: 3 | 105472 / 114272 | training loss: 0.03935771808028221\n",
      "epoch: 3 | 105504 / 114272 | training loss: 0.004636896774172783\n",
      "epoch: 3 | 105536 / 114272 | training loss: 0.027663329616189003\n",
      "epoch: 3 | 105568 / 114272 | training loss: 0.0030931970104575157\n",
      "epoch: 3 | 105600 / 114272 | training loss: 0.07191357761621475\n",
      "epoch: 3 | 105632 / 114272 | training loss: 0.16030728816986084\n",
      "epoch: 3 | 105664 / 114272 | training loss: 0.18457981944084167\n",
      "epoch: 3 | 105696 / 114272 | training loss: 0.17728380858898163\n",
      "epoch: 3 | 105728 / 114272 | training loss: 0.12962377071380615\n",
      "epoch: 3 | 105760 / 114272 | training loss: 0.006663961801677942\n",
      "epoch: 3 | 105792 / 114272 | training loss: 0.00909893587231636\n",
      "epoch: 3 | 105824 / 114272 | training loss: 0.2656953036785126\n",
      "epoch: 3 | 105856 / 114272 | training loss: 0.004623393528163433\n",
      "epoch: 3 | 105888 / 114272 | training loss: 0.005735919810831547\n",
      "epoch: 3 | 105920 / 114272 | training loss: 0.007610605098307133\n",
      "epoch: 3 | 105952 / 114272 | training loss: 0.03401170298457146\n",
      "epoch: 3 | 105984 / 114272 | training loss: 0.23542264103889465\n",
      "epoch: 3 | 106016 / 114272 | training loss: 0.012718437239527702\n",
      "epoch: 3 | 106048 / 114272 | training loss: 0.01740666665136814\n",
      "epoch: 3 | 106080 / 114272 | training loss: 0.015813585370779037\n",
      "epoch: 3 | 106112 / 114272 | training loss: 0.009099171496927738\n",
      "epoch: 3 | 106144 / 114272 | training loss: 0.052218105643987656\n",
      "epoch: 3 | 106176 / 114272 | training loss: 0.10396577417850494\n",
      "epoch: 3 | 106208 / 114272 | training loss: 0.008300616405904293\n",
      "epoch: 3 | 106240 / 114272 | training loss: 0.01150189246982336\n",
      "epoch: 3 | 106272 / 114272 | training loss: 0.004418699070811272\n",
      "epoch: 3 | 106304 / 114272 | training loss: 0.10283826291561127\n",
      "epoch: 3 | 106336 / 114272 | training loss: 0.12831205129623413\n",
      "epoch: 3 | 106368 / 114272 | training loss: 0.005922601092606783\n",
      "epoch: 3 | 106400 / 114272 | training loss: 0.15468981862068176\n",
      "epoch: 3 | 106432 / 114272 | training loss: 0.1294940710067749\n",
      "epoch: 3 | 106464 / 114272 | training loss: 0.012117979116737843\n",
      "epoch: 3 | 106496 / 114272 | training loss: 0.007562761660665274\n",
      "epoch: 3 | 106528 / 114272 | training loss: 0.029722902923822403\n",
      "epoch: 3 | 106560 / 114272 | training loss: 0.011424838565289974\n",
      "epoch: 3 | 106592 / 114272 | training loss: 0.1492711752653122\n",
      "epoch: 3 | 106624 / 114272 | training loss: 0.03565763682126999\n",
      "epoch: 3 | 106656 / 114272 | training loss: 0.08774440735578537\n",
      "epoch: 3 | 106688 / 114272 | training loss: 0.09896880388259888\n",
      "epoch: 3 | 106720 / 114272 | training loss: 0.007728702388703823\n",
      "epoch: 3 | 106752 / 114272 | training loss: 0.002890902804210782\n",
      "epoch: 3 | 106784 / 114272 | training loss: 0.16634830832481384\n",
      "epoch: 3 | 106816 / 114272 | training loss: 0.01789405196905136\n",
      "epoch: 3 | 106848 / 114272 | training loss: 0.040776100009679794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 106880 / 114272 | training loss: 0.005842940881848335\n",
      "epoch: 3 | 106912 / 114272 | training loss: 0.025087829679250717\n",
      "epoch: 3 | 106944 / 114272 | training loss: 0.009851980023086071\n",
      "epoch: 3 | 106976 / 114272 | training loss: 0.005660759285092354\n",
      "epoch: 3 | 107008 / 114272 | training loss: 0.12788929045200348\n",
      "epoch: 3 | 107040 / 114272 | training loss: 0.006521115079522133\n",
      "epoch: 3 | 107072 / 114272 | training loss: 0.0920373722910881\n",
      "epoch: 3 | 107104 / 114272 | training loss: 0.09883540123701096\n",
      "epoch: 3 | 107136 / 114272 | training loss: 0.003482218598946929\n",
      "epoch: 3 | 107168 / 114272 | training loss: 0.12086334824562073\n",
      "epoch: 3 | 107200 / 114272 | training loss: 0.07290219515562057\n",
      "epoch: 3 | 107232 / 114272 | training loss: 0.02397109381854534\n",
      "epoch: 3 | 107264 / 114272 | training loss: 0.10591444373130798\n",
      "epoch: 3 | 107296 / 114272 | training loss: 0.13864558935165405\n",
      "epoch: 3 | 107328 / 114272 | training loss: 0.009232152253389359\n",
      "epoch: 3 | 107360 / 114272 | training loss: 0.0590335875749588\n",
      "epoch: 3 | 107392 / 114272 | training loss: 0.006551455706357956\n",
      "epoch: 3 | 107424 / 114272 | training loss: 0.02151772752404213\n",
      "epoch: 3 | 107456 / 114272 | training loss: 0.17241859436035156\n",
      "epoch: 3 | 107488 / 114272 | training loss: 0.0025740100536495447\n",
      "epoch: 3 | 107520 / 114272 | training loss: 0.09634215384721756\n",
      "epoch: 3 | 107552 / 114272 | training loss: 0.18595604598522186\n",
      "epoch: 3 | 107584 / 114272 | training loss: 0.03245297074317932\n",
      "epoch: 3 | 107616 / 114272 | training loss: 0.0038770700339227915\n",
      "epoch: 3 | 107648 / 114272 | training loss: 0.1215568333864212\n",
      "epoch: 3 | 107680 / 114272 | training loss: 0.2726152241230011\n",
      "epoch: 3 | 107712 / 114272 | training loss: 0.15286654233932495\n",
      "epoch: 3 | 107744 / 114272 | training loss: 0.0055349403992295265\n",
      "epoch: 3 | 107776 / 114272 | training loss: 0.180092915892601\n",
      "epoch: 3 | 107808 / 114272 | training loss: 0.003056475194171071\n",
      "epoch: 3 | 107840 / 114272 | training loss: 0.010598164051771164\n",
      "epoch: 3 | 107872 / 114272 | training loss: 0.006052133161574602\n",
      "epoch: 3 | 107904 / 114272 | training loss: 0.006729088257998228\n",
      "epoch: 3 | 107936 / 114272 | training loss: 0.005882925819605589\n",
      "epoch: 3 | 107968 / 114272 | training loss: 0.009918933734297752\n",
      "epoch: 3 | 108000 / 114272 | training loss: 0.007848098874092102\n",
      "epoch: 3 | 108032 / 114272 | training loss: 0.07142945379018784\n",
      "epoch: 3 | 108064 / 114272 | training loss: 0.012716371566057205\n",
      "epoch: 3 | 108096 / 114272 | training loss: 0.01914508081972599\n",
      "epoch: 3 | 108128 / 114272 | training loss: 0.0075255995616316795\n",
      "epoch: 3 | 108160 / 114272 | training loss: 0.030703797936439514\n",
      "epoch: 3 | 108192 / 114272 | training loss: 0.04994162917137146\n",
      "epoch: 3 | 108224 / 114272 | training loss: 0.013280875980854034\n",
      "epoch: 3 | 108256 / 114272 | training loss: 0.056226424872875214\n",
      "epoch: 3 | 108288 / 114272 | training loss: 0.13474038243293762\n",
      "epoch: 3 | 108320 / 114272 | training loss: 0.01389484852552414\n",
      "epoch: 3 | 108352 / 114272 | training loss: 0.06624701619148254\n",
      "epoch: 3 | 108384 / 114272 | training loss: 0.10441192239522934\n",
      "epoch: 3 | 108416 / 114272 | training loss: 0.03132777661085129\n",
      "epoch: 3 | 108448 / 114272 | training loss: 0.012669858522713184\n",
      "epoch: 3 | 108480 / 114272 | training loss: 0.06521490961313248\n",
      "epoch: 3 | 108512 / 114272 | training loss: 0.0035456372424960136\n",
      "epoch: 3 | 108544 / 114272 | training loss: 0.008053532801568508\n",
      "epoch: 3 | 108576 / 114272 | training loss: 0.012817101553082466\n",
      "epoch: 3 | 108608 / 114272 | training loss: 0.00226972415111959\n",
      "epoch: 3 | 108640 / 114272 | training loss: 0.0035525690764188766\n",
      "epoch: 3 | 108672 / 114272 | training loss: 0.01536059845238924\n",
      "epoch: 3 | 108704 / 114272 | training loss: 0.025884563103318214\n",
      "epoch: 3 | 108736 / 114272 | training loss: 0.008176824077963829\n",
      "epoch: 3 | 108768 / 114272 | training loss: 0.1872408539056778\n",
      "epoch: 3 | 108800 / 114272 | training loss: 0.3825603723526001\n",
      "epoch: 3 | 108832 / 114272 | training loss: 0.08793586492538452\n",
      "epoch: 3 | 108864 / 114272 | training loss: 0.0020968534518033266\n",
      "epoch: 3 | 108896 / 114272 | training loss: 0.011671568267047405\n",
      "epoch: 3 | 108928 / 114272 | training loss: 0.0029738314915448427\n",
      "epoch: 3 | 108960 / 114272 | training loss: 0.1349438577890396\n",
      "epoch: 3 | 108992 / 114272 | training loss: 0.0332457609474659\n",
      "epoch: 3 | 109024 / 114272 | training loss: 0.1832880824804306\n",
      "epoch: 3 | 109056 / 114272 | training loss: 0.08816245198249817\n",
      "epoch: 3 | 109088 / 114272 | training loss: 0.007104058749973774\n",
      "epoch: 3 | 109120 / 114272 | training loss: 0.10481637716293335\n",
      "epoch: 3 | 109152 / 114272 | training loss: 0.004598214756697416\n",
      "epoch: 3 | 109184 / 114272 | training loss: 0.13057748973369598\n",
      "epoch: 3 | 109216 / 114272 | training loss: 0.2560388147830963\n",
      "epoch: 3 | 109248 / 114272 | training loss: 0.012485668994486332\n",
      "epoch: 3 | 109280 / 114272 | training loss: 0.08829556405544281\n",
      "epoch: 3 | 109312 / 114272 | training loss: 0.07015115022659302\n",
      "epoch: 3 | 109344 / 114272 | training loss: 0.01278261560946703\n",
      "epoch: 3 | 109376 / 114272 | training loss: 0.024546969681978226\n",
      "epoch: 3 | 109408 / 114272 | training loss: 0.0032768012024462223\n",
      "epoch: 3 | 109440 / 114272 | training loss: 0.0029599007684737444\n",
      "epoch: 3 | 109472 / 114272 | training loss: 0.03526774421334267\n",
      "epoch: 3 | 109504 / 114272 | training loss: 0.003940076567232609\n",
      "epoch: 3 | 109536 / 114272 | training loss: 0.1590043008327484\n",
      "epoch: 3 | 109568 / 114272 | training loss: 0.0022089448757469654\n",
      "epoch: 3 | 109600 / 114272 | training loss: 0.002617784310132265\n",
      "epoch: 3 | 109632 / 114272 | training loss: 0.004207842517644167\n",
      "epoch: 3 | 109664 / 114272 | training loss: 0.0028704276774078608\n",
      "epoch: 3 | 109696 / 114272 | training loss: 0.12545572221279144\n",
      "epoch: 3 | 109728 / 114272 | training loss: 0.09601964056491852\n",
      "epoch: 3 | 109760 / 114272 | training loss: 0.0022960815113037825\n",
      "epoch: 3 | 109792 / 114272 | training loss: 0.0018022144213318825\n",
      "epoch: 3 | 109824 / 114272 | training loss: 0.0038137463852763176\n",
      "epoch: 3 | 109856 / 114272 | training loss: 0.08484279364347458\n",
      "epoch: 3 | 109888 / 114272 | training loss: 0.1839822381734848\n",
      "epoch: 3 | 109920 / 114272 | training loss: 0.1663176268339157\n",
      "epoch: 3 | 109952 / 114272 | training loss: 0.0754171833395958\n",
      "epoch: 3 | 109984 / 114272 | training loss: 0.004048856440931559\n",
      "epoch: 3 | 110016 / 114272 | training loss: 0.005218931939452887\n",
      "epoch: 3 | 110048 / 114272 | training loss: 0.005225139204412699\n",
      "epoch: 3 | 110080 / 114272 | training loss: 0.060929324477910995\n",
      "epoch: 3 | 110112 / 114272 | training loss: 0.07763610780239105\n",
      "epoch: 3 | 110144 / 114272 | training loss: 0.009500839747488499\n",
      "epoch: 3 | 110176 / 114272 | training loss: 0.0442107655107975\n",
      "epoch: 3 | 110208 / 114272 | training loss: 0.0032630835194140673\n",
      "epoch: 3 | 110240 / 114272 | training loss: 0.007473546080291271\n",
      "epoch: 3 | 110272 / 114272 | training loss: 0.005841368343681097\n",
      "epoch: 3 | 110304 / 114272 | training loss: 0.057850390672683716\n",
      "epoch: 3 | 110336 / 114272 | training loss: 0.0048395562916994095\n",
      "epoch: 3 | 110368 / 114272 | training loss: 0.12103879451751709\n",
      "epoch: 3 | 110400 / 114272 | training loss: 0.007980822585523129\n",
      "epoch: 3 | 110432 / 114272 | training loss: 0.007471291348338127\n",
      "epoch: 3 | 110464 / 114272 | training loss: 0.006824706215411425\n",
      "epoch: 3 | 110496 / 114272 | training loss: 0.07672733813524246\n",
      "epoch: 3 | 110528 / 114272 | training loss: 0.00930190458893776\n",
      "epoch: 3 | 110560 / 114272 | training loss: 0.08519390970468521\n",
      "epoch: 3 | 110592 / 114272 | training loss: 0.2917971909046173\n",
      "epoch: 3 | 110624 / 114272 | training loss: 0.02464263327419758\n",
      "epoch: 3 | 110656 / 114272 | training loss: 0.02992994338274002\n",
      "epoch: 3 | 110688 / 114272 | training loss: 0.07628776878118515\n",
      "epoch: 3 | 110720 / 114272 | training loss: 0.13059541583061218\n",
      "epoch: 3 | 110752 / 114272 | training loss: 0.2382708042860031\n",
      "epoch: 3 | 110784 / 114272 | training loss: 0.3222236931324005\n",
      "epoch: 3 | 110816 / 114272 | training loss: 0.0018185260705649853\n",
      "epoch: 3 | 110848 / 114272 | training loss: 0.002861239481717348\n",
      "epoch: 3 | 110880 / 114272 | training loss: 0.002201196039095521\n",
      "epoch: 3 | 110912 / 114272 | training loss: 0.33483824133872986\n",
      "epoch: 3 | 110944 / 114272 | training loss: 0.004308649338781834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | 110976 / 114272 | training loss: 0.01332180853933096\n",
      "epoch: 3 | 111008 / 114272 | training loss: 0.007991468533873558\n",
      "epoch: 3 | 111040 / 114272 | training loss: 0.05542789399623871\n",
      "epoch: 3 | 111072 / 114272 | training loss: 0.211501806974411\n",
      "epoch: 3 | 111104 / 114272 | training loss: 0.12355615198612213\n",
      "epoch: 3 | 111136 / 114272 | training loss: 0.0014495051000267267\n",
      "epoch: 3 | 111168 / 114272 | training loss: 0.004921504762023687\n",
      "epoch: 3 | 111200 / 114272 | training loss: 0.11520805954933167\n",
      "epoch: 3 | 111232 / 114272 | training loss: 0.002368169603869319\n",
      "epoch: 3 | 111264 / 114272 | training loss: 0.013746349141001701\n",
      "epoch: 3 | 111296 / 114272 | training loss: 0.15677417814731598\n",
      "epoch: 3 | 111328 / 114272 | training loss: 0.12132205814123154\n",
      "epoch: 3 | 111360 / 114272 | training loss: 0.1491580307483673\n",
      "epoch: 3 | 111392 / 114272 | training loss: 0.2931843101978302\n",
      "epoch: 3 | 111424 / 114272 | training loss: 0.0033174892887473106\n",
      "epoch: 3 | 111456 / 114272 | training loss: 0.036172054708004\n",
      "epoch: 3 | 111488 / 114272 | training loss: 0.06609348952770233\n",
      "epoch: 3 | 111520 / 114272 | training loss: 0.22591114044189453\n",
      "epoch: 3 | 111552 / 114272 | training loss: 0.17726121842861176\n",
      "epoch: 3 | 111584 / 114272 | training loss: 0.24968992173671722\n",
      "epoch: 3 | 111616 / 114272 | training loss: 0.01687525026500225\n",
      "epoch: 3 | 111648 / 114272 | training loss: 0.00392856914550066\n",
      "epoch: 3 | 111680 / 114272 | training loss: 0.05450788140296936\n",
      "epoch: 3 | 111712 / 114272 | training loss: 0.0016963968519121408\n",
      "epoch: 3 | 111744 / 114272 | training loss: 0.013459579087793827\n",
      "epoch: 3 | 111776 / 114272 | training loss: 0.14773200452327728\n",
      "epoch: 3 | 111808 / 114272 | training loss: 0.05532197654247284\n",
      "epoch: 3 | 111840 / 114272 | training loss: 0.012164494022727013\n",
      "epoch: 3 | 111872 / 114272 | training loss: 0.11401861906051636\n",
      "epoch: 3 | 111904 / 114272 | training loss: 0.16547031700611115\n",
      "epoch: 3 | 111936 / 114272 | training loss: 0.026932096108794212\n",
      "epoch: 3 | 111968 / 114272 | training loss: 0.043517112731933594\n",
      "epoch: 3 | 112000 / 114272 | training loss: 0.010516684502363205\n",
      "epoch: 3 | 112032 / 114272 | training loss: 0.003313508117571473\n",
      "epoch: 3 | 112064 / 114272 | training loss: 0.006846704054623842\n",
      "epoch: 3 | 112096 / 114272 | training loss: 0.0032008166890591383\n",
      "epoch: 3 | 112128 / 114272 | training loss: 0.004828290548175573\n",
      "epoch: 3 | 112160 / 114272 | training loss: 0.1220022439956665\n",
      "epoch: 3 | 112192 / 114272 | training loss: 0.009385515004396439\n",
      "epoch: 3 | 112224 / 114272 | training loss: 0.14341765642166138\n",
      "epoch: 3 | 112256 / 114272 | training loss: 0.004023292101919651\n",
      "epoch: 3 | 112288 / 114272 | training loss: 0.0038843692746013403\n",
      "epoch: 3 | 112320 / 114272 | training loss: 0.005419784225523472\n",
      "epoch: 3 | 112352 / 114272 | training loss: 0.009476463310420513\n",
      "epoch: 3 | 112384 / 114272 | training loss: 0.004535126034170389\n",
      "epoch: 3 | 112416 / 114272 | training loss: 0.015823669731616974\n",
      "epoch: 3 | 112448 / 114272 | training loss: 0.09051646292209625\n",
      "epoch: 3 | 112480 / 114272 | training loss: 0.006740911863744259\n",
      "epoch: 3 | 112512 / 114272 | training loss: 0.0031564421951770782\n",
      "epoch: 3 | 112544 / 114272 | training loss: 0.003772701835259795\n",
      "epoch: 3 | 112576 / 114272 | training loss: 0.0972292423248291\n",
      "epoch: 3 | 112608 / 114272 | training loss: 0.002096629235893488\n",
      "epoch: 3 | 112640 / 114272 | training loss: 0.004777827300131321\n",
      "epoch: 3 | 112672 / 114272 | training loss: 0.04399895295500755\n",
      "epoch: 3 | 112704 / 114272 | training loss: 0.003764556720852852\n",
      "epoch: 3 | 112736 / 114272 | training loss: 0.19598427414894104\n",
      "epoch: 3 | 112768 / 114272 | training loss: 0.009242944419384003\n",
      "epoch: 3 | 112800 / 114272 | training loss: 0.009162753820419312\n",
      "epoch: 3 | 112832 / 114272 | training loss: 0.006940846331417561\n",
      "epoch: 3 | 112864 / 114272 | training loss: 0.005700199399143457\n",
      "epoch: 3 | 112896 / 114272 | training loss: 0.0025018511805683374\n",
      "epoch: 3 | 112928 / 114272 | training loss: 0.0860598161816597\n",
      "epoch: 3 | 112960 / 114272 | training loss: 0.003246214473620057\n",
      "epoch: 3 | 112992 / 114272 | training loss: 0.0033662901259958744\n",
      "epoch: 3 | 113024 / 114272 | training loss: 0.01810598559677601\n",
      "epoch: 3 | 113056 / 114272 | training loss: 0.0012814358342438936\n",
      "epoch: 3 | 113088 / 114272 | training loss: 0.0009716008207760751\n",
      "epoch: 3 | 113120 / 114272 | training loss: 0.15067839622497559\n",
      "epoch: 3 | 113152 / 114272 | training loss: 0.004475702531635761\n",
      "epoch: 3 | 113184 / 114272 | training loss: 0.0026258095167577267\n",
      "epoch: 3 | 113216 / 114272 | training loss: 0.004888993687927723\n",
      "epoch: 3 | 113248 / 114272 | training loss: 0.003559904405847192\n",
      "epoch: 3 | 113280 / 114272 | training loss: 0.10370074212551117\n",
      "epoch: 3 | 113312 / 114272 | training loss: 0.028582017868757248\n",
      "epoch: 3 | 113344 / 114272 | training loss: 0.1390998363494873\n",
      "epoch: 3 | 113376 / 114272 | training loss: 0.10405480861663818\n",
      "epoch: 3 | 113408 / 114272 | training loss: 0.1048838421702385\n",
      "epoch: 3 | 113440 / 114272 | training loss: 0.001304206089116633\n",
      "epoch: 3 | 113472 / 114272 | training loss: 0.002183417323976755\n",
      "epoch: 3 | 113504 / 114272 | training loss: 0.13414020836353302\n",
      "epoch: 3 | 113536 / 114272 | training loss: 0.08530617505311966\n",
      "epoch: 3 | 113568 / 114272 | training loss: 0.0018295884365215898\n",
      "epoch: 3 | 113600 / 114272 | training loss: 0.02536194957792759\n",
      "epoch: 3 | 113632 / 114272 | training loss: 0.20129650831222534\n",
      "epoch: 3 | 113664 / 114272 | training loss: 0.2024405300617218\n",
      "epoch: 3 | 113696 / 114272 | training loss: 0.21109072864055634\n",
      "epoch: 3 | 113728 / 114272 | training loss: 0.003249610774219036\n",
      "epoch: 3 | 113760 / 114272 | training loss: 0.1783730536699295\n",
      "epoch: 3 | 113792 / 114272 | training loss: 0.013106303289532661\n",
      "epoch: 3 | 113824 / 114272 | training loss: 0.06815481185913086\n",
      "epoch: 3 | 113856 / 114272 | training loss: 0.15455377101898193\n",
      "epoch: 3 | 113888 / 114272 | training loss: 0.08933839201927185\n",
      "epoch: 3 | 113920 / 114272 | training loss: 0.029175998643040657\n",
      "epoch: 3 | 113952 / 114272 | training loss: 0.03379178047180176\n",
      "epoch: 3 | 113984 / 114272 | training loss: 0.09690548479557037\n",
      "epoch: 3 | 114016 / 114272 | training loss: 0.18457405269145966\n",
      "epoch: 3 | 114048 / 114272 | training loss: 0.00831780955195427\n",
      "epoch: 3 | 114080 / 114272 | training loss: 0.002209817059338093\n",
      "epoch: 3 | 114112 / 114272 | training loss: 0.2549566328525543\n",
      "epoch: 3 | 114144 / 114272 | training loss: 0.12480171769857407\n",
      "epoch: 3 | 114176 / 114272 | training loss: 0.05264433100819588\n",
      "epoch: 3 | 114208 / 114272 | training loss: 0.16650258004665375\n",
      "epoch: 3 | 114240 / 114272 | training loss: 0.009218661114573479\n",
      "Training epoch 3 done! Average loss: 0.06365233494485688. Accuracy: 0.9818065667880147\n",
      "Validation epoch 3 done! Average loss: 0.1863551308644177. Accurage: 0.9541387024608501\n",
      "Epoch 5 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 4 | 0 / 114272 | training loss: 0.18288438022136688\n",
      "epoch: 4 | 32 / 114272 | training loss: 0.006531263701617718\n",
      "epoch: 4 | 64 / 114272 | training loss: 0.025573691353201866\n",
      "epoch: 4 | 96 / 114272 | training loss: 0.016838297247886658\n",
      "epoch: 4 | 128 / 114272 | training loss: 0.15650714933872223\n",
      "epoch: 4 | 160 / 114272 | training loss: 0.09199409931898117\n",
      "epoch: 4 | 192 / 114272 | training loss: 0.0035037300549447536\n",
      "epoch: 4 | 224 / 114272 | training loss: 0.00802632700651884\n",
      "epoch: 4 | 256 / 114272 | training loss: 0.011467743664979935\n",
      "epoch: 4 | 288 / 114272 | training loss: 0.1255963295698166\n",
      "epoch: 4 | 320 / 114272 | training loss: 0.0034365118481218815\n",
      "epoch: 4 | 352 / 114272 | training loss: 0.10130094736814499\n",
      "epoch: 4 | 384 / 114272 | training loss: 0.003580700373277068\n",
      "epoch: 4 | 416 / 114272 | training loss: 0.011485131457448006\n",
      "epoch: 4 | 448 / 114272 | training loss: 0.011046321131289005\n",
      "epoch: 4 | 480 / 114272 | training loss: 0.0056205028668046\n",
      "epoch: 4 | 512 / 114272 | training loss: 0.25053393840789795\n",
      "epoch: 4 | 544 / 114272 | training loss: 0.024804305285215378\n",
      "epoch: 4 | 576 / 114272 | training loss: 0.23578238487243652\n",
      "epoch: 4 | 608 / 114272 | training loss: 0.006964834872633219\n",
      "epoch: 4 | 640 / 114272 | training loss: 0.003289418760687113\n",
      "epoch: 4 | 672 / 114272 | training loss: 0.008379212580621243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 704 / 114272 | training loss: 0.12471254914999008\n",
      "epoch: 4 | 736 / 114272 | training loss: 0.1073155403137207\n",
      "epoch: 4 | 768 / 114272 | training loss: 0.05377857759594917\n",
      "epoch: 4 | 800 / 114272 | training loss: 0.07848893105983734\n",
      "epoch: 4 | 832 / 114272 | training loss: 0.09610146284103394\n",
      "epoch: 4 | 864 / 114272 | training loss: 0.17753159999847412\n",
      "epoch: 4 | 896 / 114272 | training loss: 0.004488343372941017\n",
      "epoch: 4 | 928 / 114272 | training loss: 0.07096616923809052\n",
      "epoch: 4 | 960 / 114272 | training loss: 0.0044105080887675285\n",
      "epoch: 4 | 992 / 114272 | training loss: 0.06528913974761963\n",
      "epoch: 4 | 1024 / 114272 | training loss: 0.07044873386621475\n",
      "epoch: 4 | 1056 / 114272 | training loss: 0.005930036772042513\n",
      "epoch: 4 | 1088 / 114272 | training loss: 0.04248441383242607\n",
      "epoch: 4 | 1120 / 114272 | training loss: 0.0065359449945390224\n",
      "epoch: 4 | 1152 / 114272 | training loss: 0.005893027409911156\n",
      "epoch: 4 | 1184 / 114272 | training loss: 0.006991243455559015\n",
      "epoch: 4 | 1216 / 114272 | training loss: 0.005118969827890396\n",
      "epoch: 4 | 1248 / 114272 | training loss: 0.0033372952602803707\n",
      "epoch: 4 | 1280 / 114272 | training loss: 0.0033209219109266996\n",
      "epoch: 4 | 1312 / 114272 | training loss: 0.00485959043726325\n",
      "epoch: 4 | 1344 / 114272 | training loss: 0.0025873249396681786\n",
      "epoch: 4 | 1376 / 114272 | training loss: 0.009206046350300312\n",
      "epoch: 4 | 1408 / 114272 | training loss: 0.08257760852575302\n",
      "epoch: 4 | 1440 / 114272 | training loss: 0.13430048525333405\n",
      "epoch: 4 | 1472 / 114272 | training loss: 0.0375606045126915\n",
      "epoch: 4 | 1504 / 114272 | training loss: 0.006512265652418137\n",
      "epoch: 4 | 1536 / 114272 | training loss: 0.0042711710557341576\n",
      "epoch: 4 | 1568 / 114272 | training loss: 0.005504964850842953\n",
      "epoch: 4 | 1600 / 114272 | training loss: 0.029906481504440308\n",
      "epoch: 4 | 1632 / 114272 | training loss: 0.003654455067589879\n",
      "epoch: 4 | 1664 / 114272 | training loss: 0.0027571292594075203\n",
      "epoch: 4 | 1696 / 114272 | training loss: 0.030167924240231514\n",
      "epoch: 4 | 1728 / 114272 | training loss: 0.004193295259028673\n",
      "epoch: 4 | 1760 / 114272 | training loss: 0.2661686837673187\n",
      "epoch: 4 | 1792 / 114272 | training loss: 0.001718965359032154\n",
      "epoch: 4 | 1824 / 114272 | training loss: 0.0013709795894101262\n",
      "epoch: 4 | 1856 / 114272 | training loss: 0.07998713850975037\n",
      "epoch: 4 | 1888 / 114272 | training loss: 0.00485912524163723\n",
      "epoch: 4 | 1920 / 114272 | training loss: 0.25816085934638977\n",
      "epoch: 4 | 1952 / 114272 | training loss: 0.035869769752025604\n",
      "epoch: 4 | 1984 / 114272 | training loss: 0.0025348104536533356\n",
      "epoch: 4 | 2016 / 114272 | training loss: 0.06559667736291885\n",
      "epoch: 4 | 2048 / 114272 | training loss: 0.0015899399295449257\n",
      "epoch: 4 | 2080 / 114272 | training loss: 0.0025871475227177143\n",
      "epoch: 4 | 2112 / 114272 | training loss: 0.23782433569431305\n",
      "epoch: 4 | 2144 / 114272 | training loss: 0.16394630074501038\n",
      "epoch: 4 | 2176 / 114272 | training loss: 0.16808854043483734\n",
      "epoch: 4 | 2208 / 114272 | training loss: 0.01294072438031435\n",
      "epoch: 4 | 2240 / 114272 | training loss: 0.006131836213171482\n",
      "epoch: 4 | 2272 / 114272 | training loss: 0.004009757190942764\n",
      "epoch: 4 | 2304 / 114272 | training loss: 0.03277287632226944\n",
      "epoch: 4 | 2336 / 114272 | training loss: 0.05268988758325577\n",
      "epoch: 4 | 2368 / 114272 | training loss: 0.003094952553510666\n",
      "epoch: 4 | 2400 / 114272 | training loss: 0.04410060867667198\n",
      "epoch: 4 | 2432 / 114272 | training loss: 0.0035149112809449434\n",
      "epoch: 4 | 2464 / 114272 | training loss: 0.0022133386228233576\n",
      "epoch: 4 | 2496 / 114272 | training loss: 0.012947913259267807\n",
      "epoch: 4 | 2528 / 114272 | training loss: 0.08994200080633163\n",
      "epoch: 4 | 2560 / 114272 | training loss: 0.0017523340648040175\n",
      "epoch: 4 | 2592 / 114272 | training loss: 0.003269889857620001\n",
      "epoch: 4 | 2624 / 114272 | training loss: 0.007625878322869539\n",
      "epoch: 4 | 2656 / 114272 | training loss: 0.003456912934780121\n",
      "epoch: 4 | 2688 / 114272 | training loss: 0.004766157362610102\n",
      "epoch: 4 | 2720 / 114272 | training loss: 0.0038327588699758053\n",
      "epoch: 4 | 2752 / 114272 | training loss: 0.009504183195531368\n",
      "epoch: 4 | 2784 / 114272 | training loss: 0.0015912401722744107\n",
      "epoch: 4 | 2816 / 114272 | training loss: 0.0018604721408337355\n",
      "epoch: 4 | 2848 / 114272 | training loss: 0.06511545926332474\n",
      "epoch: 4 | 2880 / 114272 | training loss: 0.05537630617618561\n",
      "epoch: 4 | 2912 / 114272 | training loss: 0.2807016372680664\n",
      "epoch: 4 | 2944 / 114272 | training loss: 0.006324972957372665\n",
      "epoch: 4 | 2976 / 114272 | training loss: 0.0028646879363805056\n",
      "epoch: 4 | 3008 / 114272 | training loss: 0.003107366617769003\n",
      "epoch: 4 | 3040 / 114272 | training loss: 0.004316625650972128\n",
      "epoch: 4 | 3072 / 114272 | training loss: 0.0019295213278383017\n",
      "epoch: 4 | 3104 / 114272 | training loss: 0.002102788770571351\n",
      "epoch: 4 | 3136 / 114272 | training loss: 0.001853810390457511\n",
      "epoch: 4 | 3168 / 114272 | training loss: 0.0033049024641513824\n",
      "epoch: 4 | 3200 / 114272 | training loss: 0.20593193173408508\n",
      "epoch: 4 | 3232 / 114272 | training loss: 0.0018128528026863933\n",
      "epoch: 4 | 3264 / 114272 | training loss: 0.00664212740957737\n",
      "epoch: 4 | 3296 / 114272 | training loss: 0.002235631225630641\n",
      "epoch: 4 | 3328 / 114272 | training loss: 0.011748616583645344\n",
      "epoch: 4 | 3360 / 114272 | training loss: 0.0026786988601088524\n",
      "epoch: 4 | 3392 / 114272 | training loss: 0.021098563447594643\n",
      "epoch: 4 | 3424 / 114272 | training loss: 0.0020005456171929836\n",
      "epoch: 4 | 3456 / 114272 | training loss: 0.001544724334962666\n",
      "epoch: 4 | 3488 / 114272 | training loss: 0.2142692655324936\n",
      "epoch: 4 | 3520 / 114272 | training loss: 0.00415240740403533\n",
      "epoch: 4 | 3552 / 114272 | training loss: 0.00538475951179862\n",
      "epoch: 4 | 3584 / 114272 | training loss: 0.00312976841814816\n",
      "epoch: 4 | 3616 / 114272 | training loss: 0.04037526994943619\n",
      "epoch: 4 | 3648 / 114272 | training loss: 0.17151936888694763\n",
      "epoch: 4 | 3680 / 114272 | training loss: 0.005453964229673147\n",
      "epoch: 4 | 3712 / 114272 | training loss: 0.002125474624335766\n",
      "epoch: 4 | 3744 / 114272 | training loss: 0.002258514054119587\n",
      "epoch: 4 | 3776 / 114272 | training loss: 0.009135441854596138\n",
      "epoch: 4 | 3808 / 114272 | training loss: 0.050329625606536865\n",
      "epoch: 4 | 3840 / 114272 | training loss: 0.02656976506114006\n",
      "epoch: 4 | 3872 / 114272 | training loss: 0.04856865853071213\n",
      "epoch: 4 | 3904 / 114272 | training loss: 0.007211850956082344\n",
      "epoch: 4 | 3936 / 114272 | training loss: 0.1668388843536377\n",
      "epoch: 4 | 3968 / 114272 | training loss: 0.0022364857140928507\n",
      "epoch: 4 | 4000 / 114272 | training loss: 0.002402304671704769\n",
      "epoch: 4 | 4032 / 114272 | training loss: 0.0025317082181572914\n",
      "epoch: 4 | 4064 / 114272 | training loss: 0.0031725256703794003\n",
      "epoch: 4 | 4096 / 114272 | training loss: 0.003731917357072234\n",
      "epoch: 4 | 4128 / 114272 | training loss: 0.01365953404456377\n",
      "epoch: 4 | 4160 / 114272 | training loss: 0.3250216245651245\n",
      "epoch: 4 | 4192 / 114272 | training loss: 0.0027928431518375874\n",
      "epoch: 4 | 4224 / 114272 | training loss: 0.3880598843097687\n",
      "epoch: 4 | 4256 / 114272 | training loss: 0.18174394965171814\n",
      "epoch: 4 | 4288 / 114272 | training loss: 0.003745702328160405\n",
      "epoch: 4 | 4320 / 114272 | training loss: 0.0020430334843695164\n",
      "epoch: 4 | 4352 / 114272 | training loss: 0.0034148283302783966\n",
      "epoch: 4 | 4384 / 114272 | training loss: 0.1601620614528656\n",
      "epoch: 4 | 4416 / 114272 | training loss: 0.0014467485016211867\n",
      "epoch: 4 | 4448 / 114272 | training loss: 0.0026113709900528193\n",
      "epoch: 4 | 4480 / 114272 | training loss: 0.03387590870261192\n",
      "epoch: 4 | 4512 / 114272 | training loss: 0.23736242949962616\n",
      "epoch: 4 | 4544 / 114272 | training loss: 0.00397077202796936\n",
      "epoch: 4 | 4576 / 114272 | training loss: 0.0034250211901962757\n",
      "epoch: 4 | 4608 / 114272 | training loss: 0.002457480411976576\n",
      "epoch: 4 | 4640 / 114272 | training loss: 0.004719930235296488\n",
      "epoch: 4 | 4672 / 114272 | training loss: 0.0046977149322628975\n",
      "epoch: 4 | 4704 / 114272 | training loss: 0.15520045161247253\n",
      "epoch: 4 | 4736 / 114272 | training loss: 0.0036997355055063963\n",
      "epoch: 4 | 4768 / 114272 | training loss: 0.08883416652679443\n",
      "epoch: 4 | 4800 / 114272 | training loss: 0.003552402602508664\n",
      "epoch: 4 | 4832 / 114272 | training loss: 0.0022795521654188633\n",
      "epoch: 4 | 4864 / 114272 | training loss: 0.0026955532375723124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 4896 / 114272 | training loss: 0.11813677847385406\n",
      "epoch: 4 | 4928 / 114272 | training loss: 0.0034551965072751045\n",
      "epoch: 4 | 4960 / 114272 | training loss: 0.004772936459630728\n",
      "epoch: 4 | 4992 / 114272 | training loss: 0.0026112068444490433\n",
      "epoch: 4 | 5024 / 114272 | training loss: 0.0032317645382136106\n",
      "epoch: 4 | 5056 / 114272 | training loss: 0.006414387375116348\n",
      "epoch: 4 | 5088 / 114272 | training loss: 0.004551855381578207\n",
      "epoch: 4 | 5120 / 114272 | training loss: 0.003975020721554756\n",
      "epoch: 4 | 5152 / 114272 | training loss: 0.006451270543038845\n",
      "epoch: 4 | 5184 / 114272 | training loss: 0.0029226047918200493\n",
      "epoch: 4 | 5216 / 114272 | training loss: 0.0821915715932846\n",
      "epoch: 4 | 5248 / 114272 | training loss: 0.002934049116447568\n",
      "epoch: 4 | 5280 / 114272 | training loss: 0.00888119637966156\n",
      "epoch: 4 | 5312 / 114272 | training loss: 0.0018221966456621885\n",
      "epoch: 4 | 5344 / 114272 | training loss: 0.0029317999724298716\n",
      "epoch: 4 | 5376 / 114272 | training loss: 0.0709708109498024\n",
      "epoch: 4 | 5408 / 114272 | training loss: 0.0046582636423408985\n",
      "epoch: 4 | 5440 / 114272 | training loss: 0.1179661974310875\n",
      "epoch: 4 | 5472 / 114272 | training loss: 0.0016265955055132508\n",
      "epoch: 4 | 5504 / 114272 | training loss: 0.0024295311886817217\n",
      "epoch: 4 | 5536 / 114272 | training loss: 0.002013806952163577\n",
      "epoch: 4 | 5568 / 114272 | training loss: 0.040277428925037384\n",
      "epoch: 4 | 5600 / 114272 | training loss: 0.004048333037644625\n",
      "epoch: 4 | 5632 / 114272 | training loss: 0.0045099458657205105\n",
      "epoch: 4 | 5664 / 114272 | training loss: 0.0024796200450509787\n",
      "epoch: 4 | 5696 / 114272 | training loss: 0.0019063056679442525\n",
      "epoch: 4 | 5728 / 114272 | training loss: 0.0028439753223210573\n",
      "epoch: 4 | 5760 / 114272 | training loss: 0.0019438236486166716\n",
      "epoch: 4 | 5792 / 114272 | training loss: 0.0012332592159509659\n",
      "epoch: 4 | 5824 / 114272 | training loss: 0.1252196580171585\n",
      "epoch: 4 | 5856 / 114272 | training loss: 0.16441859304904938\n",
      "epoch: 4 | 5888 / 114272 | training loss: 0.20476007461547852\n",
      "epoch: 4 | 5920 / 114272 | training loss: 0.001650006859563291\n",
      "epoch: 4 | 5952 / 114272 | training loss: 0.0024112905375659466\n",
      "epoch: 4 | 5984 / 114272 | training loss: 0.05812625586986542\n",
      "epoch: 4 | 6016 / 114272 | training loss: 0.0023167163599282503\n",
      "epoch: 4 | 6048 / 114272 | training loss: 0.06911051273345947\n",
      "epoch: 4 | 6080 / 114272 | training loss: 0.002504160162061453\n",
      "epoch: 4 | 6112 / 114272 | training loss: 0.002406596438959241\n",
      "epoch: 4 | 6144 / 114272 | training loss: 0.0040014986880123615\n",
      "epoch: 4 | 6176 / 114272 | training loss: 0.09092628210783005\n",
      "epoch: 4 | 6208 / 114272 | training loss: 0.0007815673598088324\n",
      "epoch: 4 | 6240 / 114272 | training loss: 0.006259045097976923\n",
      "epoch: 4 | 6272 / 114272 | training loss: 0.12677696347236633\n",
      "epoch: 4 | 6304 / 114272 | training loss: 0.004083855543285608\n",
      "epoch: 4 | 6336 / 114272 | training loss: 0.1907632201910019\n",
      "epoch: 4 | 6368 / 114272 | training loss: 0.0014970217598602176\n",
      "epoch: 4 | 6400 / 114272 | training loss: 0.0617215558886528\n",
      "epoch: 4 | 6432 / 114272 | training loss: 0.004885990172624588\n",
      "epoch: 4 | 6464 / 114272 | training loss: 0.0017889078008010983\n",
      "epoch: 4 | 6496 / 114272 | training loss: 0.11547542363405228\n",
      "epoch: 4 | 6528 / 114272 | training loss: 0.002056962577626109\n",
      "epoch: 4 | 6560 / 114272 | training loss: 0.0033742827363312244\n",
      "epoch: 4 | 6592 / 114272 | training loss: 0.0018085858318954706\n",
      "epoch: 4 | 6624 / 114272 | training loss: 0.0015143903438001871\n",
      "epoch: 4 | 6656 / 114272 | training loss: 0.0014934036880731583\n",
      "epoch: 4 | 6688 / 114272 | training loss: 0.13103890419006348\n",
      "epoch: 4 | 6720 / 114272 | training loss: 0.09136375784873962\n",
      "epoch: 4 | 6752 / 114272 | training loss: 0.001104936352930963\n",
      "epoch: 4 | 6784 / 114272 | training loss: 0.004780200310051441\n",
      "epoch: 4 | 6816 / 114272 | training loss: 0.001422010245732963\n",
      "epoch: 4 | 6848 / 114272 | training loss: 0.004965584725141525\n",
      "epoch: 4 | 6880 / 114272 | training loss: 0.1454775184392929\n",
      "epoch: 4 | 6912 / 114272 | training loss: 0.0014796257019042969\n",
      "epoch: 4 | 6944 / 114272 | training loss: 0.008789616636931896\n",
      "epoch: 4 | 6976 / 114272 | training loss: 0.159647598862648\n",
      "epoch: 4 | 7008 / 114272 | training loss: 0.061038076877593994\n",
      "epoch: 4 | 7040 / 114272 | training loss: 0.0017973714275285602\n",
      "epoch: 4 | 7072 / 114272 | training loss: 0.05181920528411865\n",
      "epoch: 4 | 7104 / 114272 | training loss: 0.0015896038385108113\n",
      "epoch: 4 | 7136 / 114272 | training loss: 0.0025517747271806\n",
      "epoch: 4 | 7168 / 114272 | training loss: 0.006433842703700066\n",
      "epoch: 4 | 7200 / 114272 | training loss: 0.002940072678029537\n",
      "epoch: 4 | 7232 / 114272 | training loss: 0.048032332211732864\n",
      "epoch: 4 | 7264 / 114272 | training loss: 0.10550196468830109\n",
      "epoch: 4 | 7296 / 114272 | training loss: 0.002188835060223937\n",
      "epoch: 4 | 7328 / 114272 | training loss: 0.002338102553039789\n",
      "epoch: 4 | 7360 / 114272 | training loss: 0.18606771528720856\n",
      "epoch: 4 | 7392 / 114272 | training loss: 0.15438979864120483\n",
      "epoch: 4 | 7424 / 114272 | training loss: 0.21710604429244995\n",
      "epoch: 4 | 7456 / 114272 | training loss: 0.0012185800587758422\n",
      "epoch: 4 | 7488 / 114272 | training loss: 0.01506839133799076\n",
      "epoch: 4 | 7520 / 114272 | training loss: 0.03845307230949402\n",
      "epoch: 4 | 7552 / 114272 | training loss: 0.0035761315375566483\n",
      "epoch: 4 | 7584 / 114272 | training loss: 0.14263489842414856\n",
      "epoch: 4 | 7616 / 114272 | training loss: 0.0017338611651211977\n",
      "epoch: 4 | 7648 / 114272 | training loss: 0.08353307843208313\n",
      "epoch: 4 | 7680 / 114272 | training loss: 0.0026971050538122654\n",
      "epoch: 4 | 7712 / 114272 | training loss: 0.1554749757051468\n",
      "epoch: 4 | 7744 / 114272 | training loss: 0.0781383141875267\n",
      "epoch: 4 | 7776 / 114272 | training loss: 0.23822391033172607\n",
      "epoch: 4 | 7808 / 114272 | training loss: 0.0310280192643404\n",
      "epoch: 4 | 7840 / 114272 | training loss: 0.02562708780169487\n",
      "epoch: 4 | 7872 / 114272 | training loss: 0.007085046730935574\n",
      "epoch: 4 | 7904 / 114272 | training loss: 0.005254112649708986\n",
      "epoch: 4 | 7936 / 114272 | training loss: 0.0065040988847613335\n",
      "epoch: 4 | 7968 / 114272 | training loss: 0.006167476065456867\n",
      "epoch: 4 | 8000 / 114272 | training loss: 0.09205260127782822\n",
      "epoch: 4 | 8032 / 114272 | training loss: 0.002543909475207329\n",
      "epoch: 4 | 8064 / 114272 | training loss: 0.007208637427538633\n",
      "epoch: 4 | 8096 / 114272 | training loss: 0.0040763188153505325\n",
      "epoch: 4 | 8128 / 114272 | training loss: 0.0602855309844017\n",
      "epoch: 4 | 8160 / 114272 | training loss: 0.0017903397092595696\n",
      "epoch: 4 | 8192 / 114272 | training loss: 0.13309599459171295\n",
      "epoch: 4 | 8224 / 114272 | training loss: 0.024190019816160202\n",
      "epoch: 4 | 8256 / 114272 | training loss: 0.156338170170784\n",
      "epoch: 4 | 8288 / 114272 | training loss: 0.004746975377202034\n",
      "epoch: 4 | 8320 / 114272 | training loss: 0.004523203242570162\n",
      "epoch: 4 | 8352 / 114272 | training loss: 0.11294812709093094\n",
      "epoch: 4 | 8384 / 114272 | training loss: 0.003947129473090172\n",
      "epoch: 4 | 8416 / 114272 | training loss: 0.006405672524124384\n",
      "epoch: 4 | 8448 / 114272 | training loss: 0.004486465826630592\n",
      "epoch: 4 | 8480 / 114272 | training loss: 0.008283477276563644\n",
      "epoch: 4 | 8512 / 114272 | training loss: 0.009106973186135292\n",
      "epoch: 4 | 8544 / 114272 | training loss: 0.005000753793865442\n",
      "epoch: 4 | 8576 / 114272 | training loss: 0.011037399992346764\n",
      "epoch: 4 | 8608 / 114272 | training loss: 0.10766325891017914\n",
      "epoch: 4 | 8640 / 114272 | training loss: 0.004133807495236397\n",
      "epoch: 4 | 8672 / 114272 | training loss: 0.00520770438015461\n",
      "epoch: 4 | 8704 / 114272 | training loss: 0.007654285524040461\n",
      "epoch: 4 | 8736 / 114272 | training loss: 0.001683662412688136\n",
      "epoch: 4 | 8768 / 114272 | training loss: 0.007142978720366955\n",
      "epoch: 4 | 8800 / 114272 | training loss: 0.0036711678840219975\n",
      "epoch: 4 | 8832 / 114272 | training loss: 0.0592079795897007\n",
      "epoch: 4 | 8864 / 114272 | training loss: 0.0030636340379714966\n",
      "epoch: 4 | 8896 / 114272 | training loss: 0.003637181594967842\n",
      "epoch: 4 | 8928 / 114272 | training loss: 0.0021556925494223833\n",
      "epoch: 4 | 8960 / 114272 | training loss: 0.005790360271930695\n",
      "epoch: 4 | 8992 / 114272 | training loss: 0.23525486886501312\n",
      "epoch: 4 | 9024 / 114272 | training loss: 0.005064809694886208\n",
      "epoch: 4 | 9056 / 114272 | training loss: 0.007615105248987675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 9088 / 114272 | training loss: 0.19563822448253632\n",
      "epoch: 4 | 9120 / 114272 | training loss: 0.3439362943172455\n",
      "epoch: 4 | 9152 / 114272 | training loss: 0.003045108634978533\n",
      "epoch: 4 | 9184 / 114272 | training loss: 0.027198078110814095\n",
      "epoch: 4 | 9216 / 114272 | training loss: 0.0011779546039178967\n",
      "epoch: 4 | 9248 / 114272 | training loss: 0.005206107161939144\n",
      "epoch: 4 | 9280 / 114272 | training loss: 0.0029083203990012407\n",
      "epoch: 4 | 9312 / 114272 | training loss: 0.231795072555542\n",
      "epoch: 4 | 9344 / 114272 | training loss: 0.04255262017250061\n",
      "epoch: 4 | 9376 / 114272 | training loss: 0.012718203477561474\n",
      "epoch: 4 | 9408 / 114272 | training loss: 0.09688308089971542\n",
      "epoch: 4 | 9440 / 114272 | training loss: 0.0021460221614688635\n",
      "epoch: 4 | 9472 / 114272 | training loss: 0.02196417935192585\n",
      "epoch: 4 | 9504 / 114272 | training loss: 0.007074167486280203\n",
      "epoch: 4 | 9536 / 114272 | training loss: 0.2528578042984009\n",
      "epoch: 4 | 9568 / 114272 | training loss: 0.23721066117286682\n",
      "epoch: 4 | 9600 / 114272 | training loss: 0.024747654795646667\n",
      "epoch: 4 | 9632 / 114272 | training loss: 0.01716400310397148\n",
      "epoch: 4 | 9664 / 114272 | training loss: 0.021977894008159637\n",
      "epoch: 4 | 9696 / 114272 | training loss: 0.012621023692190647\n",
      "epoch: 4 | 9728 / 114272 | training loss: 0.0022736769169569016\n",
      "epoch: 4 | 9760 / 114272 | training loss: 0.005072042346000671\n",
      "epoch: 4 | 9792 / 114272 | training loss: 0.0036814608611166477\n",
      "epoch: 4 | 9824 / 114272 | training loss: 0.003140456508845091\n",
      "epoch: 4 | 9856 / 114272 | training loss: 0.002572827273979783\n",
      "epoch: 4 | 9888 / 114272 | training loss: 0.00837306585162878\n",
      "epoch: 4 | 9920 / 114272 | training loss: 0.005145182833075523\n",
      "epoch: 4 | 9952 / 114272 | training loss: 0.004388651344925165\n",
      "epoch: 4 | 9984 / 114272 | training loss: 0.003087319666519761\n",
      "epoch: 4 | 10016 / 114272 | training loss: 0.0028295633383095264\n",
      "epoch: 4 | 10048 / 114272 | training loss: 0.00394878163933754\n",
      "epoch: 4 | 10080 / 114272 | training loss: 0.003448321484029293\n",
      "epoch: 4 | 10112 / 114272 | training loss: 0.003331694519147277\n",
      "epoch: 4 | 10144 / 114272 | training loss: 0.0045272186398506165\n",
      "epoch: 4 | 10176 / 114272 | training loss: 0.001967610092833638\n",
      "epoch: 4 | 10208 / 114272 | training loss: 0.022503281012177467\n",
      "epoch: 4 | 10240 / 114272 | training loss: 0.10542328655719757\n",
      "epoch: 4 | 10272 / 114272 | training loss: 0.24642428755760193\n",
      "epoch: 4 | 10304 / 114272 | training loss: 0.1403333991765976\n",
      "epoch: 4 | 10336 / 114272 | training loss: 0.0022191659081727266\n",
      "epoch: 4 | 10368 / 114272 | training loss: 0.007240544073283672\n",
      "epoch: 4 | 10400 / 114272 | training loss: 0.14452886581420898\n",
      "epoch: 4 | 10432 / 114272 | training loss: 0.0061902874149382114\n",
      "epoch: 4 | 10464 / 114272 | training loss: 0.0037691527977585793\n",
      "epoch: 4 | 10496 / 114272 | training loss: 0.005834756884723902\n",
      "epoch: 4 | 10528 / 114272 | training loss: 0.001223133411258459\n",
      "epoch: 4 | 10560 / 114272 | training loss: 0.003767892252653837\n",
      "epoch: 4 | 10592 / 114272 | training loss: 0.002768907230347395\n",
      "epoch: 4 | 10624 / 114272 | training loss: 0.002825984265655279\n",
      "epoch: 4 | 10656 / 114272 | training loss: 0.0029093539342284203\n",
      "epoch: 4 | 10688 / 114272 | training loss: 0.0018201987259089947\n",
      "epoch: 4 | 10720 / 114272 | training loss: 0.0043453434482216835\n",
      "epoch: 4 | 10752 / 114272 | training loss: 0.0022240332327783108\n",
      "epoch: 4 | 10784 / 114272 | training loss: 0.17014546692371368\n",
      "epoch: 4 | 10816 / 114272 | training loss: 0.0030975935515016317\n",
      "epoch: 4 | 10848 / 114272 | training loss: 0.0839541107416153\n",
      "epoch: 4 | 10880 / 114272 | training loss: 0.0024007782340049744\n",
      "epoch: 4 | 10912 / 114272 | training loss: 0.06392437219619751\n",
      "epoch: 4 | 10944 / 114272 | training loss: 0.0017756547313183546\n",
      "epoch: 4 | 10976 / 114272 | training loss: 0.0027261818759143353\n",
      "epoch: 4 | 11008 / 114272 | training loss: 0.003768421243876219\n",
      "epoch: 4 | 11040 / 114272 | training loss: 0.002602802822366357\n",
      "epoch: 4 | 11072 / 114272 | training loss: 0.0026422825176268816\n",
      "epoch: 4 | 11104 / 114272 | training loss: 0.2234085649251938\n",
      "epoch: 4 | 11136 / 114272 | training loss: 0.003602039534598589\n",
      "epoch: 4 | 11168 / 114272 | training loss: 0.005038158502429724\n",
      "epoch: 4 | 11200 / 114272 | training loss: 0.14817239344120026\n",
      "epoch: 4 | 11232 / 114272 | training loss: 0.004297696053981781\n",
      "epoch: 4 | 11264 / 114272 | training loss: 0.0034464735072106123\n",
      "epoch: 4 | 11296 / 114272 | training loss: 0.002667761407792568\n",
      "epoch: 4 | 11328 / 114272 | training loss: 0.002696361392736435\n",
      "epoch: 4 | 11360 / 114272 | training loss: 0.31050562858581543\n",
      "epoch: 4 | 11392 / 114272 | training loss: 0.15827548503875732\n",
      "epoch: 4 | 11424 / 114272 | training loss: 0.002315447898581624\n",
      "epoch: 4 | 11456 / 114272 | training loss: 0.07666097581386566\n",
      "epoch: 4 | 11488 / 114272 | training loss: 0.0060483659617602825\n",
      "epoch: 4 | 11520 / 114272 | training loss: 0.0019980526994913816\n",
      "epoch: 4 | 11552 / 114272 | training loss: 0.0036669804248958826\n",
      "epoch: 4 | 11584 / 114272 | training loss: 0.0018402172718197107\n",
      "epoch: 4 | 11616 / 114272 | training loss: 0.002040801104158163\n",
      "epoch: 4 | 11648 / 114272 | training loss: 0.0033371848985552788\n",
      "epoch: 4 | 11680 / 114272 | training loss: 0.04889014735817909\n",
      "epoch: 4 | 11712 / 114272 | training loss: 0.002479719230905175\n",
      "epoch: 4 | 11744 / 114272 | training loss: 0.20503944158554077\n",
      "epoch: 4 | 11776 / 114272 | training loss: 0.1230473592877388\n",
      "epoch: 4 | 11808 / 114272 | training loss: 0.0024780260864645243\n",
      "epoch: 4 | 11840 / 114272 | training loss: 0.05706140771508217\n",
      "epoch: 4 | 11872 / 114272 | training loss: 0.0019344683969393373\n",
      "epoch: 4 | 11904 / 114272 | training loss: 0.13489772379398346\n",
      "epoch: 4 | 11936 / 114272 | training loss: 0.10601654648780823\n",
      "epoch: 4 | 11968 / 114272 | training loss: 0.002285580150783062\n",
      "epoch: 4 | 12000 / 114272 | training loss: 0.00549705233424902\n",
      "epoch: 4 | 12032 / 114272 | training loss: 0.00457995617762208\n",
      "epoch: 4 | 12064 / 114272 | training loss: 0.01558852568268776\n",
      "epoch: 4 | 12096 / 114272 | training loss: 0.005249206442385912\n",
      "epoch: 4 | 12128 / 114272 | training loss: 0.00335295288823545\n",
      "epoch: 4 | 12160 / 114272 | training loss: 0.0009573508286848664\n",
      "epoch: 4 | 12192 / 114272 | training loss: 0.005750273820012808\n",
      "epoch: 4 | 12224 / 114272 | training loss: 0.00432919804006815\n",
      "epoch: 4 | 12256 / 114272 | training loss: 0.0052666449919342995\n",
      "epoch: 4 | 12288 / 114272 | training loss: 0.0017256771679967642\n",
      "epoch: 4 | 12320 / 114272 | training loss: 0.003636194858700037\n",
      "epoch: 4 | 12352 / 114272 | training loss: 0.00377760105766356\n",
      "epoch: 4 | 12384 / 114272 | training loss: 0.004697428550571203\n",
      "epoch: 4 | 12416 / 114272 | training loss: 0.16611100733280182\n",
      "epoch: 4 | 12448 / 114272 | training loss: 0.002254504943266511\n",
      "epoch: 4 | 12480 / 114272 | training loss: 0.008642778731882572\n",
      "epoch: 4 | 12512 / 114272 | training loss: 0.08880695700645447\n",
      "epoch: 4 | 12544 / 114272 | training loss: 0.004767828620970249\n",
      "epoch: 4 | 12576 / 114272 | training loss: 0.010459939017891884\n",
      "epoch: 4 | 12608 / 114272 | training loss: 0.005458962172269821\n",
      "epoch: 4 | 12640 / 114272 | training loss: 0.0027675768360495567\n",
      "epoch: 4 | 12672 / 114272 | training loss: 0.008452309295535088\n",
      "epoch: 4 | 12704 / 114272 | training loss: 0.24371610581874847\n",
      "epoch: 4 | 12736 / 114272 | training loss: 0.1903562694787979\n",
      "epoch: 4 | 12768 / 114272 | training loss: 0.0015890088398009539\n",
      "epoch: 4 | 12800 / 114272 | training loss: 0.05459827557206154\n",
      "epoch: 4 | 12832 / 114272 | training loss: 0.001046172110363841\n",
      "epoch: 4 | 12864 / 114272 | training loss: 0.15097932517528534\n",
      "epoch: 4 | 12896 / 114272 | training loss: 0.0024870657362043858\n",
      "epoch: 4 | 12928 / 114272 | training loss: 0.3537681996822357\n",
      "epoch: 4 | 12960 / 114272 | training loss: 0.0014980812557041645\n",
      "epoch: 4 | 12992 / 114272 | training loss: 0.01071220077574253\n",
      "epoch: 4 | 13024 / 114272 | training loss: 0.05391176417469978\n",
      "epoch: 4 | 13056 / 114272 | training loss: 0.005381822120398283\n",
      "epoch: 4 | 13088 / 114272 | training loss: 0.057473208755254745\n",
      "epoch: 4 | 13120 / 114272 | training loss: 0.0013452942948788404\n",
      "epoch: 4 | 13152 / 114272 | training loss: 0.31510037183761597\n",
      "epoch: 4 | 13184 / 114272 | training loss: 0.005853274371474981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 13216 / 114272 | training loss: 0.014586606994271278\n",
      "epoch: 4 | 13248 / 114272 | training loss: 0.0013797861756756902\n",
      "epoch: 4 | 13280 / 114272 | training loss: 0.13943184912204742\n",
      "epoch: 4 | 13312 / 114272 | training loss: 0.025391357019543648\n",
      "epoch: 4 | 13344 / 114272 | training loss: 0.002061632927507162\n",
      "epoch: 4 | 13376 / 114272 | training loss: 0.0060560982674360275\n",
      "epoch: 4 | 13408 / 114272 | training loss: 0.007677267771214247\n",
      "epoch: 4 | 13440 / 114272 | training loss: 0.00438939081504941\n",
      "epoch: 4 | 13472 / 114272 | training loss: 0.23543712496757507\n",
      "epoch: 4 | 13504 / 114272 | training loss: 0.010022681206464767\n",
      "epoch: 4 | 13536 / 114272 | training loss: 0.0019464263459667563\n",
      "epoch: 4 | 13568 / 114272 | training loss: 0.003844029502943158\n",
      "epoch: 4 | 13600 / 114272 | training loss: 0.0041788783855736256\n",
      "epoch: 4 | 13632 / 114272 | training loss: 0.08053498715162277\n",
      "epoch: 4 | 13664 / 114272 | training loss: 0.002363678300753236\n",
      "epoch: 4 | 13696 / 114272 | training loss: 0.0037996280007064342\n",
      "epoch: 4 | 13728 / 114272 | training loss: 0.12393505871295929\n",
      "epoch: 4 | 13760 / 114272 | training loss: 0.11382893472909927\n",
      "epoch: 4 | 13792 / 114272 | training loss: 0.10230199992656708\n",
      "epoch: 4 | 13824 / 114272 | training loss: 0.002687311265617609\n",
      "epoch: 4 | 13856 / 114272 | training loss: 0.0023126888554543257\n",
      "epoch: 4 | 13888 / 114272 | training loss: 0.08138591796159744\n",
      "epoch: 4 | 13920 / 114272 | training loss: 0.003031446598470211\n",
      "epoch: 4 | 13952 / 114272 | training loss: 0.002263458678498864\n",
      "epoch: 4 | 13984 / 114272 | training loss: 0.04088165983557701\n",
      "epoch: 4 | 14016 / 114272 | training loss: 0.15321652591228485\n",
      "epoch: 4 | 14048 / 114272 | training loss: 0.003531508380547166\n",
      "epoch: 4 | 14080 / 114272 | training loss: 0.002136565512046218\n",
      "epoch: 4 | 14112 / 114272 | training loss: 0.005857543554157019\n",
      "epoch: 4 | 14144 / 114272 | training loss: 0.018253397196531296\n",
      "epoch: 4 | 14176 / 114272 | training loss: 0.00336698186583817\n",
      "epoch: 4 | 14208 / 114272 | training loss: 0.008667469955980778\n",
      "epoch: 4 | 14240 / 114272 | training loss: 0.18218974769115448\n",
      "epoch: 4 | 14272 / 114272 | training loss: 0.013695911504328251\n",
      "epoch: 4 | 14304 / 114272 | training loss: 0.002543078735470772\n",
      "epoch: 4 | 14336 / 114272 | training loss: 0.0034208002034574747\n",
      "epoch: 4 | 14368 / 114272 | training loss: 0.09415896236896515\n",
      "epoch: 4 | 14400 / 114272 | training loss: 0.004601102322340012\n",
      "epoch: 4 | 14432 / 114272 | training loss: 0.08256252110004425\n",
      "epoch: 4 | 14464 / 114272 | training loss: 0.10777448862791061\n",
      "epoch: 4 | 14496 / 114272 | training loss: 0.007381208706647158\n",
      "epoch: 4 | 14528 / 114272 | training loss: 0.005706754047423601\n",
      "epoch: 4 | 14560 / 114272 | training loss: 0.007945846766233444\n",
      "epoch: 4 | 14592 / 114272 | training loss: 0.0029834050219506025\n",
      "epoch: 4 | 14624 / 114272 | training loss: 0.02184283174574375\n",
      "epoch: 4 | 14656 / 114272 | training loss: 0.005072418600320816\n",
      "epoch: 4 | 14688 / 114272 | training loss: 0.0016110396245494485\n",
      "epoch: 4 | 14720 / 114272 | training loss: 0.07499846071004868\n",
      "epoch: 4 | 14752 / 114272 | training loss: 0.0017291419208049774\n",
      "epoch: 4 | 14784 / 114272 | training loss: 0.007238141261041164\n",
      "epoch: 4 | 14816 / 114272 | training loss: 0.11859152466058731\n",
      "epoch: 4 | 14848 / 114272 | training loss: 0.002722205361351371\n",
      "epoch: 4 | 14880 / 114272 | training loss: 0.003899890696629882\n",
      "epoch: 4 | 14912 / 114272 | training loss: 0.0030978783033788204\n",
      "epoch: 4 | 14944 / 114272 | training loss: 0.16602912545204163\n",
      "epoch: 4 | 14976 / 114272 | training loss: 0.0038547362200915813\n",
      "epoch: 4 | 15008 / 114272 | training loss: 0.20787948369979858\n",
      "epoch: 4 | 15040 / 114272 | training loss: 0.013943339698016644\n",
      "epoch: 4 | 15072 / 114272 | training loss: 0.0021255770698189735\n",
      "epoch: 4 | 15104 / 114272 | training loss: 0.002236015163362026\n",
      "epoch: 4 | 15136 / 114272 | training loss: 0.0019054808653891087\n",
      "epoch: 4 | 15168 / 114272 | training loss: 0.003964337985962629\n",
      "epoch: 4 | 15200 / 114272 | training loss: 0.09310688823461533\n",
      "epoch: 4 | 15232 / 114272 | training loss: 0.12939688563346863\n",
      "epoch: 4 | 15264 / 114272 | training loss: 0.002118851989507675\n",
      "epoch: 4 | 15296 / 114272 | training loss: 0.009415071457624435\n",
      "epoch: 4 | 15328 / 114272 | training loss: 0.001664240495301783\n",
      "epoch: 4 | 15360 / 114272 | training loss: 0.0020040760282427073\n",
      "epoch: 4 | 15392 / 114272 | training loss: 0.0012400508858263493\n",
      "epoch: 4 | 15424 / 114272 | training loss: 0.0034829378128051758\n",
      "epoch: 4 | 15456 / 114272 | training loss: 0.004129380919039249\n",
      "epoch: 4 | 15488 / 114272 | training loss: 0.01931864768266678\n",
      "epoch: 4 | 15520 / 114272 | training loss: 0.0017732116393744946\n",
      "epoch: 4 | 15552 / 114272 | training loss: 0.0038482914678752422\n",
      "epoch: 4 | 15584 / 114272 | training loss: 0.0008546392200514674\n",
      "epoch: 4 | 15616 / 114272 | training loss: 0.002258649095892906\n",
      "epoch: 4 | 15648 / 114272 | training loss: 0.0017978018149733543\n",
      "epoch: 4 | 15680 / 114272 | training loss: 0.0025261803530156612\n",
      "epoch: 4 | 15712 / 114272 | training loss: 0.0020325384102761745\n",
      "epoch: 4 | 15744 / 114272 | training loss: 0.00197713659144938\n",
      "epoch: 4 | 15776 / 114272 | training loss: 0.0028102551586925983\n",
      "epoch: 4 | 15808 / 114272 | training loss: 0.008840184658765793\n",
      "epoch: 4 | 15840 / 114272 | training loss: 0.18182776868343353\n",
      "epoch: 4 | 15872 / 114272 | training loss: 0.06216539070010185\n",
      "epoch: 4 | 15904 / 114272 | training loss: 0.22063110768795013\n",
      "epoch: 4 | 15936 / 114272 | training loss: 0.0017589053604751825\n",
      "epoch: 4 | 15968 / 114272 | training loss: 0.0013752009253948927\n",
      "epoch: 4 | 16000 / 114272 | training loss: 0.0019294761586934328\n",
      "epoch: 4 | 16032 / 114272 | training loss: 0.0014769944828003645\n",
      "epoch: 4 | 16064 / 114272 | training loss: 0.007396466098725796\n",
      "epoch: 4 | 16096 / 114272 | training loss: 0.006343621760606766\n",
      "epoch: 4 | 16128 / 114272 | training loss: 0.0019766727928072214\n",
      "epoch: 4 | 16160 / 114272 | training loss: 0.0018425914458930492\n",
      "epoch: 4 | 16192 / 114272 | training loss: 0.001426998060196638\n",
      "epoch: 4 | 16224 / 114272 | training loss: 0.0032022518571466208\n",
      "epoch: 4 | 16256 / 114272 | training loss: 0.0009493361576460302\n",
      "epoch: 4 | 16288 / 114272 | training loss: 0.009670242667198181\n",
      "epoch: 4 | 16320 / 114272 | training loss: 0.0019619951490312815\n",
      "epoch: 4 | 16352 / 114272 | training loss: 0.07681335508823395\n",
      "epoch: 4 | 16384 / 114272 | training loss: 0.003534104907885194\n",
      "epoch: 4 | 16416 / 114272 | training loss: 0.10288835316896439\n",
      "epoch: 4 | 16448 / 114272 | training loss: 0.009055395610630512\n",
      "epoch: 4 | 16480 / 114272 | training loss: 0.004147575702518225\n",
      "epoch: 4 | 16512 / 114272 | training loss: 0.0007511567091569304\n",
      "epoch: 4 | 16544 / 114272 | training loss: 0.07130767405033112\n",
      "epoch: 4 | 16576 / 114272 | training loss: 0.019016223028302193\n",
      "epoch: 4 | 16608 / 114272 | training loss: 0.0007900992641225457\n",
      "epoch: 4 | 16640 / 114272 | training loss: 0.0015255652833729982\n",
      "epoch: 4 | 16672 / 114272 | training loss: 0.0017421836964786053\n",
      "epoch: 4 | 16704 / 114272 | training loss: 0.003878963878378272\n",
      "epoch: 4 | 16736 / 114272 | training loss: 0.001724411966279149\n",
      "epoch: 4 | 16768 / 114272 | training loss: 0.0009244203683920205\n",
      "epoch: 4 | 16800 / 114272 | training loss: 0.0011858462821692228\n",
      "epoch: 4 | 16832 / 114272 | training loss: 0.1587686985731125\n",
      "epoch: 4 | 16864 / 114272 | training loss: 0.0015321444952860475\n",
      "epoch: 4 | 16896 / 114272 | training loss: 0.000861146196257323\n",
      "epoch: 4 | 16928 / 114272 | training loss: 0.0023785855155438185\n",
      "epoch: 4 | 16960 / 114272 | training loss: 0.021945247426629066\n",
      "epoch: 4 | 16992 / 114272 | training loss: 0.04770367592573166\n",
      "epoch: 4 | 17024 / 114272 | training loss: 0.002592710079625249\n",
      "epoch: 4 | 17056 / 114272 | training loss: 0.06018481031060219\n",
      "epoch: 4 | 17088 / 114272 | training loss: 0.002192865591496229\n",
      "epoch: 4 | 17120 / 114272 | training loss: 0.2790195643901825\n",
      "epoch: 4 | 17152 / 114272 | training loss: 0.002227457007393241\n",
      "epoch: 4 | 17184 / 114272 | training loss: 0.0010825115023180842\n",
      "epoch: 4 | 17216 / 114272 | training loss: 0.0017657472053542733\n",
      "epoch: 4 | 17248 / 114272 | training loss: 0.12318199872970581\n",
      "epoch: 4 | 17280 / 114272 | training loss: 0.0010769283398985863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 17312 / 114272 | training loss: 0.0005584338214248419\n",
      "epoch: 4 | 17344 / 114272 | training loss: 0.08849670737981796\n",
      "epoch: 4 | 17376 / 114272 | training loss: 0.001785683911293745\n",
      "epoch: 4 | 17408 / 114272 | training loss: 0.0018758655060082674\n",
      "epoch: 4 | 17440 / 114272 | training loss: 0.3768095374107361\n",
      "epoch: 4 | 17472 / 114272 | training loss: 0.004295714665204287\n",
      "epoch: 4 | 17504 / 114272 | training loss: 0.0020997629035264254\n",
      "epoch: 4 | 17536 / 114272 | training loss: 0.0014506132574751973\n",
      "epoch: 4 | 17568 / 114272 | training loss: 0.005726697389036417\n",
      "epoch: 4 | 17600 / 114272 | training loss: 0.09309684485197067\n",
      "epoch: 4 | 17632 / 114272 | training loss: 0.0014251144602894783\n",
      "epoch: 4 | 17664 / 114272 | training loss: 0.0013051312416791916\n",
      "epoch: 4 | 17696 / 114272 | training loss: 0.0017781835049390793\n",
      "epoch: 4 | 17728 / 114272 | training loss: 0.0037875028792768717\n",
      "epoch: 4 | 17760 / 114272 | training loss: 0.0012064942857250571\n",
      "epoch: 4 | 17792 / 114272 | training loss: 0.002201787196099758\n",
      "epoch: 4 | 17824 / 114272 | training loss: 0.17899088561534882\n",
      "epoch: 4 | 17856 / 114272 | training loss: 0.1408238410949707\n",
      "epoch: 4 | 17888 / 114272 | training loss: 0.0016212009359151125\n",
      "epoch: 4 | 17920 / 114272 | training loss: 0.0025957031175494194\n",
      "epoch: 4 | 17952 / 114272 | training loss: 0.00231804046779871\n",
      "epoch: 4 | 17984 / 114272 | training loss: 0.0017276310827583075\n",
      "epoch: 4 | 18016 / 114272 | training loss: 0.003927217796444893\n",
      "epoch: 4 | 18048 / 114272 | training loss: 0.0012119484599679708\n",
      "epoch: 4 | 18080 / 114272 | training loss: 0.008226887322962284\n",
      "epoch: 4 | 18112 / 114272 | training loss: 0.001557083218358457\n",
      "epoch: 4 | 18144 / 114272 | training loss: 0.11373961716890335\n",
      "epoch: 4 | 18176 / 114272 | training loss: 0.002486875280737877\n",
      "epoch: 4 | 18208 / 114272 | training loss: 0.002896777121350169\n",
      "epoch: 4 | 18240 / 114272 | training loss: 0.23010791838169098\n",
      "epoch: 4 | 18272 / 114272 | training loss: 0.001289500156417489\n",
      "epoch: 4 | 18304 / 114272 | training loss: 0.0011132338549941778\n",
      "epoch: 4 | 18336 / 114272 | training loss: 0.06202163174748421\n",
      "epoch: 4 | 18368 / 114272 | training loss: 0.20983867347240448\n",
      "epoch: 4 | 18400 / 114272 | training loss: 0.07415344566106796\n",
      "epoch: 4 | 18432 / 114272 | training loss: 0.004717511124908924\n",
      "epoch: 4 | 18464 / 114272 | training loss: 0.0017820450011640787\n",
      "epoch: 4 | 18496 / 114272 | training loss: 0.001353314844891429\n",
      "epoch: 4 | 18528 / 114272 | training loss: 0.001207350636832416\n",
      "epoch: 4 | 18560 / 114272 | training loss: 0.0015062459278851748\n",
      "epoch: 4 | 18592 / 114272 | training loss: 0.005199291277676821\n",
      "epoch: 4 | 18624 / 114272 | training loss: 0.0013747132616117597\n",
      "epoch: 4 | 18656 / 114272 | training loss: 0.001538435579277575\n",
      "epoch: 4 | 18688 / 114272 | training loss: 0.01270004641264677\n",
      "epoch: 4 | 18720 / 114272 | training loss: 0.24507538974285126\n",
      "epoch: 4 | 18752 / 114272 | training loss: 0.004700252320617437\n",
      "epoch: 4 | 18784 / 114272 | training loss: 0.0023479019291698933\n",
      "epoch: 4 | 18816 / 114272 | training loss: 0.0012184178922325373\n",
      "epoch: 4 | 18848 / 114272 | training loss: 0.07778578996658325\n",
      "epoch: 4 | 18880 / 114272 | training loss: 0.0007764322217553854\n",
      "epoch: 4 | 18912 / 114272 | training loss: 0.00492134178057313\n",
      "epoch: 4 | 18944 / 114272 | training loss: 0.0011830440489575267\n",
      "epoch: 4 | 18976 / 114272 | training loss: 0.09453529864549637\n",
      "epoch: 4 | 19008 / 114272 | training loss: 0.0008828327408991754\n",
      "epoch: 4 | 19040 / 114272 | training loss: 0.013048428110778332\n",
      "epoch: 4 | 19072 / 114272 | training loss: 0.013308154419064522\n",
      "epoch: 4 | 19104 / 114272 | training loss: 0.0011101207928732038\n",
      "epoch: 4 | 19136 / 114272 | training loss: 0.002536146203055978\n",
      "epoch: 4 | 19168 / 114272 | training loss: 0.06789937615394592\n",
      "epoch: 4 | 19200 / 114272 | training loss: 0.0011365243699401617\n",
      "epoch: 4 | 19232 / 114272 | training loss: 0.00943993590772152\n",
      "epoch: 4 | 19264 / 114272 | training loss: 0.0022557557094842196\n",
      "epoch: 4 | 19296 / 114272 | training loss: 0.001495297416113317\n",
      "epoch: 4 | 19328 / 114272 | training loss: 0.000817909836769104\n",
      "epoch: 4 | 19360 / 114272 | training loss: 0.0017631693044677377\n",
      "epoch: 4 | 19392 / 114272 | training loss: 0.10634199529886246\n",
      "epoch: 4 | 19424 / 114272 | training loss: 0.0007989181904122233\n",
      "epoch: 4 | 19456 / 114272 | training loss: 0.05521509796380997\n",
      "epoch: 4 | 19488 / 114272 | training loss: 0.052164070308208466\n",
      "epoch: 4 | 19520 / 114272 | training loss: 0.0026216201949864626\n",
      "epoch: 4 | 19552 / 114272 | training loss: 0.19284668564796448\n",
      "epoch: 4 | 19584 / 114272 | training loss: 0.008536719717085361\n",
      "epoch: 4 | 19616 / 114272 | training loss: 0.001118683023378253\n",
      "epoch: 4 | 19648 / 114272 | training loss: 0.0008760258206166327\n",
      "epoch: 4 | 19680 / 114272 | training loss: 0.015064278617501259\n",
      "epoch: 4 | 19712 / 114272 | training loss: 0.309360146522522\n",
      "epoch: 4 | 19744 / 114272 | training loss: 0.00073097029235214\n",
      "epoch: 4 | 19776 / 114272 | training loss: 0.11514783650636673\n",
      "epoch: 4 | 19808 / 114272 | training loss: 0.006647848524153233\n",
      "epoch: 4 | 19840 / 114272 | training loss: 0.0019265395822003484\n",
      "epoch: 4 | 19872 / 114272 | training loss: 0.1778165102005005\n",
      "epoch: 4 | 19904 / 114272 | training loss: 0.16775691509246826\n",
      "epoch: 4 | 19936 / 114272 | training loss: 0.0015018832636997104\n",
      "epoch: 4 | 19968 / 114272 | training loss: 0.001612069783732295\n",
      "epoch: 4 | 20000 / 114272 | training loss: 0.0011474972125142813\n",
      "epoch: 4 | 20032 / 114272 | training loss: 0.001141065382398665\n",
      "epoch: 4 | 20064 / 114272 | training loss: 0.0005946323508396745\n",
      "epoch: 4 | 20096 / 114272 | training loss: 0.0009034388931468129\n",
      "epoch: 4 | 20128 / 114272 | training loss: 0.15747816860675812\n",
      "epoch: 4 | 20160 / 114272 | training loss: 0.001165539026260376\n",
      "epoch: 4 | 20192 / 114272 | training loss: 0.0038222461007535458\n",
      "epoch: 4 | 20224 / 114272 | training loss: 0.0029893433675169945\n",
      "epoch: 4 | 20256 / 114272 | training loss: 0.0007588206208311021\n",
      "epoch: 4 | 20288 / 114272 | training loss: 0.0029191854409873486\n",
      "epoch: 4 | 20320 / 114272 | training loss: 0.0009277291828766465\n",
      "epoch: 4 | 20352 / 114272 | training loss: 0.2652909755706787\n",
      "epoch: 4 | 20384 / 114272 | training loss: 0.0014818855561316013\n",
      "epoch: 4 | 20416 / 114272 | training loss: 0.0012984013883396983\n",
      "epoch: 4 | 20448 / 114272 | training loss: 0.0006826752214692533\n",
      "epoch: 4 | 20480 / 114272 | training loss: 0.11263018101453781\n",
      "epoch: 4 | 20512 / 114272 | training loss: 0.2311936914920807\n",
      "epoch: 4 | 20544 / 114272 | training loss: 0.0027984813787043095\n",
      "epoch: 4 | 20576 / 114272 | training loss: 0.0019278698600828648\n",
      "epoch: 4 | 20608 / 114272 | training loss: 0.0030907478649169207\n",
      "epoch: 4 | 20640 / 114272 | training loss: 0.1202605590224266\n",
      "epoch: 4 | 20672 / 114272 | training loss: 0.0014709793031215668\n",
      "epoch: 4 | 20704 / 114272 | training loss: 0.3073786199092865\n",
      "epoch: 4 | 20736 / 114272 | training loss: 0.00197537150233984\n",
      "epoch: 4 | 20768 / 114272 | training loss: 0.13060812652111053\n",
      "epoch: 4 | 20800 / 114272 | training loss: 0.28148946166038513\n",
      "epoch: 4 | 20832 / 114272 | training loss: 0.000566764734685421\n",
      "epoch: 4 | 20864 / 114272 | training loss: 0.0015208200784400105\n",
      "epoch: 4 | 20896 / 114272 | training loss: 0.0010738344863057137\n",
      "epoch: 4 | 20928 / 114272 | training loss: 0.001555758062750101\n",
      "epoch: 4 | 20960 / 114272 | training loss: 0.001321415533311665\n",
      "epoch: 4 | 20992 / 114272 | training loss: 0.129133939743042\n",
      "epoch: 4 | 21024 / 114272 | training loss: 0.18024809658527374\n",
      "epoch: 4 | 21056 / 114272 | training loss: 0.006656457204371691\n",
      "epoch: 4 | 21088 / 114272 | training loss: 0.002707278123125434\n",
      "epoch: 4 | 21120 / 114272 | training loss: 0.0019682366400957108\n",
      "epoch: 4 | 21152 / 114272 | training loss: 0.0009641230572015047\n",
      "epoch: 4 | 21184 / 114272 | training loss: 0.0010119180660694838\n",
      "epoch: 4 | 21216 / 114272 | training loss: 0.0064890217036008835\n",
      "epoch: 4 | 21248 / 114272 | training loss: 0.0066204797476530075\n",
      "epoch: 4 | 21280 / 114272 | training loss: 0.0024193888530135155\n",
      "epoch: 4 | 21312 / 114272 | training loss: 0.0010222875280305743\n",
      "epoch: 4 | 21344 / 114272 | training loss: 0.003900148207321763\n",
      "epoch: 4 | 21376 / 114272 | training loss: 0.0018144003115594387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 21408 / 114272 | training loss: 0.004697173368185759\n",
      "epoch: 4 | 21440 / 114272 | training loss: 0.0013856574660167098\n",
      "epoch: 4 | 21472 / 114272 | training loss: 0.021072925999760628\n",
      "epoch: 4 | 21504 / 114272 | training loss: 0.0052979919128119946\n",
      "epoch: 4 | 21536 / 114272 | training loss: 0.0012662920635193586\n",
      "epoch: 4 | 21568 / 114272 | training loss: 0.0021457476541399956\n",
      "epoch: 4 | 21600 / 114272 | training loss: 0.0030815661884844303\n",
      "epoch: 4 | 21632 / 114272 | training loss: 0.0043989610858261585\n",
      "epoch: 4 | 21664 / 114272 | training loss: 0.00068971689324826\n",
      "epoch: 4 | 21696 / 114272 | training loss: 0.001178662059828639\n",
      "epoch: 4 | 21728 / 114272 | training loss: 0.007438239641487598\n",
      "epoch: 4 | 21760 / 114272 | training loss: 0.04446522518992424\n",
      "epoch: 4 | 21792 / 114272 | training loss: 0.0010385828791186213\n",
      "epoch: 4 | 21824 / 114272 | training loss: 0.00977417454123497\n",
      "epoch: 4 | 21856 / 114272 | training loss: 0.0011011991882696748\n",
      "epoch: 4 | 21888 / 114272 | training loss: 0.0016044741496443748\n",
      "epoch: 4 | 21920 / 114272 | training loss: 0.0012430115602910519\n",
      "epoch: 4 | 21952 / 114272 | training loss: 0.001208203611895442\n",
      "epoch: 4 | 21984 / 114272 | training loss: 0.1485501527786255\n",
      "epoch: 4 | 22016 / 114272 | training loss: 0.0017972292844206095\n",
      "epoch: 4 | 22048 / 114272 | training loss: 0.00203436310403049\n",
      "epoch: 4 | 22080 / 114272 | training loss: 0.001721748849377036\n",
      "epoch: 4 | 22112 / 114272 | training loss: 0.0013424649368971586\n",
      "epoch: 4 | 22144 / 114272 | training loss: 0.0015192610444501042\n",
      "epoch: 4 | 22176 / 114272 | training loss: 0.0014692309778183699\n",
      "epoch: 4 | 22208 / 114272 | training loss: 0.0011049744207412004\n",
      "epoch: 4 | 22240 / 114272 | training loss: 0.14433549344539642\n",
      "epoch: 4 | 22272 / 114272 | training loss: 0.0008126600878313184\n",
      "epoch: 4 | 22304 / 114272 | training loss: 0.003565170569345355\n",
      "epoch: 4 | 22336 / 114272 | training loss: 0.1093175858259201\n",
      "epoch: 4 | 22368 / 114272 | training loss: 0.08748604357242584\n",
      "epoch: 4 | 22400 / 114272 | training loss: 0.001637885463424027\n",
      "epoch: 4 | 22432 / 114272 | training loss: 0.0011959781404584646\n",
      "epoch: 4 | 22464 / 114272 | training loss: 0.1554795652627945\n",
      "epoch: 4 | 22496 / 114272 | training loss: 0.002079884521663189\n",
      "epoch: 4 | 22528 / 114272 | training loss: 0.0021923575550317764\n",
      "epoch: 4 | 22560 / 114272 | training loss: 0.003449060022830963\n",
      "epoch: 4 | 22592 / 114272 | training loss: 0.0009364008437842131\n",
      "epoch: 4 | 22624 / 114272 | training loss: 0.0016407205257564783\n",
      "epoch: 4 | 22656 / 114272 | training loss: 0.007176661863923073\n",
      "epoch: 4 | 22688 / 114272 | training loss: 0.0008162861922755837\n",
      "epoch: 4 | 22720 / 114272 | training loss: 0.0029941268730908632\n",
      "epoch: 4 | 22752 / 114272 | training loss: 0.003304725745692849\n",
      "epoch: 4 | 22784 / 114272 | training loss: 0.08907421678304672\n",
      "epoch: 4 | 22816 / 114272 | training loss: 0.0037111968267709017\n",
      "epoch: 4 | 22848 / 114272 | training loss: 0.1037154272198677\n",
      "epoch: 4 | 22880 / 114272 | training loss: 0.34004542231559753\n",
      "epoch: 4 | 22912 / 114272 | training loss: 0.007022649981081486\n",
      "epoch: 4 | 22944 / 114272 | training loss: 0.0021101769525557756\n",
      "epoch: 4 | 22976 / 114272 | training loss: 0.004523856565356255\n",
      "epoch: 4 | 23008 / 114272 | training loss: 0.1252039521932602\n",
      "epoch: 4 | 23040 / 114272 | training loss: 0.001040373696014285\n",
      "epoch: 4 | 23072 / 114272 | training loss: 0.0010963501408696175\n",
      "epoch: 4 | 23104 / 114272 | training loss: 0.0007631463813595474\n",
      "epoch: 4 | 23136 / 114272 | training loss: 0.001015238813124597\n",
      "epoch: 4 | 23168 / 114272 | training loss: 0.11411105841398239\n",
      "epoch: 4 | 23200 / 114272 | training loss: 0.008922046981751919\n",
      "epoch: 4 | 23232 / 114272 | training loss: 0.0034273164346814156\n",
      "epoch: 4 | 23264 / 114272 | training loss: 0.0034278861712664366\n",
      "epoch: 4 | 23296 / 114272 | training loss: 0.36593300104141235\n",
      "epoch: 4 | 23328 / 114272 | training loss: 0.003426366951316595\n",
      "epoch: 4 | 23360 / 114272 | training loss: 0.0017676176503300667\n",
      "epoch: 4 | 23392 / 114272 | training loss: 0.1602991670370102\n",
      "epoch: 4 | 23424 / 114272 | training loss: 0.001014476758427918\n",
      "epoch: 4 | 23456 / 114272 | training loss: 0.0017377170734107494\n",
      "epoch: 4 | 23488 / 114272 | training loss: 0.11378753185272217\n",
      "epoch: 4 | 23520 / 114272 | training loss: 0.0013615434290841222\n",
      "epoch: 4 | 23552 / 114272 | training loss: 0.0020949116442352533\n",
      "epoch: 4 | 23584 / 114272 | training loss: 0.0022720431443303823\n",
      "epoch: 4 | 23616 / 114272 | training loss: 0.002130345441401005\n",
      "epoch: 4 | 23648 / 114272 | training loss: 0.08295292407274246\n",
      "epoch: 4 | 23680 / 114272 | training loss: 0.22795981168746948\n",
      "epoch: 4 | 23712 / 114272 | training loss: 0.0027359616942703724\n",
      "epoch: 4 | 23744 / 114272 | training loss: 0.00493098609149456\n",
      "epoch: 4 | 23776 / 114272 | training loss: 0.004321980755776167\n",
      "epoch: 4 | 23808 / 114272 | training loss: 0.0031179331708699465\n",
      "epoch: 4 | 23840 / 114272 | training loss: 0.003625595010817051\n",
      "epoch: 4 | 23872 / 114272 | training loss: 0.004785037599503994\n",
      "epoch: 4 | 23904 / 114272 | training loss: 0.07743676006793976\n",
      "epoch: 4 | 23936 / 114272 | training loss: 0.005009795073419809\n",
      "epoch: 4 | 23968 / 114272 | training loss: 0.0022984587121754885\n",
      "epoch: 4 | 24000 / 114272 | training loss: 0.082315593957901\n",
      "epoch: 4 | 24032 / 114272 | training loss: 0.0024182286579161882\n",
      "epoch: 4 | 24064 / 114272 | training loss: 0.0012826472520828247\n",
      "epoch: 4 | 24096 / 114272 | training loss: 0.0017095436342060566\n",
      "epoch: 4 | 24128 / 114272 | training loss: 0.003603757359087467\n",
      "epoch: 4 | 24160 / 114272 | training loss: 0.0016194154741242528\n",
      "epoch: 4 | 24192 / 114272 | training loss: 0.14107634127140045\n",
      "epoch: 4 | 24224 / 114272 | training loss: 0.15171031653881073\n",
      "epoch: 4 | 24256 / 114272 | training loss: 0.00212514353916049\n",
      "epoch: 4 | 24288 / 114272 | training loss: 0.0013441432965919375\n",
      "epoch: 4 | 24320 / 114272 | training loss: 0.2106395959854126\n",
      "epoch: 4 | 24352 / 114272 | training loss: 0.0023133810609579086\n",
      "epoch: 4 | 24384 / 114272 | training loss: 0.0026913993060588837\n",
      "epoch: 4 | 24416 / 114272 | training loss: 0.06251198053359985\n",
      "epoch: 4 | 24448 / 114272 | training loss: 0.00210943934507668\n",
      "epoch: 4 | 24480 / 114272 | training loss: 0.14148221909999847\n",
      "epoch: 4 | 24512 / 114272 | training loss: 0.06567943841218948\n",
      "epoch: 4 | 24544 / 114272 | training loss: 0.0021790023893117905\n",
      "epoch: 4 | 24576 / 114272 | training loss: 0.0025971559807658195\n",
      "epoch: 4 | 24608 / 114272 | training loss: 0.002114661270752549\n",
      "epoch: 4 | 24640 / 114272 | training loss: 0.16321897506713867\n",
      "epoch: 4 | 24672 / 114272 | training loss: 0.0013352778041735291\n",
      "epoch: 4 | 24704 / 114272 | training loss: 0.004704514052718878\n",
      "epoch: 4 | 24736 / 114272 | training loss: 0.0014133664080873132\n",
      "epoch: 4 | 24768 / 114272 | training loss: 0.08879697322845459\n",
      "epoch: 4 | 24800 / 114272 | training loss: 0.003416322637349367\n",
      "epoch: 4 | 24832 / 114272 | training loss: 0.14661039412021637\n",
      "epoch: 4 | 24864 / 114272 | training loss: 0.003177929436787963\n",
      "epoch: 4 | 24896 / 114272 | training loss: 0.002601471496745944\n",
      "epoch: 4 | 24928 / 114272 | training loss: 0.04252210259437561\n",
      "epoch: 4 | 24960 / 114272 | training loss: 0.004900239408016205\n",
      "epoch: 4 | 24992 / 114272 | training loss: 0.07388105988502502\n",
      "epoch: 4 | 25024 / 114272 | training loss: 0.01406096387654543\n",
      "epoch: 4 | 25056 / 114272 | training loss: 0.07858563214540482\n",
      "epoch: 4 | 25088 / 114272 | training loss: 0.0071211024187505245\n",
      "epoch: 4 | 25120 / 114272 | training loss: 0.004265760071575642\n",
      "epoch: 4 | 25152 / 114272 | training loss: 0.007826781831681728\n",
      "epoch: 4 | 25184 / 114272 | training loss: 0.006090584211051464\n",
      "epoch: 4 | 25216 / 114272 | training loss: 0.024043742567300797\n",
      "epoch: 4 | 25248 / 114272 | training loss: 0.0028622157406061888\n",
      "epoch: 4 | 25280 / 114272 | training loss: 0.0024804845452308655\n",
      "epoch: 4 | 25312 / 114272 | training loss: 0.2811451852321625\n",
      "epoch: 4 | 25344 / 114272 | training loss: 0.1307426244020462\n",
      "epoch: 4 | 25376 / 114272 | training loss: 0.01665337383747101\n",
      "epoch: 4 | 25408 / 114272 | training loss: 0.0011480984976515174\n",
      "epoch: 4 | 25440 / 114272 | training loss: 0.00159012945368886\n",
      "epoch: 4 | 25472 / 114272 | training loss: 0.10936936736106873\n",
      "epoch: 4 | 25504 / 114272 | training loss: 0.08988121896982193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 25536 / 114272 | training loss: 0.002121083438396454\n",
      "epoch: 4 | 25568 / 114272 | training loss: 0.010798290371894836\n",
      "epoch: 4 | 25600 / 114272 | training loss: 0.0035368448588997126\n",
      "epoch: 4 | 25632 / 114272 | training loss: 0.16046926379203796\n",
      "epoch: 4 | 25664 / 114272 | training loss: 0.03685072809457779\n",
      "epoch: 4 | 25696 / 114272 | training loss: 0.07773392647504807\n",
      "epoch: 4 | 25728 / 114272 | training loss: 0.29465919733047485\n",
      "epoch: 4 | 25760 / 114272 | training loss: 0.009558405727148056\n",
      "epoch: 4 | 25792 / 114272 | training loss: 0.0018921146402135491\n",
      "epoch: 4 | 25824 / 114272 | training loss: 0.09112835675477982\n",
      "epoch: 4 | 25856 / 114272 | training loss: 0.0010654411744326353\n",
      "epoch: 4 | 25888 / 114272 | training loss: 0.0030879261903464794\n",
      "epoch: 4 | 25920 / 114272 | training loss: 0.014495671726763248\n",
      "epoch: 4 | 25952 / 114272 | training loss: 0.00213910941965878\n",
      "epoch: 4 | 25984 / 114272 | training loss: 0.010184570215642452\n",
      "epoch: 4 | 26016 / 114272 | training loss: 0.09088656306266785\n",
      "epoch: 4 | 26048 / 114272 | training loss: 0.0030352328903973103\n",
      "epoch: 4 | 26080 / 114272 | training loss: 0.007051605265587568\n",
      "epoch: 4 | 26112 / 114272 | training loss: 0.0013737084809690714\n",
      "epoch: 4 | 26144 / 114272 | training loss: 0.002811253536492586\n",
      "epoch: 4 | 26176 / 114272 | training loss: 0.14409159123897552\n",
      "epoch: 4 | 26208 / 114272 | training loss: 0.05574343353509903\n",
      "epoch: 4 | 26240 / 114272 | training loss: 0.007800462655723095\n",
      "epoch: 4 | 26272 / 114272 | training loss: 0.0038112320471554995\n",
      "epoch: 4 | 26304 / 114272 | training loss: 0.00366334430873394\n",
      "epoch: 4 | 26336 / 114272 | training loss: 0.004270651843398809\n",
      "epoch: 4 | 26368 / 114272 | training loss: 0.09198491275310516\n",
      "epoch: 4 | 26400 / 114272 | training loss: 0.06512840837240219\n",
      "epoch: 4 | 26432 / 114272 | training loss: 0.00887398049235344\n",
      "epoch: 4 | 26464 / 114272 | training loss: 0.0017323058564215899\n",
      "epoch: 4 | 26496 / 114272 | training loss: 0.01749100722372532\n",
      "epoch: 4 | 26528 / 114272 | training loss: 0.09408929944038391\n",
      "epoch: 4 | 26560 / 114272 | training loss: 0.2873259484767914\n",
      "epoch: 4 | 26592 / 114272 | training loss: 0.2467535138130188\n",
      "epoch: 4 | 26624 / 114272 | training loss: 0.0010636140359565616\n",
      "epoch: 4 | 26656 / 114272 | training loss: 0.0024908012710511684\n",
      "epoch: 4 | 26688 / 114272 | training loss: 0.022647129371762276\n",
      "epoch: 4 | 26720 / 114272 | training loss: 0.003781277686357498\n",
      "epoch: 4 | 26752 / 114272 | training loss: 0.15229657292366028\n",
      "epoch: 4 | 26784 / 114272 | training loss: 0.0033592074178159237\n",
      "epoch: 4 | 26816 / 114272 | training loss: 0.10443446040153503\n",
      "epoch: 4 | 26848 / 114272 | training loss: 0.24727924168109894\n",
      "epoch: 4 | 26880 / 114272 | training loss: 0.015676096081733704\n",
      "epoch: 4 | 26912 / 114272 | training loss: 0.004264643415808678\n",
      "epoch: 4 | 26944 / 114272 | training loss: 0.011037025600671768\n",
      "epoch: 4 | 26976 / 114272 | training loss: 0.005415380001068115\n",
      "epoch: 4 | 27008 / 114272 | training loss: 0.0016484950901940465\n",
      "epoch: 4 | 27040 / 114272 | training loss: 0.0043614828027784824\n",
      "epoch: 4 | 27072 / 114272 | training loss: 0.2575010657310486\n",
      "epoch: 4 | 27104 / 114272 | training loss: 0.05001402646303177\n",
      "epoch: 4 | 27136 / 114272 | training loss: 0.013285034336149693\n",
      "epoch: 4 | 27168 / 114272 | training loss: 0.005859479308128357\n",
      "epoch: 4 | 27200 / 114272 | training loss: 0.053831927478313446\n",
      "epoch: 4 | 27232 / 114272 | training loss: 0.006403184961527586\n",
      "epoch: 4 | 27264 / 114272 | training loss: 0.003477721707895398\n",
      "epoch: 4 | 27296 / 114272 | training loss: 0.005364473909139633\n",
      "epoch: 4 | 27328 / 114272 | training loss: 0.22625166177749634\n",
      "epoch: 4 | 27360 / 114272 | training loss: 0.0020638389978557825\n",
      "epoch: 4 | 27392 / 114272 | training loss: 0.0038943507242947817\n",
      "epoch: 4 | 27424 / 114272 | training loss: 0.005303976126015186\n",
      "epoch: 4 | 27456 / 114272 | training loss: 0.15405583381652832\n",
      "epoch: 4 | 27488 / 114272 | training loss: 0.002782891970127821\n",
      "epoch: 4 | 27520 / 114272 | training loss: 0.012587307021021843\n",
      "epoch: 4 | 27552 / 114272 | training loss: 0.0067413244396448135\n",
      "epoch: 4 | 27584 / 114272 | training loss: 0.0023132406640797853\n",
      "epoch: 4 | 27616 / 114272 | training loss: 0.0014559379778802395\n",
      "epoch: 4 | 27648 / 114272 | training loss: 0.00281883985735476\n",
      "epoch: 4 | 27680 / 114272 | training loss: 0.16001193225383759\n",
      "epoch: 4 | 27712 / 114272 | training loss: 0.0017379591008648276\n",
      "epoch: 4 | 27744 / 114272 | training loss: 0.002252369187772274\n",
      "epoch: 4 | 27776 / 114272 | training loss: 0.09934607148170471\n",
      "epoch: 4 | 27808 / 114272 | training loss: 0.15114401280879974\n",
      "epoch: 4 | 27840 / 114272 | training loss: 0.011275079101324081\n",
      "epoch: 4 | 27872 / 114272 | training loss: 0.002202587900683284\n",
      "epoch: 4 | 27904 / 114272 | training loss: 0.026728441938757896\n",
      "epoch: 4 | 27936 / 114272 | training loss: 0.1080358698964119\n",
      "epoch: 4 | 27968 / 114272 | training loss: 0.0019592365715652704\n",
      "epoch: 4 | 28000 / 114272 | training loss: 0.004548531491309404\n",
      "epoch: 4 | 28032 / 114272 | training loss: 0.0038985926657915115\n",
      "epoch: 4 | 28064 / 114272 | training loss: 0.00393002899363637\n",
      "epoch: 4 | 28096 / 114272 | training loss: 0.0027515811379998922\n",
      "epoch: 4 | 28128 / 114272 | training loss: 0.1373913288116455\n",
      "epoch: 4 | 28160 / 114272 | training loss: 0.00276735401712358\n",
      "epoch: 4 | 28192 / 114272 | training loss: 0.0025096724275499582\n",
      "epoch: 4 | 28224 / 114272 | training loss: 0.008096772246062756\n",
      "epoch: 4 | 28256 / 114272 | training loss: 0.0015285119879990816\n",
      "epoch: 4 | 28288 / 114272 | training loss: 0.003124983049929142\n",
      "epoch: 4 | 28320 / 114272 | training loss: 0.003890379099175334\n",
      "epoch: 4 | 28352 / 114272 | training loss: 0.004992921836674213\n",
      "epoch: 4 | 28384 / 114272 | training loss: 0.00435525132343173\n",
      "epoch: 4 | 28416 / 114272 | training loss: 0.013483172282576561\n",
      "epoch: 4 | 28448 / 114272 | training loss: 0.0018657112959772348\n",
      "epoch: 4 | 28480 / 114272 | training loss: 0.07773754000663757\n",
      "epoch: 4 | 28512 / 114272 | training loss: 0.0012210025452077389\n",
      "epoch: 4 | 28544 / 114272 | training loss: 0.0010639955289661884\n",
      "epoch: 4 | 28576 / 114272 | training loss: 0.0031992089934647083\n",
      "epoch: 4 | 28608 / 114272 | training loss: 0.0011848980793729424\n",
      "epoch: 4 | 28640 / 114272 | training loss: 0.0038755086716264486\n",
      "epoch: 4 | 28672 / 114272 | training loss: 0.0020006620325148106\n",
      "epoch: 4 | 28704 / 114272 | training loss: 0.001055027125403285\n",
      "epoch: 4 | 28736 / 114272 | training loss: 0.0019023402128368616\n",
      "epoch: 4 | 28768 / 114272 | training loss: 0.16097989678382874\n",
      "epoch: 4 | 28800 / 114272 | training loss: 0.001531708287075162\n",
      "epoch: 4 | 28832 / 114272 | training loss: 0.0042562102898955345\n",
      "epoch: 4 | 28864 / 114272 | training loss: 0.020599789917469025\n",
      "epoch: 4 | 28896 / 114272 | training loss: 0.013163444586098194\n",
      "epoch: 4 | 28928 / 114272 | training loss: 0.002742909826338291\n",
      "epoch: 4 | 28960 / 114272 | training loss: 0.003981654066592455\n",
      "epoch: 4 | 28992 / 114272 | training loss: 0.0047528124414384365\n",
      "epoch: 4 | 29024 / 114272 | training loss: 0.0009705492993816733\n",
      "epoch: 4 | 29056 / 114272 | training loss: 0.1126822903752327\n",
      "epoch: 4 | 29088 / 114272 | training loss: 0.06291665881872177\n",
      "epoch: 4 | 29120 / 114272 | training loss: 0.0023970722686499357\n",
      "epoch: 4 | 29152 / 114272 | training loss: 0.10173830389976501\n",
      "epoch: 4 | 29184 / 114272 | training loss: 0.012169538997113705\n",
      "epoch: 4 | 29216 / 114272 | training loss: 0.10093851387500763\n",
      "epoch: 4 | 29248 / 114272 | training loss: 0.005960441660135984\n",
      "epoch: 4 | 29280 / 114272 | training loss: 0.001724807545542717\n",
      "epoch: 4 | 29312 / 114272 | training loss: 0.07102391868829727\n",
      "epoch: 4 | 29344 / 114272 | training loss: 0.016999663785099983\n",
      "epoch: 4 | 29376 / 114272 | training loss: 0.04260670021176338\n",
      "epoch: 4 | 29408 / 114272 | training loss: 0.002308119321241975\n",
      "epoch: 4 | 29440 / 114272 | training loss: 0.023959359154105186\n",
      "epoch: 4 | 29472 / 114272 | training loss: 0.00266841147094965\n",
      "epoch: 4 | 29504 / 114272 | training loss: 0.12790179252624512\n",
      "epoch: 4 | 29536 / 114272 | training loss: 0.08674907684326172\n",
      "epoch: 4 | 29568 / 114272 | training loss: 0.1461133509874344\n",
      "epoch: 4 | 29600 / 114272 | training loss: 0.0015697653871029615\n",
      "epoch: 4 | 29632 / 114272 | training loss: 0.18810181319713593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 29664 / 114272 | training loss: 0.0023842123337090015\n",
      "epoch: 4 | 29696 / 114272 | training loss: 0.002543768612667918\n",
      "epoch: 4 | 29728 / 114272 | training loss: 0.2041589766740799\n",
      "epoch: 4 | 29760 / 114272 | training loss: 0.002588575938716531\n",
      "epoch: 4 | 29792 / 114272 | training loss: 0.00203949143178761\n",
      "epoch: 4 | 29824 / 114272 | training loss: 0.0011196675477549434\n",
      "epoch: 4 | 29856 / 114272 | training loss: 0.0022740012500435114\n",
      "epoch: 4 | 29888 / 114272 | training loss: 0.010418972000479698\n",
      "epoch: 4 | 29920 / 114272 | training loss: 0.0013109049759805202\n",
      "epoch: 4 | 29952 / 114272 | training loss: 0.07135800272226334\n",
      "epoch: 4 | 29984 / 114272 | training loss: 0.001471646479330957\n",
      "epoch: 4 | 30016 / 114272 | training loss: 0.007273611146956682\n",
      "epoch: 4 | 30048 / 114272 | training loss: 0.013639182783663273\n",
      "epoch: 4 | 30080 / 114272 | training loss: 0.0018441027496010065\n",
      "epoch: 4 | 30112 / 114272 | training loss: 0.08200280368328094\n",
      "epoch: 4 | 30144 / 114272 | training loss: 0.0010230491170659661\n",
      "epoch: 4 | 30176 / 114272 | training loss: 0.26184216141700745\n",
      "epoch: 4 | 30208 / 114272 | training loss: 0.0013599637895822525\n",
      "epoch: 4 | 30240 / 114272 | training loss: 0.009662159718573093\n",
      "epoch: 4 | 30272 / 114272 | training loss: 0.13225416839122772\n",
      "epoch: 4 | 30304 / 114272 | training loss: 0.0012733275070786476\n",
      "epoch: 4 | 30336 / 114272 | training loss: 0.004557029809802771\n",
      "epoch: 4 | 30368 / 114272 | training loss: 0.003320705145597458\n",
      "epoch: 4 | 30400 / 114272 | training loss: 0.12840208411216736\n",
      "epoch: 4 | 30432 / 114272 | training loss: 0.0062486850656569\n",
      "epoch: 4 | 30464 / 114272 | training loss: 0.0011320312041789293\n",
      "epoch: 4 | 30496 / 114272 | training loss: 0.11859956383705139\n",
      "epoch: 4 | 30528 / 114272 | training loss: 0.2200644165277481\n",
      "epoch: 4 | 30560 / 114272 | training loss: 0.0010836031287908554\n",
      "epoch: 4 | 30592 / 114272 | training loss: 0.0007503048982471228\n",
      "epoch: 4 | 30624 / 114272 | training loss: 0.0022492497228085995\n",
      "epoch: 4 | 30656 / 114272 | training loss: 0.003259996883571148\n",
      "epoch: 4 | 30688 / 114272 | training loss: 0.1561259776353836\n",
      "epoch: 4 | 30720 / 114272 | training loss: 0.13196933269500732\n",
      "epoch: 4 | 30752 / 114272 | training loss: 0.14418542385101318\n",
      "epoch: 4 | 30784 / 114272 | training loss: 0.0007272947113960981\n",
      "epoch: 4 | 30816 / 114272 | training loss: 0.005043982993811369\n",
      "epoch: 4 | 30848 / 114272 | training loss: 0.007999317720532417\n",
      "epoch: 4 | 30880 / 114272 | training loss: 0.001498070894740522\n",
      "epoch: 4 | 30912 / 114272 | training loss: 0.000977959018200636\n",
      "epoch: 4 | 30944 / 114272 | training loss: 0.0025034912396222353\n",
      "epoch: 4 | 30976 / 114272 | training loss: 0.0034938936587423086\n",
      "epoch: 4 | 31008 / 114272 | training loss: 0.0015024831518530846\n",
      "epoch: 4 | 31040 / 114272 | training loss: 0.012536229565739632\n",
      "epoch: 4 | 31072 / 114272 | training loss: 0.0017479229718446732\n",
      "epoch: 4 | 31104 / 114272 | training loss: 0.24463452398777008\n",
      "epoch: 4 | 31136 / 114272 | training loss: 0.058480676263570786\n",
      "epoch: 4 | 31168 / 114272 | training loss: 0.0014542352873831987\n",
      "epoch: 4 | 31200 / 114272 | training loss: 0.09272628277540207\n",
      "epoch: 4 | 31232 / 114272 | training loss: 0.0013958640629425645\n",
      "epoch: 4 | 31264 / 114272 | training loss: 0.062159206718206406\n",
      "epoch: 4 | 31296 / 114272 | training loss: 0.06948289275169373\n",
      "epoch: 4 | 31328 / 114272 | training loss: 0.015403490513563156\n",
      "epoch: 4 | 31360 / 114272 | training loss: 0.01500390749424696\n",
      "epoch: 4 | 31392 / 114272 | training loss: 0.01070930901914835\n",
      "epoch: 4 | 31424 / 114272 | training loss: 0.24708619713783264\n",
      "epoch: 4 | 31456 / 114272 | training loss: 0.10085127502679825\n",
      "epoch: 4 | 31488 / 114272 | training loss: 0.0016761539736762643\n",
      "epoch: 4 | 31520 / 114272 | training loss: 0.0021368474699556828\n",
      "epoch: 4 | 31552 / 114272 | training loss: 0.13898156583309174\n",
      "epoch: 4 | 31584 / 114272 | training loss: 0.0005640789167955518\n",
      "epoch: 4 | 31616 / 114272 | training loss: 0.004505230113863945\n",
      "epoch: 4 | 31648 / 114272 | training loss: 0.0009290110901929438\n",
      "epoch: 4 | 31680 / 114272 | training loss: 0.0016701032873243093\n",
      "epoch: 4 | 31712 / 114272 | training loss: 0.06395503133535385\n",
      "epoch: 4 | 31744 / 114272 | training loss: 0.0009920925367623568\n",
      "epoch: 4 | 31776 / 114272 | training loss: 0.0012377651873975992\n",
      "epoch: 4 | 31808 / 114272 | training loss: 0.004902399145066738\n",
      "epoch: 4 | 31840 / 114272 | training loss: 0.015733681619167328\n",
      "epoch: 4 | 31872 / 114272 | training loss: 0.00946585088968277\n",
      "epoch: 4 | 31904 / 114272 | training loss: 0.0018249348504468799\n",
      "epoch: 4 | 31936 / 114272 | training loss: 0.20379291474819183\n",
      "epoch: 4 | 31968 / 114272 | training loss: 0.009501018561422825\n",
      "epoch: 4 | 32000 / 114272 | training loss: 0.009851394221186638\n",
      "epoch: 4 | 32032 / 114272 | training loss: 0.2552696168422699\n",
      "epoch: 4 | 32064 / 114272 | training loss: 0.001641957787796855\n",
      "epoch: 4 | 32096 / 114272 | training loss: 0.06672629714012146\n",
      "epoch: 4 | 32128 / 114272 | training loss: 0.13164398074150085\n",
      "epoch: 4 | 32160 / 114272 | training loss: 0.3931356966495514\n",
      "epoch: 4 | 32192 / 114272 | training loss: 0.007106899283826351\n",
      "epoch: 4 | 32224 / 114272 | training loss: 0.0014173657400533557\n",
      "epoch: 4 | 32256 / 114272 | training loss: 0.003753027645871043\n",
      "epoch: 4 | 32288 / 114272 | training loss: 0.09940490871667862\n",
      "epoch: 4 | 32320 / 114272 | training loss: 0.002045112894847989\n",
      "epoch: 4 | 32352 / 114272 | training loss: 0.0029655396938323975\n",
      "epoch: 4 | 32384 / 114272 | training loss: 0.003062125062569976\n",
      "epoch: 4 | 32416 / 114272 | training loss: 0.12667731940746307\n",
      "epoch: 4 | 32448 / 114272 | training loss: 0.0884082093834877\n",
      "epoch: 4 | 32480 / 114272 | training loss: 0.09762383997440338\n",
      "epoch: 4 | 32512 / 114272 | training loss: 0.0784895047545433\n",
      "epoch: 4 | 32544 / 114272 | training loss: 0.1921674609184265\n",
      "epoch: 4 | 32576 / 114272 | training loss: 0.006945383735001087\n",
      "epoch: 4 | 32608 / 114272 | training loss: 0.07358766347169876\n",
      "epoch: 4 | 32640 / 114272 | training loss: 0.001455668592825532\n",
      "epoch: 4 | 32672 / 114272 | training loss: 0.001996940467506647\n",
      "epoch: 4 | 32704 / 114272 | training loss: 0.06249106302857399\n",
      "epoch: 4 | 32736 / 114272 | training loss: 0.047059930860996246\n",
      "epoch: 4 | 32768 / 114272 | training loss: 0.20003008842468262\n",
      "epoch: 4 | 32800 / 114272 | training loss: 0.002432107226923108\n",
      "epoch: 4 | 32832 / 114272 | training loss: 0.0028640846721827984\n",
      "epoch: 4 | 32864 / 114272 | training loss: 0.0012978940503671765\n",
      "epoch: 4 | 32896 / 114272 | training loss: 0.0015543859917670488\n",
      "epoch: 4 | 32928 / 114272 | training loss: 0.004406479187309742\n",
      "epoch: 4 | 32960 / 114272 | training loss: 0.0054919361136853695\n",
      "epoch: 4 | 32992 / 114272 | training loss: 0.0012944653863087296\n",
      "epoch: 4 | 33024 / 114272 | training loss: 0.06358897686004639\n",
      "epoch: 4 | 33056 / 114272 | training loss: 0.1848999708890915\n",
      "epoch: 4 | 33088 / 114272 | training loss: 0.0011112686479464173\n",
      "epoch: 4 | 33120 / 114272 | training loss: 0.0016921195201575756\n",
      "epoch: 4 | 33152 / 114272 | training loss: 0.0018807441229000688\n",
      "epoch: 4 | 33184 / 114272 | training loss: 0.24357153475284576\n",
      "epoch: 4 | 33216 / 114272 | training loss: 0.0020676206331700087\n",
      "epoch: 4 | 33248 / 114272 | training loss: 0.001149081508629024\n",
      "epoch: 4 | 33280 / 114272 | training loss: 0.001381638809107244\n",
      "epoch: 4 | 33312 / 114272 | training loss: 0.0018301390809938312\n",
      "epoch: 4 | 33344 / 114272 | training loss: 0.001497543533332646\n",
      "epoch: 4 | 33376 / 114272 | training loss: 0.0018669129349291325\n",
      "epoch: 4 | 33408 / 114272 | training loss: 0.004013902973383665\n",
      "epoch: 4 | 33440 / 114272 | training loss: 0.1053408607840538\n",
      "epoch: 4 | 33472 / 114272 | training loss: 0.01090911217033863\n",
      "epoch: 4 | 33504 / 114272 | training loss: 0.0036961655132472515\n",
      "epoch: 4 | 33536 / 114272 | training loss: 0.0014442342799156904\n",
      "epoch: 4 | 33568 / 114272 | training loss: 0.0009859908604994416\n",
      "epoch: 4 | 33600 / 114272 | training loss: 0.0830521285533905\n",
      "epoch: 4 | 33632 / 114272 | training loss: 0.003204797627404332\n",
      "epoch: 4 | 33664 / 114272 | training loss: 0.15371733903884888\n",
      "epoch: 4 | 33696 / 114272 | training loss: 0.003919949289411306\n",
      "epoch: 4 | 33728 / 114272 | training loss: 0.178786963224411\n",
      "epoch: 4 | 33760 / 114272 | training loss: 0.0027006815653294325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 33792 / 114272 | training loss: 0.008459623903036118\n",
      "epoch: 4 | 33824 / 114272 | training loss: 0.0021776272915303707\n",
      "epoch: 4 | 33856 / 114272 | training loss: 0.0017342959763482213\n",
      "epoch: 4 | 33888 / 114272 | training loss: 0.06355298310518265\n",
      "epoch: 4 | 33920 / 114272 | training loss: 0.018655410036444664\n",
      "epoch: 4 | 33952 / 114272 | training loss: 0.0021098873112350702\n",
      "epoch: 4 | 33984 / 114272 | training loss: 0.0014603903982788324\n",
      "epoch: 4 | 34016 / 114272 | training loss: 0.008410057984292507\n",
      "epoch: 4 | 34048 / 114272 | training loss: 0.10264745354652405\n",
      "epoch: 4 | 34080 / 114272 | training loss: 0.0013647022424265742\n",
      "epoch: 4 | 34112 / 114272 | training loss: 0.0013718756381422281\n",
      "epoch: 4 | 34144 / 114272 | training loss: 0.004074116237461567\n",
      "epoch: 4 | 34176 / 114272 | training loss: 0.0054660364985466\n",
      "epoch: 4 | 34208 / 114272 | training loss: 0.002518210792914033\n",
      "epoch: 4 | 34240 / 114272 | training loss: 0.01523090060800314\n",
      "epoch: 4 | 34272 / 114272 | training loss: 0.00949191115796566\n",
      "epoch: 4 | 34304 / 114272 | training loss: 0.002156376838684082\n",
      "epoch: 4 | 34336 / 114272 | training loss: 0.1340317726135254\n",
      "epoch: 4 | 34368 / 114272 | training loss: 0.003602738957852125\n",
      "epoch: 4 | 34400 / 114272 | training loss: 0.1815403252840042\n",
      "epoch: 4 | 34432 / 114272 | training loss: 0.19544892013072968\n",
      "epoch: 4 | 34464 / 114272 | training loss: 0.0025871507823467255\n",
      "epoch: 4 | 34496 / 114272 | training loss: 0.1468551903963089\n",
      "epoch: 4 | 34528 / 114272 | training loss: 0.002398807555437088\n",
      "epoch: 4 | 34560 / 114272 | training loss: 0.0021125979255884886\n",
      "epoch: 4 | 34592 / 114272 | training loss: 0.007272742222994566\n",
      "epoch: 4 | 34624 / 114272 | training loss: 0.08667638897895813\n",
      "epoch: 4 | 34656 / 114272 | training loss: 0.015266258269548416\n",
      "epoch: 4 | 34688 / 114272 | training loss: 0.002415951807051897\n",
      "epoch: 4 | 34720 / 114272 | training loss: 0.12504665553569794\n",
      "epoch: 4 | 34752 / 114272 | training loss: 0.004240076057612896\n",
      "epoch: 4 | 34784 / 114272 | training loss: 0.008474851958453655\n",
      "epoch: 4 | 34816 / 114272 | training loss: 0.06378431618213654\n",
      "epoch: 4 | 34848 / 114272 | training loss: 0.23200879991054535\n",
      "epoch: 4 | 34880 / 114272 | training loss: 0.08701524138450623\n",
      "epoch: 4 | 34912 / 114272 | training loss: 0.008203130215406418\n",
      "epoch: 4 | 34944 / 114272 | training loss: 0.007420825771987438\n",
      "epoch: 4 | 34976 / 114272 | training loss: 0.0022528537083417177\n",
      "epoch: 4 | 35008 / 114272 | training loss: 0.008805928751826286\n",
      "epoch: 4 | 35040 / 114272 | training loss: 0.10879679024219513\n",
      "epoch: 4 | 35072 / 114272 | training loss: 0.003644447075203061\n",
      "epoch: 4 | 35104 / 114272 | training loss: 0.013968152925372124\n",
      "epoch: 4 | 35136 / 114272 | training loss: 0.007347268983721733\n",
      "epoch: 4 | 35168 / 114272 | training loss: 0.07714106887578964\n",
      "epoch: 4 | 35200 / 114272 | training loss: 0.002715814160183072\n",
      "epoch: 4 | 35232 / 114272 | training loss: 0.0016534485621377826\n",
      "epoch: 4 | 35264 / 114272 | training loss: 0.0017639374127611518\n",
      "epoch: 4 | 35296 / 114272 | training loss: 0.0022551536094397306\n",
      "epoch: 4 | 35328 / 114272 | training loss: 0.003600009251385927\n",
      "epoch: 4 | 35360 / 114272 | training loss: 0.2651994824409485\n",
      "epoch: 4 | 35392 / 114272 | training loss: 0.22535325586795807\n",
      "epoch: 4 | 35424 / 114272 | training loss: 0.001018774462863803\n",
      "epoch: 4 | 35456 / 114272 | training loss: 0.0021609144750982523\n",
      "epoch: 4 | 35488 / 114272 | training loss: 0.0023666024208068848\n",
      "epoch: 4 | 35520 / 114272 | training loss: 0.010198187083005905\n",
      "epoch: 4 | 35552 / 114272 | training loss: 0.004501575604081154\n",
      "epoch: 4 | 35584 / 114272 | training loss: 0.1623155027627945\n",
      "epoch: 4 | 35616 / 114272 | training loss: 0.002741806907579303\n",
      "epoch: 4 | 35648 / 114272 | training loss: 0.17878961563110352\n",
      "epoch: 4 | 35680 / 114272 | training loss: 0.04425247386097908\n",
      "epoch: 4 | 35712 / 114272 | training loss: 0.001941317692399025\n",
      "epoch: 4 | 35744 / 114272 | training loss: 0.0012384706642478704\n",
      "epoch: 4 | 35776 / 114272 | training loss: 0.003983157686889172\n",
      "epoch: 4 | 35808 / 114272 | training loss: 0.0030876570381224155\n",
      "epoch: 4 | 35840 / 114272 | training loss: 0.05197858065366745\n",
      "epoch: 4 | 35872 / 114272 | training loss: 0.0012990864925086498\n",
      "epoch: 4 | 35904 / 114272 | training loss: 0.0010791212553158402\n",
      "epoch: 4 | 35936 / 114272 | training loss: 0.0016543101519346237\n",
      "epoch: 4 | 35968 / 114272 | training loss: 0.01004865113645792\n",
      "epoch: 4 | 36000 / 114272 | training loss: 0.0034394587855786085\n",
      "epoch: 4 | 36032 / 114272 | training loss: 0.0011049278546124697\n",
      "epoch: 4 | 36064 / 114272 | training loss: 0.0009670836734585464\n",
      "epoch: 4 | 36096 / 114272 | training loss: 0.001131037250161171\n",
      "epoch: 4 | 36128 / 114272 | training loss: 0.1678275614976883\n",
      "epoch: 4 | 36160 / 114272 | training loss: 0.011927515268325806\n",
      "epoch: 4 | 36192 / 114272 | training loss: 0.2760618329048157\n",
      "epoch: 4 | 36224 / 114272 | training loss: 0.002683119848370552\n",
      "epoch: 4 | 36256 / 114272 | training loss: 0.0018437632825225592\n",
      "epoch: 4 | 36288 / 114272 | training loss: 0.0011385318357497454\n",
      "epoch: 4 | 36320 / 114272 | training loss: 0.06270799040794373\n",
      "epoch: 4 | 36352 / 114272 | training loss: 0.004798580426722765\n",
      "epoch: 4 | 36384 / 114272 | training loss: 0.10936371982097626\n",
      "epoch: 4 | 36416 / 114272 | training loss: 0.15838287770748138\n",
      "epoch: 4 | 36448 / 114272 | training loss: 0.005853841546922922\n",
      "epoch: 4 | 36480 / 114272 | training loss: 0.11095141619443893\n",
      "epoch: 4 | 36512 / 114272 | training loss: 0.10746701061725616\n",
      "epoch: 4 | 36544 / 114272 | training loss: 0.0013670892221853137\n",
      "epoch: 4 | 36576 / 114272 | training loss: 0.0015004132874310017\n",
      "epoch: 4 | 36608 / 114272 | training loss: 0.00792091153562069\n",
      "epoch: 4 | 36640 / 114272 | training loss: 0.004341846331954002\n",
      "epoch: 4 | 36672 / 114272 | training loss: 0.0020744483917951584\n",
      "epoch: 4 | 36704 / 114272 | training loss: 0.00895115826278925\n",
      "epoch: 4 | 36736 / 114272 | training loss: 0.06285621970891953\n",
      "epoch: 4 | 36768 / 114272 | training loss: 0.017528554424643517\n",
      "epoch: 4 | 36800 / 114272 | training loss: 0.003830100642517209\n",
      "epoch: 4 | 36832 / 114272 | training loss: 0.15600119531154633\n",
      "epoch: 4 | 36864 / 114272 | training loss: 0.0041106706485152245\n",
      "epoch: 4 | 36896 / 114272 | training loss: 0.04129622504115105\n",
      "epoch: 4 | 36928 / 114272 | training loss: 0.001028559054248035\n",
      "epoch: 4 | 36960 / 114272 | training loss: 0.12903346121311188\n",
      "epoch: 4 | 36992 / 114272 | training loss: 0.1751307100057602\n",
      "epoch: 4 | 37024 / 114272 | training loss: 0.17745791375637054\n",
      "epoch: 4 | 37056 / 114272 | training loss: 0.0013654088834300637\n",
      "epoch: 4 | 37088 / 114272 | training loss: 0.0030612635891884565\n",
      "epoch: 4 | 37120 / 114272 | training loss: 0.001898343674838543\n",
      "epoch: 4 | 37152 / 114272 | training loss: 0.005708683747798204\n",
      "epoch: 4 | 37184 / 114272 | training loss: 0.11598975956439972\n",
      "epoch: 4 | 37216 / 114272 | training loss: 0.1234024167060852\n",
      "epoch: 4 | 37248 / 114272 | training loss: 0.002928470727056265\n",
      "epoch: 4 | 37280 / 114272 | training loss: 0.0013453545980155468\n",
      "epoch: 4 | 37312 / 114272 | training loss: 0.00897718220949173\n",
      "epoch: 4 | 37344 / 114272 | training loss: 0.00617569824680686\n",
      "epoch: 4 | 37376 / 114272 | training loss: 0.0073425075970590115\n",
      "epoch: 4 | 37408 / 114272 | training loss: 0.002875454258173704\n",
      "epoch: 4 | 37440 / 114272 | training loss: 0.0012270306469872594\n",
      "epoch: 4 | 37472 / 114272 | training loss: 0.004716028459370136\n",
      "epoch: 4 | 37504 / 114272 | training loss: 0.00253197830170393\n",
      "epoch: 4 | 37536 / 114272 | training loss: 0.18469387292861938\n",
      "epoch: 4 | 37568 / 114272 | training loss: 0.00596056180074811\n",
      "epoch: 4 | 37600 / 114272 | training loss: 0.00225743162445724\n",
      "epoch: 4 | 37632 / 114272 | training loss: 0.10231862962245941\n",
      "epoch: 4 | 37664 / 114272 | training loss: 0.09869074076414108\n",
      "epoch: 4 | 37696 / 114272 | training loss: 0.0023017977364361286\n",
      "epoch: 4 | 37728 / 114272 | training loss: 0.0020687198266386986\n",
      "epoch: 4 | 37760 / 114272 | training loss: 0.0891956239938736\n",
      "epoch: 4 | 37792 / 114272 | training loss: 0.0013127630809322\n",
      "epoch: 4 | 37824 / 114272 | training loss: 0.0240834541618824\n",
      "epoch: 4 | 37856 / 114272 | training loss: 0.00403872225433588\n",
      "epoch: 4 | 37888 / 114272 | training loss: 0.002775064902380109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 37920 / 114272 | training loss: 0.0012702177045866847\n",
      "epoch: 4 | 37952 / 114272 | training loss: 0.0010550008155405521\n",
      "epoch: 4 | 37984 / 114272 | training loss: 0.10373316705226898\n",
      "epoch: 4 | 38016 / 114272 | training loss: 0.02885715290904045\n",
      "epoch: 4 | 38048 / 114272 | training loss: 0.0029686943162232637\n",
      "epoch: 4 | 38080 / 114272 | training loss: 0.0016722093569114804\n",
      "epoch: 4 | 38112 / 114272 | training loss: 0.0015498663997277617\n",
      "epoch: 4 | 38144 / 114272 | training loss: 0.002269239630550146\n",
      "epoch: 4 | 38176 / 114272 | training loss: 0.0010970390867441893\n",
      "epoch: 4 | 38208 / 114272 | training loss: 0.0011373840970918536\n",
      "epoch: 4 | 38240 / 114272 | training loss: 0.001577527611516416\n",
      "epoch: 4 | 38272 / 114272 | training loss: 0.09294398128986359\n",
      "epoch: 4 | 38304 / 114272 | training loss: 0.0010332450037822127\n",
      "epoch: 4 | 38336 / 114272 | training loss: 0.15111620724201202\n",
      "epoch: 4 | 38368 / 114272 | training loss: 0.13177752494812012\n",
      "epoch: 4 | 38400 / 114272 | training loss: 0.0013994319597259164\n",
      "epoch: 4 | 38432 / 114272 | training loss: 0.001541032688692212\n",
      "epoch: 4 | 38464 / 114272 | training loss: 0.15954342484474182\n",
      "epoch: 4 | 38496 / 114272 | training loss: 0.0015367674641311169\n",
      "epoch: 4 | 38528 / 114272 | training loss: 0.0015914668329060078\n",
      "epoch: 4 | 38560 / 114272 | training loss: 0.0008337536128237844\n",
      "epoch: 4 | 38592 / 114272 | training loss: 0.00293062929995358\n",
      "epoch: 4 | 38624 / 114272 | training loss: 0.003239477053284645\n",
      "epoch: 4 | 38656 / 114272 | training loss: 0.017583077773451805\n",
      "epoch: 4 | 38688 / 114272 | training loss: 0.009391502477228642\n",
      "epoch: 4 | 38720 / 114272 | training loss: 0.11221010982990265\n",
      "epoch: 4 | 38752 / 114272 | training loss: 0.0012224300298839808\n",
      "epoch: 4 | 38784 / 114272 | training loss: 0.18723852932453156\n",
      "epoch: 4 | 38816 / 114272 | training loss: 0.0010508473496884108\n",
      "epoch: 4 | 38848 / 114272 | training loss: 0.002096446231007576\n",
      "epoch: 4 | 38880 / 114272 | training loss: 0.0050576659850776196\n",
      "epoch: 4 | 38912 / 114272 | training loss: 0.0018615268636494875\n",
      "epoch: 4 | 38944 / 114272 | training loss: 0.0268089696764946\n",
      "epoch: 4 | 38976 / 114272 | training loss: 0.002672541420906782\n",
      "epoch: 4 | 39008 / 114272 | training loss: 0.004028751514852047\n",
      "epoch: 4 | 39040 / 114272 | training loss: 0.0011784011730924249\n",
      "epoch: 4 | 39072 / 114272 | training loss: 0.0431785024702549\n",
      "epoch: 4 | 39104 / 114272 | training loss: 0.0528150238096714\n",
      "epoch: 4 | 39136 / 114272 | training loss: 0.001143828616477549\n",
      "epoch: 4 | 39168 / 114272 | training loss: 0.002554117701947689\n",
      "epoch: 4 | 39200 / 114272 | training loss: 0.0017381710931658745\n",
      "epoch: 4 | 39232 / 114272 | training loss: 0.0018208445981144905\n",
      "epoch: 4 | 39264 / 114272 | training loss: 0.004368156660348177\n",
      "epoch: 4 | 39296 / 114272 | training loss: 0.1558457762002945\n",
      "epoch: 4 | 39328 / 114272 | training loss: 0.012801120057702065\n",
      "epoch: 4 | 39360 / 114272 | training loss: 0.0013146292185410857\n",
      "epoch: 4 | 39392 / 114272 | training loss: 0.005444987677037716\n",
      "epoch: 4 | 39424 / 114272 | training loss: 0.003378201276063919\n",
      "epoch: 4 | 39456 / 114272 | training loss: 0.008887151256203651\n",
      "epoch: 4 | 39488 / 114272 | training loss: 0.20152559876441956\n",
      "epoch: 4 | 39520 / 114272 | training loss: 0.005546132102608681\n",
      "epoch: 4 | 39552 / 114272 | training loss: 0.0011835332261398435\n",
      "epoch: 4 | 39584 / 114272 | training loss: 0.09411194920539856\n",
      "epoch: 4 | 39616 / 114272 | training loss: 0.044967688620090485\n",
      "epoch: 4 | 39648 / 114272 | training loss: 0.0038681745063513517\n",
      "epoch: 4 | 39680 / 114272 | training loss: 0.0028128621634095907\n",
      "epoch: 4 | 39712 / 114272 | training loss: 0.0019519879715517163\n",
      "epoch: 4 | 39744 / 114272 | training loss: 0.002030174247920513\n",
      "epoch: 4 | 39776 / 114272 | training loss: 0.0022075443994253874\n",
      "epoch: 4 | 39808 / 114272 | training loss: 0.002366623841226101\n",
      "epoch: 4 | 39840 / 114272 | training loss: 0.002091209637001157\n",
      "epoch: 4 | 39872 / 114272 | training loss: 0.0008748098043724895\n",
      "epoch: 4 | 39904 / 114272 | training loss: 0.00121393078006804\n",
      "epoch: 4 | 39936 / 114272 | training loss: 0.0732872486114502\n",
      "epoch: 4 | 39968 / 114272 | training loss: 0.003397952066734433\n",
      "epoch: 4 | 40000 / 114272 | training loss: 0.0016537964111194015\n",
      "epoch: 4 | 40032 / 114272 | training loss: 0.20538096129894257\n",
      "epoch: 4 | 40064 / 114272 | training loss: 0.0019097712356597185\n",
      "epoch: 4 | 40096 / 114272 | training loss: 0.0006593665457330644\n",
      "epoch: 4 | 40128 / 114272 | training loss: 0.00043226208072155714\n",
      "epoch: 4 | 40160 / 114272 | training loss: 0.24724990129470825\n",
      "epoch: 4 | 40192 / 114272 | training loss: 0.0017814550083130598\n",
      "epoch: 4 | 40224 / 114272 | training loss: 0.0008266792865470052\n",
      "epoch: 4 | 40256 / 114272 | training loss: 0.0017798265907913446\n",
      "epoch: 4 | 40288 / 114272 | training loss: 0.017854928970336914\n",
      "epoch: 4 | 40320 / 114272 | training loss: 0.005370153579860926\n",
      "epoch: 4 | 40352 / 114272 | training loss: 0.0016737012192606926\n",
      "epoch: 4 | 40384 / 114272 | training loss: 0.006420923862606287\n",
      "epoch: 4 | 40416 / 114272 | training loss: 0.004147227853536606\n",
      "epoch: 4 | 40448 / 114272 | training loss: 0.004241803660988808\n",
      "epoch: 4 | 40480 / 114272 | training loss: 0.08099675923585892\n",
      "epoch: 4 | 40512 / 114272 | training loss: 0.07330577075481415\n",
      "epoch: 4 | 40544 / 114272 | training loss: 0.0008608835050836205\n",
      "epoch: 4 | 40576 / 114272 | training loss: 0.0010358471190556884\n",
      "epoch: 4 | 40608 / 114272 | training loss: 0.0012126548681408167\n",
      "epoch: 4 | 40640 / 114272 | training loss: 0.23630894720554352\n",
      "epoch: 4 | 40672 / 114272 | training loss: 0.09161731600761414\n",
      "epoch: 4 | 40704 / 114272 | training loss: 0.003461829386651516\n",
      "epoch: 4 | 40736 / 114272 | training loss: 0.08943337202072144\n",
      "epoch: 4 | 40768 / 114272 | training loss: 0.003956194967031479\n",
      "epoch: 4 | 40800 / 114272 | training loss: 0.003076177788898349\n",
      "epoch: 4 | 40832 / 114272 | training loss: 0.002068138215690851\n",
      "epoch: 4 | 40864 / 114272 | training loss: 0.007414241321384907\n",
      "epoch: 4 | 40896 / 114272 | training loss: 0.0012606444070115685\n",
      "epoch: 4 | 40928 / 114272 | training loss: 0.09553390741348267\n",
      "epoch: 4 | 40960 / 114272 | training loss: 0.0015490378718823195\n",
      "epoch: 4 | 40992 / 114272 | training loss: 0.004344286397099495\n",
      "epoch: 4 | 41024 / 114272 | training loss: 0.0012211116263642907\n",
      "epoch: 4 | 41056 / 114272 | training loss: 0.2404077798128128\n",
      "epoch: 4 | 41088 / 114272 | training loss: 0.07694235444068909\n",
      "epoch: 4 | 41120 / 114272 | training loss: 0.007725866511464119\n",
      "epoch: 4 | 41152 / 114272 | training loss: 0.0019925199449062347\n",
      "epoch: 4 | 41184 / 114272 | training loss: 0.00234253890812397\n",
      "epoch: 4 | 41216 / 114272 | training loss: 0.006036757491528988\n",
      "epoch: 4 | 41248 / 114272 | training loss: 0.37659987807273865\n",
      "epoch: 4 | 41280 / 114272 | training loss: 0.0007837120210751891\n",
      "epoch: 4 | 41312 / 114272 | training loss: 0.23928974568843842\n",
      "epoch: 4 | 41344 / 114272 | training loss: 0.14783379435539246\n",
      "epoch: 4 | 41376 / 114272 | training loss: 0.0009750367025844753\n",
      "epoch: 4 | 41408 / 114272 | training loss: 0.00801535788923502\n",
      "epoch: 4 | 41440 / 114272 | training loss: 0.0940055251121521\n",
      "epoch: 4 | 41472 / 114272 | training loss: 0.001102396403439343\n",
      "epoch: 4 | 41504 / 114272 | training loss: 0.0955652967095375\n",
      "epoch: 4 | 41536 / 114272 | training loss: 0.006549084559082985\n",
      "epoch: 4 | 41568 / 114272 | training loss: 0.03452986106276512\n",
      "epoch: 4 | 41600 / 114272 | training loss: 0.0015676092589274049\n",
      "epoch: 4 | 41632 / 114272 | training loss: 0.002491260413080454\n",
      "epoch: 4 | 41664 / 114272 | training loss: 0.0010974362958222628\n",
      "epoch: 4 | 41696 / 114272 | training loss: 0.001309398328885436\n",
      "epoch: 4 | 41728 / 114272 | training loss: 0.0021814322099089622\n",
      "epoch: 4 | 41760 / 114272 | training loss: 0.16750110685825348\n",
      "epoch: 4 | 41792 / 114272 | training loss: 0.0023416432086378336\n",
      "epoch: 4 | 41824 / 114272 | training loss: 0.3067456781864166\n",
      "epoch: 4 | 41856 / 114272 | training loss: 0.0020702944602817297\n",
      "epoch: 4 | 41888 / 114272 | training loss: 0.09542984515428543\n",
      "epoch: 4 | 41920 / 114272 | training loss: 0.0015448137419298291\n",
      "epoch: 4 | 41952 / 114272 | training loss: 0.0032949005253612995\n",
      "epoch: 4 | 41984 / 114272 | training loss: 0.12911199033260345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 42016 / 114272 | training loss: 0.0017211523372679949\n",
      "epoch: 4 | 42048 / 114272 | training loss: 0.004322404507547617\n",
      "epoch: 4 | 42080 / 114272 | training loss: 0.17847798764705658\n",
      "epoch: 4 | 42112 / 114272 | training loss: 0.05760917440056801\n",
      "epoch: 4 | 42144 / 114272 | training loss: 0.0023201610893011093\n",
      "epoch: 4 | 42176 / 114272 | training loss: 0.0024246193934231997\n",
      "epoch: 4 | 42208 / 114272 | training loss: 0.18800069391727448\n",
      "epoch: 4 | 42240 / 114272 | training loss: 0.0074380445294082165\n",
      "epoch: 4 | 42272 / 114272 | training loss: 0.00769128929823637\n",
      "epoch: 4 | 42304 / 114272 | training loss: 0.0017414900939911604\n",
      "epoch: 4 | 42336 / 114272 | training loss: 0.22212976217269897\n",
      "epoch: 4 | 42368 / 114272 | training loss: 0.0012153350980952382\n",
      "epoch: 4 | 42400 / 114272 | training loss: 0.0024061535950750113\n",
      "epoch: 4 | 42432 / 114272 | training loss: 0.0011382171651348472\n",
      "epoch: 4 | 42464 / 114272 | training loss: 0.005564900580793619\n",
      "epoch: 4 | 42496 / 114272 | training loss: 0.12321630120277405\n",
      "epoch: 4 | 42528 / 114272 | training loss: 0.001933618332259357\n",
      "epoch: 4 | 42560 / 114272 | training loss: 0.17952556908130646\n",
      "epoch: 4 | 42592 / 114272 | training loss: 0.008328762836754322\n",
      "epoch: 4 | 42624 / 114272 | training loss: 0.0009928483050316572\n",
      "epoch: 4 | 42656 / 114272 | training loss: 0.0020598811097443104\n",
      "epoch: 4 | 42688 / 114272 | training loss: 0.02196452021598816\n",
      "epoch: 4 | 42720 / 114272 | training loss: 0.039585065096616745\n",
      "epoch: 4 | 42752 / 114272 | training loss: 0.019039228558540344\n",
      "epoch: 4 | 42784 / 114272 | training loss: 0.002512906678020954\n",
      "epoch: 4 | 42816 / 114272 | training loss: 0.0025446591898798943\n",
      "epoch: 4 | 42848 / 114272 | training loss: 0.001898868940770626\n",
      "epoch: 4 | 42880 / 114272 | training loss: 0.0029926218558102846\n",
      "epoch: 4 | 42912 / 114272 | training loss: 0.0035495725460350513\n",
      "epoch: 4 | 42944 / 114272 | training loss: 0.0017902167746797204\n",
      "epoch: 4 | 42976 / 114272 | training loss: 0.03642655536532402\n",
      "epoch: 4 | 43008 / 114272 | training loss: 0.002623665379360318\n",
      "epoch: 4 | 43040 / 114272 | training loss: 0.002408175030723214\n",
      "epoch: 4 | 43072 / 114272 | training loss: 0.0027188395615667105\n",
      "epoch: 4 | 43104 / 114272 | training loss: 0.0038523469120264053\n",
      "epoch: 4 | 43136 / 114272 | training loss: 0.25780966877937317\n",
      "epoch: 4 | 43168 / 114272 | training loss: 0.0013244831934571266\n",
      "epoch: 4 | 43200 / 114272 | training loss: 0.00226755253970623\n",
      "epoch: 4 | 43232 / 114272 | training loss: 0.010245095938444138\n",
      "epoch: 4 | 43264 / 114272 | training loss: 0.003236766904592514\n",
      "epoch: 4 | 43296 / 114272 | training loss: 0.0008800958748906851\n",
      "epoch: 4 | 43328 / 114272 | training loss: 0.0021105287596583366\n",
      "epoch: 4 | 43360 / 114272 | training loss: 0.0021591070108115673\n",
      "epoch: 4 | 43392 / 114272 | training loss: 0.0009853204246610403\n",
      "epoch: 4 | 43424 / 114272 | training loss: 0.0015665487153455615\n",
      "epoch: 4 | 43456 / 114272 | training loss: 0.0013863123022019863\n",
      "epoch: 4 | 43488 / 114272 | training loss: 0.0011164713650941849\n",
      "epoch: 4 | 43520 / 114272 | training loss: 0.0009117107838392258\n",
      "epoch: 4 | 43552 / 114272 | training loss: 0.2663714587688446\n",
      "epoch: 4 | 43584 / 114272 | training loss: 0.16336777806282043\n",
      "epoch: 4 | 43616 / 114272 | training loss: 0.17631688714027405\n",
      "epoch: 4 | 43648 / 114272 | training loss: 0.008012698031961918\n",
      "epoch: 4 | 43680 / 114272 | training loss: 0.06120920553803444\n",
      "epoch: 4 | 43712 / 114272 | training loss: 0.001378325978294015\n",
      "epoch: 4 | 43744 / 114272 | training loss: 0.003113021142780781\n",
      "epoch: 4 | 43776 / 114272 | training loss: 0.0010579138761386275\n",
      "epoch: 4 | 43808 / 114272 | training loss: 0.15687118470668793\n",
      "epoch: 4 | 43840 / 114272 | training loss: 0.0021314467303454876\n",
      "epoch: 4 | 43872 / 114272 | training loss: 0.06596431881189346\n",
      "epoch: 4 | 43904 / 114272 | training loss: 0.10985315591096878\n",
      "epoch: 4 | 43936 / 114272 | training loss: 0.006475560367107391\n",
      "epoch: 4 | 43968 / 114272 | training loss: 0.0023393782321363688\n",
      "epoch: 4 | 44000 / 114272 | training loss: 0.008255614899098873\n",
      "epoch: 4 | 44032 / 114272 | training loss: 0.005034390836954117\n",
      "epoch: 4 | 44064 / 114272 | training loss: 0.003406003350391984\n",
      "epoch: 4 | 44096 / 114272 | training loss: 0.11016924679279327\n",
      "epoch: 4 | 44128 / 114272 | training loss: 0.0022693376522511244\n",
      "epoch: 4 | 44160 / 114272 | training loss: 0.003936737310141325\n",
      "epoch: 4 | 44192 / 114272 | training loss: 0.0013873546849936247\n",
      "epoch: 4 | 44224 / 114272 | training loss: 0.0018638384062796831\n",
      "epoch: 4 | 44256 / 114272 | training loss: 0.09572496265172958\n",
      "epoch: 4 | 44288 / 114272 | training loss: 0.001217372715473175\n",
      "epoch: 4 | 44320 / 114272 | training loss: 0.0023937236983329058\n",
      "epoch: 4 | 44352 / 114272 | training loss: 0.0031600610818713903\n",
      "epoch: 4 | 44384 / 114272 | training loss: 0.11919011175632477\n",
      "epoch: 4 | 44416 / 114272 | training loss: 0.06198448687791824\n",
      "epoch: 4 | 44448 / 114272 | training loss: 0.0030535326804965734\n",
      "epoch: 4 | 44480 / 114272 | training loss: 0.0415644533932209\n",
      "epoch: 4 | 44512 / 114272 | training loss: 0.001345184282399714\n",
      "epoch: 4 | 44544 / 114272 | training loss: 0.0015224312664940953\n",
      "epoch: 4 | 44576 / 114272 | training loss: 0.0510697141289711\n",
      "epoch: 4 | 44608 / 114272 | training loss: 0.002068258123472333\n",
      "epoch: 4 | 44640 / 114272 | training loss: 0.10226863622665405\n",
      "epoch: 4 | 44672 / 114272 | training loss: 0.019065797328948975\n",
      "epoch: 4 | 44704 / 114272 | training loss: 0.30818408727645874\n",
      "epoch: 4 | 44736 / 114272 | training loss: 0.0036937990225851536\n",
      "epoch: 4 | 44768 / 114272 | training loss: 0.08062613010406494\n",
      "epoch: 4 | 44800 / 114272 | training loss: 0.08306272327899933\n",
      "epoch: 4 | 44832 / 114272 | training loss: 0.003107941010966897\n",
      "epoch: 4 | 44864 / 114272 | training loss: 0.005112072918564081\n",
      "epoch: 4 | 44896 / 114272 | training loss: 0.09808681160211563\n",
      "epoch: 4 | 44928 / 114272 | training loss: 0.009939247742295265\n",
      "epoch: 4 | 44960 / 114272 | training loss: 0.19720610976219177\n",
      "epoch: 4 | 44992 / 114272 | training loss: 0.0019991942681372166\n",
      "epoch: 4 | 45024 / 114272 | training loss: 0.0025302069261670113\n",
      "epoch: 4 | 45056 / 114272 | training loss: 0.006494698114693165\n",
      "epoch: 4 | 45088 / 114272 | training loss: 0.0021566739305853844\n",
      "epoch: 4 | 45120 / 114272 | training loss: 0.003855881281197071\n",
      "epoch: 4 | 45152 / 114272 | training loss: 0.136595219373703\n",
      "epoch: 4 | 45184 / 114272 | training loss: 0.003294636495411396\n",
      "epoch: 4 | 45216 / 114272 | training loss: 0.0015837937826290727\n",
      "epoch: 4 | 45248 / 114272 | training loss: 0.002138506853953004\n",
      "epoch: 4 | 45280 / 114272 | training loss: 0.08895520120859146\n",
      "epoch: 4 | 45312 / 114272 | training loss: 0.005674412939697504\n",
      "epoch: 4 | 45344 / 114272 | training loss: 0.002000647597014904\n",
      "epoch: 4 | 45376 / 114272 | training loss: 0.00250413385219872\n",
      "epoch: 4 | 45408 / 114272 | training loss: 0.0021869344636797905\n",
      "epoch: 4 | 45440 / 114272 | training loss: 0.004957228899002075\n",
      "epoch: 4 | 45472 / 114272 | training loss: 0.11004127562046051\n",
      "epoch: 4 | 45504 / 114272 | training loss: 0.006724176928400993\n",
      "epoch: 4 | 45536 / 114272 | training loss: 0.11867769807577133\n",
      "epoch: 4 | 45568 / 114272 | training loss: 0.004911733325570822\n",
      "epoch: 4 | 45600 / 114272 | training loss: 0.003069535130634904\n",
      "epoch: 4 | 45632 / 114272 | training loss: 0.10272355377674103\n",
      "epoch: 4 | 45664 / 114272 | training loss: 0.003842089558020234\n",
      "epoch: 4 | 45696 / 114272 | training loss: 0.010255303233861923\n",
      "epoch: 4 | 45728 / 114272 | training loss: 0.40655678510665894\n",
      "epoch: 4 | 45760 / 114272 | training loss: 0.08037275820970535\n",
      "epoch: 4 | 45792 / 114272 | training loss: 0.00470813037827611\n",
      "epoch: 4 | 45824 / 114272 | training loss: 0.13367393612861633\n",
      "epoch: 4 | 45856 / 114272 | training loss: 0.001840829849243164\n",
      "epoch: 4 | 45888 / 114272 | training loss: 0.13735511898994446\n",
      "epoch: 4 | 45920 / 114272 | training loss: 0.002008887007832527\n",
      "epoch: 4 | 45952 / 114272 | training loss: 0.001733318786136806\n",
      "epoch: 4 | 45984 / 114272 | training loss: 0.0031236412469297647\n",
      "epoch: 4 | 46016 / 114272 | training loss: 0.008406532928347588\n",
      "epoch: 4 | 46048 / 114272 | training loss: 0.0026210390496999025\n",
      "epoch: 4 | 46080 / 114272 | training loss: 0.0039449031464755535\n",
      "epoch: 4 | 46112 / 114272 | training loss: 0.019080182537436485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 46144 / 114272 | training loss: 0.003133282996714115\n",
      "epoch: 4 | 46176 / 114272 | training loss: 0.018627064302563667\n",
      "epoch: 4 | 46208 / 114272 | training loss: 0.003945615608245134\n",
      "epoch: 4 | 46240 / 114272 | training loss: 0.22567279636859894\n",
      "epoch: 4 | 46272 / 114272 | training loss: 0.17123253643512726\n",
      "epoch: 4 | 46304 / 114272 | training loss: 0.0031967160757631063\n",
      "epoch: 4 | 46336 / 114272 | training loss: 0.08923265337944031\n",
      "epoch: 4 | 46368 / 114272 | training loss: 0.002703804522752762\n",
      "epoch: 4 | 46400 / 114272 | training loss: 0.01440523099154234\n",
      "epoch: 4 | 46432 / 114272 | training loss: 0.003333961358293891\n",
      "epoch: 4 | 46464 / 114272 | training loss: 0.005033585708588362\n",
      "epoch: 4 | 46496 / 114272 | training loss: 0.0039512235671281815\n",
      "epoch: 4 | 46528 / 114272 | training loss: 0.004602618049830198\n",
      "epoch: 4 | 46560 / 114272 | training loss: 0.0013886707602068782\n",
      "epoch: 4 | 46592 / 114272 | training loss: 0.005295814014971256\n",
      "epoch: 4 | 46624 / 114272 | training loss: 0.0019652042537927628\n",
      "epoch: 4 | 46656 / 114272 | training loss: 0.005267359782010317\n",
      "epoch: 4 | 46688 / 114272 | training loss: 0.002658874960616231\n",
      "epoch: 4 | 46720 / 114272 | training loss: 0.002754113869741559\n",
      "epoch: 4 | 46752 / 114272 | training loss: 0.004579351749271154\n",
      "epoch: 4 | 46784 / 114272 | training loss: 0.1609276533126831\n",
      "epoch: 4 | 46816 / 114272 | training loss: 0.001624185941182077\n",
      "epoch: 4 | 46848 / 114272 | training loss: 0.07001985609531403\n",
      "epoch: 4 | 46880 / 114272 | training loss: 0.43338122963905334\n",
      "epoch: 4 | 46912 / 114272 | training loss: 0.11375470459461212\n",
      "epoch: 4 | 46944 / 114272 | training loss: 0.0037946482188999653\n",
      "epoch: 4 | 46976 / 114272 | training loss: 0.006363042164593935\n",
      "epoch: 4 | 47008 / 114272 | training loss: 0.002002011751756072\n",
      "epoch: 4 | 47040 / 114272 | training loss: 0.0013299088459461927\n",
      "epoch: 4 | 47072 / 114272 | training loss: 0.0024165972135961056\n",
      "epoch: 4 | 47104 / 114272 | training loss: 0.0037012796383351088\n",
      "epoch: 4 | 47136 / 114272 | training loss: 0.024630945175886154\n",
      "epoch: 4 | 47168 / 114272 | training loss: 0.12872330844402313\n",
      "epoch: 4 | 47200 / 114272 | training loss: 0.003647235222160816\n",
      "epoch: 4 | 47232 / 114272 | training loss: 0.1329602301120758\n",
      "epoch: 4 | 47264 / 114272 | training loss: 0.1557788848876953\n",
      "epoch: 4 | 47296 / 114272 | training loss: 0.0062240599654614925\n",
      "epoch: 4 | 47328 / 114272 | training loss: 0.0032094295602291822\n",
      "epoch: 4 | 47360 / 114272 | training loss: 0.003598853014409542\n",
      "epoch: 4 | 47392 / 114272 | training loss: 0.004617059137672186\n",
      "epoch: 4 | 47424 / 114272 | training loss: 0.1290746033191681\n",
      "epoch: 4 | 47456 / 114272 | training loss: 0.0065251621417701244\n",
      "epoch: 4 | 47488 / 114272 | training loss: 0.002773505635559559\n",
      "epoch: 4 | 47520 / 114272 | training loss: 0.002073225798085332\n",
      "epoch: 4 | 47552 / 114272 | training loss: 0.16165363788604736\n",
      "epoch: 4 | 47584 / 114272 | training loss: 0.0018281760858371854\n",
      "epoch: 4 | 47616 / 114272 | training loss: 0.0010510721476748586\n",
      "epoch: 4 | 47648 / 114272 | training loss: 0.0017912150360643864\n",
      "epoch: 4 | 47680 / 114272 | training loss: 0.1095820739865303\n",
      "epoch: 4 | 47712 / 114272 | training loss: 0.002656739903613925\n",
      "epoch: 4 | 47744 / 114272 | training loss: 0.01932746171951294\n",
      "epoch: 4 | 47776 / 114272 | training loss: 0.20926553010940552\n",
      "epoch: 4 | 47808 / 114272 | training loss: 0.10076490789651871\n",
      "epoch: 4 | 47840 / 114272 | training loss: 0.018643351271748543\n",
      "epoch: 4 | 47872 / 114272 | training loss: 0.001960643334314227\n",
      "epoch: 4 | 47904 / 114272 | training loss: 0.003347597550600767\n",
      "epoch: 4 | 47936 / 114272 | training loss: 0.005461119115352631\n",
      "epoch: 4 | 47968 / 114272 | training loss: 0.060709256678819656\n",
      "epoch: 4 | 48000 / 114272 | training loss: 0.005202353931963444\n",
      "epoch: 4 | 48032 / 114272 | training loss: 0.013592271134257317\n",
      "epoch: 4 | 48064 / 114272 | training loss: 0.010572914034128189\n",
      "epoch: 4 | 48096 / 114272 | training loss: 0.009726101532578468\n",
      "epoch: 4 | 48128 / 114272 | training loss: 0.15333805978298187\n",
      "epoch: 4 | 48160 / 114272 | training loss: 0.007957950234413147\n",
      "epoch: 4 | 48192 / 114272 | training loss: 0.0030609143432229757\n",
      "epoch: 4 | 48224 / 114272 | training loss: 0.1138380914926529\n",
      "epoch: 4 | 48256 / 114272 | training loss: 0.016008419916033745\n",
      "epoch: 4 | 48288 / 114272 | training loss: 0.0021549237426370382\n",
      "epoch: 4 | 48320 / 114272 | training loss: 0.004895512480288744\n",
      "epoch: 4 | 48352 / 114272 | training loss: 0.11720462888479233\n",
      "epoch: 4 | 48384 / 114272 | training loss: 0.0035675421822816133\n",
      "epoch: 4 | 48416 / 114272 | training loss: 0.008534948341548443\n",
      "epoch: 4 | 48448 / 114272 | training loss: 0.007780624553561211\n",
      "epoch: 4 | 48480 / 114272 | training loss: 0.051326192915439606\n",
      "epoch: 4 | 48512 / 114272 | training loss: 0.007234236225485802\n",
      "epoch: 4 | 48544 / 114272 | training loss: 0.007109705358743668\n",
      "epoch: 4 | 48576 / 114272 | training loss: 0.0069959829561412334\n",
      "epoch: 4 | 48608 / 114272 | training loss: 0.005548372864723206\n",
      "epoch: 4 | 48640 / 114272 | training loss: 0.003723189001902938\n",
      "epoch: 4 | 48672 / 114272 | training loss: 0.18709051609039307\n",
      "epoch: 4 | 48704 / 114272 | training loss: 0.005952439736574888\n",
      "epoch: 4 | 48736 / 114272 | training loss: 0.18736185133457184\n",
      "epoch: 4 | 48768 / 114272 | training loss: 0.13503475487232208\n",
      "epoch: 4 | 48800 / 114272 | training loss: 0.002161522163078189\n",
      "epoch: 4 | 48832 / 114272 | training loss: 0.17792969942092896\n",
      "epoch: 4 | 48864 / 114272 | training loss: 0.005283384583890438\n",
      "epoch: 4 | 48896 / 114272 | training loss: 0.005633610766381025\n",
      "epoch: 4 | 48928 / 114272 | training loss: 0.004082166124135256\n",
      "epoch: 4 | 48960 / 114272 | training loss: 0.003187877591699362\n",
      "epoch: 4 | 48992 / 114272 | training loss: 0.0031379929278045893\n",
      "epoch: 4 | 49024 / 114272 | training loss: 0.01065264642238617\n",
      "epoch: 4 | 49056 / 114272 | training loss: 0.001287343678995967\n",
      "epoch: 4 | 49088 / 114272 | training loss: 0.0045365989208221436\n",
      "epoch: 4 | 49120 / 114272 | training loss: 0.003114735707640648\n",
      "epoch: 4 | 49152 / 114272 | training loss: 0.01033487357199192\n",
      "epoch: 4 | 49184 / 114272 | training loss: 0.005134932696819305\n",
      "epoch: 4 | 49216 / 114272 | training loss: 0.08559829741716385\n",
      "epoch: 4 | 49248 / 114272 | training loss: 0.004835894331336021\n",
      "epoch: 4 | 49280 / 114272 | training loss: 0.0029695280827581882\n",
      "epoch: 4 | 49312 / 114272 | training loss: 0.0034617697820067406\n",
      "epoch: 4 | 49344 / 114272 | training loss: 0.003032973501831293\n",
      "epoch: 4 | 49376 / 114272 | training loss: 0.0018516065320000052\n",
      "epoch: 4 | 49408 / 114272 | training loss: 0.0023382471408694983\n",
      "epoch: 4 | 49440 / 114272 | training loss: 0.008448975160717964\n",
      "epoch: 4 | 49472 / 114272 | training loss: 0.1870345175266266\n",
      "epoch: 4 | 49504 / 114272 | training loss: 0.0906393975019455\n",
      "epoch: 4 | 49536 / 114272 | training loss: 0.05396142229437828\n",
      "epoch: 4 | 49568 / 114272 | training loss: 0.1199708804488182\n",
      "epoch: 4 | 49600 / 114272 | training loss: 0.021149875596165657\n",
      "epoch: 4 | 49632 / 114272 | training loss: 0.0007439326727762818\n",
      "epoch: 4 | 49664 / 114272 | training loss: 0.0056184385903179646\n",
      "epoch: 4 | 49696 / 114272 | training loss: 0.001438648789189756\n",
      "epoch: 4 | 49728 / 114272 | training loss: 0.002560463035479188\n",
      "epoch: 4 | 49760 / 114272 | training loss: 0.0020053829066455364\n",
      "epoch: 4 | 49792 / 114272 | training loss: 0.0010122323874384165\n",
      "epoch: 4 | 49824 / 114272 | training loss: 0.0029349057003855705\n",
      "epoch: 4 | 49856 / 114272 | training loss: 0.00243209907785058\n",
      "epoch: 4 | 49888 / 114272 | training loss: 0.003057547379285097\n",
      "epoch: 4 | 49920 / 114272 | training loss: 0.0018935261759907007\n",
      "epoch: 4 | 49952 / 114272 | training loss: 0.16999992728233337\n",
      "epoch: 4 | 49984 / 114272 | training loss: 0.007291036192327738\n",
      "epoch: 4 | 50016 / 114272 | training loss: 0.0021357203368097544\n",
      "epoch: 4 | 50048 / 114272 | training loss: 0.24500152468681335\n",
      "epoch: 4 | 50080 / 114272 | training loss: 0.0017472472973167896\n",
      "epoch: 4 | 50112 / 114272 | training loss: 0.0020161354914307594\n",
      "epoch: 4 | 50144 / 114272 | training loss: 0.011261524632573128\n",
      "epoch: 4 | 50176 / 114272 | training loss: 0.001328974962234497\n",
      "epoch: 4 | 50208 / 114272 | training loss: 0.004817334469407797\n",
      "epoch: 4 | 50240 / 114272 | training loss: 0.0027191517874598503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 50272 / 114272 | training loss: 0.0023795145098119974\n",
      "epoch: 4 | 50304 / 114272 | training loss: 0.015334398485720158\n",
      "epoch: 4 | 50336 / 114272 | training loss: 0.0022302535362541676\n",
      "epoch: 4 | 50368 / 114272 | training loss: 0.002961278660222888\n",
      "epoch: 4 | 50400 / 114272 | training loss: 0.006400787737220526\n",
      "epoch: 4 | 50432 / 114272 | training loss: 0.09087072312831879\n",
      "epoch: 4 | 50464 / 114272 | training loss: 0.002057393779978156\n",
      "epoch: 4 | 50496 / 114272 | training loss: 0.006541862152516842\n",
      "epoch: 4 | 50528 / 114272 | training loss: 0.120687335729599\n",
      "epoch: 4 | 50560 / 114272 | training loss: 0.0021081832237541676\n",
      "epoch: 4 | 50592 / 114272 | training loss: 0.14693669974803925\n",
      "epoch: 4 | 50624 / 114272 | training loss: 0.001823930419050157\n",
      "epoch: 4 | 50656 / 114272 | training loss: 0.0016159818042069674\n",
      "epoch: 4 | 50688 / 114272 | training loss: 0.0008715095464140177\n",
      "epoch: 4 | 50720 / 114272 | training loss: 0.1955653429031372\n",
      "epoch: 4 | 50752 / 114272 | training loss: 0.0014523231657221913\n",
      "epoch: 4 | 50784 / 114272 | training loss: 0.00556422071531415\n",
      "epoch: 4 | 50816 / 114272 | training loss: 0.001040938775986433\n",
      "epoch: 4 | 50848 / 114272 | training loss: 0.1455930471420288\n",
      "epoch: 4 | 50880 / 114272 | training loss: 0.0024810596369206905\n",
      "epoch: 4 | 50912 / 114272 | training loss: 0.004359591752290726\n",
      "epoch: 4 | 50944 / 114272 | training loss: 0.0017523481510579586\n",
      "epoch: 4 | 50976 / 114272 | training loss: 0.012806382030248642\n",
      "epoch: 4 | 51008 / 114272 | training loss: 0.005130380392074585\n",
      "epoch: 4 | 51040 / 114272 | training loss: 0.14797751605510712\n",
      "epoch: 4 | 51072 / 114272 | training loss: 0.0009640315547585487\n",
      "epoch: 4 | 51104 / 114272 | training loss: 0.0011562728323042393\n",
      "epoch: 4 | 51136 / 114272 | training loss: 0.0019572204910218716\n",
      "epoch: 4 | 51168 / 114272 | training loss: 0.1161155253648758\n",
      "epoch: 4 | 51200 / 114272 | training loss: 0.0017911707982420921\n",
      "epoch: 4 | 51232 / 114272 | training loss: 0.002357886638492346\n",
      "epoch: 4 | 51264 / 114272 | training loss: 0.003129413118585944\n",
      "epoch: 4 | 51296 / 114272 | training loss: 0.005228373222053051\n",
      "epoch: 4 | 51328 / 114272 | training loss: 0.00397399440407753\n",
      "epoch: 4 | 51360 / 114272 | training loss: 0.0017973552457988262\n",
      "epoch: 4 | 51392 / 114272 | training loss: 0.004708173684775829\n",
      "epoch: 4 | 51424 / 114272 | training loss: 0.0331946462392807\n",
      "epoch: 4 | 51456 / 114272 | training loss: 0.0019507046090438962\n",
      "epoch: 4 | 51488 / 114272 | training loss: 0.0033545212354511023\n",
      "epoch: 4 | 51520 / 114272 | training loss: 0.0030264391098171473\n",
      "epoch: 4 | 51552 / 114272 | training loss: 0.2767433226108551\n",
      "epoch: 4 | 51584 / 114272 | training loss: 0.0028982108924537897\n",
      "epoch: 4 | 51616 / 114272 | training loss: 0.00248846597969532\n",
      "epoch: 4 | 51648 / 114272 | training loss: 0.11100970208644867\n",
      "epoch: 4 | 51680 / 114272 | training loss: 0.0061509027145802975\n",
      "epoch: 4 | 51712 / 114272 | training loss: 0.26115503907203674\n",
      "epoch: 4 | 51744 / 114272 | training loss: 0.0018095255363732576\n",
      "epoch: 4 | 51776 / 114272 | training loss: 0.2673564553260803\n",
      "epoch: 4 | 51808 / 114272 | training loss: 0.0032318986486643553\n",
      "epoch: 4 | 51840 / 114272 | training loss: 0.0031217881478369236\n",
      "epoch: 4 | 51872 / 114272 | training loss: 0.005636387504637241\n",
      "epoch: 4 | 51904 / 114272 | training loss: 0.0021479958668351173\n",
      "epoch: 4 | 51936 / 114272 | training loss: 0.001570715568959713\n",
      "epoch: 4 | 51968 / 114272 | training loss: 0.0811992883682251\n",
      "epoch: 4 | 52000 / 114272 | training loss: 0.0028639568481594324\n",
      "epoch: 4 | 52032 / 114272 | training loss: 0.0027615446597337723\n",
      "epoch: 4 | 52064 / 114272 | training loss: 0.00275118974968791\n",
      "epoch: 4 | 52096 / 114272 | training loss: 0.002908474998548627\n",
      "epoch: 4 | 52128 / 114272 | training loss: 0.0015904989559203386\n",
      "epoch: 4 | 52160 / 114272 | training loss: 0.0014386483235284686\n",
      "epoch: 4 | 52192 / 114272 | training loss: 0.010730303823947906\n",
      "epoch: 4 | 52224 / 114272 | training loss: 0.006396820768713951\n",
      "epoch: 4 | 52256 / 114272 | training loss: 0.0014158899430185556\n",
      "epoch: 4 | 52288 / 114272 | training loss: 0.0019241963746026158\n",
      "epoch: 4 | 52320 / 114272 | training loss: 0.1752287745475769\n",
      "epoch: 4 | 52352 / 114272 | training loss: 0.0016118403291329741\n",
      "epoch: 4 | 52384 / 114272 | training loss: 0.183527871966362\n",
      "epoch: 4 | 52416 / 114272 | training loss: 0.018329095095396042\n",
      "epoch: 4 | 52448 / 114272 | training loss: 0.1429690718650818\n",
      "epoch: 4 | 52480 / 114272 | training loss: 0.0022351655643433332\n",
      "epoch: 4 | 52512 / 114272 | training loss: 0.0030042510479688644\n",
      "epoch: 4 | 52544 / 114272 | training loss: 0.0021620860788971186\n",
      "epoch: 4 | 52576 / 114272 | training loss: 0.0015610393602401018\n",
      "epoch: 4 | 52608 / 114272 | training loss: 0.18743492662906647\n",
      "epoch: 4 | 52640 / 114272 | training loss: 0.0009347554296255112\n",
      "epoch: 4 | 52672 / 114272 | training loss: 0.146250918507576\n",
      "epoch: 4 | 52704 / 114272 | training loss: 0.002389016794040799\n",
      "epoch: 4 | 52736 / 114272 | training loss: 0.4322085678577423\n",
      "epoch: 4 | 52768 / 114272 | training loss: 0.003258434124290943\n",
      "epoch: 4 | 52800 / 114272 | training loss: 0.0018451788928359747\n",
      "epoch: 4 | 52832 / 114272 | training loss: 0.0007130926824174821\n",
      "epoch: 4 | 52864 / 114272 | training loss: 0.0029114075005054474\n",
      "epoch: 4 | 52896 / 114272 | training loss: 0.001215777243487537\n",
      "epoch: 4 | 52928 / 114272 | training loss: 0.0027778265066444874\n",
      "epoch: 4 | 52960 / 114272 | training loss: 0.0010209925239905715\n",
      "epoch: 4 | 52992 / 114272 | training loss: 0.0030052270740270615\n",
      "epoch: 4 | 53024 / 114272 | training loss: 0.003741856198757887\n",
      "epoch: 4 | 53056 / 114272 | training loss: 0.0025934779550880194\n",
      "epoch: 4 | 53088 / 114272 | training loss: 0.00257158768363297\n",
      "epoch: 4 | 53120 / 114272 | training loss: 0.002883878070861101\n",
      "epoch: 4 | 53152 / 114272 | training loss: 0.0023293406702578068\n",
      "epoch: 4 | 53184 / 114272 | training loss: 0.08034560829401016\n",
      "epoch: 4 | 53216 / 114272 | training loss: 0.0018423632718622684\n",
      "epoch: 4 | 53248 / 114272 | training loss: 0.06265391409397125\n",
      "epoch: 4 | 53280 / 114272 | training loss: 0.0017989572370424867\n",
      "epoch: 4 | 53312 / 114272 | training loss: 0.005707570351660252\n",
      "epoch: 4 | 53344 / 114272 | training loss: 0.003589673899114132\n",
      "epoch: 4 | 53376 / 114272 | training loss: 0.002796463668346405\n",
      "epoch: 4 | 53408 / 114272 | training loss: 0.003408567514270544\n",
      "epoch: 4 | 53440 / 114272 | training loss: 0.0014722907217219472\n",
      "epoch: 4 | 53472 / 114272 | training loss: 0.002197473542764783\n",
      "epoch: 4 | 53504 / 114272 | training loss: 0.0012769353343173862\n",
      "epoch: 4 | 53536 / 114272 | training loss: 0.0005311754066497087\n",
      "epoch: 4 | 53568 / 114272 | training loss: 0.43832579255104065\n",
      "epoch: 4 | 53600 / 114272 | training loss: 0.08631791919469833\n",
      "epoch: 4 | 53632 / 114272 | training loss: 0.0007364487391896546\n",
      "epoch: 4 | 53664 / 114272 | training loss: 0.05060839653015137\n",
      "epoch: 4 | 53696 / 114272 | training loss: 0.14128729701042175\n",
      "epoch: 4 | 53728 / 114272 | training loss: 0.0004112190508749336\n",
      "epoch: 4 | 53760 / 114272 | training loss: 0.0022526346147060394\n",
      "epoch: 4 | 53792 / 114272 | training loss: 0.0030175524298101664\n",
      "epoch: 4 | 53824 / 114272 | training loss: 0.03870667517185211\n",
      "epoch: 4 | 53856 / 114272 | training loss: 0.002136251423507929\n",
      "epoch: 4 | 53888 / 114272 | training loss: 0.11352850496768951\n",
      "epoch: 4 | 53920 / 114272 | training loss: 0.0018388242460787296\n",
      "epoch: 4 | 53952 / 114272 | training loss: 0.28106147050857544\n",
      "epoch: 4 | 53984 / 114272 | training loss: 0.002370133064687252\n",
      "epoch: 4 | 54016 / 114272 | training loss: 0.013437625020742416\n",
      "epoch: 4 | 54048 / 114272 | training loss: 0.11152492463588715\n",
      "epoch: 4 | 54080 / 114272 | training loss: 0.003228258341550827\n",
      "epoch: 4 | 54112 / 114272 | training loss: 0.0019174817716702819\n",
      "epoch: 4 | 54144 / 114272 | training loss: 0.1024940237402916\n",
      "epoch: 4 | 54176 / 114272 | training loss: 0.0008904435089789331\n",
      "epoch: 4 | 54208 / 114272 | training loss: 0.002550124656409025\n",
      "epoch: 4 | 54240 / 114272 | training loss: 0.004708020016551018\n",
      "epoch: 4 | 54272 / 114272 | training loss: 0.003311147913336754\n",
      "epoch: 4 | 54304 / 114272 | training loss: 0.002477677771821618\n",
      "epoch: 4 | 54336 / 114272 | training loss: 0.003209273098036647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 54368 / 114272 | training loss: 0.0035339388996362686\n",
      "epoch: 4 | 54400 / 114272 | training loss: 0.0023087323643267155\n",
      "epoch: 4 | 54432 / 114272 | training loss: 0.001128423842601478\n",
      "epoch: 4 | 54464 / 114272 | training loss: 0.05560716614127159\n",
      "epoch: 4 | 54496 / 114272 | training loss: 0.0037654940970242023\n",
      "epoch: 4 | 54528 / 114272 | training loss: 0.001123218797147274\n",
      "epoch: 4 | 54560 / 114272 | training loss: 0.01213087048381567\n",
      "epoch: 4 | 54592 / 114272 | training loss: 0.002295846352353692\n",
      "epoch: 4 | 54624 / 114272 | training loss: 0.4211362302303314\n",
      "epoch: 4 | 54656 / 114272 | training loss: 0.0023426013067364693\n",
      "epoch: 4 | 54688 / 114272 | training loss: 0.0030784609261900187\n",
      "epoch: 4 | 54720 / 114272 | training loss: 0.12892653048038483\n",
      "epoch: 4 | 54752 / 114272 | training loss: 0.11169405281543732\n",
      "epoch: 4 | 54784 / 114272 | training loss: 0.001069860067218542\n",
      "epoch: 4 | 54816 / 114272 | training loss: 0.0018990952521562576\n",
      "epoch: 4 | 54848 / 114272 | training loss: 0.06966602057218552\n",
      "epoch: 4 | 54880 / 114272 | training loss: 0.15206600725650787\n",
      "epoch: 4 | 54912 / 114272 | training loss: 0.006094775162637234\n",
      "epoch: 4 | 54944 / 114272 | training loss: 0.07925280928611755\n",
      "epoch: 4 | 54976 / 114272 | training loss: 0.05580046400427818\n",
      "epoch: 4 | 55008 / 114272 | training loss: 0.19839181005954742\n",
      "epoch: 4 | 55040 / 114272 | training loss: 0.0019369118381291628\n",
      "epoch: 4 | 55072 / 114272 | training loss: 0.0030844092834740877\n",
      "epoch: 4 | 55104 / 114272 | training loss: 0.0020653284154832363\n",
      "epoch: 4 | 55136 / 114272 | training loss: 0.00365022593177855\n",
      "epoch: 4 | 55168 / 114272 | training loss: 0.1320226937532425\n",
      "epoch: 4 | 55200 / 114272 | training loss: 0.05662310868501663\n",
      "epoch: 4 | 55232 / 114272 | training loss: 0.11050263792276382\n",
      "epoch: 4 | 55264 / 114272 | training loss: 0.09691077470779419\n",
      "epoch: 4 | 55296 / 114272 | training loss: 0.14383669197559357\n",
      "epoch: 4 | 55328 / 114272 | training loss: 0.0017871587770059705\n",
      "epoch: 4 | 55360 / 114272 | training loss: 0.003809940302744508\n",
      "epoch: 4 | 55392 / 114272 | training loss: 0.022835534065961838\n",
      "epoch: 4 | 55424 / 114272 | training loss: 0.004829023964703083\n",
      "epoch: 4 | 55456 / 114272 | training loss: 0.0025917799212038517\n",
      "epoch: 4 | 55488 / 114272 | training loss: 0.004384962376207113\n",
      "epoch: 4 | 55520 / 114272 | training loss: 0.0888683944940567\n",
      "epoch: 4 | 55552 / 114272 | training loss: 0.2222422957420349\n",
      "epoch: 4 | 55584 / 114272 | training loss: 0.003677370958030224\n",
      "epoch: 4 | 55616 / 114272 | training loss: 0.003913159482181072\n",
      "epoch: 4 | 55648 / 114272 | training loss: 0.0024360951501876116\n",
      "epoch: 4 | 55680 / 114272 | training loss: 0.06499314308166504\n",
      "epoch: 4 | 55712 / 114272 | training loss: 0.003082720097154379\n",
      "epoch: 4 | 55744 / 114272 | training loss: 0.14893029630184174\n",
      "epoch: 4 | 55776 / 114272 | training loss: 0.008832615800201893\n",
      "epoch: 4 | 55808 / 114272 | training loss: 0.008989092893898487\n",
      "epoch: 4 | 55840 / 114272 | training loss: 0.010829601436853409\n",
      "epoch: 4 | 55872 / 114272 | training loss: 0.004097240045666695\n",
      "epoch: 4 | 55904 / 114272 | training loss: 0.004191511310636997\n",
      "epoch: 4 | 55936 / 114272 | training loss: 0.0062592606991529465\n",
      "epoch: 4 | 55968 / 114272 | training loss: 0.16195465624332428\n",
      "epoch: 4 | 56000 / 114272 | training loss: 0.06610994786024094\n",
      "epoch: 4 | 56032 / 114272 | training loss: 0.003001465229317546\n",
      "epoch: 4 | 56064 / 114272 | training loss: 0.003929987549781799\n",
      "epoch: 4 | 56096 / 114272 | training loss: 0.0032148968894034624\n",
      "epoch: 4 | 56128 / 114272 | training loss: 0.0033627834636718035\n",
      "epoch: 4 | 56160 / 114272 | training loss: 0.0042417943477630615\n",
      "epoch: 4 | 56192 / 114272 | training loss: 0.015659881755709648\n",
      "epoch: 4 | 56224 / 114272 | training loss: 0.06767649203538895\n",
      "epoch: 4 | 56256 / 114272 | training loss: 0.004020951222628355\n",
      "epoch: 4 | 56288 / 114272 | training loss: 0.0058357310481369495\n",
      "epoch: 4 | 56320 / 114272 | training loss: 0.005205295514315367\n",
      "epoch: 4 | 56352 / 114272 | training loss: 0.002659137826412916\n",
      "epoch: 4 | 56384 / 114272 | training loss: 0.005097357090562582\n",
      "epoch: 4 | 56416 / 114272 | training loss: 0.003661053953692317\n",
      "epoch: 4 | 56448 / 114272 | training loss: 0.005279276520013809\n",
      "epoch: 4 | 56480 / 114272 | training loss: 0.00854729674756527\n",
      "epoch: 4 | 56512 / 114272 | training loss: 0.003337895032018423\n",
      "epoch: 4 | 56544 / 114272 | training loss: 0.0038546898867934942\n",
      "epoch: 4 | 56576 / 114272 | training loss: 0.10889258980751038\n",
      "epoch: 4 | 56608 / 114272 | training loss: 0.02052289992570877\n",
      "epoch: 4 | 56640 / 114272 | training loss: 0.0011687063379213214\n",
      "epoch: 4 | 56672 / 114272 | training loss: 0.16165079176425934\n",
      "epoch: 4 | 56704 / 114272 | training loss: 0.001790366368368268\n",
      "epoch: 4 | 56736 / 114272 | training loss: 0.00475765997543931\n",
      "epoch: 4 | 56768 / 114272 | training loss: 0.002206666860729456\n",
      "epoch: 4 | 56800 / 114272 | training loss: 0.0038440728094428778\n",
      "epoch: 4 | 56832 / 114272 | training loss: 0.04509507119655609\n",
      "epoch: 4 | 56864 / 114272 | training loss: 0.002121086698025465\n",
      "epoch: 4 | 56896 / 114272 | training loss: 0.0025085590314120054\n",
      "epoch: 4 | 56928 / 114272 | training loss: 0.002072724746540189\n",
      "epoch: 4 | 56960 / 114272 | training loss: 0.0010868058307096362\n",
      "epoch: 4 | 56992 / 114272 | training loss: 0.0034779426641762257\n",
      "epoch: 4 | 57024 / 114272 | training loss: 0.37586408853530884\n",
      "epoch: 4 | 57056 / 114272 | training loss: 0.35916510224342346\n",
      "epoch: 4 | 57088 / 114272 | training loss: 0.0013471436686813831\n",
      "epoch: 4 | 57120 / 114272 | training loss: 0.0019189657177776098\n",
      "epoch: 4 | 57152 / 114272 | training loss: 0.11787858605384827\n",
      "epoch: 4 | 57184 / 114272 | training loss: 0.0015873649390414357\n",
      "epoch: 4 | 57216 / 114272 | training loss: 0.02425946295261383\n",
      "epoch: 4 | 57248 / 114272 | training loss: 0.002936223754659295\n",
      "epoch: 4 | 57280 / 114272 | training loss: 0.0007790288073010743\n",
      "epoch: 4 | 57312 / 114272 | training loss: 0.10735396295785904\n",
      "epoch: 4 | 57344 / 114272 | training loss: 0.0022654724307358265\n",
      "epoch: 4 | 57376 / 114272 | training loss: 0.0019240255933254957\n",
      "epoch: 4 | 57408 / 114272 | training loss: 0.001003336743451655\n",
      "epoch: 4 | 57440 / 114272 | training loss: 0.0010330849327147007\n",
      "epoch: 4 | 57472 / 114272 | training loss: 0.0016464564250782132\n",
      "epoch: 4 | 57504 / 114272 | training loss: 0.0021188626997172832\n",
      "epoch: 4 | 57536 / 114272 | training loss: 0.0019360564183443785\n",
      "epoch: 4 | 57568 / 114272 | training loss: 0.0005011125467717648\n",
      "epoch: 4 | 57600 / 114272 | training loss: 0.001720959204249084\n",
      "epoch: 4 | 57632 / 114272 | training loss: 0.0030402466654777527\n",
      "epoch: 4 | 57664 / 114272 | training loss: 0.036164622753858566\n",
      "epoch: 4 | 57696 / 114272 | training loss: 0.011742061004042625\n",
      "epoch: 4 | 57728 / 114272 | training loss: 0.23804901540279388\n",
      "epoch: 4 | 57760 / 114272 | training loss: 0.11581984907388687\n",
      "epoch: 4 | 57792 / 114272 | training loss: 0.1448071002960205\n",
      "epoch: 4 | 57824 / 114272 | training loss: 0.017057405784726143\n",
      "epoch: 4 | 57856 / 114272 | training loss: 0.0018231086432933807\n",
      "epoch: 4 | 57888 / 114272 | training loss: 0.10541792958974838\n",
      "epoch: 4 | 57920 / 114272 | training loss: 0.0009496526909060776\n",
      "epoch: 4 | 57952 / 114272 | training loss: 0.34936660528182983\n",
      "epoch: 4 | 57984 / 114272 | training loss: 0.001290441257879138\n",
      "epoch: 4 | 58016 / 114272 | training loss: 0.0014502234989777207\n",
      "epoch: 4 | 58048 / 114272 | training loss: 0.0021962574683129787\n",
      "epoch: 4 | 58080 / 114272 | training loss: 0.15062211453914642\n",
      "epoch: 4 | 58112 / 114272 | training loss: 0.07310236990451813\n",
      "epoch: 4 | 58144 / 114272 | training loss: 0.012824985198676586\n",
      "epoch: 4 | 58176 / 114272 | training loss: 0.10950185358524323\n",
      "epoch: 4 | 58208 / 114272 | training loss: 0.2815793454647064\n",
      "epoch: 4 | 58240 / 114272 | training loss: 0.09498947113752365\n",
      "epoch: 4 | 58272 / 114272 | training loss: 0.0010959579376503825\n",
      "epoch: 4 | 58304 / 114272 | training loss: 0.07377700507640839\n",
      "epoch: 4 | 58336 / 114272 | training loss: 0.005946650635451078\n",
      "epoch: 4 | 58368 / 114272 | training loss: 0.0017118975520133972\n",
      "epoch: 4 | 58400 / 114272 | training loss: 0.1291349083185196\n",
      "epoch: 4 | 58432 / 114272 | training loss: 0.001627312391065061\n",
      "epoch: 4 | 58464 / 114272 | training loss: 0.0029845028184354305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 58496 / 114272 | training loss: 0.22769665718078613\n",
      "epoch: 4 | 58528 / 114272 | training loss: 0.0006837465334683657\n",
      "epoch: 4 | 58560 / 114272 | training loss: 0.01864853873848915\n",
      "epoch: 4 | 58592 / 114272 | training loss: 0.0038879557978361845\n",
      "epoch: 4 | 58624 / 114272 | training loss: 0.0019986217375844717\n",
      "epoch: 4 | 58656 / 114272 | training loss: 0.0032697429414838552\n",
      "epoch: 4 | 58688 / 114272 | training loss: 0.0034987693652510643\n",
      "epoch: 4 | 58720 / 114272 | training loss: 0.001062211929820478\n",
      "epoch: 4 | 58752 / 114272 | training loss: 0.06894317269325256\n",
      "epoch: 4 | 58784 / 114272 | training loss: 0.05625497177243233\n",
      "epoch: 4 | 58816 / 114272 | training loss: 0.0018393301870673895\n",
      "epoch: 4 | 58848 / 114272 | training loss: 0.25844883918762207\n",
      "epoch: 4 | 58880 / 114272 | training loss: 0.03787143900990486\n",
      "epoch: 4 | 58912 / 114272 | training loss: 0.0013060925994068384\n",
      "epoch: 4 | 58944 / 114272 | training loss: 0.09579706192016602\n",
      "epoch: 4 | 58976 / 114272 | training loss: 0.0022703511640429497\n",
      "epoch: 4 | 59008 / 114272 | training loss: 0.15774467587471008\n",
      "epoch: 4 | 59040 / 114272 | training loss: 0.07039769738912582\n",
      "epoch: 4 | 59072 / 114272 | training loss: 0.1887749880552292\n",
      "epoch: 4 | 59104 / 114272 | training loss: 0.003097152803093195\n",
      "epoch: 4 | 59136 / 114272 | training loss: 0.0786994993686676\n",
      "epoch: 4 | 59168 / 114272 | training loss: 0.30548667907714844\n",
      "epoch: 4 | 59200 / 114272 | training loss: 0.005462417844682932\n",
      "epoch: 4 | 59232 / 114272 | training loss: 0.13382788002490997\n",
      "epoch: 4 | 59264 / 114272 | training loss: 0.0005840802914462984\n",
      "epoch: 4 | 59296 / 114272 | training loss: 0.015432151034474373\n",
      "epoch: 4 | 59328 / 114272 | training loss: 0.0034173158928751945\n",
      "epoch: 4 | 59360 / 114272 | training loss: 0.08472229540348053\n",
      "epoch: 4 | 59392 / 114272 | training loss: 0.0012686868431046605\n",
      "epoch: 4 | 59424 / 114272 | training loss: 0.0044964756816625595\n",
      "epoch: 4 | 59456 / 114272 | training loss: 0.08561792969703674\n",
      "epoch: 4 | 59488 / 114272 | training loss: 0.007227984722703695\n",
      "epoch: 4 | 59520 / 114272 | training loss: 0.05082997679710388\n",
      "epoch: 4 | 59552 / 114272 | training loss: 0.022655723616480827\n",
      "epoch: 4 | 59584 / 114272 | training loss: 0.00627942243590951\n",
      "epoch: 4 | 59616 / 114272 | training loss: 0.0060775200836360455\n",
      "epoch: 4 | 59648 / 114272 | training loss: 0.22118230164051056\n",
      "epoch: 4 | 59680 / 114272 | training loss: 0.001717122970148921\n",
      "epoch: 4 | 59712 / 114272 | training loss: 0.003065716940909624\n",
      "epoch: 4 | 59744 / 114272 | training loss: 0.0033355385530740023\n",
      "epoch: 4 | 59776 / 114272 | training loss: 0.06719870865345001\n",
      "epoch: 4 | 59808 / 114272 | training loss: 0.008085432462394238\n",
      "epoch: 4 | 59840 / 114272 | training loss: 0.0020034441258758307\n",
      "epoch: 4 | 59872 / 114272 | training loss: 0.08107223361730576\n",
      "epoch: 4 | 59904 / 114272 | training loss: 0.014934010803699493\n",
      "epoch: 4 | 59936 / 114272 | training loss: 0.004781821276992559\n",
      "epoch: 4 | 59968 / 114272 | training loss: 0.06703732907772064\n",
      "epoch: 4 | 60000 / 114272 | training loss: 0.012823210097849369\n",
      "epoch: 4 | 60032 / 114272 | training loss: 0.029865993186831474\n",
      "epoch: 4 | 60064 / 114272 | training loss: 0.01026648934930563\n",
      "epoch: 4 | 60096 / 114272 | training loss: 0.002737737726420164\n",
      "epoch: 4 | 60128 / 114272 | training loss: 0.00557416956871748\n",
      "epoch: 4 | 60160 / 114272 | training loss: 0.06627114862203598\n",
      "epoch: 4 | 60192 / 114272 | training loss: 0.002493599895387888\n",
      "epoch: 4 | 60224 / 114272 | training loss: 0.0035183485597372055\n",
      "epoch: 4 | 60256 / 114272 | training loss: 0.0026617071125656366\n",
      "epoch: 4 | 60288 / 114272 | training loss: 0.06854970753192902\n",
      "epoch: 4 | 60320 / 114272 | training loss: 0.006963820196688175\n",
      "epoch: 4 | 60352 / 114272 | training loss: 0.0480911023914814\n",
      "epoch: 4 | 60384 / 114272 | training loss: 0.0017922220285981894\n",
      "epoch: 4 | 60416 / 114272 | training loss: 0.011241951957345009\n",
      "epoch: 4 | 60448 / 114272 | training loss: 0.007523225154727697\n",
      "epoch: 4 | 60480 / 114272 | training loss: 0.0018125389469787478\n",
      "epoch: 4 | 60512 / 114272 | training loss: 0.0014291235711425543\n",
      "epoch: 4 | 60544 / 114272 | training loss: 0.004067474976181984\n",
      "epoch: 4 | 60576 / 114272 | training loss: 0.0007194175850600004\n",
      "epoch: 4 | 60608 / 114272 | training loss: 0.11975563317537308\n",
      "epoch: 4 | 60640 / 114272 | training loss: 0.06236867606639862\n",
      "epoch: 4 | 60672 / 114272 | training loss: 0.00101091421674937\n",
      "epoch: 4 | 60704 / 114272 | training loss: 0.08589326590299606\n",
      "epoch: 4 | 60736 / 114272 | training loss: 0.1043919026851654\n",
      "epoch: 4 | 60768 / 114272 | training loss: 0.21630088984966278\n",
      "epoch: 4 | 60800 / 114272 | training loss: 0.004506608005613089\n",
      "epoch: 4 | 60832 / 114272 | training loss: 0.0017667800420895219\n",
      "epoch: 4 | 60864 / 114272 | training loss: 0.8412936329841614\n",
      "epoch: 4 | 60896 / 114272 | training loss: 0.0009262753883376718\n",
      "epoch: 4 | 60928 / 114272 | training loss: 0.0008733341237530112\n",
      "epoch: 4 | 60960 / 114272 | training loss: 0.11218013614416122\n",
      "epoch: 4 | 60992 / 114272 | training loss: 0.002595509635284543\n",
      "epoch: 4 | 61024 / 114272 | training loss: 0.23774906992912292\n",
      "epoch: 4 | 61056 / 114272 | training loss: 0.16568045318126678\n",
      "epoch: 4 | 61088 / 114272 | training loss: 0.0010887893149629235\n",
      "epoch: 4 | 61120 / 114272 | training loss: 0.0012641584035009146\n",
      "epoch: 4 | 61152 / 114272 | training loss: 0.15068291127681732\n",
      "epoch: 4 | 61184 / 114272 | training loss: 0.002281251596286893\n",
      "epoch: 4 | 61216 / 114272 | training loss: 0.005002379883080721\n",
      "epoch: 4 | 61248 / 114272 | training loss: 0.0007002008496783674\n",
      "epoch: 4 | 61280 / 114272 | training loss: 0.001627531135454774\n",
      "epoch: 4 | 61312 / 114272 | training loss: 0.011058393865823746\n",
      "epoch: 4 | 61344 / 114272 | training loss: 0.05104940012097359\n",
      "epoch: 4 | 61376 / 114272 | training loss: 0.008161978796124458\n",
      "epoch: 4 | 61408 / 114272 | training loss: 0.0037703823763877153\n",
      "epoch: 4 | 61440 / 114272 | training loss: 0.002805630676448345\n",
      "epoch: 4 | 61472 / 114272 | training loss: 0.006185763981193304\n",
      "epoch: 4 | 61504 / 114272 | training loss: 0.0018556388095021248\n",
      "epoch: 4 | 61536 / 114272 | training loss: 0.0029317885637283325\n",
      "epoch: 4 | 61568 / 114272 | training loss: 0.06511109322309494\n",
      "epoch: 4 | 61600 / 114272 | training loss: 0.003932596649974585\n",
      "epoch: 4 | 61632 / 114272 | training loss: 0.0007829802925698459\n",
      "epoch: 4 | 61664 / 114272 | training loss: 0.0039575472474098206\n",
      "epoch: 4 | 61696 / 114272 | training loss: 0.0009982915362343192\n",
      "epoch: 4 | 61728 / 114272 | training loss: 0.07416095584630966\n",
      "epoch: 4 | 61760 / 114272 | training loss: 0.012638906016945839\n",
      "epoch: 4 | 61792 / 114272 | training loss: 0.005591387394815683\n",
      "epoch: 4 | 61824 / 114272 | training loss: 0.0041830758564174175\n",
      "epoch: 4 | 61856 / 114272 | training loss: 0.013196638785302639\n",
      "epoch: 4 | 61888 / 114272 | training loss: 0.011901282705366611\n",
      "epoch: 4 | 61920 / 114272 | training loss: 0.021785074844956398\n",
      "epoch: 4 | 61952 / 114272 | training loss: 0.0009440007852390409\n",
      "epoch: 4 | 61984 / 114272 | training loss: 0.008512560278177261\n",
      "epoch: 4 | 62016 / 114272 | training loss: 0.10163554549217224\n",
      "epoch: 4 | 62048 / 114272 | training loss: 0.1418665200471878\n",
      "epoch: 4 | 62080 / 114272 | training loss: 0.0011452764738351107\n",
      "epoch: 4 | 62112 / 114272 | training loss: 0.0016701617278158665\n",
      "epoch: 4 | 62144 / 114272 | training loss: 0.03676225617527962\n",
      "epoch: 4 | 62176 / 114272 | training loss: 0.0014015405904501677\n",
      "epoch: 4 | 62208 / 114272 | training loss: 0.07003993541002274\n",
      "epoch: 4 | 62240 / 114272 | training loss: 0.0007911472348496318\n",
      "epoch: 4 | 62272 / 114272 | training loss: 0.0026475314516574144\n",
      "epoch: 4 | 62304 / 114272 | training loss: 0.21571913361549377\n",
      "epoch: 4 | 62336 / 114272 | training loss: 0.10809166729450226\n",
      "epoch: 4 | 62368 / 114272 | training loss: 0.2050432562828064\n",
      "epoch: 4 | 62400 / 114272 | training loss: 0.0008620754815638065\n",
      "epoch: 4 | 62432 / 114272 | training loss: 0.1268308460712433\n",
      "epoch: 4 | 62464 / 114272 | training loss: 0.06574851274490356\n",
      "epoch: 4 | 62496 / 114272 | training loss: 0.06255233287811279\n",
      "epoch: 4 | 62528 / 114272 | training loss: 0.01174493134021759\n",
      "epoch: 4 | 62560 / 114272 | training loss: 0.2266603708267212\n",
      "epoch: 4 | 62592 / 114272 | training loss: 0.10834202915430069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 62624 / 114272 | training loss: 0.10532822459936142\n",
      "epoch: 4 | 62656 / 114272 | training loss: 0.08021312952041626\n",
      "epoch: 4 | 62688 / 114272 | training loss: 0.19893427193164825\n",
      "epoch: 4 | 62720 / 114272 | training loss: 0.00384401879273355\n",
      "epoch: 4 | 62752 / 114272 | training loss: 0.09163270145654678\n",
      "epoch: 4 | 62784 / 114272 | training loss: 0.003077386412769556\n",
      "epoch: 4 | 62816 / 114272 | training loss: 0.0018187646055594087\n",
      "epoch: 4 | 62848 / 114272 | training loss: 0.013009865768253803\n",
      "epoch: 4 | 62880 / 114272 | training loss: 0.045997992157936096\n",
      "epoch: 4 | 62912 / 114272 | training loss: 0.011669115163385868\n",
      "epoch: 4 | 62944 / 114272 | training loss: 0.001921506249345839\n",
      "epoch: 4 | 62976 / 114272 | training loss: 0.06969551742076874\n",
      "epoch: 4 | 63008 / 114272 | training loss: 0.014230971224606037\n",
      "epoch: 4 | 63040 / 114272 | training loss: 0.0057503897696733475\n",
      "epoch: 4 | 63072 / 114272 | training loss: 0.006196280941367149\n",
      "epoch: 4 | 63104 / 114272 | training loss: 0.005844644270837307\n",
      "epoch: 4 | 63136 / 114272 | training loss: 0.002016741316765547\n",
      "epoch: 4 | 63168 / 114272 | training loss: 0.15976285934448242\n",
      "epoch: 4 | 63200 / 114272 | training loss: 0.025222787633538246\n",
      "epoch: 4 | 63232 / 114272 | training loss: 0.006143953651189804\n",
      "epoch: 4 | 63264 / 114272 | training loss: 0.002284900052472949\n",
      "epoch: 4 | 63296 / 114272 | training loss: 0.40513524413108826\n",
      "epoch: 4 | 63328 / 114272 | training loss: 0.0068007237277925014\n",
      "epoch: 4 | 63360 / 114272 | training loss: 0.002992442110553384\n",
      "epoch: 4 | 63392 / 114272 | training loss: 0.0028894415590912104\n",
      "epoch: 4 | 63424 / 114272 | training loss: 0.005418422631919384\n",
      "epoch: 4 | 63456 / 114272 | training loss: 0.0010771629167720675\n",
      "epoch: 4 | 63488 / 114272 | training loss: 0.001518747885711491\n",
      "epoch: 4 | 63520 / 114272 | training loss: 0.00241700722835958\n",
      "epoch: 4 | 63552 / 114272 | training loss: 0.007449520286172628\n",
      "epoch: 4 | 63584 / 114272 | training loss: 0.008914007805287838\n",
      "epoch: 4 | 63616 / 114272 | training loss: 0.19008304178714752\n",
      "epoch: 4 | 63648 / 114272 | training loss: 0.10279341042041779\n",
      "epoch: 4 | 63680 / 114272 | training loss: 0.0027985554188489914\n",
      "epoch: 4 | 63712 / 114272 | training loss: 0.005989724304527044\n",
      "epoch: 4 | 63744 / 114272 | training loss: 0.003383985487744212\n",
      "epoch: 4 | 63776 / 114272 | training loss: 0.004715269431471825\n",
      "epoch: 4 | 63808 / 114272 | training loss: 0.06355395913124084\n",
      "epoch: 4 | 63840 / 114272 | training loss: 0.013464274816215038\n",
      "epoch: 4 | 63872 / 114272 | training loss: 0.016922427341341972\n",
      "epoch: 4 | 63904 / 114272 | training loss: 0.004893884994089603\n",
      "epoch: 4 | 63936 / 114272 | training loss: 0.001995931612327695\n",
      "epoch: 4 | 63968 / 114272 | training loss: 0.028704466298222542\n",
      "epoch: 4 | 64000 / 114272 | training loss: 0.001375015126541257\n",
      "epoch: 4 | 64032 / 114272 | training loss: 0.0005176389240659773\n",
      "epoch: 4 | 64064 / 114272 | training loss: 0.0027034461963921785\n",
      "epoch: 4 | 64096 / 114272 | training loss: 0.1427839994430542\n",
      "epoch: 4 | 64128 / 114272 | training loss: 0.07448496669530869\n",
      "epoch: 4 | 64160 / 114272 | training loss: 0.0031360567081719637\n",
      "epoch: 4 | 64192 / 114272 | training loss: 0.03769908472895622\n",
      "epoch: 4 | 64224 / 114272 | training loss: 0.0009889777284115553\n",
      "epoch: 4 | 64256 / 114272 | training loss: 0.002390516921877861\n",
      "epoch: 4 | 64288 / 114272 | training loss: 0.1462022364139557\n",
      "epoch: 4 | 64320 / 114272 | training loss: 0.2522696852684021\n",
      "epoch: 4 | 64352 / 114272 | training loss: 0.02446809969842434\n",
      "epoch: 4 | 64384 / 114272 | training loss: 0.15365275740623474\n",
      "epoch: 4 | 64416 / 114272 | training loss: 0.0005619784351438284\n",
      "epoch: 4 | 64448 / 114272 | training loss: 0.01631823740899563\n",
      "epoch: 4 | 64480 / 114272 | training loss: 0.10234585404396057\n",
      "epoch: 4 | 64512 / 114272 | training loss: 0.06950563937425613\n",
      "epoch: 4 | 64544 / 114272 | training loss: 0.003265941049903631\n",
      "epoch: 4 | 64576 / 114272 | training loss: 0.3264578878879547\n",
      "epoch: 4 | 64608 / 114272 | training loss: 0.0013114996254444122\n",
      "epoch: 4 | 64640 / 114272 | training loss: 0.05886344239115715\n",
      "epoch: 4 | 64672 / 114272 | training loss: 0.07031058520078659\n",
      "epoch: 4 | 64704 / 114272 | training loss: 0.12550443410873413\n",
      "epoch: 4 | 64736 / 114272 | training loss: 0.007236636243760586\n",
      "epoch: 4 | 64768 / 114272 | training loss: 0.0032150554470717907\n",
      "epoch: 4 | 64800 / 114272 | training loss: 0.011301888152956963\n",
      "epoch: 4 | 64832 / 114272 | training loss: 0.18987810611724854\n",
      "epoch: 4 | 64864 / 114272 | training loss: 0.004871258977800608\n",
      "epoch: 4 | 64896 / 114272 | training loss: 0.0035303072072565556\n",
      "epoch: 4 | 64928 / 114272 | training loss: 0.006920346990227699\n",
      "epoch: 4 | 64960 / 114272 | training loss: 0.011077872477471828\n",
      "epoch: 4 | 64992 / 114272 | training loss: 0.17641574144363403\n",
      "epoch: 4 | 65024 / 114272 | training loss: 0.005861201323568821\n",
      "epoch: 4 | 65056 / 114272 | training loss: 0.004205922596156597\n",
      "epoch: 4 | 65088 / 114272 | training loss: 0.0026435740292072296\n",
      "epoch: 4 | 65120 / 114272 | training loss: 0.006601202301681042\n",
      "epoch: 4 | 65152 / 114272 | training loss: 0.0010010346304625273\n",
      "epoch: 4 | 65184 / 114272 | training loss: 0.003243513870984316\n",
      "epoch: 4 | 65216 / 114272 | training loss: 0.0053459657356143\n",
      "epoch: 4 | 65248 / 114272 | training loss: 0.008298211731016636\n",
      "epoch: 4 | 65280 / 114272 | training loss: 0.0025791237130761147\n",
      "epoch: 4 | 65312 / 114272 | training loss: 0.0032966984435915947\n",
      "epoch: 4 | 65344 / 114272 | training loss: 0.00461891945451498\n",
      "epoch: 4 | 65376 / 114272 | training loss: 0.026227572932839394\n",
      "epoch: 4 | 65408 / 114272 | training loss: 0.26087242364883423\n",
      "epoch: 4 | 65440 / 114272 | training loss: 0.004339249804615974\n",
      "epoch: 4 | 65472 / 114272 | training loss: 0.028578627854585648\n",
      "epoch: 4 | 65504 / 114272 | training loss: 0.002628656569868326\n",
      "epoch: 4 | 65536 / 114272 | training loss: 0.17542339861392975\n",
      "epoch: 4 | 65568 / 114272 | training loss: 0.0010410796385258436\n",
      "epoch: 4 | 65600 / 114272 | training loss: 0.0011477527441456914\n",
      "epoch: 4 | 65632 / 114272 | training loss: 0.0408298596739769\n",
      "epoch: 4 | 65664 / 114272 | training loss: 0.0008800657815299928\n",
      "epoch: 4 | 65696 / 114272 | training loss: 0.0015137448208406568\n",
      "epoch: 4 | 65728 / 114272 | training loss: 0.0013030359987169504\n",
      "epoch: 4 | 65760 / 114272 | training loss: 0.0009979530004784465\n",
      "epoch: 4 | 65792 / 114272 | training loss: 0.09745272994041443\n",
      "epoch: 4 | 65824 / 114272 | training loss: 0.0028413438703864813\n",
      "epoch: 4 | 65856 / 114272 | training loss: 0.0015321996761485934\n",
      "epoch: 4 | 65888 / 114272 | training loss: 0.12881337106227875\n",
      "epoch: 4 | 65920 / 114272 | training loss: 0.0022078268229961395\n",
      "epoch: 4 | 65952 / 114272 | training loss: 0.000714193272870034\n",
      "epoch: 4 | 65984 / 114272 | training loss: 0.001763894222676754\n",
      "epoch: 4 | 66016 / 114272 | training loss: 0.14270424842834473\n",
      "epoch: 4 | 66048 / 114272 | training loss: 0.0017383702797815204\n",
      "epoch: 4 | 66080 / 114272 | training loss: 0.007317313924431801\n",
      "epoch: 4 | 66112 / 114272 | training loss: 0.0005757608450949192\n",
      "epoch: 4 | 66144 / 114272 | training loss: 0.0027460569981485605\n",
      "epoch: 4 | 66176 / 114272 | training loss: 0.06458599865436554\n",
      "epoch: 4 | 66208 / 114272 | training loss: 0.001349735539406538\n",
      "epoch: 4 | 66240 / 114272 | training loss: 0.0013391802785918117\n",
      "epoch: 4 | 66272 / 114272 | training loss: 0.003796077100560069\n",
      "epoch: 4 | 66304 / 114272 | training loss: 0.0018063614843413234\n",
      "epoch: 4 | 66336 / 114272 | training loss: 0.001207891502417624\n",
      "epoch: 4 | 66368 / 114272 | training loss: 0.004806638229638338\n",
      "epoch: 4 | 66400 / 114272 | training loss: 0.10072120279073715\n",
      "epoch: 4 | 66432 / 114272 | training loss: 0.12361650168895721\n",
      "epoch: 4 | 66464 / 114272 | training loss: 0.002328253583982587\n",
      "epoch: 4 | 66496 / 114272 | training loss: 0.1374291330575943\n",
      "epoch: 4 | 66528 / 114272 | training loss: 0.004300432279706001\n",
      "epoch: 4 | 66560 / 114272 | training loss: 0.0018790594767779112\n",
      "epoch: 4 | 66592 / 114272 | training loss: 0.18142662942409515\n",
      "epoch: 4 | 66624 / 114272 | training loss: 0.031172102317214012\n",
      "epoch: 4 | 66656 / 114272 | training loss: 0.0036646933294832706\n",
      "epoch: 4 | 66688 / 114272 | training loss: 0.0025750042404979467\n",
      "epoch: 4 | 66720 / 114272 | training loss: 0.012972396798431873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 66752 / 114272 | training loss: 0.0014096741797402501\n",
      "epoch: 4 | 66784 / 114272 | training loss: 0.0009023969178088009\n",
      "epoch: 4 | 66816 / 114272 | training loss: 0.0024033752270042896\n",
      "epoch: 4 | 66848 / 114272 | training loss: 0.19606967270374298\n",
      "epoch: 4 | 66880 / 114272 | training loss: 0.10090115666389465\n",
      "epoch: 4 | 66912 / 114272 | training loss: 0.0030305623076856136\n",
      "epoch: 4 | 66944 / 114272 | training loss: 0.0036704298108816147\n",
      "epoch: 4 | 66976 / 114272 | training loss: 0.003644623328000307\n",
      "epoch: 4 | 67008 / 114272 | training loss: 0.004676158539950848\n",
      "epoch: 4 | 67040 / 114272 | training loss: 0.2484900951385498\n",
      "epoch: 4 | 67072 / 114272 | training loss: 0.0011329769622534513\n",
      "epoch: 4 | 67104 / 114272 | training loss: 0.0011977292597293854\n",
      "epoch: 4 | 67136 / 114272 | training loss: 0.15271741151809692\n",
      "epoch: 4 | 67168 / 114272 | training loss: 0.08237133920192719\n",
      "epoch: 4 | 67200 / 114272 | training loss: 0.07328778505325317\n",
      "epoch: 4 | 67232 / 114272 | training loss: 0.0043783774599432945\n",
      "epoch: 4 | 67264 / 114272 | training loss: 0.2527557909488678\n",
      "epoch: 4 | 67296 / 114272 | training loss: 0.0012022972805425525\n",
      "epoch: 4 | 67328 / 114272 | training loss: 0.010900268331170082\n",
      "epoch: 4 | 67360 / 114272 | training loss: 0.09270071238279343\n",
      "epoch: 4 | 67392 / 114272 | training loss: 0.0036116375122219324\n",
      "epoch: 4 | 67424 / 114272 | training loss: 0.0018001346616074443\n",
      "epoch: 4 | 67456 / 114272 | training loss: 0.2707890570163727\n",
      "epoch: 4 | 67488 / 114272 | training loss: 0.0022623585537075996\n",
      "epoch: 4 | 67520 / 114272 | training loss: 0.0018903331365436316\n",
      "epoch: 4 | 67552 / 114272 | training loss: 0.0026847422122955322\n",
      "epoch: 4 | 67584 / 114272 | training loss: 0.09077169001102448\n",
      "epoch: 4 | 67616 / 114272 | training loss: 0.00620536133646965\n",
      "epoch: 4 | 67648 / 114272 | training loss: 0.0706033930182457\n",
      "epoch: 4 | 67680 / 114272 | training loss: 0.007450253237038851\n",
      "epoch: 4 | 67712 / 114272 | training loss: 0.0010038657346740365\n",
      "epoch: 4 | 67744 / 114272 | training loss: 0.0012060395674780011\n",
      "epoch: 4 | 67776 / 114272 | training loss: 0.08744856715202332\n",
      "epoch: 4 | 67808 / 114272 | training loss: 0.002593105426058173\n",
      "epoch: 4 | 67840 / 114272 | training loss: 0.021364567801356316\n",
      "epoch: 4 | 67872 / 114272 | training loss: 0.002394780283793807\n",
      "epoch: 4 | 67904 / 114272 | training loss: 0.0017264760099351406\n",
      "epoch: 4 | 67936 / 114272 | training loss: 0.0038599653635174036\n",
      "epoch: 4 | 67968 / 114272 | training loss: 0.0058397953398525715\n",
      "epoch: 4 | 68000 / 114272 | training loss: 0.08677899837493896\n",
      "epoch: 4 | 68032 / 114272 | training loss: 0.034204982221126556\n",
      "epoch: 4 | 68064 / 114272 | training loss: 0.0024045316968113184\n",
      "epoch: 4 | 68096 / 114272 | training loss: 0.09239955991506577\n",
      "epoch: 4 | 68128 / 114272 | training loss: 0.16660147905349731\n",
      "epoch: 4 | 68160 / 114272 | training loss: 0.21524646878242493\n",
      "epoch: 4 | 68192 / 114272 | training loss: 0.00788047257810831\n",
      "epoch: 4 | 68224 / 114272 | training loss: 0.0016713117947801948\n",
      "epoch: 4 | 68256 / 114272 | training loss: 0.1316230595111847\n",
      "epoch: 4 | 68288 / 114272 | training loss: 0.003836395451799035\n",
      "epoch: 4 | 68320 / 114272 | training loss: 0.003756485879421234\n",
      "epoch: 4 | 68352 / 114272 | training loss: 0.004983562044799328\n",
      "epoch: 4 | 68384 / 114272 | training loss: 0.0017841735389083624\n",
      "epoch: 4 | 68416 / 114272 | training loss: 0.15057052671909332\n",
      "epoch: 4 | 68448 / 114272 | training loss: 0.004747770726680756\n",
      "epoch: 4 | 68480 / 114272 | training loss: 0.0014546170132234693\n",
      "epoch: 4 | 68512 / 114272 | training loss: 0.0011108701582998037\n",
      "epoch: 4 | 68544 / 114272 | training loss: 0.002021266147494316\n",
      "epoch: 4 | 68576 / 114272 | training loss: 0.0016895824810490012\n",
      "epoch: 4 | 68608 / 114272 | training loss: 0.003609318286180496\n",
      "epoch: 4 | 68640 / 114272 | training loss: 0.15839147567749023\n",
      "epoch: 4 | 68672 / 114272 | training loss: 0.00496209692209959\n",
      "epoch: 4 | 68704 / 114272 | training loss: 0.13546834886074066\n",
      "epoch: 4 | 68736 / 114272 | training loss: 0.004077521618455648\n",
      "epoch: 4 | 68768 / 114272 | training loss: 0.0034544963855296373\n",
      "epoch: 4 | 68800 / 114272 | training loss: 0.0016569994622841477\n",
      "epoch: 4 | 68832 / 114272 | training loss: 0.008197079412639141\n",
      "epoch: 4 | 68864 / 114272 | training loss: 0.002778369467705488\n",
      "epoch: 4 | 68896 / 114272 | training loss: 0.3671046197414398\n",
      "epoch: 4 | 68928 / 114272 | training loss: 0.0013816591817885637\n",
      "epoch: 4 | 68960 / 114272 | training loss: 0.0015478694112971425\n",
      "epoch: 4 | 68992 / 114272 | training loss: 0.0019650713074952364\n",
      "epoch: 4 | 69024 / 114272 | training loss: 0.039437782019376755\n",
      "epoch: 4 | 69056 / 114272 | training loss: 0.011033385992050171\n",
      "epoch: 4 | 69088 / 114272 | training loss: 0.0018666349351406097\n",
      "epoch: 4 | 69120 / 114272 | training loss: 0.14161095023155212\n",
      "epoch: 4 | 69152 / 114272 | training loss: 0.0018947997596114874\n",
      "epoch: 4 | 69184 / 114272 | training loss: 0.004709379747509956\n",
      "epoch: 4 | 69216 / 114272 | training loss: 0.0017932138871401548\n",
      "epoch: 4 | 69248 / 114272 | training loss: 0.002093581249937415\n",
      "epoch: 4 | 69280 / 114272 | training loss: 0.15279895067214966\n",
      "epoch: 4 | 69312 / 114272 | training loss: 0.002594469580799341\n",
      "epoch: 4 | 69344 / 114272 | training loss: 0.1555059850215912\n",
      "epoch: 4 | 69376 / 114272 | training loss: 0.002136874245479703\n",
      "epoch: 4 | 69408 / 114272 | training loss: 0.0015224891249090433\n",
      "epoch: 4 | 69440 / 114272 | training loss: 0.003536645323038101\n",
      "epoch: 4 | 69472 / 114272 | training loss: 0.006605288479477167\n",
      "epoch: 4 | 69504 / 114272 | training loss: 0.1264571249485016\n",
      "epoch: 4 | 69536 / 114272 | training loss: 0.007367657497525215\n",
      "epoch: 4 | 69568 / 114272 | training loss: 0.23420029878616333\n",
      "epoch: 4 | 69600 / 114272 | training loss: 0.0015502710593864322\n",
      "epoch: 4 | 69632 / 114272 | training loss: 0.011857869103550911\n",
      "epoch: 4 | 69664 / 114272 | training loss: 0.0012262602103874087\n",
      "epoch: 4 | 69696 / 114272 | training loss: 0.11930720508098602\n",
      "epoch: 4 | 69728 / 114272 | training loss: 0.08537633717060089\n",
      "epoch: 4 | 69760 / 114272 | training loss: 0.0017577260732650757\n",
      "epoch: 4 | 69792 / 114272 | training loss: 0.005153350066393614\n",
      "epoch: 4 | 69824 / 114272 | training loss: 0.15165111422538757\n",
      "epoch: 4 | 69856 / 114272 | training loss: 0.33653202652931213\n",
      "epoch: 4 | 69888 / 114272 | training loss: 0.11408776044845581\n",
      "epoch: 4 | 69920 / 114272 | training loss: 0.002460830844938755\n",
      "epoch: 4 | 69952 / 114272 | training loss: 0.0029171558562666178\n",
      "epoch: 4 | 69984 / 114272 | training loss: 0.0022889862302690744\n",
      "epoch: 4 | 70016 / 114272 | training loss: 0.002368627581745386\n",
      "epoch: 4 | 70048 / 114272 | training loss: 0.0031387603376060724\n",
      "epoch: 4 | 70080 / 114272 | training loss: 0.0042569600045681\n",
      "epoch: 4 | 70112 / 114272 | training loss: 0.17737756669521332\n",
      "epoch: 4 | 70144 / 114272 | training loss: 0.002797272289171815\n",
      "epoch: 4 | 70176 / 114272 | training loss: 0.0039648134261369705\n",
      "epoch: 4 | 70208 / 114272 | training loss: 0.0023117621894925833\n",
      "epoch: 4 | 70240 / 114272 | training loss: 0.11306046694517136\n",
      "epoch: 4 | 70272 / 114272 | training loss: 0.08735307306051254\n",
      "epoch: 4 | 70304 / 114272 | training loss: 0.20031976699829102\n",
      "epoch: 4 | 70336 / 114272 | training loss: 0.07587407529354095\n",
      "epoch: 4 | 70368 / 114272 | training loss: 0.15955683588981628\n",
      "epoch: 4 | 70400 / 114272 | training loss: 0.09526761621236801\n",
      "epoch: 4 | 70432 / 114272 | training loss: 0.06192360073328018\n",
      "epoch: 4 | 70464 / 114272 | training loss: 0.11754308640956879\n",
      "epoch: 4 | 70496 / 114272 | training loss: 0.003350969636812806\n",
      "epoch: 4 | 70528 / 114272 | training loss: 0.06757247447967529\n",
      "epoch: 4 | 70560 / 114272 | training loss: 0.001718240207992494\n",
      "epoch: 4 | 70592 / 114272 | training loss: 0.00411946652457118\n",
      "epoch: 4 | 70624 / 114272 | training loss: 0.0025389513466507196\n",
      "epoch: 4 | 70656 / 114272 | training loss: 0.0031053200364112854\n",
      "epoch: 4 | 70688 / 114272 | training loss: 0.0028069133404642344\n",
      "epoch: 4 | 70720 / 114272 | training loss: 0.0032900231890380383\n",
      "epoch: 4 | 70752 / 114272 | training loss: 0.004430106841027737\n",
      "epoch: 4 | 70784 / 114272 | training loss: 0.005123177543282509\n",
      "epoch: 4 | 70816 / 114272 | training loss: 0.003995536360889673\n",
      "epoch: 4 | 70848 / 114272 | training loss: 0.00276715075597167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 70880 / 114272 | training loss: 0.0066205416806042194\n",
      "epoch: 4 | 70912 / 114272 | training loss: 0.0041091893799602985\n",
      "epoch: 4 | 70944 / 114272 | training loss: 0.01197111327201128\n",
      "epoch: 4 | 70976 / 114272 | training loss: 0.001052731298841536\n",
      "epoch: 4 | 71008 / 114272 | training loss: 0.005845241714268923\n",
      "epoch: 4 | 71040 / 114272 | training loss: 0.13816221058368683\n",
      "epoch: 4 | 71072 / 114272 | training loss: 0.036685388535261154\n",
      "epoch: 4 | 71104 / 114272 | training loss: 0.0023320105392485857\n",
      "epoch: 4 | 71136 / 114272 | training loss: 0.002152759348973632\n",
      "epoch: 4 | 71168 / 114272 | training loss: 0.003434723010286689\n",
      "epoch: 4 | 71200 / 114272 | training loss: 0.009744355455040932\n",
      "epoch: 4 | 71232 / 114272 | training loss: 0.02956230193376541\n",
      "epoch: 4 | 71264 / 114272 | training loss: 0.0011881068348884583\n",
      "epoch: 4 | 71296 / 114272 | training loss: 0.0027848249301314354\n",
      "epoch: 4 | 71328 / 114272 | training loss: 0.006695093121379614\n",
      "epoch: 4 | 71360 / 114272 | training loss: 0.0029959455132484436\n",
      "epoch: 4 | 71392 / 114272 | training loss: 0.08198248594999313\n",
      "epoch: 4 | 71424 / 114272 | training loss: 0.004405813291668892\n",
      "epoch: 4 | 71456 / 114272 | training loss: 0.0029928390868008137\n",
      "epoch: 4 | 71488 / 114272 | training loss: 0.030311232432723045\n",
      "epoch: 4 | 71520 / 114272 | training loss: 0.35902291536331177\n",
      "epoch: 4 | 71552 / 114272 | training loss: 0.002533017424866557\n",
      "epoch: 4 | 71584 / 114272 | training loss: 0.0026639816351234913\n",
      "epoch: 4 | 71616 / 114272 | training loss: 0.0020128258038312197\n",
      "epoch: 4 | 71648 / 114272 | training loss: 0.0013768611242994666\n",
      "epoch: 4 | 71680 / 114272 | training loss: 0.31215164065361023\n",
      "epoch: 4 | 71712 / 114272 | training loss: 0.0035203483421355486\n",
      "epoch: 4 | 71744 / 114272 | training loss: 0.0010737049160525203\n",
      "epoch: 4 | 71776 / 114272 | training loss: 0.0009943257318809628\n",
      "epoch: 4 | 71808 / 114272 | training loss: 0.0015705195255577564\n",
      "epoch: 4 | 71840 / 114272 | training loss: 0.08355775475502014\n",
      "epoch: 4 | 71872 / 114272 | training loss: 0.06604484468698502\n",
      "epoch: 4 | 71904 / 114272 | training loss: 0.0013597928918898106\n",
      "epoch: 4 | 71936 / 114272 | training loss: 0.0018227911787107587\n",
      "epoch: 4 | 71968 / 114272 | training loss: 0.0013521427754312754\n",
      "epoch: 4 | 72000 / 114272 | training loss: 0.001359673566184938\n",
      "epoch: 4 | 72032 / 114272 | training loss: 0.0029467784333974123\n",
      "epoch: 4 | 72064 / 114272 | training loss: 0.15408413112163544\n",
      "epoch: 4 | 72096 / 114272 | training loss: 0.0024805297143757343\n",
      "epoch: 4 | 72128 / 114272 | training loss: 0.007749796379357576\n",
      "epoch: 4 | 72160 / 114272 | training loss: 0.0026056331116706133\n",
      "epoch: 4 | 72192 / 114272 | training loss: 0.08894955366849899\n",
      "epoch: 4 | 72224 / 114272 | training loss: 0.15361087024211884\n",
      "epoch: 4 | 72256 / 114272 | training loss: 0.11812146753072739\n",
      "epoch: 4 | 72288 / 114272 | training loss: 0.0012434476520866156\n",
      "epoch: 4 | 72320 / 114272 | training loss: 0.2509525716304779\n",
      "epoch: 4 | 72352 / 114272 | training loss: 0.22044092416763306\n",
      "epoch: 4 | 72384 / 114272 | training loss: 0.0009011832298710942\n",
      "epoch: 4 | 72416 / 114272 | training loss: 0.0027561006136238575\n",
      "epoch: 4 | 72448 / 114272 | training loss: 0.05671580880880356\n",
      "epoch: 4 | 72480 / 114272 | training loss: 0.0017193527892231941\n",
      "epoch: 4 | 72512 / 114272 | training loss: 0.13923226296901703\n",
      "epoch: 4 | 72544 / 114272 | training loss: 0.0032549009192734957\n",
      "epoch: 4 | 72576 / 114272 | training loss: 0.005663635209202766\n",
      "epoch: 4 | 72608 / 114272 | training loss: 0.0051737697795033455\n",
      "epoch: 4 | 72640 / 114272 | training loss: 0.0037181025836616755\n",
      "epoch: 4 | 72672 / 114272 | training loss: 0.001315222936682403\n",
      "epoch: 4 | 72704 / 114272 | training loss: 0.19905252754688263\n",
      "epoch: 4 | 72736 / 114272 | training loss: 0.1142411082983017\n",
      "epoch: 4 | 72768 / 114272 | training loss: 0.0077469064854085445\n",
      "epoch: 4 | 72800 / 114272 | training loss: 0.0016929892590269446\n",
      "epoch: 4 | 72832 / 114272 | training loss: 0.02101081795990467\n",
      "epoch: 4 | 72864 / 114272 | training loss: 0.005872534587979317\n",
      "epoch: 4 | 72896 / 114272 | training loss: 0.0066239782609045506\n",
      "epoch: 4 | 72928 / 114272 | training loss: 0.008663373999297619\n",
      "epoch: 4 | 72960 / 114272 | training loss: 0.004134259652346373\n",
      "epoch: 4 | 72992 / 114272 | training loss: 0.010896505787968636\n",
      "epoch: 4 | 73024 / 114272 | training loss: 0.26604440808296204\n",
      "epoch: 4 | 73056 / 114272 | training loss: 0.003230756614357233\n",
      "epoch: 4 | 73088 / 114272 | training loss: 0.002875336678698659\n",
      "epoch: 4 | 73120 / 114272 | training loss: 0.0019086181418970227\n",
      "epoch: 4 | 73152 / 114272 | training loss: 0.0038187559694051743\n",
      "epoch: 4 | 73184 / 114272 | training loss: 0.15230850875377655\n",
      "epoch: 4 | 73216 / 114272 | training loss: 0.0036401019897311926\n",
      "epoch: 4 | 73248 / 114272 | training loss: 0.12986458837985992\n",
      "epoch: 4 | 73280 / 114272 | training loss: 0.06958061456680298\n",
      "epoch: 4 | 73312 / 114272 | training loss: 0.0014101210981607437\n",
      "epoch: 4 | 73344 / 114272 | training loss: 0.0034182399976998568\n",
      "epoch: 4 | 73376 / 114272 | training loss: 0.0024458046536892653\n",
      "epoch: 4 | 73408 / 114272 | training loss: 0.08170492947101593\n",
      "epoch: 4 | 73440 / 114272 | training loss: 0.16984707117080688\n",
      "epoch: 4 | 73472 / 114272 | training loss: 0.003014687215909362\n",
      "epoch: 4 | 73504 / 114272 | training loss: 0.0067835175432264805\n",
      "epoch: 4 | 73536 / 114272 | training loss: 0.004746790509670973\n",
      "epoch: 4 | 73568 / 114272 | training loss: 0.13276539742946625\n",
      "epoch: 4 | 73600 / 114272 | training loss: 0.009191610850393772\n",
      "epoch: 4 | 73632 / 114272 | training loss: 0.08553698658943176\n",
      "epoch: 4 | 73664 / 114272 | training loss: 0.0046526421792805195\n",
      "epoch: 4 | 73696 / 114272 | training loss: 0.11823684722185135\n",
      "epoch: 4 | 73728 / 114272 | training loss: 0.00850910134613514\n",
      "epoch: 4 | 73760 / 114272 | training loss: 0.09490657597780228\n",
      "epoch: 4 | 73792 / 114272 | training loss: 0.005770754534751177\n",
      "epoch: 4 | 73824 / 114272 | training loss: 0.053229499608278275\n",
      "epoch: 4 | 73856 / 114272 | training loss: 0.09598252922296524\n",
      "epoch: 4 | 73888 / 114272 | training loss: 0.09065166860818863\n",
      "epoch: 4 | 73920 / 114272 | training loss: 0.002451011212542653\n",
      "epoch: 4 | 73952 / 114272 | training loss: 0.010497726500034332\n",
      "epoch: 4 | 73984 / 114272 | training loss: 0.03791838139295578\n",
      "epoch: 4 | 74016 / 114272 | training loss: 0.009781956672668457\n",
      "epoch: 4 | 74048 / 114272 | training loss: 0.009650074876844883\n",
      "epoch: 4 | 74080 / 114272 | training loss: 0.011532329022884369\n",
      "epoch: 4 | 74112 / 114272 | training loss: 0.05293932929635048\n",
      "epoch: 4 | 74144 / 114272 | training loss: 0.007583010941743851\n",
      "epoch: 4 | 74176 / 114272 | training loss: 0.1634031981229782\n",
      "epoch: 4 | 74208 / 114272 | training loss: 0.011663499288260937\n",
      "epoch: 4 | 74240 / 114272 | training loss: 0.23217090964317322\n",
      "epoch: 4 | 74272 / 114272 | training loss: 0.03496628627181053\n",
      "epoch: 4 | 74304 / 114272 | training loss: 0.012279721908271313\n",
      "epoch: 4 | 74336 / 114272 | training loss: 0.10415638238191605\n",
      "epoch: 4 | 74368 / 114272 | training loss: 0.0006301933899521828\n",
      "epoch: 4 | 74400 / 114272 | training loss: 0.014725083485245705\n",
      "epoch: 4 | 74432 / 114272 | training loss: 0.2610599994659424\n",
      "epoch: 4 | 74464 / 114272 | training loss: 0.01061253435909748\n",
      "epoch: 4 | 74496 / 114272 | training loss: 0.09937568008899689\n",
      "epoch: 4 | 74528 / 114272 | training loss: 0.0033552851527929306\n",
      "epoch: 4 | 74560 / 114272 | training loss: 0.004339949227869511\n",
      "epoch: 4 | 74592 / 114272 | training loss: 0.004412205424159765\n",
      "epoch: 4 | 74624 / 114272 | training loss: 0.003474240889772773\n",
      "epoch: 4 | 74656 / 114272 | training loss: 0.12168373912572861\n",
      "epoch: 4 | 74688 / 114272 | training loss: 0.0015849503688514233\n",
      "epoch: 4 | 74720 / 114272 | training loss: 0.16363993287086487\n",
      "epoch: 4 | 74752 / 114272 | training loss: 0.10297761112451553\n",
      "epoch: 4 | 74784 / 114272 | training loss: 0.04513491690158844\n",
      "epoch: 4 | 74816 / 114272 | training loss: 0.0036683836951851845\n",
      "epoch: 4 | 74848 / 114272 | training loss: 0.003406018717214465\n",
      "epoch: 4 | 74880 / 114272 | training loss: 0.3283858895301819\n",
      "epoch: 4 | 74912 / 114272 | training loss: 0.0035185515880584717\n",
      "epoch: 4 | 74944 / 114272 | training loss: 0.0025214203633368015\n",
      "epoch: 4 | 74976 / 114272 | training loss: 0.0021507018245756626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 75008 / 114272 | training loss: 0.014260719530284405\n",
      "epoch: 4 | 75040 / 114272 | training loss: 0.0019906850066035986\n",
      "epoch: 4 | 75072 / 114272 | training loss: 0.045130085200071335\n",
      "epoch: 4 | 75104 / 114272 | training loss: 0.01108766719698906\n",
      "epoch: 4 | 75136 / 114272 | training loss: 0.12663495540618896\n",
      "epoch: 4 | 75168 / 114272 | training loss: 0.005977023392915726\n",
      "epoch: 4 | 75200 / 114272 | training loss: 0.004416531417518854\n",
      "epoch: 4 | 75232 / 114272 | training loss: 0.0013633009511977434\n",
      "epoch: 4 | 75264 / 114272 | training loss: 0.22947609424591064\n",
      "epoch: 4 | 75296 / 114272 | training loss: 0.002715157112106681\n",
      "epoch: 4 | 75328 / 114272 | training loss: 0.15586483478546143\n",
      "epoch: 4 | 75360 / 114272 | training loss: 0.006910860538482666\n",
      "epoch: 4 | 75392 / 114272 | training loss: 0.1180570051074028\n",
      "epoch: 4 | 75424 / 114272 | training loss: 0.0038948424626141787\n",
      "epoch: 4 | 75456 / 114272 | training loss: 0.003180082654580474\n",
      "epoch: 4 | 75488 / 114272 | training loss: 0.002308883238583803\n",
      "epoch: 4 | 75520 / 114272 | training loss: 0.003196924924850464\n",
      "epoch: 4 | 75552 / 114272 | training loss: 0.20367076992988586\n",
      "epoch: 4 | 75584 / 114272 | training loss: 0.005226530134677887\n",
      "epoch: 4 | 75616 / 114272 | training loss: 0.0044417171739041805\n",
      "epoch: 4 | 75648 / 114272 | training loss: 0.0035856759641319513\n",
      "epoch: 4 | 75680 / 114272 | training loss: 0.003917299211025238\n",
      "epoch: 4 | 75712 / 114272 | training loss: 0.010383306071162224\n",
      "epoch: 4 | 75744 / 114272 | training loss: 0.0026387758553028107\n",
      "epoch: 4 | 75776 / 114272 | training loss: 0.10744103044271469\n",
      "epoch: 4 | 75808 / 114272 | training loss: 0.1372644454240799\n",
      "epoch: 4 | 75840 / 114272 | training loss: 0.011389468796551228\n",
      "epoch: 4 | 75872 / 114272 | training loss: 0.31958436965942383\n",
      "epoch: 4 | 75904 / 114272 | training loss: 0.09110219776630402\n",
      "epoch: 4 | 75936 / 114272 | training loss: 0.006259456742554903\n",
      "epoch: 4 | 75968 / 114272 | training loss: 0.04022279009222984\n",
      "epoch: 4 | 76000 / 114272 | training loss: 0.006012374069541693\n",
      "epoch: 4 | 76032 / 114272 | training loss: 0.030016612261533737\n",
      "epoch: 4 | 76064 / 114272 | training loss: 0.0021072516683489084\n",
      "epoch: 4 | 76096 / 114272 | training loss: 0.002264568582177162\n",
      "epoch: 4 | 76128 / 114272 | training loss: 0.01862148940563202\n",
      "epoch: 4 | 76160 / 114272 | training loss: 0.006579208187758923\n",
      "epoch: 4 | 76192 / 114272 | training loss: 0.039866987615823746\n",
      "epoch: 4 | 76224 / 114272 | training loss: 0.031415343284606934\n",
      "epoch: 4 | 76256 / 114272 | training loss: 0.0032989925239235163\n",
      "epoch: 4 | 76288 / 114272 | training loss: 0.005925311241298914\n",
      "epoch: 4 | 76320 / 114272 | training loss: 0.007536254823207855\n",
      "epoch: 4 | 76352 / 114272 | training loss: 0.004467112477868795\n",
      "epoch: 4 | 76384 / 114272 | training loss: 0.06677103787660599\n",
      "epoch: 4 | 76416 / 114272 | training loss: 0.002768700011074543\n",
      "epoch: 4 | 76448 / 114272 | training loss: 0.004565276205539703\n",
      "epoch: 4 | 76480 / 114272 | training loss: 0.005762879271060228\n",
      "epoch: 4 | 76512 / 114272 | training loss: 0.004114669747650623\n",
      "epoch: 4 | 76544 / 114272 | training loss: 0.12497659027576447\n",
      "epoch: 4 | 76576 / 114272 | training loss: 0.0017209297511726618\n",
      "epoch: 4 | 76608 / 114272 | training loss: 0.07993195205926895\n",
      "epoch: 4 | 76640 / 114272 | training loss: 0.0019265614682808518\n",
      "epoch: 4 | 76672 / 114272 | training loss: 0.004162718541920185\n",
      "epoch: 4 | 76704 / 114272 | training loss: 0.0036186750512570143\n",
      "epoch: 4 | 76736 / 114272 | training loss: 0.0018474541138857603\n",
      "epoch: 4 | 76768 / 114272 | training loss: 0.10972419381141663\n",
      "epoch: 4 | 76800 / 114272 | training loss: 0.00154591235332191\n",
      "epoch: 4 | 76832 / 114272 | training loss: 0.0011457584332674742\n",
      "epoch: 4 | 76864 / 114272 | training loss: 0.0021523491013795137\n",
      "epoch: 4 | 76896 / 114272 | training loss: 0.001892568776383996\n",
      "epoch: 4 | 76928 / 114272 | training loss: 0.04180433973670006\n",
      "epoch: 4 | 76960 / 114272 | training loss: 0.005275844596326351\n",
      "epoch: 4 | 76992 / 114272 | training loss: 0.2918217182159424\n",
      "epoch: 4 | 77024 / 114272 | training loss: 0.0014273673295974731\n",
      "epoch: 4 | 77056 / 114272 | training loss: 0.002243311610072851\n",
      "epoch: 4 | 77088 / 114272 | training loss: 0.057229578495025635\n",
      "epoch: 4 | 77120 / 114272 | training loss: 0.00354788382537663\n",
      "epoch: 4 | 77152 / 114272 | training loss: 0.0014121135463938117\n",
      "epoch: 4 | 77184 / 114272 | training loss: 0.0850038155913353\n",
      "epoch: 4 | 77216 / 114272 | training loss: 0.003853820962831378\n",
      "epoch: 4 | 77248 / 114272 | training loss: 0.002025282010436058\n",
      "epoch: 4 | 77280 / 114272 | training loss: 0.0022693003993481398\n",
      "epoch: 4 | 77312 / 114272 | training loss: 0.022065985947847366\n",
      "epoch: 4 | 77344 / 114272 | training loss: 0.006131094880402088\n",
      "epoch: 4 | 77376 / 114272 | training loss: 0.0022327920887619257\n",
      "epoch: 4 | 77408 / 114272 | training loss: 0.000874941935762763\n",
      "epoch: 4 | 77440 / 114272 | training loss: 0.008145062252879143\n",
      "epoch: 4 | 77472 / 114272 | training loss: 0.0014506867155432701\n",
      "epoch: 4 | 77504 / 114272 | training loss: 0.0026535154320299625\n",
      "epoch: 4 | 77536 / 114272 | training loss: 0.0007368173100985587\n",
      "epoch: 4 | 77568 / 114272 | training loss: 0.0445270761847496\n",
      "epoch: 4 | 77600 / 114272 | training loss: 0.005433341022580862\n",
      "epoch: 4 | 77632 / 114272 | training loss: 0.0010111731244251132\n",
      "epoch: 4 | 77664 / 114272 | training loss: 0.004999079275876284\n",
      "epoch: 4 | 77696 / 114272 | training loss: 0.0008553763036616147\n",
      "epoch: 4 | 77728 / 114272 | training loss: 0.27080363035202026\n",
      "epoch: 4 | 77760 / 114272 | training loss: 0.00561852753162384\n",
      "epoch: 4 | 77792 / 114272 | training loss: 0.001410394674167037\n",
      "epoch: 4 | 77824 / 114272 | training loss: 0.0008915134822018445\n",
      "epoch: 4 | 77856 / 114272 | training loss: 0.000997288734652102\n",
      "epoch: 4 | 77888 / 114272 | training loss: 0.025076573714613914\n",
      "epoch: 4 | 77920 / 114272 | training loss: 0.0020367768593132496\n",
      "epoch: 4 | 77952 / 114272 | training loss: 0.2123015820980072\n",
      "epoch: 4 | 77984 / 114272 | training loss: 0.003250428941100836\n",
      "epoch: 4 | 78016 / 114272 | training loss: 0.00795821100473404\n",
      "epoch: 4 | 78048 / 114272 | training loss: 0.0012189662083983421\n",
      "epoch: 4 | 78080 / 114272 | training loss: 0.20182524621486664\n",
      "epoch: 4 | 78112 / 114272 | training loss: 0.009192543104290962\n",
      "epoch: 4 | 78144 / 114272 | training loss: 0.4032631516456604\n",
      "epoch: 4 | 78176 / 114272 | training loss: 0.0013851628173142672\n",
      "epoch: 4 | 78208 / 114272 | training loss: 0.0011409445432946086\n",
      "epoch: 4 | 78240 / 114272 | training loss: 0.0013187521835789084\n",
      "epoch: 4 | 78272 / 114272 | training loss: 0.001715285237878561\n",
      "epoch: 4 | 78304 / 114272 | training loss: 0.0019361189333721995\n",
      "epoch: 4 | 78336 / 114272 | training loss: 0.0008242310723289847\n",
      "epoch: 4 | 78368 / 114272 | training loss: 0.0006258833454921842\n",
      "epoch: 4 | 78400 / 114272 | training loss: 0.0013646162115037441\n",
      "epoch: 4 | 78432 / 114272 | training loss: 0.25055190920829773\n",
      "epoch: 4 | 78464 / 114272 | training loss: 0.010814967565238476\n",
      "epoch: 4 | 78496 / 114272 | training loss: 0.0016228086315095425\n",
      "epoch: 4 | 78528 / 114272 | training loss: 0.13529838621616364\n",
      "epoch: 4 | 78560 / 114272 | training loss: 0.11686626076698303\n",
      "epoch: 4 | 78592 / 114272 | training loss: 0.003617932554334402\n",
      "epoch: 4 | 78624 / 114272 | training loss: 0.15810297429561615\n",
      "epoch: 4 | 78656 / 114272 | training loss: 0.0010023667709901929\n",
      "epoch: 4 | 78688 / 114272 | training loss: 0.0008406300912611187\n",
      "epoch: 4 | 78720 / 114272 | training loss: 0.0018092339159920812\n",
      "epoch: 4 | 78752 / 114272 | training loss: 0.00595405139029026\n",
      "epoch: 4 | 78784 / 114272 | training loss: 0.002225665608420968\n",
      "epoch: 4 | 78816 / 114272 | training loss: 0.005142590496689081\n",
      "epoch: 4 | 78848 / 114272 | training loss: 0.001979036722332239\n",
      "epoch: 4 | 78880 / 114272 | training loss: 0.10952529311180115\n",
      "epoch: 4 | 78912 / 114272 | training loss: 0.004598699975758791\n",
      "epoch: 4 | 78944 / 114272 | training loss: 0.0006618876359425485\n",
      "epoch: 4 | 78976 / 114272 | training loss: 0.0008759559132158756\n",
      "epoch: 4 | 79008 / 114272 | training loss: 0.0013036623131483793\n",
      "epoch: 4 | 79040 / 114272 | training loss: 0.11632932722568512\n",
      "epoch: 4 | 79072 / 114272 | training loss: 0.24818766117095947\n",
      "epoch: 4 | 79104 / 114272 | training loss: 0.0020129072945564985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 79136 / 114272 | training loss: 0.0025465558283030987\n",
      "epoch: 4 | 79168 / 114272 | training loss: 0.09919396042823792\n",
      "epoch: 4 | 79200 / 114272 | training loss: 0.008316502906382084\n",
      "epoch: 4 | 79232 / 114272 | training loss: 0.0048759751953184605\n",
      "epoch: 4 | 79264 / 114272 | training loss: 0.0028443278279155493\n",
      "epoch: 4 | 79296 / 114272 | training loss: 0.003888831241056323\n",
      "epoch: 4 | 79328 / 114272 | training loss: 0.09234848618507385\n",
      "epoch: 4 | 79360 / 114272 | training loss: 0.001766272704117\n",
      "epoch: 4 | 79392 / 114272 | training loss: 0.0633886530995369\n",
      "epoch: 4 | 79424 / 114272 | training loss: 0.007104980293661356\n",
      "epoch: 4 | 79456 / 114272 | training loss: 0.23787453770637512\n",
      "epoch: 4 | 79488 / 114272 | training loss: 0.0030548744834959507\n",
      "epoch: 4 | 79520 / 114272 | training loss: 0.003491717390716076\n",
      "epoch: 4 | 79552 / 114272 | training loss: 0.0017790811834856868\n",
      "epoch: 4 | 79584 / 114272 | training loss: 0.11112935096025467\n",
      "epoch: 4 | 79616 / 114272 | training loss: 0.21502834558486938\n",
      "epoch: 4 | 79648 / 114272 | training loss: 0.0028878357261419296\n",
      "epoch: 4 | 79680 / 114272 | training loss: 0.005438434891402721\n",
      "epoch: 4 | 79712 / 114272 | training loss: 0.005569780245423317\n",
      "epoch: 4 | 79744 / 114272 | training loss: 0.0027637723833322525\n",
      "epoch: 4 | 79776 / 114272 | training loss: 0.006620966363698244\n",
      "epoch: 4 | 79808 / 114272 | training loss: 0.000910902745090425\n",
      "epoch: 4 | 79840 / 114272 | training loss: 0.3060154616832733\n",
      "epoch: 4 | 79872 / 114272 | training loss: 0.006784672848880291\n",
      "epoch: 4 | 79904 / 114272 | training loss: 0.004783925134688616\n",
      "epoch: 4 | 79936 / 114272 | training loss: 0.001758089754730463\n",
      "epoch: 4 | 79968 / 114272 | training loss: 0.062148913741111755\n",
      "epoch: 4 | 80000 / 114272 | training loss: 0.002047336660325527\n",
      "epoch: 4 | 80032 / 114272 | training loss: 0.003215854987502098\n",
      "epoch: 4 | 80064 / 114272 | training loss: 0.014108113013207912\n",
      "epoch: 4 | 80096 / 114272 | training loss: 0.004367421381175518\n",
      "epoch: 4 | 80128 / 114272 | training loss: 0.1769043654203415\n",
      "epoch: 4 | 80160 / 114272 | training loss: 0.009148969314992428\n",
      "epoch: 4 | 80192 / 114272 | training loss: 0.0038077342323958874\n",
      "epoch: 4 | 80224 / 114272 | training loss: 0.09312249720096588\n",
      "epoch: 4 | 80256 / 114272 | training loss: 0.0019412836991250515\n",
      "epoch: 4 | 80288 / 114272 | training loss: 0.0029986051376909018\n",
      "epoch: 4 | 80320 / 114272 | training loss: 0.0009911649394780397\n",
      "epoch: 4 | 80352 / 114272 | training loss: 0.014193141832947731\n",
      "epoch: 4 | 80384 / 114272 | training loss: 0.0034189182333648205\n",
      "epoch: 4 | 80416 / 114272 | training loss: 0.39806482195854187\n",
      "epoch: 4 | 80448 / 114272 | training loss: 0.005465529393404722\n",
      "epoch: 4 | 80480 / 114272 | training loss: 0.0037373497616499662\n",
      "epoch: 4 | 80512 / 114272 | training loss: 0.0026212758384644985\n",
      "epoch: 4 | 80544 / 114272 | training loss: 0.057535238564014435\n",
      "epoch: 4 | 80576 / 114272 | training loss: 0.003162548877298832\n",
      "epoch: 4 | 80608 / 114272 | training loss: 0.0007815404678694904\n",
      "epoch: 4 | 80640 / 114272 | training loss: 0.00202424474991858\n",
      "epoch: 4 | 80672 / 114272 | training loss: 0.0033791055902838707\n",
      "epoch: 4 | 80704 / 114272 | training loss: 0.0008770244894549251\n",
      "epoch: 4 | 80736 / 114272 | training loss: 0.0008956067031249404\n",
      "epoch: 4 | 80768 / 114272 | training loss: 0.0010894108563661575\n",
      "epoch: 4 | 80800 / 114272 | training loss: 0.058869995176792145\n",
      "epoch: 4 | 80832 / 114272 | training loss: 0.0009286804706789553\n",
      "epoch: 4 | 80864 / 114272 | training loss: 0.29756051301956177\n",
      "epoch: 4 | 80896 / 114272 | training loss: 0.0013949911808595061\n",
      "epoch: 4 | 80928 / 114272 | training loss: 0.0013185319257900119\n",
      "epoch: 4 | 80960 / 114272 | training loss: 0.08985243737697601\n",
      "epoch: 4 | 80992 / 114272 | training loss: 0.0016362720634788275\n",
      "epoch: 4 | 81024 / 114272 | training loss: 0.228648379445076\n",
      "epoch: 4 | 81056 / 114272 | training loss: 0.001256655203178525\n",
      "epoch: 4 | 81088 / 114272 | training loss: 0.03794232755899429\n",
      "epoch: 4 | 81120 / 114272 | training loss: 0.0007594542112201452\n",
      "epoch: 4 | 81152 / 114272 | training loss: 0.45658957958221436\n",
      "epoch: 4 | 81184 / 114272 | training loss: 0.0012195417657494545\n",
      "epoch: 4 | 81216 / 114272 | training loss: 0.0015967569779604673\n",
      "epoch: 4 | 81248 / 114272 | training loss: 0.0019912661518901587\n",
      "epoch: 4 | 81280 / 114272 | training loss: 0.22919081151485443\n",
      "epoch: 4 | 81312 / 114272 | training loss: 0.004662452265620232\n",
      "epoch: 4 | 81344 / 114272 | training loss: 0.1671983152627945\n",
      "epoch: 4 | 81376 / 114272 | training loss: 0.0011818240163847804\n",
      "epoch: 4 | 81408 / 114272 | training loss: 0.00128659897018224\n",
      "epoch: 4 | 81440 / 114272 | training loss: 0.0005784491659142077\n",
      "epoch: 4 | 81472 / 114272 | training loss: 0.002876619342714548\n",
      "epoch: 4 | 81504 / 114272 | training loss: 0.004279271233826876\n",
      "epoch: 4 | 81536 / 114272 | training loss: 0.003707000520080328\n",
      "epoch: 4 | 81568 / 114272 | training loss: 0.0012333494378253818\n",
      "epoch: 4 | 81600 / 114272 | training loss: 0.0010340128792449832\n",
      "epoch: 4 | 81632 / 114272 | training loss: 0.18541358411312103\n",
      "epoch: 4 | 81664 / 114272 | training loss: 0.17816299200057983\n",
      "epoch: 4 | 81696 / 114272 | training loss: 0.0008356713806279004\n",
      "epoch: 4 | 81728 / 114272 | training loss: 0.001210263930261135\n",
      "epoch: 4 | 81760 / 114272 | training loss: 0.0008993392693810165\n",
      "epoch: 4 | 81792 / 114272 | training loss: 0.0806269571185112\n",
      "epoch: 4 | 81824 / 114272 | training loss: 0.0007557118078693748\n",
      "epoch: 4 | 81856 / 114272 | training loss: 0.13027314841747284\n",
      "epoch: 4 | 81888 / 114272 | training loss: 0.015956204384565353\n",
      "epoch: 4 | 81920 / 114272 | training loss: 0.27747946977615356\n",
      "epoch: 4 | 81952 / 114272 | training loss: 0.1304280310869217\n",
      "epoch: 4 | 81984 / 114272 | training loss: 0.0011564269661903381\n",
      "epoch: 4 | 82016 / 114272 | training loss: 0.0014816710026934743\n",
      "epoch: 4 | 82048 / 114272 | training loss: 0.0014087611343711615\n",
      "epoch: 4 | 82080 / 114272 | training loss: 0.07055328041315079\n",
      "epoch: 4 | 82112 / 114272 | training loss: 0.05829139053821564\n",
      "epoch: 4 | 82144 / 114272 | training loss: 0.1974569410085678\n",
      "epoch: 4 | 82176 / 114272 | training loss: 0.12397707998752594\n",
      "epoch: 4 | 82208 / 114272 | training loss: 0.12427432835102081\n",
      "epoch: 4 | 82240 / 114272 | training loss: 0.0011613017413765192\n",
      "epoch: 4 | 82272 / 114272 | training loss: 0.18065151572227478\n",
      "epoch: 4 | 82304 / 114272 | training loss: 0.002695586532354355\n",
      "epoch: 4 | 82336 / 114272 | training loss: 0.0010530706495046616\n",
      "epoch: 4 | 82368 / 114272 | training loss: 0.0013788429787382483\n",
      "epoch: 4 | 82400 / 114272 | training loss: 0.14516715705394745\n",
      "epoch: 4 | 82432 / 114272 | training loss: 0.16946889460086823\n",
      "epoch: 4 | 82464 / 114272 | training loss: 0.018440699204802513\n",
      "epoch: 4 | 82496 / 114272 | training loss: 0.002985832281410694\n",
      "epoch: 4 | 82528 / 114272 | training loss: 0.10660400986671448\n",
      "epoch: 4 | 82560 / 114272 | training loss: 0.2460351437330246\n",
      "epoch: 4 | 82592 / 114272 | training loss: 0.0042810034938156605\n",
      "epoch: 4 | 82624 / 114272 | training loss: 0.006618865299969912\n",
      "epoch: 4 | 82656 / 114272 | training loss: 0.08533958345651627\n",
      "epoch: 4 | 82688 / 114272 | training loss: 0.002371523529291153\n",
      "epoch: 4 | 82720 / 114272 | training loss: 0.09512761235237122\n",
      "epoch: 4 | 82752 / 114272 | training loss: 0.0020878990180790424\n",
      "epoch: 4 | 82784 / 114272 | training loss: 0.07686500996351242\n",
      "epoch: 4 | 82816 / 114272 | training loss: 0.002511034719645977\n",
      "epoch: 4 | 82848 / 114272 | training loss: 0.0017501437105238438\n",
      "epoch: 4 | 82880 / 114272 | training loss: 0.0043419161811470985\n",
      "epoch: 4 | 82912 / 114272 | training loss: 0.07378051429986954\n",
      "epoch: 4 | 82944 / 114272 | training loss: 0.13897916674613953\n",
      "epoch: 4 | 82976 / 114272 | training loss: 0.0016399386804550886\n",
      "epoch: 4 | 83008 / 114272 | training loss: 0.0030949462670832872\n",
      "epoch: 4 | 83040 / 114272 | training loss: 0.004453376401215792\n",
      "epoch: 4 | 83072 / 114272 | training loss: 0.24294567108154297\n",
      "epoch: 4 | 83104 / 114272 | training loss: 0.006951913237571716\n",
      "epoch: 4 | 83136 / 114272 | training loss: 0.03776660934090614\n",
      "epoch: 4 | 83168 / 114272 | training loss: 0.06527183949947357\n",
      "epoch: 4 | 83200 / 114272 | training loss: 0.11440806090831757\n",
      "epoch: 4 | 83232 / 114272 | training loss: 0.08092980086803436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 83264 / 114272 | training loss: 0.001011131564155221\n",
      "epoch: 4 | 83296 / 114272 | training loss: 0.0024664297234266996\n",
      "epoch: 4 | 83328 / 114272 | training loss: 0.12855298817157745\n",
      "epoch: 4 | 83360 / 114272 | training loss: 0.07263544201850891\n",
      "epoch: 4 | 83392 / 114272 | training loss: 0.10362428426742554\n",
      "epoch: 4 | 83424 / 114272 | training loss: 0.3358955681324005\n",
      "epoch: 4 | 83456 / 114272 | training loss: 0.002797140507027507\n",
      "epoch: 4 | 83488 / 114272 | training loss: 0.015913423150777817\n",
      "epoch: 4 | 83520 / 114272 | training loss: 0.001968707889318466\n",
      "epoch: 4 | 83552 / 114272 | training loss: 0.004189136903733015\n",
      "epoch: 4 | 83584 / 114272 | training loss: 0.13858193159103394\n",
      "epoch: 4 | 83616 / 114272 | training loss: 0.01018965058028698\n",
      "epoch: 4 | 83648 / 114272 | training loss: 0.010051516816020012\n",
      "epoch: 4 | 83680 / 114272 | training loss: 0.004417297430336475\n",
      "epoch: 4 | 83712 / 114272 | training loss: 0.006897600367665291\n",
      "epoch: 4 | 83744 / 114272 | training loss: 0.002471315208822489\n",
      "epoch: 4 | 83776 / 114272 | training loss: 0.0027396513614803553\n",
      "epoch: 4 | 83808 / 114272 | training loss: 0.008551879785954952\n",
      "epoch: 4 | 83840 / 114272 | training loss: 0.002450136933475733\n",
      "epoch: 4 | 83872 / 114272 | training loss: 0.05034313723444939\n",
      "epoch: 4 | 83904 / 114272 | training loss: 0.21103739738464355\n",
      "epoch: 4 | 83936 / 114272 | training loss: 0.026563184335827827\n",
      "epoch: 4 | 83968 / 114272 | training loss: 0.10649537295103073\n",
      "epoch: 4 | 84000 / 114272 | training loss: 0.0036454058717936277\n",
      "epoch: 4 | 84032 / 114272 | training loss: 0.29472216963768005\n",
      "epoch: 4 | 84064 / 114272 | training loss: 0.151799738407135\n",
      "epoch: 4 | 84096 / 114272 | training loss: 0.2694462239742279\n",
      "epoch: 4 | 84128 / 114272 | training loss: 0.006276186089962721\n",
      "epoch: 4 | 84160 / 114272 | training loss: 0.07275049388408661\n",
      "epoch: 4 | 84192 / 114272 | training loss: 0.010403109714388847\n",
      "epoch: 4 | 84224 / 114272 | training loss: 0.08166861534118652\n",
      "epoch: 4 | 84256 / 114272 | training loss: 0.06103555113077164\n",
      "epoch: 4 | 84288 / 114272 | training loss: 0.0030630568508058786\n",
      "epoch: 4 | 84320 / 114272 | training loss: 0.007501285523176193\n",
      "epoch: 4 | 84352 / 114272 | training loss: 0.0031397186685353518\n",
      "epoch: 4 | 84384 / 114272 | training loss: 0.0045011527836322784\n",
      "epoch: 4 | 84416 / 114272 | training loss: 0.011610315181314945\n",
      "epoch: 4 | 84448 / 114272 | training loss: 0.001249819528311491\n",
      "epoch: 4 | 84480 / 114272 | training loss: 0.06496143341064453\n",
      "epoch: 4 | 84512 / 114272 | training loss: 0.003667325945571065\n",
      "epoch: 4 | 84544 / 114272 | training loss: 0.057112667709589005\n",
      "epoch: 4 | 84576 / 114272 | training loss: 0.0022307438775897026\n",
      "epoch: 4 | 84608 / 114272 | training loss: 0.12339643388986588\n",
      "epoch: 4 | 84640 / 114272 | training loss: 0.2005205899477005\n",
      "epoch: 4 | 84672 / 114272 | training loss: 0.006251587066799402\n",
      "epoch: 4 | 84704 / 114272 | training loss: 0.005886498373001814\n",
      "epoch: 4 | 84736 / 114272 | training loss: 0.009085897356271744\n",
      "epoch: 4 | 84768 / 114272 | training loss: 0.021870996803045273\n",
      "epoch: 4 | 84800 / 114272 | training loss: 0.05451374128460884\n",
      "epoch: 4 | 84832 / 114272 | training loss: 0.0030825724825263023\n",
      "epoch: 4 | 84864 / 114272 | training loss: 0.1491660177707672\n",
      "epoch: 4 | 84896 / 114272 | training loss: 0.0012911257799714804\n",
      "epoch: 4 | 84928 / 114272 | training loss: 0.009482753463089466\n",
      "epoch: 4 | 84960 / 114272 | training loss: 0.0020552705973386765\n",
      "epoch: 4 | 84992 / 114272 | training loss: 0.002228221157565713\n",
      "epoch: 4 | 85024 / 114272 | training loss: 0.08075879514217377\n",
      "epoch: 4 | 85056 / 114272 | training loss: 0.06313182413578033\n",
      "epoch: 4 | 85088 / 114272 | training loss: 0.005407112650573254\n",
      "epoch: 4 | 85120 / 114272 | training loss: 0.0029701997991651297\n",
      "epoch: 4 | 85152 / 114272 | training loss: 0.0024473569355905056\n",
      "epoch: 4 | 85184 / 114272 | training loss: 0.003912659361958504\n",
      "epoch: 4 | 85216 / 114272 | training loss: 0.004138322081416845\n",
      "epoch: 4 | 85248 / 114272 | training loss: 0.002416044706478715\n",
      "epoch: 4 | 85280 / 114272 | training loss: 0.18671797215938568\n",
      "epoch: 4 | 85312 / 114272 | training loss: 0.005543069913983345\n",
      "epoch: 4 | 85344 / 114272 | training loss: 0.35932645201683044\n",
      "epoch: 4 | 85376 / 114272 | training loss: 0.0017744862707331777\n",
      "epoch: 4 | 85408 / 114272 | training loss: 0.009900369681417942\n",
      "epoch: 4 | 85440 / 114272 | training loss: 0.0063003115355968475\n",
      "epoch: 4 | 85472 / 114272 | training loss: 0.0020328909158706665\n",
      "epoch: 4 | 85504 / 114272 | training loss: 0.001162537606433034\n",
      "epoch: 4 | 85536 / 114272 | training loss: 0.006185521371662617\n",
      "epoch: 4 | 85568 / 114272 | training loss: 0.002655527787283063\n",
      "epoch: 4 | 85600 / 114272 | training loss: 0.002962110796943307\n",
      "epoch: 4 | 85632 / 114272 | training loss: 0.14784672856330872\n",
      "epoch: 4 | 85664 / 114272 | training loss: 0.001291581429541111\n",
      "epoch: 4 | 85696 / 114272 | training loss: 0.005012864712625742\n",
      "epoch: 4 | 85728 / 114272 | training loss: 0.011110477149486542\n",
      "epoch: 4 | 85760 / 114272 | training loss: 0.16885489225387573\n",
      "epoch: 4 | 85792 / 114272 | training loss: 0.28256189823150635\n",
      "epoch: 4 | 85824 / 114272 | training loss: 0.0025768703781068325\n",
      "epoch: 4 | 85856 / 114272 | training loss: 0.12493815273046494\n",
      "epoch: 4 | 85888 / 114272 | training loss: 0.0028610231820493937\n",
      "epoch: 4 | 85920 / 114272 | training loss: 0.003542137099429965\n",
      "epoch: 4 | 85952 / 114272 | training loss: 0.09391318261623383\n",
      "epoch: 4 | 85984 / 114272 | training loss: 0.001198610058054328\n",
      "epoch: 4 | 86016 / 114272 | training loss: 0.040161311626434326\n",
      "epoch: 4 | 86048 / 114272 | training loss: 0.005525604356080294\n",
      "epoch: 4 | 86080 / 114272 | training loss: 0.0032161094713956118\n",
      "epoch: 4 | 86112 / 114272 | training loss: 0.0017827361589297652\n",
      "epoch: 4 | 86144 / 114272 | training loss: 0.011926762759685516\n",
      "epoch: 4 | 86176 / 114272 | training loss: 0.001376876956783235\n",
      "epoch: 4 | 86208 / 114272 | training loss: 0.005205177702009678\n",
      "epoch: 4 | 86240 / 114272 | training loss: 0.0011433128966018558\n",
      "epoch: 4 | 86272 / 114272 | training loss: 0.43101292848587036\n",
      "epoch: 4 | 86304 / 114272 | training loss: 0.004344951827079058\n",
      "epoch: 4 | 86336 / 114272 | training loss: 0.016616640612483025\n",
      "epoch: 4 | 86368 / 114272 | training loss: 0.09992953389883041\n",
      "epoch: 4 | 86400 / 114272 | training loss: 0.006588691845536232\n",
      "epoch: 4 | 86432 / 114272 | training loss: 0.00316395191475749\n",
      "epoch: 4 | 86464 / 114272 | training loss: 0.001567639410495758\n",
      "epoch: 4 | 86496 / 114272 | training loss: 0.003311529289931059\n",
      "epoch: 4 | 86528 / 114272 | training loss: 0.14950083196163177\n",
      "epoch: 4 | 86560 / 114272 | training loss: 0.180669367313385\n",
      "epoch: 4 | 86592 / 114272 | training loss: 0.00471365824341774\n",
      "epoch: 4 | 86624 / 114272 | training loss: 0.0015887407353147864\n",
      "epoch: 4 | 86656 / 114272 | training loss: 0.001956711057573557\n",
      "epoch: 4 | 86688 / 114272 | training loss: 0.1341952234506607\n",
      "epoch: 4 | 86720 / 114272 | training loss: 0.0015760189853608608\n",
      "epoch: 4 | 86752 / 114272 | training loss: 0.035145413130521774\n",
      "epoch: 4 | 86784 / 114272 | training loss: 0.004520612768828869\n",
      "epoch: 4 | 86816 / 114272 | training loss: 0.005370249040424824\n",
      "epoch: 4 | 86848 / 114272 | training loss: 0.11529970169067383\n",
      "epoch: 4 | 86880 / 114272 | training loss: 0.003374245949089527\n",
      "epoch: 4 | 86912 / 114272 | training loss: 0.0015849414048716426\n",
      "epoch: 4 | 86944 / 114272 | training loss: 0.28651338815689087\n",
      "epoch: 4 | 86976 / 114272 | training loss: 0.002271488308906555\n",
      "epoch: 4 | 87008 / 114272 | training loss: 0.007364457473158836\n",
      "epoch: 4 | 87040 / 114272 | training loss: 0.00122992810793221\n",
      "epoch: 4 | 87072 / 114272 | training loss: 0.049317698925733566\n",
      "epoch: 4 | 87104 / 114272 | training loss: 0.2513468563556671\n",
      "epoch: 4 | 87136 / 114272 | training loss: 0.11004141718149185\n",
      "epoch: 4 | 87168 / 114272 | training loss: 0.005495286080986261\n",
      "epoch: 4 | 87200 / 114272 | training loss: 0.003835274139419198\n",
      "epoch: 4 | 87232 / 114272 | training loss: 0.003284064820036292\n",
      "epoch: 4 | 87264 / 114272 | training loss: 0.0032618301920592785\n",
      "epoch: 4 | 87296 / 114272 | training loss: 0.004071605391800404\n",
      "epoch: 4 | 87328 / 114272 | training loss: 0.005009348038583994\n",
      "epoch: 4 | 87360 / 114272 | training loss: 0.004609155002981424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 87392 / 114272 | training loss: 0.17051082849502563\n",
      "epoch: 4 | 87424 / 114272 | training loss: 0.003738688537850976\n",
      "epoch: 4 | 87456 / 114272 | training loss: 0.004169443156570196\n",
      "epoch: 4 | 87488 / 114272 | training loss: 0.004299934487789869\n",
      "epoch: 4 | 87520 / 114272 | training loss: 0.007908446714282036\n",
      "epoch: 4 | 87552 / 114272 | training loss: 0.001556121977046132\n",
      "epoch: 4 | 87584 / 114272 | training loss: 0.006775952875614166\n",
      "epoch: 4 | 87616 / 114272 | training loss: 0.14553070068359375\n",
      "epoch: 4 | 87648 / 114272 | training loss: 0.0873907133936882\n",
      "epoch: 4 | 87680 / 114272 | training loss: 0.0023867744021117687\n",
      "epoch: 4 | 87712 / 114272 | training loss: 0.09672276675701141\n",
      "epoch: 4 | 87744 / 114272 | training loss: 0.0013699602568522096\n",
      "epoch: 4 | 87776 / 114272 | training loss: 0.002574790734797716\n",
      "epoch: 4 | 87808 / 114272 | training loss: 0.002975613344460726\n",
      "epoch: 4 | 87840 / 114272 | training loss: 0.003468182170763612\n",
      "epoch: 4 | 87872 / 114272 | training loss: 0.04408940672874451\n",
      "epoch: 4 | 87904 / 114272 | training loss: 0.004767925478518009\n",
      "epoch: 4 | 87936 / 114272 | training loss: 0.005914963316172361\n",
      "epoch: 4 | 87968 / 114272 | training loss: 0.20754629373550415\n",
      "epoch: 4 | 88000 / 114272 | training loss: 0.007558315992355347\n",
      "epoch: 4 | 88032 / 114272 | training loss: 0.14198631048202515\n",
      "epoch: 4 | 88064 / 114272 | training loss: 0.08773230016231537\n",
      "epoch: 4 | 88096 / 114272 | training loss: 0.00426550442352891\n",
      "epoch: 4 | 88128 / 114272 | training loss: 0.0012408248148858547\n",
      "epoch: 4 | 88160 / 114272 | training loss: 0.0012634005397558212\n",
      "epoch: 4 | 88192 / 114272 | training loss: 0.002222523558884859\n",
      "epoch: 4 | 88224 / 114272 | training loss: 0.28938260674476624\n",
      "epoch: 4 | 88256 / 114272 | training loss: 0.002709651365876198\n",
      "epoch: 4 | 88288 / 114272 | training loss: 0.009217088110744953\n",
      "epoch: 4 | 88320 / 114272 | training loss: 0.09279168397188187\n",
      "epoch: 4 | 88352 / 114272 | training loss: 0.0008347080438397825\n",
      "epoch: 4 | 88384 / 114272 | training loss: 0.22107630968093872\n",
      "epoch: 4 | 88416 / 114272 | training loss: 0.07563377171754837\n",
      "epoch: 4 | 88448 / 114272 | training loss: 0.16682204604148865\n",
      "epoch: 4 | 88480 / 114272 | training loss: 0.0025072728749364614\n",
      "epoch: 4 | 88512 / 114272 | training loss: 0.03352012857794762\n",
      "epoch: 4 | 88544 / 114272 | training loss: 0.22378967702388763\n",
      "epoch: 4 | 88576 / 114272 | training loss: 0.002261343179270625\n",
      "epoch: 4 | 88608 / 114272 | training loss: 0.11268233507871628\n",
      "epoch: 4 | 88640 / 114272 | training loss: 0.0009649795247241855\n",
      "epoch: 4 | 88672 / 114272 | training loss: 0.004995899274945259\n",
      "epoch: 4 | 88704 / 114272 | training loss: 0.01964697241783142\n",
      "epoch: 4 | 88736 / 114272 | training loss: 0.006273484323173761\n",
      "epoch: 4 | 88768 / 114272 | training loss: 0.003148742951452732\n",
      "epoch: 4 | 88800 / 114272 | training loss: 0.0933590829372406\n",
      "epoch: 4 | 88832 / 114272 | training loss: 0.00452959630638361\n",
      "epoch: 4 | 88864 / 114272 | training loss: 0.14150013029575348\n",
      "epoch: 4 | 88896 / 114272 | training loss: 0.0035592701751738787\n",
      "epoch: 4 | 88928 / 114272 | training loss: 0.002084274310618639\n",
      "epoch: 4 | 88960 / 114272 | training loss: 0.002027626149356365\n",
      "epoch: 4 | 88992 / 114272 | training loss: 0.010712094604969025\n",
      "epoch: 4 | 89024 / 114272 | training loss: 0.00909718032926321\n",
      "epoch: 4 | 89056 / 114272 | training loss: 0.004681800492107868\n",
      "epoch: 4 | 89088 / 114272 | training loss: 0.0024325745180249214\n",
      "epoch: 4 | 89120 / 114272 | training loss: 0.11088401079177856\n",
      "epoch: 4 | 89152 / 114272 | training loss: 0.005720641929656267\n",
      "epoch: 4 | 89184 / 114272 | training loss: 0.003826827509328723\n",
      "epoch: 4 | 89216 / 114272 | training loss: 0.004020579159259796\n",
      "epoch: 4 | 89248 / 114272 | training loss: 0.022423775866627693\n",
      "epoch: 4 | 89280 / 114272 | training loss: 0.0038933593314141035\n",
      "epoch: 4 | 89312 / 114272 | training loss: 0.23665234446525574\n",
      "epoch: 4 | 89344 / 114272 | training loss: 0.1433085799217224\n",
      "epoch: 4 | 89376 / 114272 | training loss: 0.0036642306949943304\n",
      "epoch: 4 | 89408 / 114272 | training loss: 0.06554581224918365\n",
      "epoch: 4 | 89440 / 114272 | training loss: 0.0019679018296301365\n",
      "epoch: 4 | 89472 / 114272 | training loss: 0.00668570538982749\n",
      "epoch: 4 | 89504 / 114272 | training loss: 0.16978958249092102\n",
      "epoch: 4 | 89536 / 114272 | training loss: 0.0033098154235631227\n",
      "epoch: 4 | 89568 / 114272 | training loss: 0.001082385890185833\n",
      "epoch: 4 | 89600 / 114272 | training loss: 0.0016078650951385498\n",
      "epoch: 4 | 89632 / 114272 | training loss: 0.0016374310944229364\n",
      "epoch: 4 | 89664 / 114272 | training loss: 0.006990004796534777\n",
      "epoch: 4 | 89696 / 114272 | training loss: 0.001949777826666832\n",
      "epoch: 4 | 89728 / 114272 | training loss: 0.0020365770906209946\n",
      "epoch: 4 | 89760 / 114272 | training loss: 0.00341326673515141\n",
      "epoch: 4 | 89792 / 114272 | training loss: 0.004133016336709261\n",
      "epoch: 4 | 89824 / 114272 | training loss: 0.0009449045173823833\n",
      "epoch: 4 | 89856 / 114272 | training loss: 0.1537109911441803\n",
      "epoch: 4 | 89888 / 114272 | training loss: 0.006390546448528767\n",
      "epoch: 4 | 89920 / 114272 | training loss: 0.002411921974271536\n",
      "epoch: 4 | 89952 / 114272 | training loss: 0.0036783770192414522\n",
      "epoch: 4 | 89984 / 114272 | training loss: 0.002537119435146451\n",
      "epoch: 4 | 90016 / 114272 | training loss: 0.002431868575513363\n",
      "epoch: 4 | 90048 / 114272 | training loss: 0.10316357016563416\n",
      "epoch: 4 | 90080 / 114272 | training loss: 0.05004548281431198\n",
      "epoch: 4 | 90112 / 114272 | training loss: 0.000999914132989943\n",
      "epoch: 4 | 90144 / 114272 | training loss: 0.004520915448665619\n",
      "epoch: 4 | 90176 / 114272 | training loss: 0.15334144234657288\n",
      "epoch: 4 | 90208 / 114272 | training loss: 0.0031606347765773535\n",
      "epoch: 4 | 90240 / 114272 | training loss: 0.0008144769235514104\n",
      "epoch: 4 | 90272 / 114272 | training loss: 0.5494721531867981\n",
      "epoch: 4 | 90304 / 114272 | training loss: 0.0886310413479805\n",
      "epoch: 4 | 90336 / 114272 | training loss: 0.10115903615951538\n",
      "epoch: 4 | 90368 / 114272 | training loss: 0.0035517504438757896\n",
      "epoch: 4 | 90400 / 114272 | training loss: 0.0010562638053670526\n",
      "epoch: 4 | 90432 / 114272 | training loss: 0.10042981058359146\n",
      "epoch: 4 | 90464 / 114272 | training loss: 0.11201122403144836\n",
      "epoch: 4 | 90496 / 114272 | training loss: 0.0033671590499579906\n",
      "epoch: 4 | 90528 / 114272 | training loss: 0.09744966775178909\n",
      "epoch: 4 | 90560 / 114272 | training loss: 0.08240460604429245\n",
      "epoch: 4 | 90592 / 114272 | training loss: 0.005475214682519436\n",
      "epoch: 4 | 90624 / 114272 | training loss: 0.00401002774015069\n",
      "epoch: 4 | 90656 / 114272 | training loss: 0.00576967466622591\n",
      "epoch: 4 | 90688 / 114272 | training loss: 0.0071437484584748745\n",
      "epoch: 4 | 90720 / 114272 | training loss: 0.02333659492433071\n",
      "epoch: 4 | 90752 / 114272 | training loss: 0.005799786187708378\n",
      "epoch: 4 | 90784 / 114272 | training loss: 0.003555792849510908\n",
      "epoch: 4 | 90816 / 114272 | training loss: 0.2424396574497223\n",
      "epoch: 4 | 90848 / 114272 | training loss: 0.0036933121737092733\n",
      "epoch: 4 | 90880 / 114272 | training loss: 0.0019945278763771057\n",
      "epoch: 4 | 90912 / 114272 | training loss: 0.010526389814913273\n",
      "epoch: 4 | 90944 / 114272 | training loss: 0.0803857147693634\n",
      "epoch: 4 | 90976 / 114272 | training loss: 0.2269524484872818\n",
      "epoch: 4 | 91008 / 114272 | training loss: 0.1477660983800888\n",
      "epoch: 4 | 91040 / 114272 | training loss: 0.2096981406211853\n",
      "epoch: 4 | 91072 / 114272 | training loss: 0.001877398113720119\n",
      "epoch: 4 | 91104 / 114272 | training loss: 0.004285153932869434\n",
      "epoch: 4 | 91136 / 114272 | training loss: 0.0014204392209649086\n",
      "epoch: 4 | 91168 / 114272 | training loss: 0.01668596640229225\n",
      "epoch: 4 | 91200 / 114272 | training loss: 0.18333210051059723\n",
      "epoch: 4 | 91232 / 114272 | training loss: 0.005753752775490284\n",
      "epoch: 4 | 91264 / 114272 | training loss: 0.001800841186195612\n",
      "epoch: 4 | 91296 / 114272 | training loss: 0.004063896834850311\n",
      "epoch: 4 | 91328 / 114272 | training loss: 0.10660991817712784\n",
      "epoch: 4 | 91360 / 114272 | training loss: 0.0022675858344882727\n",
      "epoch: 4 | 91392 / 114272 | training loss: 0.011976121924817562\n",
      "epoch: 4 | 91424 / 114272 | training loss: 0.003779433201998472\n",
      "epoch: 4 | 91456 / 114272 | training loss: 0.07642953097820282\n",
      "epoch: 4 | 91488 / 114272 | training loss: 0.0011205078335478902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 91520 / 114272 | training loss: 0.27859315276145935\n",
      "epoch: 4 | 91552 / 114272 | training loss: 0.15026189386844635\n",
      "epoch: 4 | 91584 / 114272 | training loss: 0.1500675529241562\n",
      "epoch: 4 | 91616 / 114272 | training loss: 0.0014692769618704915\n",
      "epoch: 4 | 91648 / 114272 | training loss: 0.28511008620262146\n",
      "epoch: 4 | 91680 / 114272 | training loss: 0.004235023632645607\n",
      "epoch: 4 | 91712 / 114272 | training loss: 0.10194459557533264\n",
      "epoch: 4 | 91744 / 114272 | training loss: 0.046710435301065445\n",
      "epoch: 4 | 91776 / 114272 | training loss: 0.0014777383767068386\n",
      "epoch: 4 | 91808 / 114272 | training loss: 0.059445179998874664\n",
      "epoch: 4 | 91840 / 114272 | training loss: 0.009758778847754002\n",
      "epoch: 4 | 91872 / 114272 | training loss: 0.2895452678203583\n",
      "epoch: 4 | 91904 / 114272 | training loss: 0.004169135354459286\n",
      "epoch: 4 | 91936 / 114272 | training loss: 0.0036895049270242453\n",
      "epoch: 4 | 91968 / 114272 | training loss: 0.003435715101659298\n",
      "epoch: 4 | 92000 / 114272 | training loss: 0.00569473672658205\n",
      "epoch: 4 | 92032 / 114272 | training loss: 0.007978124544024467\n",
      "epoch: 4 | 92064 / 114272 | training loss: 0.006491266191005707\n",
      "epoch: 4 | 92096 / 114272 | training loss: 0.07224445044994354\n",
      "epoch: 4 | 92128 / 114272 | training loss: 0.010817280039191246\n",
      "epoch: 4 | 92160 / 114272 | training loss: 0.002725183265283704\n",
      "epoch: 4 | 92192 / 114272 | training loss: 0.008977515622973442\n",
      "epoch: 4 | 92224 / 114272 | training loss: 0.0056871990673244\n",
      "epoch: 4 | 92256 / 114272 | training loss: 0.003472121199592948\n",
      "epoch: 4 | 92288 / 114272 | training loss: 0.0017782109789550304\n",
      "epoch: 4 | 92320 / 114272 | training loss: 0.0026808620896190405\n",
      "epoch: 4 | 92352 / 114272 | training loss: 0.0017630943330004811\n",
      "epoch: 4 | 92384 / 114272 | training loss: 0.005685701966285706\n",
      "epoch: 4 | 92416 / 114272 | training loss: 0.014779560267925262\n",
      "epoch: 4 | 92448 / 114272 | training loss: 0.002249842742457986\n",
      "epoch: 4 | 92480 / 114272 | training loss: 0.07722142338752747\n",
      "epoch: 4 | 92512 / 114272 | training loss: 0.16991011798381805\n",
      "epoch: 4 | 92544 / 114272 | training loss: 0.001338456990197301\n",
      "epoch: 4 | 92576 / 114272 | training loss: 0.0016809883527457714\n",
      "epoch: 4 | 92608 / 114272 | training loss: 0.00138384522870183\n",
      "epoch: 4 | 92640 / 114272 | training loss: 0.0019215698121115565\n",
      "epoch: 4 | 92672 / 114272 | training loss: 0.0014123918954283\n",
      "epoch: 4 | 92704 / 114272 | training loss: 0.0010140808299183846\n",
      "epoch: 4 | 92736 / 114272 | training loss: 0.171868696808815\n",
      "epoch: 4 | 92768 / 114272 | training loss: 0.010011635720729828\n",
      "epoch: 4 | 92800 / 114272 | training loss: 0.0028689741156995296\n",
      "epoch: 4 | 92832 / 114272 | training loss: 0.0016922749346122146\n",
      "epoch: 4 | 92864 / 114272 | training loss: 0.0014948192983865738\n",
      "epoch: 4 | 92896 / 114272 | training loss: 0.0034099037293344736\n",
      "epoch: 4 | 92928 / 114272 | training loss: 0.00296321720816195\n",
      "epoch: 4 | 92960 / 114272 | training loss: 0.013316487893462181\n",
      "epoch: 4 | 92992 / 114272 | training loss: 0.004557674750685692\n",
      "epoch: 4 | 93024 / 114272 | training loss: 0.019508840516209602\n",
      "epoch: 4 | 93056 / 114272 | training loss: 0.0031635721679776907\n",
      "epoch: 4 | 93088 / 114272 | training loss: 0.0015227985568344593\n",
      "epoch: 4 | 93120 / 114272 | training loss: 0.15609070658683777\n",
      "epoch: 4 | 93152 / 114272 | training loss: 0.0021765620913356543\n",
      "epoch: 4 | 93184 / 114272 | training loss: 0.005860598292201757\n",
      "epoch: 4 | 93216 / 114272 | training loss: 0.0028175278566777706\n",
      "epoch: 4 | 93248 / 114272 | training loss: 0.0022598491050302982\n",
      "epoch: 4 | 93280 / 114272 | training loss: 0.0017362749204039574\n",
      "epoch: 4 | 93312 / 114272 | training loss: 0.09774883836507797\n",
      "epoch: 4 | 93344 / 114272 | training loss: 0.002341389423236251\n",
      "epoch: 4 | 93376 / 114272 | training loss: 0.0013801725581288338\n",
      "epoch: 4 | 93408 / 114272 | training loss: 0.11876443028450012\n",
      "epoch: 4 | 93440 / 114272 | training loss: 0.0015009959461167455\n",
      "epoch: 4 | 93472 / 114272 | training loss: 0.0025842939503490925\n",
      "epoch: 4 | 93504 / 114272 | training loss: 0.25965988636016846\n",
      "epoch: 4 | 93536 / 114272 | training loss: 0.0014934833161532879\n",
      "epoch: 4 | 93568 / 114272 | training loss: 0.03736589476466179\n",
      "epoch: 4 | 93600 / 114272 | training loss: 0.0029686286579817533\n",
      "epoch: 4 | 93632 / 114272 | training loss: 0.0015756635693833232\n",
      "epoch: 4 | 93664 / 114272 | training loss: 0.002977772383019328\n",
      "epoch: 4 | 93696 / 114272 | training loss: 0.22379152476787567\n",
      "epoch: 4 | 93728 / 114272 | training loss: 0.00569101981818676\n",
      "epoch: 4 | 93760 / 114272 | training loss: 0.0017267813673242927\n",
      "epoch: 4 | 93792 / 114272 | training loss: 0.008551758714020252\n",
      "epoch: 4 | 93824 / 114272 | training loss: 0.008261546492576599\n",
      "epoch: 4 | 93856 / 114272 | training loss: 0.1022268682718277\n",
      "epoch: 4 | 93888 / 114272 | training loss: 0.3794020712375641\n",
      "epoch: 4 | 93920 / 114272 | training loss: 0.0058405110612511635\n",
      "epoch: 4 | 93952 / 114272 | training loss: 0.33309394121170044\n",
      "epoch: 4 | 93984 / 114272 | training loss: 0.008719860576093197\n",
      "epoch: 4 | 94016 / 114272 | training loss: 0.003752102144062519\n",
      "epoch: 4 | 94048 / 114272 | training loss: 0.05010969564318657\n",
      "epoch: 4 | 94080 / 114272 | training loss: 0.0010985396802425385\n",
      "epoch: 4 | 94112 / 114272 | training loss: 0.006305956281721592\n",
      "epoch: 4 | 94144 / 114272 | training loss: 0.02014031819999218\n",
      "epoch: 4 | 94176 / 114272 | training loss: 0.011202352121472359\n",
      "epoch: 4 | 94208 / 114272 | training loss: 0.0034908028319478035\n",
      "epoch: 4 | 94240 / 114272 | training loss: 0.062164682894945145\n",
      "epoch: 4 | 94272 / 114272 | training loss: 0.0018816548399627209\n",
      "epoch: 4 | 94304 / 114272 | training loss: 0.002718444215133786\n",
      "epoch: 4 | 94336 / 114272 | training loss: 0.1326838731765747\n",
      "epoch: 4 | 94368 / 114272 | training loss: 0.006510178092867136\n",
      "epoch: 4 | 94400 / 114272 | training loss: 0.09077657014131546\n",
      "epoch: 4 | 94432 / 114272 | training loss: 0.002547444077208638\n",
      "epoch: 4 | 94464 / 114272 | training loss: 0.2607012987136841\n",
      "epoch: 4 | 94496 / 114272 | training loss: 0.03848816826939583\n",
      "epoch: 4 | 94528 / 114272 | training loss: 0.0022910835687071085\n",
      "epoch: 4 | 94560 / 114272 | training loss: 0.054257120937108994\n",
      "epoch: 4 | 94592 / 114272 | training loss: 0.0018243968952447176\n",
      "epoch: 4 | 94624 / 114272 | training loss: 0.3091960847377777\n",
      "epoch: 4 | 94656 / 114272 | training loss: 0.17253348231315613\n",
      "epoch: 4 | 94688 / 114272 | training loss: 0.005183087196201086\n",
      "epoch: 4 | 94720 / 114272 | training loss: 0.0026793491560965776\n",
      "epoch: 4 | 94752 / 114272 | training loss: 0.0022460988257080317\n",
      "epoch: 4 | 94784 / 114272 | training loss: 0.06909086555242538\n",
      "epoch: 4 | 94816 / 114272 | training loss: 0.18288001418113708\n",
      "epoch: 4 | 94848 / 114272 | training loss: 0.10709763318300247\n",
      "epoch: 4 | 94880 / 114272 | training loss: 0.0009771987097337842\n",
      "epoch: 4 | 94912 / 114272 | training loss: 0.001613274565897882\n",
      "epoch: 4 | 94944 / 114272 | training loss: 0.005472540855407715\n",
      "epoch: 4 | 94976 / 114272 | training loss: 0.0861298069357872\n",
      "epoch: 4 | 95008 / 114272 | training loss: 0.19325581192970276\n",
      "epoch: 4 | 95040 / 114272 | training loss: 0.0032503290567547083\n",
      "epoch: 4 | 95072 / 114272 | training loss: 0.00365286017768085\n",
      "epoch: 4 | 95104 / 114272 | training loss: 0.003672167891636491\n",
      "epoch: 4 | 95136 / 114272 | training loss: 0.007505952380597591\n",
      "epoch: 4 | 95168 / 114272 | training loss: 0.011809490621089935\n",
      "epoch: 4 | 95200 / 114272 | training loss: 0.20144161581993103\n",
      "epoch: 4 | 95232 / 114272 | training loss: 0.0013458292232826352\n",
      "epoch: 4 | 95264 / 114272 | training loss: 0.0014757334720343351\n",
      "epoch: 4 | 95296 / 114272 | training loss: 0.26261746883392334\n",
      "epoch: 4 | 95328 / 114272 | training loss: 0.23543235659599304\n",
      "epoch: 4 | 95360 / 114272 | training loss: 0.002135358052328229\n",
      "epoch: 4 | 95392 / 114272 | training loss: 0.006343455985188484\n",
      "epoch: 4 | 95424 / 114272 | training loss: 0.0016923591028898954\n",
      "epoch: 4 | 95456 / 114272 | training loss: 0.0015692447777837515\n",
      "epoch: 4 | 95488 / 114272 | training loss: 0.005532972048968077\n",
      "epoch: 4 | 95520 / 114272 | training loss: 0.006847755052149296\n",
      "epoch: 4 | 95552 / 114272 | training loss: 0.0030382811091840267\n",
      "epoch: 4 | 95584 / 114272 | training loss: 0.17308564484119415\n",
      "epoch: 4 | 95616 / 114272 | training loss: 0.004704724997282028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 95648 / 114272 | training loss: 0.005376036278903484\n",
      "epoch: 4 | 95680 / 114272 | training loss: 0.0021546261850744486\n",
      "epoch: 4 | 95712 / 114272 | training loss: 0.3115958273410797\n",
      "epoch: 4 | 95744 / 114272 | training loss: 0.002325537381693721\n",
      "epoch: 4 | 95776 / 114272 | training loss: 0.0021068211644887924\n",
      "epoch: 4 | 95808 / 114272 | training loss: 0.0017761908238753676\n",
      "epoch: 4 | 95840 / 114272 | training loss: 0.0018786382861435413\n",
      "epoch: 4 | 95872 / 114272 | training loss: 0.0025945650413632393\n",
      "epoch: 4 | 95904 / 114272 | training loss: 0.09080523997545242\n",
      "epoch: 4 | 95936 / 114272 | training loss: 0.005332628730684519\n",
      "epoch: 4 | 95968 / 114272 | training loss: 0.1884172409772873\n",
      "epoch: 4 | 96000 / 114272 | training loss: 0.003341589355841279\n",
      "epoch: 4 | 96032 / 114272 | training loss: 0.001426947070285678\n",
      "epoch: 4 | 96064 / 114272 | training loss: 0.18843692541122437\n",
      "epoch: 4 | 96096 / 114272 | training loss: 0.0020731943659484386\n",
      "epoch: 4 | 96128 / 114272 | training loss: 0.06686437875032425\n",
      "epoch: 4 | 96160 / 114272 | training loss: 0.0068074436858296394\n",
      "epoch: 4 | 96192 / 114272 | training loss: 0.0027483676094561815\n",
      "epoch: 4 | 96224 / 114272 | training loss: 0.016445886343717575\n",
      "epoch: 4 | 96256 / 114272 | training loss: 0.0028238212689757347\n",
      "epoch: 4 | 96288 / 114272 | training loss: 0.0025039867032319307\n",
      "epoch: 4 | 96320 / 114272 | training loss: 0.0030405977740883827\n",
      "epoch: 4 | 96352 / 114272 | training loss: 0.0025263542775064707\n",
      "epoch: 4 | 96384 / 114272 | training loss: 0.15396946668624878\n",
      "epoch: 4 | 96416 / 114272 | training loss: 0.3416413962841034\n",
      "epoch: 4 | 96448 / 114272 | training loss: 0.004928118549287319\n",
      "epoch: 4 | 96480 / 114272 | training loss: 0.0039666988886892796\n",
      "epoch: 4 | 96512 / 114272 | training loss: 0.10894490033388138\n",
      "epoch: 4 | 96544 / 114272 | training loss: 0.10359840095043182\n",
      "epoch: 4 | 96576 / 114272 | training loss: 0.004774336703121662\n",
      "epoch: 4 | 96608 / 114272 | training loss: 0.037130214273929596\n",
      "epoch: 4 | 96640 / 114272 | training loss: 0.0029419693164527416\n",
      "epoch: 4 | 96672 / 114272 | training loss: 0.07781374454498291\n",
      "epoch: 4 | 96704 / 114272 | training loss: 0.001363407471217215\n",
      "epoch: 4 | 96736 / 114272 | training loss: 0.3000154197216034\n",
      "epoch: 4 | 96768 / 114272 | training loss: 0.006189075298607349\n",
      "epoch: 4 | 96800 / 114272 | training loss: 0.0018790848553180695\n",
      "epoch: 4 | 96832 / 114272 | training loss: 0.30059415102005005\n",
      "epoch: 4 | 96864 / 114272 | training loss: 0.14153771102428436\n",
      "epoch: 4 | 96896 / 114272 | training loss: 0.0035884869284927845\n",
      "epoch: 4 | 96928 / 114272 | training loss: 0.0037908738013356924\n",
      "epoch: 4 | 96960 / 114272 | training loss: 0.003451630473136902\n",
      "epoch: 4 | 96992 / 114272 | training loss: 0.004329909570515156\n",
      "epoch: 4 | 97024 / 114272 | training loss: 0.006607320159673691\n",
      "epoch: 4 | 97056 / 114272 | training loss: 0.11708223074674606\n",
      "epoch: 4 | 97088 / 114272 | training loss: 0.002040182938799262\n",
      "epoch: 4 | 97120 / 114272 | training loss: 0.001382965361699462\n",
      "epoch: 4 | 97152 / 114272 | training loss: 0.002398967044427991\n",
      "epoch: 4 | 97184 / 114272 | training loss: 0.00744661083444953\n",
      "epoch: 4 | 97216 / 114272 | training loss: 0.0075519937090575695\n",
      "epoch: 4 | 97248 / 114272 | training loss: 0.003029797226190567\n",
      "epoch: 4 | 97280 / 114272 | training loss: 0.12494252622127533\n",
      "epoch: 4 | 97312 / 114272 | training loss: 0.07597916573286057\n",
      "epoch: 4 | 97344 / 114272 | training loss: 0.005069315899163485\n",
      "epoch: 4 | 97376 / 114272 | training loss: 0.12307412177324295\n",
      "epoch: 4 | 97408 / 114272 | training loss: 0.016691509634256363\n",
      "epoch: 4 | 97440 / 114272 | training loss: 0.0029024214018136263\n",
      "epoch: 4 | 97472 / 114272 | training loss: 0.1466570645570755\n",
      "epoch: 4 | 97504 / 114272 | training loss: 0.004021198023110628\n",
      "epoch: 4 | 97536 / 114272 | training loss: 0.005748461000621319\n",
      "epoch: 4 | 97568 / 114272 | training loss: 0.10479411482810974\n",
      "epoch: 4 | 97600 / 114272 | training loss: 0.005986110772937536\n",
      "epoch: 4 | 97632 / 114272 | training loss: 0.002992409048601985\n",
      "epoch: 4 | 97664 / 114272 | training loss: 0.005177298095077276\n",
      "epoch: 4 | 97696 / 114272 | training loss: 0.009448553435504436\n",
      "epoch: 4 | 97728 / 114272 | training loss: 0.11248710751533508\n",
      "epoch: 4 | 97760 / 114272 | training loss: 0.0029774224385619164\n",
      "epoch: 4 | 97792 / 114272 | training loss: 0.04843779280781746\n",
      "epoch: 4 | 97824 / 114272 | training loss: 0.04735337570309639\n",
      "epoch: 4 | 97856 / 114272 | training loss: 0.0011078582610934973\n",
      "epoch: 4 | 97888 / 114272 | training loss: 0.00514958193525672\n",
      "epoch: 4 | 97920 / 114272 | training loss: 0.004465232603251934\n",
      "epoch: 4 | 97952 / 114272 | training loss: 0.18055401742458344\n",
      "epoch: 4 | 97984 / 114272 | training loss: 0.00535290502011776\n",
      "epoch: 4 | 98016 / 114272 | training loss: 0.13365206122398376\n",
      "epoch: 4 | 98048 / 114272 | training loss: 0.003321056254208088\n",
      "epoch: 4 | 98080 / 114272 | training loss: 0.0034143454395234585\n",
      "epoch: 4 | 98112 / 114272 | training loss: 0.00220391433686018\n",
      "epoch: 4 | 98144 / 114272 | training loss: 0.0026156040839850903\n",
      "epoch: 4 | 98176 / 114272 | training loss: 0.006270758807659149\n",
      "epoch: 4 | 98208 / 114272 | training loss: 0.1159207820892334\n",
      "epoch: 4 | 98240 / 114272 | training loss: 0.0025088891852647066\n",
      "epoch: 4 | 98272 / 114272 | training loss: 0.004220087081193924\n",
      "epoch: 4 | 98304 / 114272 | training loss: 0.07594794780015945\n",
      "epoch: 4 | 98336 / 114272 | training loss: 0.001960947411134839\n",
      "epoch: 4 | 98368 / 114272 | training loss: 0.1317051202058792\n",
      "epoch: 4 | 98400 / 114272 | training loss: 0.008310340344905853\n",
      "epoch: 4 | 98432 / 114272 | training loss: 0.001608117832802236\n",
      "epoch: 4 | 98464 / 114272 | training loss: 0.007805005181580782\n",
      "epoch: 4 | 98496 / 114272 | training loss: 0.10102817416191101\n",
      "epoch: 4 | 98528 / 114272 | training loss: 0.05112891271710396\n",
      "epoch: 4 | 98560 / 114272 | training loss: 0.002839918015524745\n",
      "epoch: 4 | 98592 / 114272 | training loss: 0.004308902192860842\n",
      "epoch: 4 | 98624 / 114272 | training loss: 0.23251459002494812\n",
      "epoch: 4 | 98656 / 114272 | training loss: 0.29746392369270325\n",
      "epoch: 4 | 98688 / 114272 | training loss: 0.31810054183006287\n",
      "epoch: 4 | 98720 / 114272 | training loss: 0.0032649473287165165\n",
      "epoch: 4 | 98752 / 114272 | training loss: 0.0012029395438730717\n",
      "epoch: 4 | 98784 / 114272 | training loss: 0.0034137899056077003\n",
      "epoch: 4 | 98816 / 114272 | training loss: 0.011943545192480087\n",
      "epoch: 4 | 98848 / 114272 | training loss: 0.007261714898049831\n",
      "epoch: 4 | 98880 / 114272 | training loss: 0.10014760494232178\n",
      "epoch: 4 | 98912 / 114272 | training loss: 0.05678325518965721\n",
      "epoch: 4 | 98944 / 114272 | training loss: 0.11388247460126877\n",
      "epoch: 4 | 98976 / 114272 | training loss: 0.18166720867156982\n",
      "epoch: 4 | 99008 / 114272 | training loss: 0.0023878905922174454\n",
      "epoch: 4 | 99040 / 114272 | training loss: 0.007926351390779018\n",
      "epoch: 4 | 99072 / 114272 | training loss: 0.08301117271184921\n",
      "epoch: 4 | 99104 / 114272 | training loss: 0.004039711784571409\n",
      "epoch: 4 | 99136 / 114272 | training loss: 0.21663454174995422\n",
      "epoch: 4 | 99168 / 114272 | training loss: 0.0053045437671244144\n",
      "epoch: 4 | 99200 / 114272 | training loss: 0.008271392434835434\n",
      "epoch: 4 | 99232 / 114272 | training loss: 0.002383830025792122\n",
      "epoch: 4 | 99264 / 114272 | training loss: 0.09818467497825623\n",
      "epoch: 4 | 99296 / 114272 | training loss: 0.004024476278573275\n",
      "epoch: 4 | 99328 / 114272 | training loss: 0.00954373273998499\n",
      "epoch: 4 | 99360 / 114272 | training loss: 0.1335284560918808\n",
      "epoch: 4 | 99392 / 114272 | training loss: 0.03942084684967995\n",
      "epoch: 4 | 99424 / 114272 | training loss: 0.11208654195070267\n",
      "epoch: 4 | 99456 / 114272 | training loss: 0.14031045138835907\n",
      "epoch: 4 | 99488 / 114272 | training loss: 0.005945819895714521\n",
      "epoch: 4 | 99520 / 114272 | training loss: 0.010932430624961853\n",
      "epoch: 4 | 99552 / 114272 | training loss: 0.09942030906677246\n",
      "epoch: 4 | 99584 / 114272 | training loss: 0.02767559140920639\n",
      "epoch: 4 | 99616 / 114272 | training loss: 0.005028692074120045\n",
      "epoch: 4 | 99648 / 114272 | training loss: 0.009579271078109741\n",
      "epoch: 4 | 99680 / 114272 | training loss: 0.1237119510769844\n",
      "epoch: 4 | 99712 / 114272 | training loss: 0.0018144468776881695\n",
      "epoch: 4 | 99744 / 114272 | training loss: 0.053620580583810806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 99776 / 114272 | training loss: 0.0036413816269487143\n",
      "epoch: 4 | 99808 / 114272 | training loss: 0.08210304379463196\n",
      "epoch: 4 | 99840 / 114272 | training loss: 0.0031804873142391443\n",
      "epoch: 4 | 99872 / 114272 | training loss: 0.004859035834670067\n",
      "epoch: 4 | 99904 / 114272 | training loss: 0.07326340675354004\n",
      "epoch: 4 | 99936 / 114272 | training loss: 0.005517631769180298\n",
      "epoch: 4 | 99968 / 114272 | training loss: 0.004028317052870989\n",
      "epoch: 4 | 100000 / 114272 | training loss: 0.0020194011740386486\n",
      "epoch: 4 | 100032 / 114272 | training loss: 0.005249444395303726\n",
      "epoch: 4 | 100064 / 114272 | training loss: 0.04963602125644684\n",
      "epoch: 4 | 100096 / 114272 | training loss: 0.11009492725133896\n",
      "epoch: 4 | 100128 / 114272 | training loss: 0.004906949587166309\n",
      "epoch: 4 | 100160 / 114272 | training loss: 0.06551504880189896\n",
      "epoch: 4 | 100192 / 114272 | training loss: 0.007182948291301727\n",
      "epoch: 4 | 100224 / 114272 | training loss: 0.005370874889194965\n",
      "epoch: 4 | 100256 / 114272 | training loss: 0.006935912650078535\n",
      "epoch: 4 | 100288 / 114272 | training loss: 0.10353555530309677\n",
      "epoch: 4 | 100320 / 114272 | training loss: 0.00615778099745512\n",
      "epoch: 4 | 100352 / 114272 | training loss: 0.005913260392844677\n",
      "epoch: 4 | 100384 / 114272 | training loss: 0.009640146046876907\n",
      "epoch: 4 | 100416 / 114272 | training loss: 0.0068500349298119545\n",
      "epoch: 4 | 100448 / 114272 | training loss: 0.033176276832818985\n",
      "epoch: 4 | 100480 / 114272 | training loss: 0.10375722497701645\n",
      "epoch: 4 | 100512 / 114272 | training loss: 0.03948148339986801\n",
      "epoch: 4 | 100544 / 114272 | training loss: 0.08914999663829803\n",
      "epoch: 4 | 100576 / 114272 | training loss: 0.0016106582479551435\n",
      "epoch: 4 | 100608 / 114272 | training loss: 0.02896343171596527\n",
      "epoch: 4 | 100640 / 114272 | training loss: 0.0031562710646539927\n",
      "epoch: 4 | 100672 / 114272 | training loss: 0.08269999921321869\n",
      "epoch: 4 | 100704 / 114272 | training loss: 0.024660583585500717\n",
      "epoch: 4 | 100736 / 114272 | training loss: 0.23339316248893738\n",
      "epoch: 4 | 100768 / 114272 | training loss: 0.0038102592807263136\n",
      "epoch: 4 | 100800 / 114272 | training loss: 0.004456149414181709\n",
      "epoch: 4 | 100832 / 114272 | training loss: 0.0018922743620350957\n",
      "epoch: 4 | 100864 / 114272 | training loss: 0.0017745208460837603\n",
      "epoch: 4 | 100896 / 114272 | training loss: 0.0013293405063450336\n",
      "epoch: 4 | 100928 / 114272 | training loss: 0.0012388686882331967\n",
      "epoch: 4 | 100960 / 114272 | training loss: 0.10497519373893738\n",
      "epoch: 4 | 100992 / 114272 | training loss: 0.005299276672303677\n",
      "epoch: 4 | 101024 / 114272 | training loss: 0.0024466952309012413\n",
      "epoch: 4 | 101056 / 114272 | training loss: 0.0016168636502698064\n",
      "epoch: 4 | 101088 / 114272 | training loss: 0.0031687510199844837\n",
      "epoch: 4 | 101120 / 114272 | training loss: 0.0013591957977041602\n",
      "epoch: 4 | 101152 / 114272 | training loss: 0.001206058426760137\n",
      "epoch: 4 | 101184 / 114272 | training loss: 0.08118969947099686\n",
      "epoch: 4 | 101216 / 114272 | training loss: 0.0015531460521742702\n",
      "epoch: 4 | 101248 / 114272 | training loss: 0.26185309886932373\n",
      "epoch: 4 | 101280 / 114272 | training loss: 0.007046365644782782\n",
      "epoch: 4 | 101312 / 114272 | training loss: 0.0012322395341470838\n",
      "epoch: 4 | 101344 / 114272 | training loss: 0.0012470065848901868\n",
      "epoch: 4 | 101376 / 114272 | training loss: 0.24486714601516724\n",
      "epoch: 4 | 101408 / 114272 | training loss: 0.0006139002507552505\n",
      "epoch: 4 | 101440 / 114272 | training loss: 0.096490278840065\n",
      "epoch: 4 | 101472 / 114272 | training loss: 0.08998732268810272\n",
      "epoch: 4 | 101504 / 114272 | training loss: 0.19252601265907288\n",
      "epoch: 4 | 101536 / 114272 | training loss: 0.36712774634361267\n",
      "epoch: 4 | 101568 / 114272 | training loss: 0.0008372332085855305\n",
      "epoch: 4 | 101600 / 114272 | training loss: 0.0039024739526212215\n",
      "epoch: 4 | 101632 / 114272 | training loss: 0.1183890625834465\n",
      "epoch: 4 | 101664 / 114272 | training loss: 0.06656922399997711\n",
      "epoch: 4 | 101696 / 114272 | training loss: 0.045568134635686874\n",
      "epoch: 4 | 101728 / 114272 | training loss: 0.0026956796646118164\n",
      "epoch: 4 | 101760 / 114272 | training loss: 0.09756214171648026\n",
      "epoch: 4 | 101792 / 114272 | training loss: 0.0723404660820961\n",
      "epoch: 4 | 101824 / 114272 | training loss: 0.001649048994295299\n",
      "epoch: 4 | 101856 / 114272 | training loss: 0.0005690862308256328\n",
      "epoch: 4 | 101888 / 114272 | training loss: 0.005505683831870556\n",
      "epoch: 4 | 101920 / 114272 | training loss: 0.1863105297088623\n",
      "epoch: 4 | 101952 / 114272 | training loss: 0.004967196378856897\n",
      "epoch: 4 | 101984 / 114272 | training loss: 0.002012742217630148\n",
      "epoch: 4 | 102016 / 114272 | training loss: 0.18134765326976776\n",
      "epoch: 4 | 102048 / 114272 | training loss: 0.005263327155262232\n",
      "epoch: 4 | 102080 / 114272 | training loss: 0.0019090702990069985\n",
      "epoch: 4 | 102112 / 114272 | training loss: 0.0024820759426802397\n",
      "epoch: 4 | 102144 / 114272 | training loss: 0.12066139280796051\n",
      "epoch: 4 | 102176 / 114272 | training loss: 0.00356311840005219\n",
      "epoch: 4 | 102208 / 114272 | training loss: 0.07003366947174072\n",
      "epoch: 4 | 102240 / 114272 | training loss: 0.06760560721158981\n",
      "epoch: 4 | 102272 / 114272 | training loss: 0.14666004478931427\n",
      "epoch: 4 | 102304 / 114272 | training loss: 0.006060367450118065\n",
      "epoch: 4 | 102336 / 114272 | training loss: 0.002004550537094474\n",
      "epoch: 4 | 102368 / 114272 | training loss: 0.002891688607633114\n",
      "epoch: 4 | 102400 / 114272 | training loss: 0.06820690631866455\n",
      "epoch: 4 | 102432 / 114272 | training loss: 0.004490228369832039\n",
      "epoch: 4 | 102464 / 114272 | training loss: 0.14060144126415253\n",
      "epoch: 4 | 102496 / 114272 | training loss: 0.09524260461330414\n",
      "epoch: 4 | 102528 / 114272 | training loss: 0.00877925381064415\n",
      "epoch: 4 | 102560 / 114272 | training loss: 0.00827738456428051\n",
      "epoch: 4 | 102592 / 114272 | training loss: 0.323556512594223\n",
      "epoch: 4 | 102624 / 114272 | training loss: 0.006427082698792219\n",
      "epoch: 4 | 102656 / 114272 | training loss: 0.007130004465579987\n",
      "epoch: 4 | 102688 / 114272 | training loss: 0.005043299403041601\n",
      "epoch: 4 | 102720 / 114272 | training loss: 0.007300667930394411\n",
      "epoch: 4 | 102752 / 114272 | training loss: 0.20078393816947937\n",
      "epoch: 4 | 102784 / 114272 | training loss: 0.11661665141582489\n",
      "epoch: 4 | 102816 / 114272 | training loss: 0.10260938853025436\n",
      "epoch: 4 | 102848 / 114272 | training loss: 0.00614086864516139\n",
      "epoch: 4 | 102880 / 114272 | training loss: 0.20271235704421997\n",
      "epoch: 4 | 102912 / 114272 | training loss: 0.18166205286979675\n",
      "epoch: 4 | 102944 / 114272 | training loss: 0.012812883593142033\n",
      "epoch: 4 | 102976 / 114272 | training loss: 0.006235362961888313\n",
      "epoch: 4 | 103008 / 114272 | training loss: 0.004536645952612162\n",
      "epoch: 4 | 103040 / 114272 | training loss: 0.05564152076840401\n",
      "epoch: 4 | 103072 / 114272 | training loss: 0.00464871758595109\n",
      "epoch: 4 | 103104 / 114272 | training loss: 0.023171361535787582\n",
      "epoch: 4 | 103136 / 114272 | training loss: 0.024647429585456848\n",
      "epoch: 4 | 103168 / 114272 | training loss: 0.0033281079959124327\n",
      "epoch: 4 | 103200 / 114272 | training loss: 0.10111968964338303\n",
      "epoch: 4 | 103232 / 114272 | training loss: 0.004421823658049107\n",
      "epoch: 4 | 103264 / 114272 | training loss: 0.0035907765850424767\n",
      "epoch: 4 | 103296 / 114272 | training loss: 0.0038169389590620995\n",
      "epoch: 4 | 103328 / 114272 | training loss: 0.002569424221292138\n",
      "epoch: 4 | 103360 / 114272 | training loss: 0.0035723531618714333\n",
      "epoch: 4 | 103392 / 114272 | training loss: 0.07364299148321152\n",
      "epoch: 4 | 103424 / 114272 | training loss: 0.21081630885601044\n",
      "epoch: 4 | 103456 / 114272 | training loss: 0.005841135047376156\n",
      "epoch: 4 | 103488 / 114272 | training loss: 0.004878843668848276\n",
      "epoch: 4 | 103520 / 114272 | training loss: 0.005828949622809887\n",
      "epoch: 4 | 103552 / 114272 | training loss: 0.0053156474605202675\n",
      "epoch: 4 | 103584 / 114272 | training loss: 0.46413570642471313\n",
      "epoch: 4 | 103616 / 114272 | training loss: 0.0032511744648218155\n",
      "epoch: 4 | 103648 / 114272 | training loss: 0.08977987617254257\n",
      "epoch: 4 | 103680 / 114272 | training loss: 0.004349701572209597\n",
      "epoch: 4 | 103712 / 114272 | training loss: 0.06309937685728073\n",
      "epoch: 4 | 103744 / 114272 | training loss: 0.005416433326900005\n",
      "epoch: 4 | 103776 / 114272 | training loss: 0.007262985687702894\n",
      "epoch: 4 | 103808 / 114272 | training loss: 0.003878240706399083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 103840 / 114272 | training loss: 0.0025717527605593204\n",
      "epoch: 4 | 103872 / 114272 | training loss: 0.2433995008468628\n",
      "epoch: 4 | 103904 / 114272 | training loss: 0.07000605762004852\n",
      "epoch: 4 | 103936 / 114272 | training loss: 0.0021953799296170473\n",
      "epoch: 4 | 103968 / 114272 | training loss: 0.20409463346004486\n",
      "epoch: 4 | 104000 / 114272 | training loss: 0.002217858098447323\n",
      "epoch: 4 | 104032 / 114272 | training loss: 0.0012640742352232337\n",
      "epoch: 4 | 104064 / 114272 | training loss: 0.2767079472541809\n",
      "epoch: 4 | 104096 / 114272 | training loss: 0.0025414556730538607\n",
      "epoch: 4 | 104128 / 114272 | training loss: 0.008211685344576836\n",
      "epoch: 4 | 104160 / 114272 | training loss: 0.08107012510299683\n",
      "epoch: 4 | 104192 / 114272 | training loss: 0.09255032241344452\n",
      "epoch: 4 | 104224 / 114272 | training loss: 0.0027782071847468615\n",
      "epoch: 4 | 104256 / 114272 | training loss: 0.08333906531333923\n",
      "epoch: 4 | 104288 / 114272 | training loss: 0.002300351159647107\n",
      "epoch: 4 | 104320 / 114272 | training loss: 0.006080930586904287\n",
      "epoch: 4 | 104352 / 114272 | training loss: 0.15463753044605255\n",
      "epoch: 4 | 104384 / 114272 | training loss: 0.0018095518462359905\n",
      "epoch: 4 | 104416 / 114272 | training loss: 0.0033918372355401516\n",
      "epoch: 4 | 104448 / 114272 | training loss: 0.06899721175432205\n",
      "epoch: 4 | 104480 / 114272 | training loss: 0.1152040958404541\n",
      "epoch: 4 | 104512 / 114272 | training loss: 0.0028439557645469904\n",
      "epoch: 4 | 104544 / 114272 | training loss: 0.018770968541502953\n",
      "epoch: 4 | 104576 / 114272 | training loss: 0.01604670099914074\n",
      "epoch: 4 | 104608 / 114272 | training loss: 0.0017933603376150131\n",
      "epoch: 4 | 104640 / 114272 | training loss: 0.0029435090254992247\n",
      "epoch: 4 | 104672 / 114272 | training loss: 0.002067851135507226\n",
      "epoch: 4 | 104704 / 114272 | training loss: 0.00188657664693892\n",
      "epoch: 4 | 104736 / 114272 | training loss: 0.08600844442844391\n",
      "epoch: 4 | 104768 / 114272 | training loss: 0.0014771465212106705\n",
      "epoch: 4 | 104800 / 114272 | training loss: 0.0067326645366847515\n",
      "epoch: 4 | 104832 / 114272 | training loss: 0.10170696675777435\n",
      "epoch: 4 | 104864 / 114272 | training loss: 0.003621244803071022\n",
      "epoch: 4 | 104896 / 114272 | training loss: 0.08453786373138428\n",
      "epoch: 4 | 104928 / 114272 | training loss: 0.12188655883073807\n",
      "epoch: 4 | 104960 / 114272 | training loss: 0.007029082626104355\n",
      "epoch: 4 | 104992 / 114272 | training loss: 0.20599518716335297\n",
      "epoch: 4 | 105024 / 114272 | training loss: 0.0013220597757026553\n",
      "epoch: 4 | 105056 / 114272 | training loss: 0.0035966970026493073\n",
      "epoch: 4 | 105088 / 114272 | training loss: 0.000871038471814245\n",
      "epoch: 4 | 105120 / 114272 | training loss: 0.054892949759960175\n",
      "epoch: 4 | 105152 / 114272 | training loss: 0.0018347299192100763\n",
      "epoch: 4 | 105184 / 114272 | training loss: 0.03323386237025261\n",
      "epoch: 4 | 105216 / 114272 | training loss: 0.0038189678452908993\n",
      "epoch: 4 | 105248 / 114272 | training loss: 0.20022748410701752\n",
      "epoch: 4 | 105280 / 114272 | training loss: 0.09255260974168777\n",
      "epoch: 4 | 105312 / 114272 | training loss: 0.2560962736606598\n",
      "epoch: 4 | 105344 / 114272 | training loss: 0.13768236339092255\n",
      "epoch: 4 | 105376 / 114272 | training loss: 0.0018445949535816908\n",
      "epoch: 4 | 105408 / 114272 | training loss: 0.023118136450648308\n",
      "epoch: 4 | 105440 / 114272 | training loss: 0.06376905739307404\n",
      "epoch: 4 | 105472 / 114272 | training loss: 0.04896366223692894\n",
      "epoch: 4 | 105504 / 114272 | training loss: 0.003962805960327387\n",
      "epoch: 4 | 105536 / 114272 | training loss: 0.004360463470220566\n",
      "epoch: 4 | 105568 / 114272 | training loss: 0.010200133547186852\n",
      "epoch: 4 | 105600 / 114272 | training loss: 0.0033165363129228354\n",
      "epoch: 4 | 105632 / 114272 | training loss: 0.001235954463481903\n",
      "epoch: 4 | 105664 / 114272 | training loss: 0.007520217448472977\n",
      "epoch: 4 | 105696 / 114272 | training loss: 0.002415028167888522\n",
      "epoch: 4 | 105728 / 114272 | training loss: 0.004572086501866579\n",
      "epoch: 4 | 105760 / 114272 | training loss: 0.009421427734196186\n",
      "epoch: 4 | 105792 / 114272 | training loss: 0.06635760515928268\n",
      "epoch: 4 | 105824 / 114272 | training loss: 0.015366892330348492\n",
      "epoch: 4 | 105856 / 114272 | training loss: 0.005715695675462484\n",
      "epoch: 4 | 105888 / 114272 | training loss: 0.004698377568274736\n",
      "epoch: 4 | 105920 / 114272 | training loss: 0.024672800675034523\n",
      "epoch: 4 | 105952 / 114272 | training loss: 0.003926937934011221\n",
      "epoch: 4 | 105984 / 114272 | training loss: 0.2564006447792053\n",
      "epoch: 4 | 106016 / 114272 | training loss: 0.0023239024449139833\n",
      "epoch: 4 | 106048 / 114272 | training loss: 0.005951541010290384\n",
      "epoch: 4 | 106080 / 114272 | training loss: 0.29189011454582214\n",
      "epoch: 4 | 106112 / 114272 | training loss: 0.05500897020101547\n",
      "epoch: 4 | 106144 / 114272 | training loss: 0.00855561625212431\n",
      "epoch: 4 | 106176 / 114272 | training loss: 0.06779153645038605\n",
      "epoch: 4 | 106208 / 114272 | training loss: 0.009460147470235825\n",
      "epoch: 4 | 106240 / 114272 | training loss: 0.0026575597003102303\n",
      "epoch: 4 | 106272 / 114272 | training loss: 0.0014117045793682337\n",
      "epoch: 4 | 106304 / 114272 | training loss: 0.002224158262833953\n",
      "epoch: 4 | 106336 / 114272 | training loss: 0.0020275569986552\n",
      "epoch: 4 | 106368 / 114272 | training loss: 0.23692744970321655\n",
      "epoch: 4 | 106400 / 114272 | training loss: 0.0015096322167664766\n",
      "epoch: 4 | 106432 / 114272 | training loss: 0.0045624407939612865\n",
      "epoch: 4 | 106464 / 114272 | training loss: 0.09879075735807419\n",
      "epoch: 4 | 106496 / 114272 | training loss: 0.23850221931934357\n",
      "epoch: 4 | 106528 / 114272 | training loss: 0.0016668223543092608\n",
      "epoch: 4 | 106560 / 114272 | training loss: 0.001998100895434618\n",
      "epoch: 4 | 106592 / 114272 | training loss: 0.23592479526996613\n",
      "epoch: 4 | 106624 / 114272 | training loss: 0.011621642857789993\n",
      "epoch: 4 | 106656 / 114272 | training loss: 0.0018453322118148208\n",
      "epoch: 4 | 106688 / 114272 | training loss: 0.22297661006450653\n",
      "epoch: 4 | 106720 / 114272 | training loss: 0.001758617116138339\n",
      "epoch: 4 | 106752 / 114272 | training loss: 0.003434418700635433\n",
      "epoch: 4 | 106784 / 114272 | training loss: 0.05787200108170509\n",
      "epoch: 4 | 106816 / 114272 | training loss: 0.007007118780165911\n",
      "epoch: 4 | 106848 / 114272 | training loss: 0.008947894908487797\n",
      "epoch: 4 | 106880 / 114272 | training loss: 0.002443579724058509\n",
      "epoch: 4 | 106912 / 114272 | training loss: 0.005741186439990997\n",
      "epoch: 4 | 106944 / 114272 | training loss: 0.15914776921272278\n",
      "epoch: 4 | 106976 / 114272 | training loss: 0.15750505030155182\n",
      "epoch: 4 | 107008 / 114272 | training loss: 0.001883696299046278\n",
      "epoch: 4 | 107040 / 114272 | training loss: 0.030986936762928963\n",
      "epoch: 4 | 107072 / 114272 | training loss: 0.04826666787266731\n",
      "epoch: 4 | 107104 / 114272 | training loss: 0.006425513885915279\n",
      "epoch: 4 | 107136 / 114272 | training loss: 0.19771458208560944\n",
      "epoch: 4 | 107168 / 114272 | training loss: 0.09022906422615051\n",
      "epoch: 4 | 107200 / 114272 | training loss: 0.0048704687505960464\n",
      "epoch: 4 | 107232 / 114272 | training loss: 0.20770438015460968\n",
      "epoch: 4 | 107264 / 114272 | training loss: 0.09933948516845703\n",
      "epoch: 4 | 107296 / 114272 | training loss: 0.004502893425524235\n",
      "epoch: 4 | 107328 / 114272 | training loss: 0.0054335384629666805\n",
      "epoch: 4 | 107360 / 114272 | training loss: 0.05281728506088257\n",
      "epoch: 4 | 107392 / 114272 | training loss: 0.005782813299447298\n",
      "epoch: 4 | 107424 / 114272 | training loss: 0.0050631240010261536\n",
      "epoch: 4 | 107456 / 114272 | training loss: 0.007377707865089178\n",
      "epoch: 4 | 107488 / 114272 | training loss: 0.0017906740540638566\n",
      "epoch: 4 | 107520 / 114272 | training loss: 0.002908274531364441\n",
      "epoch: 4 | 107552 / 114272 | training loss: 0.21574321389198303\n",
      "epoch: 4 | 107584 / 114272 | training loss: 0.0034241355024278164\n",
      "epoch: 4 | 107616 / 114272 | training loss: 0.002065497450530529\n",
      "epoch: 4 | 107648 / 114272 | training loss: 0.006522083654999733\n",
      "epoch: 4 | 107680 / 114272 | training loss: 0.012506725266575813\n",
      "epoch: 4 | 107712 / 114272 | training loss: 0.0034994015004485846\n",
      "epoch: 4 | 107744 / 114272 | training loss: 0.03787938505411148\n",
      "epoch: 4 | 107776 / 114272 | training loss: 0.006342072505503893\n",
      "epoch: 4 | 107808 / 114272 | training loss: 0.2496219128370285\n",
      "epoch: 4 | 107840 / 114272 | training loss: 0.002813150640577078\n",
      "epoch: 4 | 107872 / 114272 | training loss: 0.0024552850518375635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 107904 / 114272 | training loss: 0.0053826686926186085\n",
      "epoch: 4 | 107936 / 114272 | training loss: 0.004047034773975611\n",
      "epoch: 4 | 107968 / 114272 | training loss: 0.002712501212954521\n",
      "epoch: 4 | 108000 / 114272 | training loss: 0.15254950523376465\n",
      "epoch: 4 | 108032 / 114272 | training loss: 0.0033472131472080946\n",
      "epoch: 4 | 108064 / 114272 | training loss: 0.0014176694676280022\n",
      "epoch: 4 | 108096 / 114272 | training loss: 0.007726941257715225\n",
      "epoch: 4 | 108128 / 114272 | training loss: 0.001771555165760219\n",
      "epoch: 4 | 108160 / 114272 | training loss: 0.001960311084985733\n",
      "epoch: 4 | 108192 / 114272 | training loss: 0.11492223292589188\n",
      "epoch: 4 | 108224 / 114272 | training loss: 0.00853434856981039\n",
      "epoch: 4 | 108256 / 114272 | training loss: 0.003221992403268814\n",
      "epoch: 4 | 108288 / 114272 | training loss: 0.0020333947613835335\n",
      "epoch: 4 | 108320 / 114272 | training loss: 0.0008094963268376887\n",
      "epoch: 4 | 108352 / 114272 | training loss: 0.010231572203338146\n",
      "epoch: 4 | 108384 / 114272 | training loss: 0.16379565000534058\n",
      "epoch: 4 | 108416 / 114272 | training loss: 0.00344925862737\n",
      "epoch: 4 | 108448 / 114272 | training loss: 0.001649501733481884\n",
      "epoch: 4 | 108480 / 114272 | training loss: 0.0040157572366297245\n",
      "epoch: 4 | 108512 / 114272 | training loss: 0.0017475188942626119\n",
      "epoch: 4 | 108544 / 114272 | training loss: 0.0939803272485733\n",
      "epoch: 4 | 108576 / 114272 | training loss: 0.20085078477859497\n",
      "epoch: 4 | 108608 / 114272 | training loss: 0.10066327452659607\n",
      "epoch: 4 | 108640 / 114272 | training loss: 0.0031962397042661905\n",
      "epoch: 4 | 108672 / 114272 | training loss: 0.14784076809883118\n",
      "epoch: 4 | 108704 / 114272 | training loss: 0.17134161293506622\n",
      "epoch: 4 | 108736 / 114272 | training loss: 0.0024919756688177586\n",
      "epoch: 4 | 108768 / 114272 | training loss: 0.00213027885183692\n",
      "epoch: 4 | 108800 / 114272 | training loss: 0.10824336856603622\n",
      "epoch: 4 | 108832 / 114272 | training loss: 0.1571698933839798\n",
      "epoch: 4 | 108864 / 114272 | training loss: 0.0013180298265069723\n",
      "epoch: 4 | 108896 / 114272 | training loss: 0.003391238860785961\n",
      "epoch: 4 | 108928 / 114272 | training loss: 0.0020678096916526556\n",
      "epoch: 4 | 108960 / 114272 | training loss: 0.0017646029591560364\n",
      "epoch: 4 | 108992 / 114272 | training loss: 0.005093660205602646\n",
      "epoch: 4 | 109024 / 114272 | training loss: 0.030720839276909828\n",
      "epoch: 4 | 109056 / 114272 | training loss: 0.11294332891702652\n",
      "epoch: 4 | 109088 / 114272 | training loss: 0.00877140648663044\n",
      "epoch: 4 | 109120 / 114272 | training loss: 0.14458556473255157\n",
      "epoch: 4 | 109152 / 114272 | training loss: 0.0026515319477766752\n",
      "epoch: 4 | 109184 / 114272 | training loss: 0.11301715672016144\n",
      "epoch: 4 | 109216 / 114272 | training loss: 0.13742965459823608\n",
      "epoch: 4 | 109248 / 114272 | training loss: 0.14028328657150269\n",
      "epoch: 4 | 109280 / 114272 | training loss: 0.0038536947686225176\n",
      "epoch: 4 | 109312 / 114272 | training loss: 0.0034181736409664154\n",
      "epoch: 4 | 109344 / 114272 | training loss: 0.14829108119010925\n",
      "epoch: 4 | 109376 / 114272 | training loss: 0.10973986983299255\n",
      "epoch: 4 | 109408 / 114272 | training loss: 0.005636752117425203\n",
      "epoch: 4 | 109440 / 114272 | training loss: 0.01265975832939148\n",
      "epoch: 4 | 109472 / 114272 | training loss: 0.005971003323793411\n",
      "epoch: 4 | 109504 / 114272 | training loss: 0.0025033578276634216\n",
      "epoch: 4 | 109536 / 114272 | training loss: 0.1428021490573883\n",
      "epoch: 4 | 109568 / 114272 | training loss: 0.17146453261375427\n",
      "epoch: 4 | 109600 / 114272 | training loss: 0.008561680093407631\n",
      "epoch: 4 | 109632 / 114272 | training loss: 0.3910418450832367\n",
      "epoch: 4 | 109664 / 114272 | training loss: 0.004739134106785059\n",
      "epoch: 4 | 109696 / 114272 | training loss: 0.09681541472673416\n",
      "epoch: 4 | 109728 / 114272 | training loss: 0.07454726099967957\n",
      "epoch: 4 | 109760 / 114272 | training loss: 0.0036077406257390976\n",
      "epoch: 4 | 109792 / 114272 | training loss: 0.12830094993114471\n",
      "epoch: 4 | 109824 / 114272 | training loss: 0.3239511549472809\n",
      "epoch: 4 | 109856 / 114272 | training loss: 0.1775544434785843\n",
      "epoch: 4 | 109888 / 114272 | training loss: 0.005674283020198345\n",
      "epoch: 4 | 109920 / 114272 | training loss: 0.020603623241186142\n",
      "epoch: 4 | 109952 / 114272 | training loss: 0.0038431035354733467\n",
      "epoch: 4 | 109984 / 114272 | training loss: 0.07782074064016342\n",
      "epoch: 4 | 110016 / 114272 | training loss: 0.2606269121170044\n",
      "epoch: 4 | 110048 / 114272 | training loss: 0.004856858402490616\n",
      "epoch: 4 | 110080 / 114272 | training loss: 0.036854397505521774\n",
      "epoch: 4 | 110112 / 114272 | training loss: 0.0036201102193444967\n",
      "epoch: 4 | 110144 / 114272 | training loss: 0.0038269585929811\n",
      "epoch: 4 | 110176 / 114272 | training loss: 0.0024865043815225363\n",
      "epoch: 4 | 110208 / 114272 | training loss: 0.00522424653172493\n",
      "epoch: 4 | 110240 / 114272 | training loss: 0.00374734029173851\n",
      "epoch: 4 | 110272 / 114272 | training loss: 0.011256656609475613\n",
      "epoch: 4 | 110304 / 114272 | training loss: 0.00561252236366272\n",
      "epoch: 4 | 110336 / 114272 | training loss: 0.008344597183167934\n",
      "epoch: 4 | 110368 / 114272 | training loss: 0.006126772612333298\n",
      "epoch: 4 | 110400 / 114272 | training loss: 0.0028391152154654264\n",
      "epoch: 4 | 110432 / 114272 | training loss: 0.02753354236483574\n",
      "epoch: 4 | 110464 / 114272 | training loss: 0.004844609647989273\n",
      "epoch: 4 | 110496 / 114272 | training loss: 0.11685273796319962\n",
      "epoch: 4 | 110528 / 114272 | training loss: 0.004474428482353687\n",
      "epoch: 4 | 110560 / 114272 | training loss: 0.06626693159341812\n",
      "epoch: 4 | 110592 / 114272 | training loss: 0.008752136491239071\n",
      "epoch: 4 | 110624 / 114272 | training loss: 0.03428027406334877\n",
      "epoch: 4 | 110656 / 114272 | training loss: 0.004658588673919439\n",
      "epoch: 4 | 110688 / 114272 | training loss: 0.21918801963329315\n",
      "epoch: 4 | 110720 / 114272 | training loss: 0.08305434882640839\n",
      "epoch: 4 | 110752 / 114272 | training loss: 0.009295324794948101\n",
      "epoch: 4 | 110784 / 114272 | training loss: 0.003168282797560096\n",
      "epoch: 4 | 110816 / 114272 | training loss: 0.0038499818183481693\n",
      "epoch: 4 | 110848 / 114272 | training loss: 0.009740094654262066\n",
      "epoch: 4 | 110880 / 114272 | training loss: 0.12238981574773788\n",
      "epoch: 4 | 110912 / 114272 | training loss: 0.003164486261084676\n",
      "epoch: 4 | 110944 / 114272 | training loss: 0.003272694069892168\n",
      "epoch: 4 | 110976 / 114272 | training loss: 0.05069446563720703\n",
      "epoch: 4 | 111008 / 114272 | training loss: 0.0038034708704799414\n",
      "epoch: 4 | 111040 / 114272 | training loss: 0.17321330308914185\n",
      "epoch: 4 | 111072 / 114272 | training loss: 0.002322702668607235\n",
      "epoch: 4 | 111104 / 114272 | training loss: 0.0032286590430885553\n",
      "epoch: 4 | 111136 / 114272 | training loss: 0.09730914980173111\n",
      "epoch: 4 | 111168 / 114272 | training loss: 0.1876039057970047\n",
      "epoch: 4 | 111200 / 114272 | training loss: 0.002131264889612794\n",
      "epoch: 4 | 111232 / 114272 | training loss: 0.0038235227111727\n",
      "epoch: 4 | 111264 / 114272 | training loss: 0.002184043638408184\n",
      "epoch: 4 | 111296 / 114272 | training loss: 0.1277381330728531\n",
      "epoch: 4 | 111328 / 114272 | training loss: 0.013334670104086399\n",
      "epoch: 4 | 111360 / 114272 | training loss: 0.008093906566500664\n",
      "epoch: 4 | 111392 / 114272 | training loss: 0.0029764254577457905\n",
      "epoch: 4 | 111424 / 114272 | training loss: 0.07850000262260437\n",
      "epoch: 4 | 111456 / 114272 | training loss: 0.0064754788763821125\n",
      "epoch: 4 | 111488 / 114272 | training loss: 0.03644609451293945\n",
      "epoch: 4 | 111520 / 114272 | training loss: 0.02273537963628769\n",
      "epoch: 4 | 111552 / 114272 | training loss: 0.004214952699840069\n",
      "epoch: 4 | 111584 / 114272 | training loss: 0.0061651128344237804\n",
      "epoch: 4 | 111616 / 114272 | training loss: 0.00875841360539198\n",
      "epoch: 4 | 111648 / 114272 | training loss: 0.05746078118681908\n",
      "epoch: 4 | 111680 / 114272 | training loss: 0.002438451163470745\n",
      "epoch: 4 | 111712 / 114272 | training loss: 0.002164217410609126\n",
      "epoch: 4 | 111744 / 114272 | training loss: 0.12572737038135529\n",
      "epoch: 4 | 111776 / 114272 | training loss: 0.0031685088761150837\n",
      "epoch: 4 | 111808 / 114272 | training loss: 0.005052199121564627\n",
      "epoch: 4 | 111840 / 114272 | training loss: 0.002052496187388897\n",
      "epoch: 4 | 111872 / 114272 | training loss: 0.004129997920244932\n",
      "epoch: 4 | 111904 / 114272 | training loss: 0.002698102267459035\n",
      "epoch: 4 | 111936 / 114272 | training loss: 0.1555258184671402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | 111968 / 114272 | training loss: 0.10836195945739746\n",
      "epoch: 4 | 112000 / 114272 | training loss: 0.002476224908605218\n",
      "epoch: 4 | 112032 / 114272 | training loss: 0.003107522614300251\n",
      "epoch: 4 | 112064 / 114272 | training loss: 0.013783453963696957\n",
      "epoch: 4 | 112096 / 114272 | training loss: 0.0020132577046751976\n",
      "epoch: 4 | 112128 / 114272 | training loss: 0.001484271022491157\n",
      "epoch: 4 | 112160 / 114272 | training loss: 0.029615001752972603\n",
      "epoch: 4 | 112192 / 114272 | training loss: 0.0009979152819141746\n",
      "epoch: 4 | 112224 / 114272 | training loss: 0.001083454117178917\n",
      "epoch: 4 | 112256 / 114272 | training loss: 0.002381484257057309\n",
      "epoch: 4 | 112288 / 114272 | training loss: 0.0016418899176642299\n",
      "epoch: 4 | 112320 / 114272 | training loss: 0.0009744292474351823\n",
      "epoch: 4 | 112352 / 114272 | training loss: 0.15156826376914978\n",
      "epoch: 4 | 112384 / 114272 | training loss: 0.002062218263745308\n",
      "epoch: 4 | 112416 / 114272 | training loss: 0.06520060449838638\n",
      "epoch: 4 | 112448 / 114272 | training loss: 0.002301492728292942\n",
      "epoch: 4 | 112480 / 114272 | training loss: 0.0012555259745568037\n",
      "epoch: 4 | 112512 / 114272 | training loss: 0.002312891185283661\n",
      "epoch: 4 | 112544 / 114272 | training loss: 0.002673499519005418\n",
      "epoch: 4 | 112576 / 114272 | training loss: 0.12490258365869522\n",
      "epoch: 4 | 112608 / 114272 | training loss: 0.0009800600819289684\n",
      "epoch: 4 | 112640 / 114272 | training loss: 0.0023649916984140873\n",
      "epoch: 4 | 112672 / 114272 | training loss: 0.0072143711149692535\n",
      "epoch: 4 | 112704 / 114272 | training loss: 0.0032616041135042906\n",
      "epoch: 4 | 112736 / 114272 | training loss: 0.05229797586798668\n",
      "epoch: 4 | 112768 / 114272 | training loss: 0.0051436033099889755\n",
      "epoch: 4 | 112800 / 114272 | training loss: 0.10081040859222412\n",
      "epoch: 4 | 112832 / 114272 | training loss: 0.0035722372122108936\n",
      "epoch: 4 | 112864 / 114272 | training loss: 0.0018698321655392647\n",
      "epoch: 4 | 112896 / 114272 | training loss: 0.20595228672027588\n",
      "epoch: 4 | 112928 / 114272 | training loss: 0.005459384992718697\n",
      "epoch: 4 | 112960 / 114272 | training loss: 0.0016357771819457412\n",
      "epoch: 4 | 112992 / 114272 | training loss: 0.17569251358509064\n",
      "epoch: 4 | 113024 / 114272 | training loss: 0.0025959755294024944\n",
      "epoch: 4 | 113056 / 114272 | training loss: 0.0042709424160420895\n",
      "epoch: 4 | 113088 / 114272 | training loss: 0.0038016794715076685\n",
      "epoch: 4 | 113120 / 114272 | training loss: 0.007387461140751839\n",
      "epoch: 4 | 113152 / 114272 | training loss: 0.0021971403621137142\n",
      "epoch: 4 | 113184 / 114272 | training loss: 0.006286090239882469\n",
      "epoch: 4 | 113216 / 114272 | training loss: 0.013622396625578403\n",
      "epoch: 4 | 113248 / 114272 | training loss: 0.0006985368672758341\n",
      "epoch: 4 | 113280 / 114272 | training loss: 0.005376282148063183\n",
      "epoch: 4 | 113312 / 114272 | training loss: 0.006683512590825558\n",
      "epoch: 4 | 113344 / 114272 | training loss: 0.18382149934768677\n",
      "epoch: 4 | 113376 / 114272 | training loss: 0.005352947395294905\n",
      "epoch: 4 | 113408 / 114272 | training loss: 0.0020934920758008957\n",
      "epoch: 4 | 113440 / 114272 | training loss: 0.11972762644290924\n",
      "epoch: 4 | 113472 / 114272 | training loss: 0.19888237118721008\n",
      "epoch: 4 | 113504 / 114272 | training loss: 0.001035482157021761\n",
      "epoch: 4 | 113536 / 114272 | training loss: 0.0036203539930284023\n",
      "epoch: 4 | 113568 / 114272 | training loss: 0.029283998534083366\n",
      "epoch: 4 | 113600 / 114272 | training loss: 0.003293626708909869\n",
      "epoch: 4 | 113632 / 114272 | training loss: 0.1329192817211151\n",
      "epoch: 4 | 113664 / 114272 | training loss: 0.0010907663963735104\n",
      "epoch: 4 | 113696 / 114272 | training loss: 0.014314478263258934\n",
      "epoch: 4 | 113728 / 114272 | training loss: 0.0026845901738852262\n",
      "epoch: 4 | 113760 / 114272 | training loss: 0.00464393338188529\n",
      "epoch: 4 | 113792 / 114272 | training loss: 0.1433693915605545\n",
      "epoch: 4 | 113824 / 114272 | training loss: 0.12338393926620483\n",
      "epoch: 4 | 113856 / 114272 | training loss: 0.09523677080869675\n",
      "epoch: 4 | 113888 / 114272 | training loss: 0.001887821708805859\n",
      "epoch: 4 | 113920 / 114272 | training loss: 0.004484604578465223\n",
      "epoch: 4 | 113952 / 114272 | training loss: 0.002751555060967803\n",
      "epoch: 4 | 113984 / 114272 | training loss: 0.14308474957942963\n",
      "epoch: 4 | 114016 / 114272 | training loss: 0.006693533156067133\n",
      "epoch: 4 | 114048 / 114272 | training loss: 0.1355358511209488\n",
      "epoch: 4 | 114080 / 114272 | training loss: 0.2429162859916687\n",
      "epoch: 4 | 114112 / 114272 | training loss: 0.15572574734687805\n",
      "epoch: 4 | 114144 / 114272 | training loss: 0.11633693426847458\n",
      "epoch: 4 | 114176 / 114272 | training loss: 0.09283749014139175\n",
      "epoch: 4 | 114208 / 114272 | training loss: 0.003378924448043108\n",
      "epoch: 4 | 114240 / 114272 | training loss: 0.0031810039654374123\n",
      "Training epoch 4 done! Average loss: 0.04570347267213693. Accuracy: 0.9882648417810138\n",
      "Validation epoch 4 done! Average loss: 0.19569514631134846. Accurage: 0.9540687919463087\n",
      "Epoch 6 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 5 | 0 / 114272 | training loss: 0.0006026424234732985\n",
      "epoch: 5 | 32 / 114272 | training loss: 0.005106234457343817\n",
      "epoch: 5 | 64 / 114272 | training loss: 0.002959327306598425\n",
      "epoch: 5 | 96 / 114272 | training loss: 0.0034373351372778416\n",
      "epoch: 5 | 128 / 114272 | training loss: 0.15653710067272186\n",
      "epoch: 5 | 160 / 114272 | training loss: 0.004450731445103884\n",
      "epoch: 5 | 192 / 114272 | training loss: 0.001821171143092215\n",
      "epoch: 5 | 224 / 114272 | training loss: 0.002988289576023817\n",
      "epoch: 5 | 256 / 114272 | training loss: 0.0030216227751225233\n",
      "epoch: 5 | 288 / 114272 | training loss: 0.0029745223000645638\n",
      "epoch: 5 | 320 / 114272 | training loss: 0.0022051967680454254\n",
      "epoch: 5 | 352 / 114272 | training loss: 0.001776626450009644\n",
      "epoch: 5 | 384 / 114272 | training loss: 0.0029751379042863846\n",
      "epoch: 5 | 416 / 114272 | training loss: 0.26875677704811096\n",
      "epoch: 5 | 448 / 114272 | training loss: 0.02745549939572811\n",
      "epoch: 5 | 480 / 114272 | training loss: 0.0026982473209500313\n",
      "epoch: 5 | 512 / 114272 | training loss: 0.002899266080930829\n",
      "epoch: 5 | 544 / 114272 | training loss: 0.006953294854611158\n",
      "epoch: 5 | 576 / 114272 | training loss: 0.1703329086303711\n",
      "epoch: 5 | 608 / 114272 | training loss: 0.0056638517417013645\n",
      "epoch: 5 | 640 / 114272 | training loss: 0.12766346335411072\n",
      "epoch: 5 | 672 / 114272 | training loss: 0.005762114655226469\n",
      "epoch: 5 | 704 / 114272 | training loss: 0.004309521988034248\n",
      "epoch: 5 | 736 / 114272 | training loss: 0.006404274143278599\n",
      "epoch: 5 | 768 / 114272 | training loss: 0.006145489402115345\n",
      "epoch: 5 | 800 / 114272 | training loss: 0.0025726300664246082\n",
      "epoch: 5 | 832 / 114272 | training loss: 0.004244511481374502\n",
      "epoch: 5 | 864 / 114272 | training loss: 0.002430876949802041\n",
      "epoch: 5 | 896 / 114272 | training loss: 0.004186789970844984\n",
      "epoch: 5 | 928 / 114272 | training loss: 0.12931081652641296\n",
      "epoch: 5 | 960 / 114272 | training loss: 0.0049078213050961494\n",
      "epoch: 5 | 992 / 114272 | training loss: 0.0036594599951058626\n",
      "epoch: 5 | 1024 / 114272 | training loss: 0.0006774620851501822\n",
      "epoch: 5 | 1056 / 114272 | training loss: 0.11876941472291946\n",
      "epoch: 5 | 1088 / 114272 | training loss: 0.06810262799263\n",
      "epoch: 5 | 1120 / 114272 | training loss: 0.003595090238377452\n",
      "epoch: 5 | 1152 / 114272 | training loss: 0.00642929133027792\n",
      "epoch: 5 | 1184 / 114272 | training loss: 0.003468229901045561\n",
      "epoch: 5 | 1216 / 114272 | training loss: 0.008377284742891788\n",
      "epoch: 5 | 1248 / 114272 | training loss: 0.00658463267609477\n",
      "epoch: 5 | 1280 / 114272 | training loss: 0.0019116619369015098\n",
      "epoch: 5 | 1312 / 114272 | training loss: 0.006266330368816853\n",
      "epoch: 5 | 1344 / 114272 | training loss: 0.004232598003000021\n",
      "epoch: 5 | 1376 / 114272 | training loss: 0.1948218196630478\n",
      "epoch: 5 | 1408 / 114272 | training loss: 0.003940317779779434\n",
      "epoch: 5 | 1440 / 114272 | training loss: 0.058485910296440125\n",
      "epoch: 5 | 1472 / 114272 | training loss: 0.006681470666080713\n",
      "epoch: 5 | 1504 / 114272 | training loss: 0.10683891177177429\n",
      "epoch: 5 | 1536 / 114272 | training loss: 0.0011445217533037066\n",
      "epoch: 5 | 1568 / 114272 | training loss: 0.004777294117957354\n",
      "epoch: 5 | 1600 / 114272 | training loss: 0.0015224380185827613\n",
      "epoch: 5 | 1632 / 114272 | training loss: 0.0030300994403660297\n",
      "epoch: 5 | 1664 / 114272 | training loss: 0.005776094738394022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 1696 / 114272 | training loss: 0.0030421200208365917\n",
      "epoch: 5 | 1728 / 114272 | training loss: 0.002714316127821803\n",
      "epoch: 5 | 1760 / 114272 | training loss: 0.006055029574781656\n",
      "epoch: 5 | 1792 / 114272 | training loss: 0.0024860582780092955\n",
      "epoch: 5 | 1824 / 114272 | training loss: 0.0031890165992081165\n",
      "epoch: 5 | 1856 / 114272 | training loss: 0.0018367893062531948\n",
      "epoch: 5 | 1888 / 114272 | training loss: 0.0032325100619345903\n",
      "epoch: 5 | 1920 / 114272 | training loss: 0.21310807764530182\n",
      "epoch: 5 | 1952 / 114272 | training loss: 0.002337600803002715\n",
      "epoch: 5 | 1984 / 114272 | training loss: 0.0016822053585201502\n",
      "epoch: 5 | 2016 / 114272 | training loss: 0.09406907856464386\n",
      "epoch: 5 | 2048 / 114272 | training loss: 0.0044008055701851845\n",
      "epoch: 5 | 2080 / 114272 | training loss: 0.0025035757571458817\n",
      "epoch: 5 | 2112 / 114272 | training loss: 0.004866878967732191\n",
      "epoch: 5 | 2144 / 114272 | training loss: 0.30789679288864136\n",
      "epoch: 5 | 2176 / 114272 | training loss: 0.005476124119013548\n",
      "epoch: 5 | 2208 / 114272 | training loss: 0.07019501179456711\n",
      "epoch: 5 | 2240 / 114272 | training loss: 0.10369133949279785\n",
      "epoch: 5 | 2272 / 114272 | training loss: 0.002092659007757902\n",
      "epoch: 5 | 2304 / 114272 | training loss: 0.10106929391622543\n",
      "epoch: 5 | 2336 / 114272 | training loss: 0.003264292608946562\n",
      "epoch: 5 | 2368 / 114272 | training loss: 0.07253341376781464\n",
      "epoch: 5 | 2400 / 114272 | training loss: 0.013166212476789951\n",
      "epoch: 5 | 2432 / 114272 | training loss: 0.0031934641301631927\n",
      "epoch: 5 | 2464 / 114272 | training loss: 0.0018459561979398131\n",
      "epoch: 5 | 2496 / 114272 | training loss: 0.14603115618228912\n",
      "epoch: 5 | 2528 / 114272 | training loss: 0.003111219732090831\n",
      "epoch: 5 | 2560 / 114272 | training loss: 0.004705127328634262\n",
      "epoch: 5 | 2592 / 114272 | training loss: 0.0019983251113444567\n",
      "epoch: 5 | 2624 / 114272 | training loss: 0.002304332097992301\n",
      "epoch: 5 | 2656 / 114272 | training loss: 0.04770539700984955\n",
      "epoch: 5 | 2688 / 114272 | training loss: 0.0018703031819313765\n",
      "epoch: 5 | 2720 / 114272 | training loss: 0.0018070307560265064\n",
      "epoch: 5 | 2752 / 114272 | training loss: 0.0022777142003178596\n",
      "epoch: 5 | 2784 / 114272 | training loss: 0.002493669278919697\n",
      "epoch: 5 | 2816 / 114272 | training loss: 0.008770328015089035\n",
      "epoch: 5 | 2848 / 114272 | training loss: 0.004821345210075378\n",
      "epoch: 5 | 2880 / 114272 | training loss: 0.002357086632400751\n",
      "epoch: 5 | 2912 / 114272 | training loss: 0.003169098636135459\n",
      "epoch: 5 | 2944 / 114272 | training loss: 0.02346389926970005\n",
      "epoch: 5 | 2976 / 114272 | training loss: 0.04145265743136406\n",
      "epoch: 5 | 3008 / 114272 | training loss: 0.0016921227797865868\n",
      "epoch: 5 | 3040 / 114272 | training loss: 0.0011053234338760376\n",
      "epoch: 5 | 3072 / 114272 | training loss: 0.13765780627727509\n",
      "epoch: 5 | 3104 / 114272 | training loss: 0.001993587240576744\n",
      "epoch: 5 | 3136 / 114272 | training loss: 0.0037185358814895153\n",
      "epoch: 5 | 3168 / 114272 | training loss: 0.0023668974172323942\n",
      "epoch: 5 | 3200 / 114272 | training loss: 0.0016651198966428638\n",
      "epoch: 5 | 3232 / 114272 | training loss: 0.0011996431276202202\n",
      "epoch: 5 | 3264 / 114272 | training loss: 0.017183102667331696\n",
      "epoch: 5 | 3296 / 114272 | training loss: 0.0030615695286542177\n",
      "epoch: 5 | 3328 / 114272 | training loss: 0.0016754069365561008\n",
      "epoch: 5 | 3360 / 114272 | training loss: 0.03267059475183487\n",
      "epoch: 5 | 3392 / 114272 | training loss: 0.0016499239718541503\n",
      "epoch: 5 | 3424 / 114272 | training loss: 0.00572305778041482\n",
      "epoch: 5 | 3456 / 114272 | training loss: 0.0012965974165126681\n",
      "epoch: 5 | 3488 / 114272 | training loss: 0.0036640791222453117\n",
      "epoch: 5 | 3520 / 114272 | training loss: 0.0033532502129673958\n",
      "epoch: 5 | 3552 / 114272 | training loss: 0.054506756365299225\n",
      "epoch: 5 | 3584 / 114272 | training loss: 0.00170719507150352\n",
      "epoch: 5 | 3616 / 114272 | training loss: 0.0025400042068213224\n",
      "epoch: 5 | 3648 / 114272 | training loss: 0.0010920626809820533\n",
      "epoch: 5 | 3680 / 114272 | training loss: 0.0014193701790645719\n",
      "epoch: 5 | 3712 / 114272 | training loss: 0.0015538178849965334\n",
      "epoch: 5 | 3744 / 114272 | training loss: 0.0010753480019047856\n",
      "epoch: 5 | 3776 / 114272 | training loss: 0.03735063225030899\n",
      "epoch: 5 | 3808 / 114272 | training loss: 0.0012390312040224671\n",
      "epoch: 5 | 3840 / 114272 | training loss: 0.001318657654337585\n",
      "epoch: 5 | 3872 / 114272 | training loss: 0.0028152046725153923\n",
      "epoch: 5 | 3904 / 114272 | training loss: 0.0025372011587023735\n",
      "epoch: 5 | 3936 / 114272 | training loss: 0.06967178732156754\n",
      "epoch: 5 | 3968 / 114272 | training loss: 0.44059011340141296\n",
      "epoch: 5 | 4000 / 114272 | training loss: 0.0017835989128798246\n",
      "epoch: 5 | 4032 / 114272 | training loss: 0.0014098073588684201\n",
      "epoch: 5 | 4064 / 114272 | training loss: 0.0015394972870126367\n",
      "epoch: 5 | 4096 / 114272 | training loss: 0.0033185305073857307\n",
      "epoch: 5 | 4128 / 114272 | training loss: 0.003794889897108078\n",
      "epoch: 5 | 4160 / 114272 | training loss: 0.0021457811817526817\n",
      "epoch: 5 | 4192 / 114272 | training loss: 0.0012746130814775825\n",
      "epoch: 5 | 4224 / 114272 | training loss: 0.008694058284163475\n",
      "epoch: 5 | 4256 / 114272 | training loss: 0.0025542364455759525\n",
      "epoch: 5 | 4288 / 114272 | training loss: 0.0020366059616208076\n",
      "epoch: 5 | 4320 / 114272 | training loss: 0.002155753318220377\n",
      "epoch: 5 | 4352 / 114272 | training loss: 0.0011517144739627838\n",
      "epoch: 5 | 4384 / 114272 | training loss: 0.0011911493493244052\n",
      "epoch: 5 | 4416 / 114272 | training loss: 0.000682295358274132\n",
      "epoch: 5 | 4448 / 114272 | training loss: 0.08316333591938019\n",
      "epoch: 5 | 4480 / 114272 | training loss: 0.0021344616543501616\n",
      "epoch: 5 | 4512 / 114272 | training loss: 0.0011792490258812904\n",
      "epoch: 5 | 4544 / 114272 | training loss: 0.0016207698499783874\n",
      "epoch: 5 | 4576 / 114272 | training loss: 0.0013110998552292585\n",
      "epoch: 5 | 4608 / 114272 | training loss: 0.0012395376106724143\n",
      "epoch: 5 | 4640 / 114272 | training loss: 0.0011661803582683206\n",
      "epoch: 5 | 4672 / 114272 | training loss: 0.1468261331319809\n",
      "epoch: 5 | 4704 / 114272 | training loss: 0.0008911181939765811\n",
      "epoch: 5 | 4736 / 114272 | training loss: 0.0027451671194285154\n",
      "epoch: 5 | 4768 / 114272 | training loss: 0.00170804257504642\n",
      "epoch: 5 | 4800 / 114272 | training loss: 0.010927421972155571\n",
      "epoch: 5 | 4832 / 114272 | training loss: 0.0026565545704215765\n",
      "epoch: 5 | 4864 / 114272 | training loss: 0.10476715117692947\n",
      "epoch: 5 | 4896 / 114272 | training loss: 0.0012464855099096894\n",
      "epoch: 5 | 4928 / 114272 | training loss: 0.0009000554564408958\n",
      "epoch: 5 | 4960 / 114272 | training loss: 0.0030700063798576593\n",
      "epoch: 5 | 4992 / 114272 | training loss: 0.001721605658531189\n",
      "epoch: 5 | 5024 / 114272 | training loss: 0.001116856117732823\n",
      "epoch: 5 | 5056 / 114272 | training loss: 0.24126258492469788\n",
      "epoch: 5 | 5088 / 114272 | training loss: 0.0009084644843824208\n",
      "epoch: 5 | 5120 / 114272 | training loss: 0.13530108332633972\n",
      "epoch: 5 | 5152 / 114272 | training loss: 0.3296758830547333\n",
      "epoch: 5 | 5184 / 114272 | training loss: 0.0013758926652371883\n",
      "epoch: 5 | 5216 / 114272 | training loss: 0.0011182595044374466\n",
      "epoch: 5 | 5248 / 114272 | training loss: 0.0015138874296098948\n",
      "epoch: 5 | 5280 / 114272 | training loss: 0.0007619535317644477\n",
      "epoch: 5 | 5312 / 114272 | training loss: 0.07419393956661224\n",
      "epoch: 5 | 5344 / 114272 | training loss: 0.3007534444332123\n",
      "epoch: 5 | 5376 / 114272 | training loss: 0.001264070044271648\n",
      "epoch: 5 | 5408 / 114272 | training loss: 0.030843518674373627\n",
      "epoch: 5 | 5440 / 114272 | training loss: 0.19332744181156158\n",
      "epoch: 5 | 5472 / 114272 | training loss: 0.001292249420657754\n",
      "epoch: 5 | 5504 / 114272 | training loss: 0.0007457016035914421\n",
      "epoch: 5 | 5536 / 114272 | training loss: 0.0021414211951196194\n",
      "epoch: 5 | 5568 / 114272 | training loss: 0.0009368828614242375\n",
      "epoch: 5 | 5600 / 114272 | training loss: 0.0010242103599011898\n",
      "epoch: 5 | 5632 / 114272 | training loss: 0.11607667803764343\n",
      "epoch: 5 | 5664 / 114272 | training loss: 0.001510832691565156\n",
      "epoch: 5 | 5696 / 114272 | training loss: 0.0007581784157082438\n",
      "epoch: 5 | 5728 / 114272 | training loss: 0.008100367151200771\n",
      "epoch: 5 | 5760 / 114272 | training loss: 0.0008966543828137219\n",
      "epoch: 5 | 5792 / 114272 | training loss: 0.0010864677606150508\n",
      "epoch: 5 | 5824 / 114272 | training loss: 0.0009648840641602874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 5856 / 114272 | training loss: 0.0007768322830088437\n",
      "epoch: 5 | 5888 / 114272 | training loss: 0.0012088080402463675\n",
      "epoch: 5 | 5920 / 114272 | training loss: 0.0018527740612626076\n",
      "epoch: 5 | 5952 / 114272 | training loss: 0.001175475656054914\n",
      "epoch: 5 | 5984 / 114272 | training loss: 0.0007602261612191796\n",
      "epoch: 5 | 6016 / 114272 | training loss: 0.0017105864826589823\n",
      "epoch: 5 | 6048 / 114272 | training loss: 0.0028450870886445045\n",
      "epoch: 5 | 6080 / 114272 | training loss: 0.025562813505530357\n",
      "epoch: 5 | 6112 / 114272 | training loss: 0.023432474583387375\n",
      "epoch: 5 | 6144 / 114272 | training loss: 0.0033251335844397545\n",
      "epoch: 5 | 6176 / 114272 | training loss: 0.0028000236488878727\n",
      "epoch: 5 | 6208 / 114272 | training loss: 0.0007118839421309531\n",
      "epoch: 5 | 6240 / 114272 | training loss: 0.0008590371580794454\n",
      "epoch: 5 | 6272 / 114272 | training loss: 0.2288612425327301\n",
      "epoch: 5 | 6304 / 114272 | training loss: 0.002161499345675111\n",
      "epoch: 5 | 6336 / 114272 | training loss: 0.0016928213881328702\n",
      "epoch: 5 | 6368 / 114272 | training loss: 0.0012321157846599817\n",
      "epoch: 5 | 6400 / 114272 | training loss: 0.013521729037165642\n",
      "epoch: 5 | 6432 / 114272 | training loss: 0.002024802379310131\n",
      "epoch: 5 | 6464 / 114272 | training loss: 0.00251380424015224\n",
      "epoch: 5 | 6496 / 114272 | training loss: 0.22771206498146057\n",
      "epoch: 5 | 6528 / 114272 | training loss: 0.0008853531908243895\n",
      "epoch: 5 | 6560 / 114272 | training loss: 0.0018810060573741794\n",
      "epoch: 5 | 6592 / 114272 | training loss: 0.002151841763406992\n",
      "epoch: 5 | 6624 / 114272 | training loss: 0.0017412729794159532\n",
      "epoch: 5 | 6656 / 114272 | training loss: 0.004555644001811743\n",
      "epoch: 5 | 6688 / 114272 | training loss: 0.0011861653765663505\n",
      "epoch: 5 | 6720 / 114272 | training loss: 0.002828926080837846\n",
      "epoch: 5 | 6752 / 114272 | training loss: 0.0019253588980063796\n",
      "epoch: 5 | 6784 / 114272 | training loss: 0.0009713608887977898\n",
      "epoch: 5 | 6816 / 114272 | training loss: 0.0008043956477195024\n",
      "epoch: 5 | 6848 / 114272 | training loss: 0.17433246970176697\n",
      "epoch: 5 | 6880 / 114272 | training loss: 0.0009377480018883944\n",
      "epoch: 5 | 6912 / 114272 | training loss: 0.0007740653236396611\n",
      "epoch: 5 | 6944 / 114272 | training loss: 0.0011669417144730687\n",
      "epoch: 5 | 6976 / 114272 | training loss: 0.21588189899921417\n",
      "epoch: 5 | 7008 / 114272 | training loss: 0.000772597617469728\n",
      "epoch: 5 | 7040 / 114272 | training loss: 0.06250086426734924\n",
      "epoch: 5 | 7072 / 114272 | training loss: 0.001698156469501555\n",
      "epoch: 5 | 7104 / 114272 | training loss: 0.0038050974253565073\n",
      "epoch: 5 | 7136 / 114272 | training loss: 0.0014425753615796566\n",
      "epoch: 5 | 7168 / 114272 | training loss: 0.11937154084444046\n",
      "epoch: 5 | 7200 / 114272 | training loss: 0.001356429886072874\n",
      "epoch: 5 | 7232 / 114272 | training loss: 0.0009031209629029036\n",
      "epoch: 5 | 7264 / 114272 | training loss: 0.0013965889811515808\n",
      "epoch: 5 | 7296 / 114272 | training loss: 0.001068658079020679\n",
      "epoch: 5 | 7328 / 114272 | training loss: 0.007362026255577803\n",
      "epoch: 5 | 7360 / 114272 | training loss: 0.163997620344162\n",
      "epoch: 5 | 7392 / 114272 | training loss: 0.0011831546435132623\n",
      "epoch: 5 | 7424 / 114272 | training loss: 0.0020006459672003984\n",
      "epoch: 5 | 7456 / 114272 | training loss: 0.0008521967683918774\n",
      "epoch: 5 | 7488 / 114272 | training loss: 0.15625569224357605\n",
      "epoch: 5 | 7520 / 114272 | training loss: 0.005473254714161158\n",
      "epoch: 5 | 7552 / 114272 | training loss: 0.0013683824799954891\n",
      "epoch: 5 | 7584 / 114272 | training loss: 0.15160441398620605\n",
      "epoch: 5 | 7616 / 114272 | training loss: 0.0022351734805852175\n",
      "epoch: 5 | 7648 / 114272 | training loss: 0.0013652332127094269\n",
      "epoch: 5 | 7680 / 114272 | training loss: 0.0023746814113110304\n",
      "epoch: 5 | 7712 / 114272 | training loss: 0.0011257018195465207\n",
      "epoch: 5 | 7744 / 114272 | training loss: 0.0011814150493592024\n",
      "epoch: 5 | 7776 / 114272 | training loss: 0.001204456901177764\n",
      "epoch: 5 | 7808 / 114272 | training loss: 0.006575826555490494\n",
      "epoch: 5 | 7840 / 114272 | training loss: 0.0025625608395785093\n",
      "epoch: 5 | 7872 / 114272 | training loss: 0.0014870186569169164\n",
      "epoch: 5 | 7904 / 114272 | training loss: 0.0022698978427797556\n",
      "epoch: 5 | 7936 / 114272 | training loss: 0.1514872908592224\n",
      "epoch: 5 | 7968 / 114272 | training loss: 0.0017545940354466438\n",
      "epoch: 5 | 8000 / 114272 | training loss: 0.002884394722059369\n",
      "epoch: 5 | 8032 / 114272 | training loss: 0.001369796460494399\n",
      "epoch: 5 | 8064 / 114272 | training loss: 0.0015018676640465856\n",
      "epoch: 5 | 8096 / 114272 | training loss: 0.0030434124637395144\n",
      "epoch: 5 | 8128 / 114272 | training loss: 0.0009682286763563752\n",
      "epoch: 5 | 8160 / 114272 | training loss: 0.008493173867464066\n",
      "epoch: 5 | 8192 / 114272 | training loss: 0.0017376893665641546\n",
      "epoch: 5 | 8224 / 114272 | training loss: 0.0019704264122992754\n",
      "epoch: 5 | 8256 / 114272 | training loss: 0.002006908878684044\n",
      "epoch: 5 | 8288 / 114272 | training loss: 0.0025175956543534994\n",
      "epoch: 5 | 8320 / 114272 | training loss: 0.0026388242840766907\n",
      "epoch: 5 | 8352 / 114272 | training loss: 0.00202926155179739\n",
      "epoch: 5 | 8384 / 114272 | training loss: 0.001449487404897809\n",
      "epoch: 5 | 8416 / 114272 | training loss: 0.002954216441139579\n",
      "epoch: 5 | 8448 / 114272 | training loss: 0.0008902268018573523\n",
      "epoch: 5 | 8480 / 114272 | training loss: 0.06181004270911217\n",
      "epoch: 5 | 8512 / 114272 | training loss: 0.006572346668690443\n",
      "epoch: 5 | 8544 / 114272 | training loss: 0.0023937539663165808\n",
      "epoch: 5 | 8576 / 114272 | training loss: 0.0009938378352671862\n",
      "epoch: 5 | 8608 / 114272 | training loss: 0.0030948759522289038\n",
      "epoch: 5 | 8640 / 114272 | training loss: 0.0010833882261067629\n",
      "epoch: 5 | 8672 / 114272 | training loss: 0.0011442058021202683\n",
      "epoch: 5 | 8704 / 114272 | training loss: 0.14817212522029877\n",
      "epoch: 5 | 8736 / 114272 | training loss: 0.002261136891320348\n",
      "epoch: 5 | 8768 / 114272 | training loss: 0.001436373102478683\n",
      "epoch: 5 | 8800 / 114272 | training loss: 0.000748292135540396\n",
      "epoch: 5 | 8832 / 114272 | training loss: 0.3424537181854248\n",
      "epoch: 5 | 8864 / 114272 | training loss: 0.0008931172778829932\n",
      "epoch: 5 | 8896 / 114272 | training loss: 0.0008590258657932281\n",
      "epoch: 5 | 8928 / 114272 | training loss: 0.034900564700365067\n",
      "epoch: 5 | 8960 / 114272 | training loss: 0.0024936541449278593\n",
      "epoch: 5 | 8992 / 114272 | training loss: 0.0005221714382059872\n",
      "epoch: 5 | 9024 / 114272 | training loss: 0.0012850044295191765\n",
      "epoch: 5 | 9056 / 114272 | training loss: 0.0014238308649510145\n",
      "epoch: 5 | 9088 / 114272 | training loss: 0.0016790470108389854\n",
      "epoch: 5 | 9120 / 114272 | training loss: 0.0009020871948450804\n",
      "epoch: 5 | 9152 / 114272 | training loss: 0.001617910573258996\n",
      "epoch: 5 | 9184 / 114272 | training loss: 0.002507608849555254\n",
      "epoch: 5 | 9216 / 114272 | training loss: 0.001315854606218636\n",
      "epoch: 5 | 9248 / 114272 | training loss: 0.0009392912616021931\n",
      "epoch: 5 | 9280 / 114272 | training loss: 0.0013287885813042521\n",
      "epoch: 5 | 9312 / 114272 | training loss: 0.023225747048854828\n",
      "epoch: 5 | 9344 / 114272 | training loss: 0.0013672129716724157\n",
      "epoch: 5 | 9376 / 114272 | training loss: 0.1900729537010193\n",
      "epoch: 5 | 9408 / 114272 | training loss: 0.0015857263933867216\n",
      "epoch: 5 | 9440 / 114272 | training loss: 0.0012677237391471863\n",
      "epoch: 5 | 9472 / 114272 | training loss: 0.0011509378673508763\n",
      "epoch: 5 | 9504 / 114272 | training loss: 0.10508471727371216\n",
      "epoch: 5 | 9536 / 114272 | training loss: 0.0009927768260240555\n",
      "epoch: 5 | 9568 / 114272 | training loss: 0.0014919432578608394\n",
      "epoch: 5 | 9600 / 114272 | training loss: 0.0009144281502813101\n",
      "epoch: 5 | 9632 / 114272 | training loss: 0.001059064525179565\n",
      "epoch: 5 | 9664 / 114272 | training loss: 0.0009476225241087377\n",
      "epoch: 5 | 9696 / 114272 | training loss: 0.0010430169058963656\n",
      "epoch: 5 | 9728 / 114272 | training loss: 0.20253443717956543\n",
      "epoch: 5 | 9760 / 114272 | training loss: 0.001636633649468422\n",
      "epoch: 5 | 9792 / 114272 | training loss: 0.12619782984256744\n",
      "epoch: 5 | 9824 / 114272 | training loss: 0.0005860436358489096\n",
      "epoch: 5 | 9856 / 114272 | training loss: 0.00179883255623281\n",
      "epoch: 5 | 9888 / 114272 | training loss: 0.0010739238932728767\n",
      "epoch: 5 | 9920 / 114272 | training loss: 0.0010445412481203675\n",
      "epoch: 5 | 9952 / 114272 | training loss: 0.15242524445056915\n",
      "epoch: 5 | 9984 / 114272 | training loss: 0.02216360904276371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 10016 / 114272 | training loss: 0.0018642396898940206\n",
      "epoch: 5 | 10048 / 114272 | training loss: 0.001735756522975862\n",
      "epoch: 5 | 10080 / 114272 | training loss: 0.0008774022571742535\n",
      "epoch: 5 | 10112 / 114272 | training loss: 0.0020038350485265255\n",
      "epoch: 5 | 10144 / 114272 | training loss: 0.029678087681531906\n",
      "epoch: 5 | 10176 / 114272 | training loss: 0.04189693555235863\n",
      "epoch: 5 | 10208 / 114272 | training loss: 0.021361609920859337\n",
      "epoch: 5 | 10240 / 114272 | training loss: 0.002111279871314764\n",
      "epoch: 5 | 10272 / 114272 | training loss: 0.08663053810596466\n",
      "epoch: 5 | 10304 / 114272 | training loss: 0.157333642244339\n",
      "epoch: 5 | 10336 / 114272 | training loss: 0.010018682107329369\n",
      "epoch: 5 | 10368 / 114272 | training loss: 0.0008330868440680206\n",
      "epoch: 5 | 10400 / 114272 | training loss: 0.001295188209041953\n",
      "epoch: 5 | 10432 / 114272 | training loss: 0.016450706869363785\n",
      "epoch: 5 | 10464 / 114272 | training loss: 0.0009960610186681151\n",
      "epoch: 5 | 10496 / 114272 | training loss: 0.0013648777967318892\n",
      "epoch: 5 | 10528 / 114272 | training loss: 0.0008882927359081805\n",
      "epoch: 5 | 10560 / 114272 | training loss: 0.0016116241458803415\n",
      "epoch: 5 | 10592 / 114272 | training loss: 0.0012176287127658725\n",
      "epoch: 5 | 10624 / 114272 | training loss: 0.0006704764091409743\n",
      "epoch: 5 | 10656 / 114272 | training loss: 0.002687374595552683\n",
      "epoch: 5 | 10688 / 114272 | training loss: 0.0018023108132183552\n",
      "epoch: 5 | 10720 / 114272 | training loss: 0.0011313633294776082\n",
      "epoch: 5 | 10752 / 114272 | training loss: 0.0011356946779415011\n",
      "epoch: 5 | 10784 / 114272 | training loss: 0.002808860968798399\n",
      "epoch: 5 | 10816 / 114272 | training loss: 0.0018719895742833614\n",
      "epoch: 5 | 10848 / 114272 | training loss: 0.0016093432204797864\n",
      "epoch: 5 | 10880 / 114272 | training loss: 0.08410396426916122\n",
      "epoch: 5 | 10912 / 114272 | training loss: 0.0017985425656661391\n",
      "epoch: 5 | 10944 / 114272 | training loss: 0.00140742352232337\n",
      "epoch: 5 | 10976 / 114272 | training loss: 0.07857605069875717\n",
      "epoch: 5 | 11008 / 114272 | training loss: 0.00220264820381999\n",
      "epoch: 5 | 11040 / 114272 | training loss: 0.16384144127368927\n",
      "epoch: 5 | 11072 / 114272 | training loss: 0.002816298045217991\n",
      "epoch: 5 | 11104 / 114272 | training loss: 0.002447090344503522\n",
      "epoch: 5 | 11136 / 114272 | training loss: 0.000990967731922865\n",
      "epoch: 5 | 11168 / 114272 | training loss: 0.0039093149825930595\n",
      "epoch: 5 | 11200 / 114272 | training loss: 0.00089068099623546\n",
      "epoch: 5 | 11232 / 114272 | training loss: 0.0019452329725027084\n",
      "epoch: 5 | 11264 / 114272 | training loss: 0.0015003267908468843\n",
      "epoch: 5 | 11296 / 114272 | training loss: 0.0011426464188843966\n",
      "epoch: 5 | 11328 / 114272 | training loss: 0.0013298725243657827\n",
      "epoch: 5 | 11360 / 114272 | training loss: 0.0035090669989585876\n",
      "epoch: 5 | 11392 / 114272 | training loss: 0.0009247938869521022\n",
      "epoch: 5 | 11424 / 114272 | training loss: 0.0011591333895921707\n",
      "epoch: 5 | 11456 / 114272 | training loss: 0.03987767547369003\n",
      "epoch: 5 | 11488 / 114272 | training loss: 0.0014389005955308676\n",
      "epoch: 5 | 11520 / 114272 | training loss: 0.0013443509815260768\n",
      "epoch: 5 | 11552 / 114272 | training loss: 0.0009384562727063894\n",
      "epoch: 5 | 11584 / 114272 | training loss: 0.0008932596538215876\n",
      "epoch: 5 | 11616 / 114272 | training loss: 0.0012483724858611822\n",
      "epoch: 5 | 11648 / 114272 | training loss: 0.0010225011501461267\n",
      "epoch: 5 | 11680 / 114272 | training loss: 0.0025022621266543865\n",
      "epoch: 5 | 11712 / 114272 | training loss: 0.013761397451162338\n",
      "epoch: 5 | 11744 / 114272 | training loss: 0.0011517575476318598\n",
      "epoch: 5 | 11776 / 114272 | training loss: 0.11933673173189163\n",
      "epoch: 5 | 11808 / 114272 | training loss: 0.12344090640544891\n",
      "epoch: 5 | 11840 / 114272 | training loss: 0.001397009240463376\n",
      "epoch: 5 | 11872 / 114272 | training loss: 0.16261328756809235\n",
      "epoch: 5 | 11904 / 114272 | training loss: 0.0007256284006871283\n",
      "epoch: 5 | 11936 / 114272 | training loss: 0.003417306113988161\n",
      "epoch: 5 | 11968 / 114272 | training loss: 0.11195521801710129\n",
      "epoch: 5 | 12000 / 114272 | training loss: 0.0008073405479080975\n",
      "epoch: 5 | 12032 / 114272 | training loss: 0.001785844098776579\n",
      "epoch: 5 | 12064 / 114272 | training loss: 0.0020037104841321707\n",
      "epoch: 5 | 12096 / 114272 | training loss: 0.13991132378578186\n",
      "epoch: 5 | 12128 / 114272 | training loss: 0.002800297224894166\n",
      "epoch: 5 | 12160 / 114272 | training loss: 0.0016846804646775126\n",
      "epoch: 5 | 12192 / 114272 | training loss: 0.0007624331046827137\n",
      "epoch: 5 | 12224 / 114272 | training loss: 0.13016468286514282\n",
      "epoch: 5 | 12256 / 114272 | training loss: 0.000819269334897399\n",
      "epoch: 5 | 12288 / 114272 | training loss: 0.0008583487360738218\n",
      "epoch: 5 | 12320 / 114272 | training loss: 0.017835039645433426\n",
      "epoch: 5 | 12352 / 114272 | training loss: 0.001143015455454588\n",
      "epoch: 5 | 12384 / 114272 | training loss: 0.00107900844886899\n",
      "epoch: 5 | 12416 / 114272 | training loss: 0.0009401603601872921\n",
      "epoch: 5 | 12448 / 114272 | training loss: 0.0008550097118131816\n",
      "epoch: 5 | 12480 / 114272 | training loss: 0.2900175154209137\n",
      "epoch: 5 | 12512 / 114272 | training loss: 0.001682783244177699\n",
      "epoch: 5 | 12544 / 114272 | training loss: 0.001484682667069137\n",
      "epoch: 5 | 12576 / 114272 | training loss: 0.003249873174354434\n",
      "epoch: 5 | 12608 / 114272 | training loss: 0.0032604066655039787\n",
      "epoch: 5 | 12640 / 114272 | training loss: 0.0008956555975601077\n",
      "epoch: 5 | 12672 / 114272 | training loss: 0.1623968929052353\n",
      "epoch: 5 | 12704 / 114272 | training loss: 0.0012650470016524196\n",
      "epoch: 5 | 12736 / 114272 | training loss: 0.0010258618276566267\n",
      "epoch: 5 | 12768 / 114272 | training loss: 0.0013457987224683166\n",
      "epoch: 5 | 12800 / 114272 | training loss: 0.0032809574622660875\n",
      "epoch: 5 | 12832 / 114272 | training loss: 0.002712344517931342\n",
      "epoch: 5 | 12864 / 114272 | training loss: 0.0011515164515003562\n",
      "epoch: 5 | 12896 / 114272 | training loss: 0.1522779017686844\n",
      "epoch: 5 | 12928 / 114272 | training loss: 0.0012373976642265916\n",
      "epoch: 5 | 12960 / 114272 | training loss: 0.09752387553453445\n",
      "epoch: 5 | 12992 / 114272 | training loss: 0.0010015942389145494\n",
      "epoch: 5 | 13024 / 114272 | training loss: 0.0013622036203742027\n",
      "epoch: 5 | 13056 / 114272 | training loss: 0.005234754644334316\n",
      "epoch: 5 | 13088 / 114272 | training loss: 0.0008061981643550098\n",
      "epoch: 5 | 13120 / 114272 | training loss: 0.001243003411218524\n",
      "epoch: 5 | 13152 / 114272 | training loss: 0.0020252205431461334\n",
      "epoch: 5 | 13184 / 114272 | training loss: 0.0009916987037286162\n",
      "epoch: 5 | 13216 / 114272 | training loss: 0.0011426096316426992\n",
      "epoch: 5 | 13248 / 114272 | training loss: 0.0010971407173201442\n",
      "epoch: 5 | 13280 / 114272 | training loss: 0.0014843107201159\n",
      "epoch: 5 | 13312 / 114272 | training loss: 0.006257519591599703\n",
      "epoch: 5 | 13344 / 114272 | training loss: 0.0007124540861696005\n",
      "epoch: 5 | 13376 / 114272 | training loss: 0.0009964506607502699\n",
      "epoch: 5 | 13408 / 114272 | training loss: 0.001228794571943581\n",
      "epoch: 5 | 13440 / 114272 | training loss: 0.0017752177082002163\n",
      "epoch: 5 | 13472 / 114272 | training loss: 0.001496034674346447\n",
      "epoch: 5 | 13504 / 114272 | training loss: 0.10131432116031647\n",
      "epoch: 5 | 13536 / 114272 | training loss: 0.26064449548721313\n",
      "epoch: 5 | 13568 / 114272 | training loss: 0.001244471874088049\n",
      "epoch: 5 | 13600 / 114272 | training loss: 0.0007636731024831533\n",
      "epoch: 5 | 13632 / 114272 | training loss: 0.0007068008999340236\n",
      "epoch: 5 | 13664 / 114272 | training loss: 0.0018620187183842063\n",
      "epoch: 5 | 13696 / 114272 | training loss: 0.0011873072944581509\n",
      "epoch: 5 | 13728 / 114272 | training loss: 0.0037494448479264975\n",
      "epoch: 5 | 13760 / 114272 | training loss: 0.03164033591747284\n",
      "epoch: 5 | 13792 / 114272 | training loss: 0.0018852343782782555\n",
      "epoch: 5 | 13824 / 114272 | training loss: 0.0009209354175254703\n",
      "epoch: 5 | 13856 / 114272 | training loss: 0.007122871000319719\n",
      "epoch: 5 | 13888 / 114272 | training loss: 0.0010494592133909464\n",
      "epoch: 5 | 13920 / 114272 | training loss: 0.0007289194618351758\n",
      "epoch: 5 | 13952 / 114272 | training loss: 0.0009917514398694038\n",
      "epoch: 5 | 13984 / 114272 | training loss: 0.001001296448521316\n",
      "epoch: 5 | 14016 / 114272 | training loss: 0.0009028950007632375\n",
      "epoch: 5 | 14048 / 114272 | training loss: 0.000625568674877286\n",
      "epoch: 5 | 14080 / 114272 | training loss: 0.005069639068096876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 14112 / 114272 | training loss: 0.0008780137868598104\n",
      "epoch: 5 | 14144 / 114272 | training loss: 0.001823796541430056\n",
      "epoch: 5 | 14176 / 114272 | training loss: 0.0034069812390953302\n",
      "epoch: 5 | 14208 / 114272 | training loss: 0.0010715598473325372\n",
      "epoch: 5 | 14240 / 114272 | training loss: 0.01844804547727108\n",
      "epoch: 5 | 14272 / 114272 | training loss: 0.0006849305354990065\n",
      "epoch: 5 | 14304 / 114272 | training loss: 0.0009268557769246399\n",
      "epoch: 5 | 14336 / 114272 | training loss: 0.02235317789018154\n",
      "epoch: 5 | 14368 / 114272 | training loss: 0.12629295885562897\n",
      "epoch: 5 | 14400 / 114272 | training loss: 0.0010793446563184261\n",
      "epoch: 5 | 14432 / 114272 | training loss: 0.000705542741343379\n",
      "epoch: 5 | 14464 / 114272 | training loss: 0.0009188004769384861\n",
      "epoch: 5 | 14496 / 114272 | training loss: 0.0007278164848685265\n",
      "epoch: 5 | 14528 / 114272 | training loss: 0.0005582462763413787\n",
      "epoch: 5 | 14560 / 114272 | training loss: 0.0007613917696289718\n",
      "epoch: 5 | 14592 / 114272 | training loss: 0.0015470532234758139\n",
      "epoch: 5 | 14624 / 114272 | training loss: 0.0008519675466232002\n",
      "epoch: 5 | 14656 / 114272 | training loss: 0.12719939649105072\n",
      "epoch: 5 | 14688 / 114272 | training loss: 0.0027613805141299963\n",
      "epoch: 5 | 14720 / 114272 | training loss: 0.0009710847516544163\n",
      "epoch: 5 | 14752 / 114272 | training loss: 0.004026700742542744\n",
      "epoch: 5 | 14784 / 114272 | training loss: 0.0006624048692174256\n",
      "epoch: 5 | 14816 / 114272 | training loss: 0.0006029937067069113\n",
      "epoch: 5 | 14848 / 114272 | training loss: 0.24435417354106903\n",
      "epoch: 5 | 14880 / 114272 | training loss: 0.0018782602855935693\n",
      "epoch: 5 | 14912 / 114272 | training loss: 0.00048170972149819136\n",
      "epoch: 5 | 14944 / 114272 | training loss: 0.10541938245296478\n",
      "epoch: 5 | 14976 / 114272 | training loss: 0.0024405778385698795\n",
      "epoch: 5 | 15008 / 114272 | training loss: 0.00036953153903596103\n",
      "epoch: 5 | 15040 / 114272 | training loss: 0.0014479545643553138\n",
      "epoch: 5 | 15072 / 114272 | training loss: 0.001697384286671877\n",
      "epoch: 5 | 15104 / 114272 | training loss: 0.0006879427237436175\n",
      "epoch: 5 | 15136 / 114272 | training loss: 0.0006427683983929455\n",
      "epoch: 5 | 15168 / 114272 | training loss: 0.0021433811634778976\n",
      "epoch: 5 | 15200 / 114272 | training loss: 0.196061909198761\n",
      "epoch: 5 | 15232 / 114272 | training loss: 0.0006608883268199861\n",
      "epoch: 5 | 15264 / 114272 | training loss: 0.03583506867289543\n",
      "epoch: 5 | 15296 / 114272 | training loss: 0.0010303304297849536\n",
      "epoch: 5 | 15328 / 114272 | training loss: 0.0023934952914714813\n",
      "epoch: 5 | 15360 / 114272 | training loss: 0.0008125827880576253\n",
      "epoch: 5 | 15392 / 114272 | training loss: 0.0005814054165966809\n",
      "epoch: 5 | 15424 / 114272 | training loss: 0.0009877127595245838\n",
      "epoch: 5 | 15456 / 114272 | training loss: 0.12683162093162537\n",
      "epoch: 5 | 15488 / 114272 | training loss: 0.0016737442929297686\n",
      "epoch: 5 | 15520 / 114272 | training loss: 0.0009356416412629187\n",
      "epoch: 5 | 15552 / 114272 | training loss: 0.001328933285549283\n",
      "epoch: 5 | 15584 / 114272 | training loss: 0.0007674272055737674\n",
      "epoch: 5 | 15616 / 114272 | training loss: 0.0006528781377710402\n",
      "epoch: 5 | 15648 / 114272 | training loss: 0.09285054355859756\n",
      "epoch: 5 | 15680 / 114272 | training loss: 0.1999206244945526\n",
      "epoch: 5 | 15712 / 114272 | training loss: 0.0008438804652541876\n",
      "epoch: 5 | 15744 / 114272 | training loss: 0.0020258252043277025\n",
      "epoch: 5 | 15776 / 114272 | training loss: 0.001715939724817872\n",
      "epoch: 5 | 15808 / 114272 | training loss: 0.0007201508851721883\n",
      "epoch: 5 | 15840 / 114272 | training loss: 0.014058077707886696\n",
      "epoch: 5 | 15872 / 114272 | training loss: 0.0009655805770307779\n",
      "epoch: 5 | 15904 / 114272 | training loss: 0.0009028080967254937\n",
      "epoch: 5 | 15936 / 114272 | training loss: 0.0006370864575728774\n",
      "epoch: 5 | 15968 / 114272 | training loss: 0.001910001621581614\n",
      "epoch: 5 | 16000 / 114272 | training loss: 0.0018970472738146782\n",
      "epoch: 5 | 16032 / 114272 | training loss: 0.001483524451032281\n",
      "epoch: 5 | 16064 / 114272 | training loss: 0.00296423165127635\n",
      "epoch: 5 | 16096 / 114272 | training loss: 0.0007655762019567192\n",
      "epoch: 5 | 16128 / 114272 | training loss: 0.0012033504899591208\n",
      "epoch: 5 | 16160 / 114272 | training loss: 0.0014038615627214313\n",
      "epoch: 5 | 16192 / 114272 | training loss: 0.0026075132191181183\n",
      "epoch: 5 | 16224 / 114272 | training loss: 0.0007264299201779068\n",
      "epoch: 5 | 16256 / 114272 | training loss: 0.0034646422136574984\n",
      "epoch: 5 | 16288 / 114272 | training loss: 0.00070595444412902\n",
      "epoch: 5 | 16320 / 114272 | training loss: 0.013724667951464653\n",
      "epoch: 5 | 16352 / 114272 | training loss: 0.0006206849939189851\n",
      "epoch: 5 | 16384 / 114272 | training loss: 0.0007352581014856696\n",
      "epoch: 5 | 16416 / 114272 | training loss: 0.000637109624221921\n",
      "epoch: 5 | 16448 / 114272 | training loss: 0.0009173230500891805\n",
      "epoch: 5 | 16480 / 114272 | training loss: 0.000838927982840687\n",
      "epoch: 5 | 16512 / 114272 | training loss: 0.09747748076915741\n",
      "epoch: 5 | 16544 / 114272 | training loss: 0.0010694442316889763\n",
      "epoch: 5 | 16576 / 114272 | training loss: 0.0007964495453052223\n",
      "epoch: 5 | 16608 / 114272 | training loss: 0.0044319722801446915\n",
      "epoch: 5 | 16640 / 114272 | training loss: 0.17144595086574554\n",
      "epoch: 5 | 16672 / 114272 | training loss: 0.0010003085481002927\n",
      "epoch: 5 | 16704 / 114272 | training loss: 0.000648748071398586\n",
      "epoch: 5 | 16736 / 114272 | training loss: 0.1326717734336853\n",
      "epoch: 5 | 16768 / 114272 | training loss: 0.0015051456866785884\n",
      "epoch: 5 | 16800 / 114272 | training loss: 0.24948735535144806\n",
      "epoch: 5 | 16832 / 114272 | training loss: 0.0005174947436898947\n",
      "epoch: 5 | 16864 / 114272 | training loss: 0.012494338676333427\n",
      "epoch: 5 | 16896 / 114272 | training loss: 0.18682728707790375\n",
      "epoch: 5 | 16928 / 114272 | training loss: 0.0006000559660606086\n",
      "epoch: 5 | 16960 / 114272 | training loss: 0.003691072342917323\n",
      "epoch: 5 | 16992 / 114272 | training loss: 0.0004767464124597609\n",
      "epoch: 5 | 17024 / 114272 | training loss: 0.08323356509208679\n",
      "epoch: 5 | 17056 / 114272 | training loss: 0.000811630510725081\n",
      "epoch: 5 | 17088 / 114272 | training loss: 0.06262027472257614\n",
      "epoch: 5 | 17120 / 114272 | training loss: 0.18620282411575317\n",
      "epoch: 5 | 17152 / 114272 | training loss: 0.007949274964630604\n",
      "epoch: 5 | 17184 / 114272 | training loss: 0.0015359017997980118\n",
      "epoch: 5 | 17216 / 114272 | training loss: 0.0018089235527440906\n",
      "epoch: 5 | 17248 / 114272 | training loss: 0.0010615905048325658\n",
      "epoch: 5 | 17280 / 114272 | training loss: 0.001806621323339641\n",
      "epoch: 5 | 17312 / 114272 | training loss: 0.00696691544726491\n",
      "epoch: 5 | 17344 / 114272 | training loss: 0.004275940824300051\n",
      "epoch: 5 | 17376 / 114272 | training loss: 0.0012520735617727041\n",
      "epoch: 5 | 17408 / 114272 | training loss: 0.004162733908742666\n",
      "epoch: 5 | 17440 / 114272 | training loss: 0.0023585332091897726\n",
      "epoch: 5 | 17472 / 114272 | training loss: 0.3457214832305908\n",
      "epoch: 5 | 17504 / 114272 | training loss: 0.16423963010311127\n",
      "epoch: 5 | 17536 / 114272 | training loss: 0.0013999755028635263\n",
      "epoch: 5 | 17568 / 114272 | training loss: 0.013998606242239475\n",
      "epoch: 5 | 17600 / 114272 | training loss: 0.0010684463195502758\n",
      "epoch: 5 | 17632 / 114272 | training loss: 0.0015900074504315853\n",
      "epoch: 5 | 17664 / 114272 | training loss: 0.07792170345783234\n",
      "epoch: 5 | 17696 / 114272 | training loss: 0.0027318561915308237\n",
      "epoch: 5 | 17728 / 114272 | training loss: 0.0028334276285022497\n",
      "epoch: 5 | 17760 / 114272 | training loss: 0.004487142898142338\n",
      "epoch: 5 | 17792 / 114272 | training loss: 0.0019246770534664392\n",
      "epoch: 5 | 17824 / 114272 | training loss: 0.0035862985532730818\n",
      "epoch: 5 | 17856 / 114272 | training loss: 0.08956022560596466\n",
      "epoch: 5 | 17888 / 114272 | training loss: 0.0013152273604646325\n",
      "epoch: 5 | 17920 / 114272 | training loss: 0.001728426432237029\n",
      "epoch: 5 | 17952 / 114272 | training loss: 0.002188426908105612\n",
      "epoch: 5 | 17984 / 114272 | training loss: 0.0018527933862060308\n",
      "epoch: 5 | 18016 / 114272 | training loss: 0.0026579031255096197\n",
      "epoch: 5 | 18048 / 114272 | training loss: 0.0023630785290151834\n",
      "epoch: 5 | 18080 / 114272 | training loss: 0.05858846381306648\n",
      "epoch: 5 | 18112 / 114272 | training loss: 0.1056244745850563\n",
      "epoch: 5 | 18144 / 114272 | training loss: 0.0022137491032481194\n",
      "epoch: 5 | 18176 / 114272 | training loss: 0.0021813027560710907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 18208 / 114272 | training loss: 0.001974190352484584\n",
      "epoch: 5 | 18240 / 114272 | training loss: 0.0019504053052514791\n",
      "epoch: 5 | 18272 / 114272 | training loss: 0.1381860226392746\n",
      "epoch: 5 | 18304 / 114272 | training loss: 0.16600777208805084\n",
      "epoch: 5 | 18336 / 114272 | training loss: 0.001327475649304688\n",
      "epoch: 5 | 18368 / 114272 | training loss: 0.0015293944161385298\n",
      "epoch: 5 | 18400 / 114272 | training loss: 0.19362439215183258\n",
      "epoch: 5 | 18432 / 114272 | training loss: 0.0020315616857260466\n",
      "epoch: 5 | 18464 / 114272 | training loss: 0.001030972576700151\n",
      "epoch: 5 | 18496 / 114272 | training loss: 0.0021799702662974596\n",
      "epoch: 5 | 18528 / 114272 | training loss: 0.1737852394580841\n",
      "epoch: 5 | 18560 / 114272 | training loss: 0.0014634029939770699\n",
      "epoch: 5 | 18592 / 114272 | training loss: 0.031621433794498444\n",
      "epoch: 5 | 18624 / 114272 | training loss: 0.001945118885487318\n",
      "epoch: 5 | 18656 / 114272 | training loss: 0.1529942750930786\n",
      "epoch: 5 | 18688 / 114272 | training loss: 0.0025018518790602684\n",
      "epoch: 5 | 18720 / 114272 | training loss: 0.15462785959243774\n",
      "epoch: 5 | 18752 / 114272 | training loss: 0.09633605927228928\n",
      "epoch: 5 | 18784 / 114272 | training loss: 0.12933403253555298\n",
      "epoch: 5 | 18816 / 114272 | training loss: 0.0015203311340883374\n",
      "epoch: 5 | 18848 / 114272 | training loss: 0.0030388988088816404\n",
      "epoch: 5 | 18880 / 114272 | training loss: 0.00554528646171093\n",
      "epoch: 5 | 18912 / 114272 | training loss: 0.0034304943401366472\n",
      "epoch: 5 | 18944 / 114272 | training loss: 0.0029056761413812637\n",
      "epoch: 5 | 18976 / 114272 | training loss: 0.00372819765470922\n",
      "epoch: 5 | 19008 / 114272 | training loss: 0.0029085378628224134\n",
      "epoch: 5 | 19040 / 114272 | training loss: 0.004423316568136215\n",
      "epoch: 5 | 19072 / 114272 | training loss: 0.002108977409079671\n",
      "epoch: 5 | 19104 / 114272 | training loss: 0.002679684432223439\n",
      "epoch: 5 | 19136 / 114272 | training loss: 0.0028340632561594248\n",
      "epoch: 5 | 19168 / 114272 | training loss: 0.00512846652418375\n",
      "epoch: 5 | 19200 / 114272 | training loss: 0.0047225672751665115\n",
      "epoch: 5 | 19232 / 114272 | training loss: 0.004115608520805836\n",
      "epoch: 5 | 19264 / 114272 | training loss: 0.003016213420778513\n",
      "epoch: 5 | 19296 / 114272 | training loss: 0.004418420139700174\n",
      "epoch: 5 | 19328 / 114272 | training loss: 0.0029886907432228327\n",
      "epoch: 5 | 19360 / 114272 | training loss: 0.17852820456027985\n",
      "epoch: 5 | 19392 / 114272 | training loss: 0.0009381918935105205\n",
      "epoch: 5 | 19424 / 114272 | training loss: 0.3010307550430298\n",
      "epoch: 5 | 19456 / 114272 | training loss: 0.14200825989246368\n",
      "epoch: 5 | 19488 / 114272 | training loss: 0.006167682819068432\n",
      "epoch: 5 | 19520 / 114272 | training loss: 0.002574187470600009\n",
      "epoch: 5 | 19552 / 114272 | training loss: 0.0019519265042617917\n",
      "epoch: 5 | 19584 / 114272 | training loss: 0.001276800176128745\n",
      "epoch: 5 | 19616 / 114272 | training loss: 0.0016577070346102118\n",
      "epoch: 5 | 19648 / 114272 | training loss: 0.25409895181655884\n",
      "epoch: 5 | 19680 / 114272 | training loss: 0.01287793181836605\n",
      "epoch: 5 | 19712 / 114272 | training loss: 0.0033103651367127895\n",
      "epoch: 5 | 19744 / 114272 | training loss: 0.006671966519206762\n",
      "epoch: 5 | 19776 / 114272 | training loss: 0.0027071007061749697\n",
      "epoch: 5 | 19808 / 114272 | training loss: 0.07086553424596786\n",
      "epoch: 5 | 19840 / 114272 | training loss: 0.0953897014260292\n",
      "epoch: 5 | 19872 / 114272 | training loss: 0.002916924189776182\n",
      "epoch: 5 | 19904 / 114272 | training loss: 0.004730390850454569\n",
      "epoch: 5 | 19936 / 114272 | training loss: 0.0021498259156942368\n",
      "epoch: 5 | 19968 / 114272 | training loss: 0.1484900861978531\n",
      "epoch: 5 | 20000 / 114272 | training loss: 0.003282059682533145\n",
      "epoch: 5 | 20032 / 114272 | training loss: 0.0024544461630284786\n",
      "epoch: 5 | 20064 / 114272 | training loss: 0.004036256577819586\n",
      "epoch: 5 | 20096 / 114272 | training loss: 0.0030360291711986065\n",
      "epoch: 5 | 20128 / 114272 | training loss: 0.002560204127803445\n",
      "epoch: 5 | 20160 / 114272 | training loss: 0.0032421245705336332\n",
      "epoch: 5 | 20192 / 114272 | training loss: 0.00241543585434556\n",
      "epoch: 5 | 20224 / 114272 | training loss: 0.0026165556628257036\n",
      "epoch: 5 | 20256 / 114272 | training loss: 0.0114610455930233\n",
      "epoch: 5 | 20288 / 114272 | training loss: 0.003670257981866598\n",
      "epoch: 5 | 20320 / 114272 | training loss: 0.02243879623711109\n",
      "epoch: 5 | 20352 / 114272 | training loss: 0.003122772090137005\n",
      "epoch: 5 | 20384 / 114272 | training loss: 0.20748655498027802\n",
      "epoch: 5 | 20416 / 114272 | training loss: 0.003325793193653226\n",
      "epoch: 5 | 20448 / 114272 | training loss: 0.005429228767752647\n",
      "epoch: 5 | 20480 / 114272 | training loss: 0.022336740046739578\n",
      "epoch: 5 | 20512 / 114272 | training loss: 0.0017482455587014556\n",
      "epoch: 5 | 20544 / 114272 | training loss: 0.002674817806109786\n",
      "epoch: 5 | 20576 / 114272 | training loss: 0.010779947973787785\n",
      "epoch: 5 | 20608 / 114272 | training loss: 0.04204563423991203\n",
      "epoch: 5 | 20640 / 114272 | training loss: 0.001431406824849546\n",
      "epoch: 5 | 20672 / 114272 | training loss: 0.0014284987701103091\n",
      "epoch: 5 | 20704 / 114272 | training loss: 0.15981259942054749\n",
      "epoch: 5 | 20736 / 114272 | training loss: 0.1505485624074936\n",
      "epoch: 5 | 20768 / 114272 | training loss: 0.0017366467509418726\n",
      "epoch: 5 | 20800 / 114272 | training loss: 0.0025454494170844555\n",
      "epoch: 5 | 20832 / 114272 | training loss: 0.21111996471881866\n",
      "epoch: 5 | 20864 / 114272 | training loss: 0.0014867803547531366\n",
      "epoch: 5 | 20896 / 114272 | training loss: 0.0017630241345614195\n",
      "epoch: 5 | 20928 / 114272 | training loss: 0.0026133814826607704\n",
      "epoch: 5 | 20960 / 114272 | training loss: 0.001954234205186367\n",
      "epoch: 5 | 20992 / 114272 | training loss: 0.03186206892132759\n",
      "epoch: 5 | 21024 / 114272 | training loss: 0.0035639952402561903\n",
      "epoch: 5 | 21056 / 114272 | training loss: 0.24593056738376617\n",
      "epoch: 5 | 21088 / 114272 | training loss: 0.002455576555803418\n",
      "epoch: 5 | 21120 / 114272 | training loss: 0.00800389889627695\n",
      "epoch: 5 | 21152 / 114272 | training loss: 0.002115069655701518\n",
      "epoch: 5 | 21184 / 114272 | training loss: 0.002435551956295967\n",
      "epoch: 5 | 21216 / 114272 | training loss: 0.02548981085419655\n",
      "epoch: 5 | 21248 / 114272 | training loss: 0.0022909853141754866\n",
      "epoch: 5 | 21280 / 114272 | training loss: 0.001311474945396185\n",
      "epoch: 5 | 21312 / 114272 | training loss: 0.002333536511287093\n",
      "epoch: 5 | 21344 / 114272 | training loss: 0.001994216814637184\n",
      "epoch: 5 | 21376 / 114272 | training loss: 0.005854947026818991\n",
      "epoch: 5 | 21408 / 114272 | training loss: 0.0016421000473201275\n",
      "epoch: 5 | 21440 / 114272 | training loss: 0.002177388872951269\n",
      "epoch: 5 | 21472 / 114272 | training loss: 0.004661643877625465\n",
      "epoch: 5 | 21504 / 114272 | training loss: 0.0015353616327047348\n",
      "epoch: 5 | 21536 / 114272 | training loss: 0.00382093689404428\n",
      "epoch: 5 | 21568 / 114272 | training loss: 0.004020582884550095\n",
      "epoch: 5 | 21600 / 114272 | training loss: 0.001476737204939127\n",
      "epoch: 5 | 21632 / 114272 | training loss: 0.01955636404454708\n",
      "epoch: 5 | 21664 / 114272 | training loss: 0.052063602954149246\n",
      "epoch: 5 | 21696 / 114272 | training loss: 0.01128668338060379\n",
      "epoch: 5 | 21728 / 114272 | training loss: 0.001043853466399014\n",
      "epoch: 5 | 21760 / 114272 | training loss: 0.001681546913459897\n",
      "epoch: 5 | 21792 / 114272 | training loss: 0.0011670145904645324\n",
      "epoch: 5 | 21824 / 114272 | training loss: 0.002550593577325344\n",
      "epoch: 5 | 21856 / 114272 | training loss: 0.0007986487471498549\n",
      "epoch: 5 | 21888 / 114272 | training loss: 0.14817902445793152\n",
      "epoch: 5 | 21920 / 114272 | training loss: 0.0018149391980841756\n",
      "epoch: 5 | 21952 / 114272 | training loss: 0.11047088354825974\n",
      "epoch: 5 | 21984 / 114272 | training loss: 0.0021255756728351116\n",
      "epoch: 5 | 22016 / 114272 | training loss: 0.004533641971647739\n",
      "epoch: 5 | 22048 / 114272 | training loss: 0.0017691046232357621\n",
      "epoch: 5 | 22080 / 114272 | training loss: 0.0019145503174513578\n",
      "epoch: 5 | 22112 / 114272 | training loss: 0.0014733171556144953\n",
      "epoch: 5 | 22144 / 114272 | training loss: 0.032688405364751816\n",
      "epoch: 5 | 22176 / 114272 | training loss: 0.0012758341617882252\n",
      "epoch: 5 | 22208 / 114272 | training loss: 0.0010988048743456602\n",
      "epoch: 5 | 22240 / 114272 | training loss: 0.12073156982660294\n",
      "epoch: 5 | 22272 / 114272 | training loss: 0.0017916536889970303\n",
      "epoch: 5 | 22304 / 114272 | training loss: 0.02416943572461605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 22336 / 114272 | training loss: 0.0038390641566365957\n",
      "epoch: 5 | 22368 / 114272 | training loss: 0.0009766754228621721\n",
      "epoch: 5 | 22400 / 114272 | training loss: 0.001571320230141282\n",
      "epoch: 5 | 22432 / 114272 | training loss: 0.0012062747264280915\n",
      "epoch: 5 | 22464 / 114272 | training loss: 0.0027988594956696033\n",
      "epoch: 5 | 22496 / 114272 | training loss: 0.0011092937784269452\n",
      "epoch: 5 | 22528 / 114272 | training loss: 0.0016404767520725727\n",
      "epoch: 5 | 22560 / 114272 | training loss: 0.0015429755439981818\n",
      "epoch: 5 | 22592 / 114272 | training loss: 0.0013938272604718804\n",
      "epoch: 5 | 22624 / 114272 | training loss: 0.0848313570022583\n",
      "epoch: 5 | 22656 / 114272 | training loss: 0.0029238613788038492\n",
      "epoch: 5 | 22688 / 114272 | training loss: 0.09790016710758209\n",
      "epoch: 5 | 22720 / 114272 | training loss: 0.004499367438256741\n",
      "epoch: 5 | 22752 / 114272 | training loss: 0.0024197206366807222\n",
      "epoch: 5 | 22784 / 114272 | training loss: 0.05857863649725914\n",
      "epoch: 5 | 22816 / 114272 | training loss: 0.03789334371685982\n",
      "epoch: 5 | 22848 / 114272 | training loss: 0.006211776752024889\n",
      "epoch: 5 | 22880 / 114272 | training loss: 0.0014742721104994416\n",
      "epoch: 5 | 22912 / 114272 | training loss: 0.002118844073265791\n",
      "epoch: 5 | 22944 / 114272 | training loss: 0.0014062193222343922\n",
      "epoch: 5 | 22976 / 114272 | training loss: 0.0015622003702446818\n",
      "epoch: 5 | 23008 / 114272 | training loss: 0.012457086704671383\n",
      "epoch: 5 | 23040 / 114272 | training loss: 0.12105124443769455\n",
      "epoch: 5 | 23072 / 114272 | training loss: 0.0015659673372283578\n",
      "epoch: 5 | 23104 / 114272 | training loss: 0.12710849940776825\n",
      "epoch: 5 | 23136 / 114272 | training loss: 0.0013174550840631127\n",
      "epoch: 5 | 23168 / 114272 | training loss: 0.11497852951288223\n",
      "epoch: 5 | 23200 / 114272 | training loss: 0.0029680468142032623\n",
      "epoch: 5 | 23232 / 114272 | training loss: 0.001351740793325007\n",
      "epoch: 5 | 23264 / 114272 | training loss: 0.0011217051651328802\n",
      "epoch: 5 | 23296 / 114272 | training loss: 0.0007229057955555618\n",
      "epoch: 5 | 23328 / 114272 | training loss: 0.0028654958587139845\n",
      "epoch: 5 | 23360 / 114272 | training loss: 0.0020735496655106544\n",
      "epoch: 5 | 23392 / 114272 | training loss: 0.07643941789865494\n",
      "epoch: 5 | 23424 / 114272 | training loss: 0.0018288693390786648\n",
      "epoch: 5 | 23456 / 114272 | training loss: 0.002302042441442609\n",
      "epoch: 5 | 23488 / 114272 | training loss: 0.0013020255137234926\n",
      "epoch: 5 | 23520 / 114272 | training loss: 0.12348338216543198\n",
      "epoch: 5 | 23552 / 114272 | training loss: 0.0005525683518499136\n",
      "epoch: 5 | 23584 / 114272 | training loss: 0.022715408354997635\n",
      "epoch: 5 | 23616 / 114272 | training loss: 0.11975865066051483\n",
      "epoch: 5 | 23648 / 114272 | training loss: 0.0011334748705849051\n",
      "epoch: 5 | 23680 / 114272 | training loss: 0.001414795289747417\n",
      "epoch: 5 | 23712 / 114272 | training loss: 0.0019966503605246544\n",
      "epoch: 5 | 23744 / 114272 | training loss: 0.001985370647162199\n",
      "epoch: 5 | 23776 / 114272 | training loss: 0.1963832527399063\n",
      "epoch: 5 | 23808 / 114272 | training loss: 0.10090279579162598\n",
      "epoch: 5 | 23840 / 114272 | training loss: 0.12440429627895355\n",
      "epoch: 5 | 23872 / 114272 | training loss: 0.0012061487650498748\n",
      "epoch: 5 | 23904 / 114272 | training loss: 0.0011929766042158008\n",
      "epoch: 5 | 23936 / 114272 | training loss: 0.0011764359660446644\n",
      "epoch: 5 | 23968 / 114272 | training loss: 0.016066331416368484\n",
      "epoch: 5 | 24000 / 114272 | training loss: 0.13326096534729004\n",
      "epoch: 5 | 24032 / 114272 | training loss: 0.0027124793268740177\n",
      "epoch: 5 | 24064 / 114272 | training loss: 0.0016111796721816063\n",
      "epoch: 5 | 24096 / 114272 | training loss: 0.000717783987056464\n",
      "epoch: 5 | 24128 / 114272 | training loss: 0.0013061813078820705\n",
      "epoch: 5 | 24160 / 114272 | training loss: 0.0009626889368519187\n",
      "epoch: 5 | 24192 / 114272 | training loss: 0.0021139467135071754\n",
      "epoch: 5 | 24224 / 114272 | training loss: 0.27143922448158264\n",
      "epoch: 5 | 24256 / 114272 | training loss: 0.08609174191951752\n",
      "epoch: 5 | 24288 / 114272 | training loss: 0.0027559411246329546\n",
      "epoch: 5 | 24320 / 114272 | training loss: 0.005555273033678532\n",
      "epoch: 5 | 24352 / 114272 | training loss: 0.000785576063208282\n",
      "epoch: 5 | 24384 / 114272 | training loss: 0.0008533752406947315\n",
      "epoch: 5 | 24416 / 114272 | training loss: 0.001015474321320653\n",
      "epoch: 5 | 24448 / 114272 | training loss: 0.00048703482025302947\n",
      "epoch: 5 | 24480 / 114272 | training loss: 0.0008943029679358006\n",
      "epoch: 5 | 24512 / 114272 | training loss: 0.0009329515742138028\n",
      "epoch: 5 | 24544 / 114272 | training loss: 0.00161143415607512\n",
      "epoch: 5 | 24576 / 114272 | training loss: 0.011873902752995491\n",
      "epoch: 5 | 24608 / 114272 | training loss: 0.10072986781597137\n",
      "epoch: 5 | 24640 / 114272 | training loss: 0.05682332441210747\n",
      "epoch: 5 | 24672 / 114272 | training loss: 0.000907695502974093\n",
      "epoch: 5 | 24704 / 114272 | training loss: 0.0015985185746103525\n",
      "epoch: 5 | 24736 / 114272 | training loss: 0.0010985773988068104\n",
      "epoch: 5 | 24768 / 114272 | training loss: 0.001822723657824099\n",
      "epoch: 5 | 24800 / 114272 | training loss: 0.0020678676664829254\n",
      "epoch: 5 | 24832 / 114272 | training loss: 0.0012821750715374947\n",
      "epoch: 5 | 24864 / 114272 | training loss: 0.001182481530122459\n",
      "epoch: 5 | 24896 / 114272 | training loss: 0.0005765699315816164\n",
      "epoch: 5 | 24928 / 114272 | training loss: 0.06645824760198593\n",
      "epoch: 5 | 24960 / 114272 | training loss: 0.0016744823660701513\n",
      "epoch: 5 | 24992 / 114272 | training loss: 0.011650626547634602\n",
      "epoch: 5 | 25024 / 114272 | training loss: 0.004821944050490856\n",
      "epoch: 5 | 25056 / 114272 | training loss: 0.16928793489933014\n",
      "epoch: 5 | 25088 / 114272 | training loss: 0.0008824689430184662\n",
      "epoch: 5 | 25120 / 114272 | training loss: 0.0008375652832910419\n",
      "epoch: 5 | 25152 / 114272 | training loss: 0.0032875293400138617\n",
      "epoch: 5 | 25184 / 114272 | training loss: 0.001442472217604518\n",
      "epoch: 5 | 25216 / 114272 | training loss: 0.0009336442453786731\n",
      "epoch: 5 | 25248 / 114272 | training loss: 0.0014170504873618484\n",
      "epoch: 5 | 25280 / 114272 | training loss: 0.0009574508294463158\n",
      "epoch: 5 | 25312 / 114272 | training loss: 0.007558553479611874\n",
      "epoch: 5 | 25344 / 114272 | training loss: 0.0010832218686118722\n",
      "epoch: 5 | 25376 / 114272 | training loss: 0.001015757443383336\n",
      "epoch: 5 | 25408 / 114272 | training loss: 0.0009559948812238872\n",
      "epoch: 5 | 25440 / 114272 | training loss: 0.0035645239986479282\n",
      "epoch: 5 | 25472 / 114272 | training loss: 0.0009005976608023047\n",
      "epoch: 5 | 25504 / 114272 | training loss: 0.0008631996461190283\n",
      "epoch: 5 | 25536 / 114272 | training loss: 0.0005893830675631762\n",
      "epoch: 5 | 25568 / 114272 | training loss: 0.0011154164094477892\n",
      "epoch: 5 | 25600 / 114272 | training loss: 0.0038396441377699375\n",
      "epoch: 5 | 25632 / 114272 | training loss: 0.002506975783035159\n",
      "epoch: 5 | 25664 / 114272 | training loss: 0.040020063519477844\n",
      "epoch: 5 | 25696 / 114272 | training loss: 0.17431886494159698\n",
      "epoch: 5 | 25728 / 114272 | training loss: 0.0010991781018674374\n",
      "epoch: 5 | 25760 / 114272 | training loss: 0.1262739896774292\n",
      "epoch: 5 | 25792 / 114272 | training loss: 0.0005752556608058512\n",
      "epoch: 5 | 25824 / 114272 | training loss: 0.0013956858310848475\n",
      "epoch: 5 | 25856 / 114272 | training loss: 0.0039235386066138744\n",
      "epoch: 5 | 25888 / 114272 | training loss: 0.0012858760310336947\n",
      "epoch: 5 | 25920 / 114272 | training loss: 0.19905254244804382\n",
      "epoch: 5 | 25952 / 114272 | training loss: 0.1455833911895752\n",
      "epoch: 5 | 25984 / 114272 | training loss: 0.14197829365730286\n",
      "epoch: 5 | 26016 / 114272 | training loss: 0.0015876360703259706\n",
      "epoch: 5 | 26048 / 114272 | training loss: 0.0010929199634119868\n",
      "epoch: 5 | 26080 / 114272 | training loss: 0.049709394574165344\n",
      "epoch: 5 | 26112 / 114272 | training loss: 0.002414438407868147\n",
      "epoch: 5 | 26144 / 114272 | training loss: 0.0021900739520788193\n",
      "epoch: 5 | 26176 / 114272 | training loss: 0.0038013739977031946\n",
      "epoch: 5 | 26208 / 114272 | training loss: 0.005802252795547247\n",
      "epoch: 5 | 26240 / 114272 | training loss: 0.0014763735234737396\n",
      "epoch: 5 | 26272 / 114272 | training loss: 0.0034291562624275684\n",
      "epoch: 5 | 26304 / 114272 | training loss: 0.0006298538064584136\n",
      "epoch: 5 | 26336 / 114272 | training loss: 0.2526567876338959\n",
      "epoch: 5 | 26368 / 114272 | training loss: 0.002755598397925496\n",
      "epoch: 5 | 26400 / 114272 | training loss: 0.14914846420288086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 26432 / 114272 | training loss: 0.005508078262209892\n",
      "epoch: 5 | 26464 / 114272 | training loss: 0.002579241059720516\n",
      "epoch: 5 | 26496 / 114272 | training loss: 0.0359325036406517\n",
      "epoch: 5 | 26528 / 114272 | training loss: 0.09979335963726044\n",
      "epoch: 5 | 26560 / 114272 | training loss: 0.003077899105846882\n",
      "epoch: 5 | 26592 / 114272 | training loss: 0.0331331305205822\n",
      "epoch: 5 | 26624 / 114272 | training loss: 0.0013127923011779785\n",
      "epoch: 5 | 26656 / 114272 | training loss: 0.0019625090062618256\n",
      "epoch: 5 | 26688 / 114272 | training loss: 0.0014761437196284533\n",
      "epoch: 5 | 26720 / 114272 | training loss: 0.0037976913154125214\n",
      "epoch: 5 | 26752 / 114272 | training loss: 0.0019155684858560562\n",
      "epoch: 5 | 26784 / 114272 | training loss: 0.36177563667297363\n",
      "epoch: 5 | 26816 / 114272 | training loss: 0.0008498767274431884\n",
      "epoch: 5 | 26848 / 114272 | training loss: 0.006788765545934439\n",
      "epoch: 5 | 26880 / 114272 | training loss: 0.0012104593915864825\n",
      "epoch: 5 | 26912 / 114272 | training loss: 0.006542388815432787\n",
      "epoch: 5 | 26944 / 114272 | training loss: 0.09216053783893585\n",
      "epoch: 5 | 26976 / 114272 | training loss: 0.004305316135287285\n",
      "epoch: 5 | 27008 / 114272 | training loss: 0.002273451304063201\n",
      "epoch: 5 | 27040 / 114272 | training loss: 0.0019181100651621819\n",
      "epoch: 5 | 27072 / 114272 | training loss: 0.14082080125808716\n",
      "epoch: 5 | 27104 / 114272 | training loss: 0.003687022253870964\n",
      "epoch: 5 | 27136 / 114272 | training loss: 0.0025680502876639366\n",
      "epoch: 5 | 27168 / 114272 | training loss: 0.20667381584644318\n",
      "epoch: 5 | 27200 / 114272 | training loss: 0.005178750492632389\n",
      "epoch: 5 | 27232 / 114272 | training loss: 0.15075302124023438\n",
      "epoch: 5 | 27264 / 114272 | training loss: 0.0025225630961358547\n",
      "epoch: 5 | 27296 / 114272 | training loss: 0.02780500426888466\n",
      "epoch: 5 | 27328 / 114272 | training loss: 0.16063936054706573\n",
      "epoch: 5 | 27360 / 114272 | training loss: 0.0018261171644553542\n",
      "epoch: 5 | 27392 / 114272 | training loss: 0.08022941648960114\n",
      "epoch: 5 | 27424 / 114272 | training loss: 0.11447375267744064\n",
      "epoch: 5 | 27456 / 114272 | training loss: 0.0005985897150821984\n",
      "epoch: 5 | 27488 / 114272 | training loss: 0.13666091859340668\n",
      "epoch: 5 | 27520 / 114272 | training loss: 0.0021067531779408455\n",
      "epoch: 5 | 27552 / 114272 | training loss: 0.002920679748058319\n",
      "epoch: 5 | 27584 / 114272 | training loss: 0.002257663756608963\n",
      "epoch: 5 | 27616 / 114272 | training loss: 0.003984549082815647\n",
      "epoch: 5 | 27648 / 114272 | training loss: 0.001493715913966298\n",
      "epoch: 5 | 27680 / 114272 | training loss: 0.12124033272266388\n",
      "epoch: 5 | 27712 / 114272 | training loss: 0.002962263999506831\n",
      "epoch: 5 | 27744 / 114272 | training loss: 0.005233405623584986\n",
      "epoch: 5 | 27776 / 114272 | training loss: 0.0008126040338538587\n",
      "epoch: 5 | 27808 / 114272 | training loss: 0.0018664051312953234\n",
      "epoch: 5 | 27840 / 114272 | training loss: 0.13749399781227112\n",
      "epoch: 5 | 27872 / 114272 | training loss: 0.005970598664134741\n",
      "epoch: 5 | 27904 / 114272 | training loss: 0.0034060601610690355\n",
      "epoch: 5 | 27936 / 114272 | training loss: 0.012651636265218258\n",
      "epoch: 5 | 27968 / 114272 | training loss: 0.024210495874285698\n",
      "epoch: 5 | 28000 / 114272 | training loss: 0.004866271745413542\n",
      "epoch: 5 | 28032 / 114272 | training loss: 0.0020871376618742943\n",
      "epoch: 5 | 28064 / 114272 | training loss: 0.003074576612561941\n",
      "epoch: 5 | 28096 / 114272 | training loss: 0.004301396664232016\n",
      "epoch: 5 | 28128 / 114272 | training loss: 0.010103579610586166\n",
      "epoch: 5 | 28160 / 114272 | training loss: 0.012272481806576252\n",
      "epoch: 5 | 28192 / 114272 | training loss: 0.09759316593408585\n",
      "epoch: 5 | 28224 / 114272 | training loss: 0.004107464104890823\n",
      "epoch: 5 | 28256 / 114272 | training loss: 0.003976680804044008\n",
      "epoch: 5 | 28288 / 114272 | training loss: 0.002853159559890628\n",
      "epoch: 5 | 28320 / 114272 | training loss: 0.0009308194275945425\n",
      "epoch: 5 | 28352 / 114272 | training loss: 0.13585926592350006\n",
      "epoch: 5 | 28384 / 114272 | training loss: 0.0024766051210463047\n",
      "epoch: 5 | 28416 / 114272 | training loss: 0.002100525191053748\n",
      "epoch: 5 | 28448 / 114272 | training loss: 0.009075111709535122\n",
      "epoch: 5 | 28480 / 114272 | training loss: 0.10265179723501205\n",
      "epoch: 5 | 28512 / 114272 | training loss: 0.0017344533698633313\n",
      "epoch: 5 | 28544 / 114272 | training loss: 0.005385372322052717\n",
      "epoch: 5 | 28576 / 114272 | training loss: 0.12873125076293945\n",
      "epoch: 5 | 28608 / 114272 | training loss: 0.0039262063801288605\n",
      "epoch: 5 | 28640 / 114272 | training loss: 0.0025665834546089172\n",
      "epoch: 5 | 28672 / 114272 | training loss: 0.006506381090730429\n",
      "epoch: 5 | 28704 / 114272 | training loss: 0.10428309440612793\n",
      "epoch: 5 | 28736 / 114272 | training loss: 0.0032997257076203823\n",
      "epoch: 5 | 28768 / 114272 | training loss: 0.03930442035198212\n",
      "epoch: 5 | 28800 / 114272 | training loss: 0.00297905202023685\n",
      "epoch: 5 | 28832 / 114272 | training loss: 0.14332331717014313\n",
      "epoch: 5 | 28864 / 114272 | training loss: 0.002623750828206539\n",
      "epoch: 5 | 28896 / 114272 | training loss: 0.0008580215508118272\n",
      "epoch: 5 | 28928 / 114272 | training loss: 0.0009546917863190174\n",
      "epoch: 5 | 28960 / 114272 | training loss: 0.0028774733655154705\n",
      "epoch: 5 | 28992 / 114272 | training loss: 0.0013621486723423004\n",
      "epoch: 5 | 29024 / 114272 | training loss: 0.14986029267311096\n",
      "epoch: 5 | 29056 / 114272 | training loss: 0.002855228492990136\n",
      "epoch: 5 | 29088 / 114272 | training loss: 0.0030769316945225\n",
      "epoch: 5 | 29120 / 114272 | training loss: 0.12247023731470108\n",
      "epoch: 5 | 29152 / 114272 | training loss: 0.004388289526104927\n",
      "epoch: 5 | 29184 / 114272 | training loss: 0.004697970114648342\n",
      "epoch: 5 | 29216 / 114272 | training loss: 0.0028639989905059338\n",
      "epoch: 5 | 29248 / 114272 | training loss: 0.002591715194284916\n",
      "epoch: 5 | 29280 / 114272 | training loss: 0.20799294114112854\n",
      "epoch: 5 | 29312 / 114272 | training loss: 0.09705092757940292\n",
      "epoch: 5 | 29344 / 114272 | training loss: 0.004464915953576565\n",
      "epoch: 5 | 29376 / 114272 | training loss: 0.011808532290160656\n",
      "epoch: 5 | 29408 / 114272 | training loss: 0.0026995688676834106\n",
      "epoch: 5 | 29440 / 114272 | training loss: 0.0039006758015602827\n",
      "epoch: 5 | 29472 / 114272 | training loss: 0.0018448522314429283\n",
      "epoch: 5 | 29504 / 114272 | training loss: 0.04914962500333786\n",
      "epoch: 5 | 29536 / 114272 | training loss: 0.001400624169036746\n",
      "epoch: 5 | 29568 / 114272 | training loss: 0.001638185465708375\n",
      "epoch: 5 | 29600 / 114272 | training loss: 0.07055038213729858\n",
      "epoch: 5 | 29632 / 114272 | training loss: 0.0022675690706819296\n",
      "epoch: 5 | 29664 / 114272 | training loss: 0.003507073735818267\n",
      "epoch: 5 | 29696 / 114272 | training loss: 0.0008495233487337828\n",
      "epoch: 5 | 29728 / 114272 | training loss: 0.0018226556712761521\n",
      "epoch: 5 | 29760 / 114272 | training loss: 0.002927484456449747\n",
      "epoch: 5 | 29792 / 114272 | training loss: 0.002752048894762993\n",
      "epoch: 5 | 29824 / 114272 | training loss: 0.0009506399510428309\n",
      "epoch: 5 | 29856 / 114272 | training loss: 0.17139728367328644\n",
      "epoch: 5 | 29888 / 114272 | training loss: 0.003253546543419361\n",
      "epoch: 5 | 29920 / 114272 | training loss: 0.011840985156595707\n",
      "epoch: 5 | 29952 / 114272 | training loss: 0.0011648848885670304\n",
      "epoch: 5 | 29984 / 114272 | training loss: 0.0008685535867698491\n",
      "epoch: 5 | 30016 / 114272 | training loss: 0.00048493422218598425\n",
      "epoch: 5 | 30048 / 114272 | training loss: 0.0022348936181515455\n",
      "epoch: 5 | 30080 / 114272 | training loss: 0.0035709657240659\n",
      "epoch: 5 | 30112 / 114272 | training loss: 0.0005709648248739541\n",
      "epoch: 5 | 30144 / 114272 | training loss: 0.15185728669166565\n",
      "epoch: 5 | 30176 / 114272 | training loss: 0.0016912687569856644\n",
      "epoch: 5 | 30208 / 114272 | training loss: 0.06307543069124222\n",
      "epoch: 5 | 30240 / 114272 | training loss: 0.0013275912497192621\n",
      "epoch: 5 | 30272 / 114272 | training loss: 0.12844817340373993\n",
      "epoch: 5 | 30304 / 114272 | training loss: 0.1288168877363205\n",
      "epoch: 5 | 30336 / 114272 | training loss: 0.009598301723599434\n",
      "epoch: 5 | 30368 / 114272 | training loss: 0.0026875841431319714\n",
      "epoch: 5 | 30400 / 114272 | training loss: 0.0007251471397466958\n",
      "epoch: 5 | 30432 / 114272 | training loss: 0.055346257984638214\n",
      "epoch: 5 | 30464 / 114272 | training loss: 0.0010115575278177857\n",
      "epoch: 5 | 30496 / 114272 | training loss: 0.0013928856933489442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 30528 / 114272 | training loss: 0.001547859632410109\n",
      "epoch: 5 | 30560 / 114272 | training loss: 0.017487473785877228\n",
      "epoch: 5 | 30592 / 114272 | training loss: 0.0005199773004278541\n",
      "epoch: 5 | 30624 / 114272 | training loss: 0.08804909884929657\n",
      "epoch: 5 | 30656 / 114272 | training loss: 0.0005510530900210142\n",
      "epoch: 5 | 30688 / 114272 | training loss: 0.04742472991347313\n",
      "epoch: 5 | 30720 / 114272 | training loss: 0.0008944335277192295\n",
      "epoch: 5 | 30752 / 114272 | training loss: 0.0028746798634529114\n",
      "epoch: 5 | 30784 / 114272 | training loss: 0.004453638568520546\n",
      "epoch: 5 | 30816 / 114272 | training loss: 0.06360187381505966\n",
      "epoch: 5 | 30848 / 114272 | training loss: 0.0016281212447211146\n",
      "epoch: 5 | 30880 / 114272 | training loss: 0.0071733868680894375\n",
      "epoch: 5 | 30912 / 114272 | training loss: 0.0010397261939942837\n",
      "epoch: 5 | 30944 / 114272 | training loss: 0.0011437826324254274\n",
      "epoch: 5 | 30976 / 114272 | training loss: 0.001519468380138278\n",
      "epoch: 5 | 31008 / 114272 | training loss: 0.1099054291844368\n",
      "epoch: 5 | 31040 / 114272 | training loss: 0.0012532564578577876\n",
      "epoch: 5 | 31072 / 114272 | training loss: 0.001067695440724492\n",
      "epoch: 5 | 31104 / 114272 | training loss: 0.004592990968376398\n",
      "epoch: 5 | 31136 / 114272 | training loss: 0.000783709401730448\n",
      "epoch: 5 | 31168 / 114272 | training loss: 0.0029278055299073458\n",
      "epoch: 5 | 31200 / 114272 | training loss: 0.011675279587507248\n",
      "epoch: 5 | 31232 / 114272 | training loss: 0.0010038240579888225\n",
      "epoch: 5 | 31264 / 114272 | training loss: 0.00040853710379451513\n",
      "epoch: 5 | 31296 / 114272 | training loss: 0.11856590956449509\n",
      "epoch: 5 | 31328 / 114272 | training loss: 0.24394990503787994\n",
      "epoch: 5 | 31360 / 114272 | training loss: 0.0014078149106353521\n",
      "epoch: 5 | 31392 / 114272 | training loss: 0.04176654666662216\n",
      "epoch: 5 | 31424 / 114272 | training loss: 0.0008719566976651549\n",
      "epoch: 5 | 31456 / 114272 | training loss: 0.0011493146885186434\n",
      "epoch: 5 | 31488 / 114272 | training loss: 0.0016018524765968323\n",
      "epoch: 5 | 31520 / 114272 | training loss: 0.0018223539227619767\n",
      "epoch: 5 | 31552 / 114272 | training loss: 0.002273542108014226\n",
      "epoch: 5 | 31584 / 114272 | training loss: 0.0013942796504125\n",
      "epoch: 5 | 31616 / 114272 | training loss: 0.0013382378965616226\n",
      "epoch: 5 | 31648 / 114272 | training loss: 0.0026804236695170403\n",
      "epoch: 5 | 31680 / 114272 | training loss: 0.0016920946072787046\n",
      "epoch: 5 | 31712 / 114272 | training loss: 0.0009012939990498126\n",
      "epoch: 5 | 31744 / 114272 | training loss: 0.0016639879904687405\n",
      "epoch: 5 | 31776 / 114272 | training loss: 0.0012567697558552027\n",
      "epoch: 5 | 31808 / 114272 | training loss: 0.0016102463705465198\n",
      "epoch: 5 | 31840 / 114272 | training loss: 0.0013123615644872189\n",
      "epoch: 5 | 31872 / 114272 | training loss: 0.0007193213677965105\n",
      "epoch: 5 | 31904 / 114272 | training loss: 0.07628494501113892\n",
      "epoch: 5 | 31936 / 114272 | training loss: 0.004253838211297989\n",
      "epoch: 5 | 31968 / 114272 | training loss: 0.0006230392609722912\n",
      "epoch: 5 | 32000 / 114272 | training loss: 0.000956425501499325\n",
      "epoch: 5 | 32032 / 114272 | training loss: 0.0009249409777112305\n",
      "epoch: 5 | 32064 / 114272 | training loss: 0.07068238407373428\n",
      "epoch: 5 | 32096 / 114272 | training loss: 0.0008216130081564188\n",
      "epoch: 5 | 32128 / 114272 | training loss: 0.0007404128555208445\n",
      "epoch: 5 | 32160 / 114272 | training loss: 0.0011068518506363034\n",
      "epoch: 5 | 32192 / 114272 | training loss: 0.0005149799981154501\n",
      "epoch: 5 | 32224 / 114272 | training loss: 0.006397029384970665\n",
      "epoch: 5 | 32256 / 114272 | training loss: 0.0007937413174659014\n",
      "epoch: 5 | 32288 / 114272 | training loss: 0.0221088957041502\n",
      "epoch: 5 | 32320 / 114272 | training loss: 0.00703476183116436\n",
      "epoch: 5 | 32352 / 114272 | training loss: 0.0017773548606783152\n",
      "epoch: 5 | 32384 / 114272 | training loss: 0.001138736610300839\n",
      "epoch: 5 | 32416 / 114272 | training loss: 0.000577261031139642\n",
      "epoch: 5 | 32448 / 114272 | training loss: 0.0007164723356254399\n",
      "epoch: 5 | 32480 / 114272 | training loss: 0.0016974065219983459\n",
      "epoch: 5 | 32512 / 114272 | training loss: 0.00038436101749539375\n",
      "epoch: 5 | 32544 / 114272 | training loss: 0.0014457966899499297\n",
      "epoch: 5 | 32576 / 114272 | training loss: 0.0015213883016258478\n",
      "epoch: 5 | 32608 / 114272 | training loss: 0.0021931128576397896\n",
      "epoch: 5 | 32640 / 114272 | training loss: 0.0006885892944410443\n",
      "epoch: 5 | 32672 / 114272 | training loss: 0.0046241856180131435\n",
      "epoch: 5 | 32704 / 114272 | training loss: 0.0016381965251639485\n",
      "epoch: 5 | 32736 / 114272 | training loss: 0.00038324223714880645\n",
      "epoch: 5 | 32768 / 114272 | training loss: 0.0027930994983762503\n",
      "epoch: 5 | 32800 / 114272 | training loss: 0.003806960303336382\n",
      "epoch: 5 | 32832 / 114272 | training loss: 0.1037551686167717\n",
      "epoch: 5 | 32864 / 114272 | training loss: 0.00048148524365387857\n",
      "epoch: 5 | 32896 / 114272 | training loss: 0.001424729940481484\n",
      "epoch: 5 | 32928 / 114272 | training loss: 0.00043853899114765227\n",
      "epoch: 5 | 32960 / 114272 | training loss: 0.0005298877949826419\n",
      "epoch: 5 | 32992 / 114272 | training loss: 0.0005299251643009484\n",
      "epoch: 5 | 33024 / 114272 | training loss: 0.0009366839076392353\n",
      "epoch: 5 | 33056 / 114272 | training loss: 0.0010201918194070458\n",
      "epoch: 5 | 33088 / 114272 | training loss: 0.0026545897126197815\n",
      "epoch: 5 | 33120 / 114272 | training loss: 0.000895650708116591\n",
      "epoch: 5 | 33152 / 114272 | training loss: 0.00073677470209077\n",
      "epoch: 5 | 33184 / 114272 | training loss: 0.0006098904414102435\n",
      "epoch: 5 | 33216 / 114272 | training loss: 0.0031155485194176435\n",
      "epoch: 5 | 33248 / 114272 | training loss: 0.04430804029107094\n",
      "epoch: 5 | 33280 / 114272 | training loss: 0.0003279659431427717\n",
      "epoch: 5 | 33312 / 114272 | training loss: 0.15709787607192993\n",
      "epoch: 5 | 33344 / 114272 | training loss: 0.0007585791172459722\n",
      "epoch: 5 | 33376 / 114272 | training loss: 0.001768863177858293\n",
      "epoch: 5 | 33408 / 114272 | training loss: 0.00038817725726403296\n",
      "epoch: 5 | 33440 / 114272 | training loss: 0.006446613930165768\n",
      "epoch: 5 | 33472 / 114272 | training loss: 0.000697772076819092\n",
      "epoch: 5 | 33504 / 114272 | training loss: 0.0013240419793874025\n",
      "epoch: 5 | 33536 / 114272 | training loss: 0.0004146773717366159\n",
      "epoch: 5 | 33568 / 114272 | training loss: 0.0006933123804628849\n",
      "epoch: 5 | 33600 / 114272 | training loss: 0.021171892061829567\n",
      "epoch: 5 | 33632 / 114272 | training loss: 0.0013062488287687302\n",
      "epoch: 5 | 33664 / 114272 | training loss: 0.0024417275562882423\n",
      "epoch: 5 | 33696 / 114272 | training loss: 0.26697564125061035\n",
      "epoch: 5 | 33728 / 114272 | training loss: 0.00032895838376134634\n",
      "epoch: 5 | 33760 / 114272 | training loss: 0.13554254174232483\n",
      "epoch: 5 | 33792 / 114272 | training loss: 0.0005065275472588837\n",
      "epoch: 5 | 33824 / 114272 | training loss: 0.0005700173205696046\n",
      "epoch: 5 | 33856 / 114272 | training loss: 0.0019607373978942633\n",
      "epoch: 5 | 33888 / 114272 | training loss: 0.0013734849635511637\n",
      "epoch: 5 | 33920 / 114272 | training loss: 0.0010830158134922385\n",
      "epoch: 5 | 33952 / 114272 | training loss: 0.0031982443761080503\n",
      "epoch: 5 | 33984 / 114272 | training loss: 0.0011463987175375223\n",
      "epoch: 5 | 34016 / 114272 | training loss: 0.0007243748987093568\n",
      "epoch: 5 | 34048 / 114272 | training loss: 0.000603754713665694\n",
      "epoch: 5 | 34080 / 114272 | training loss: 0.0012136291479691863\n",
      "epoch: 5 | 34112 / 114272 | training loss: 0.00039525216561742127\n",
      "epoch: 5 | 34144 / 114272 | training loss: 0.000683952821418643\n",
      "epoch: 5 | 34176 / 114272 | training loss: 0.0005912219639867544\n",
      "epoch: 5 | 34208 / 114272 | training loss: 0.0007264060550369322\n",
      "epoch: 5 | 34240 / 114272 | training loss: 0.0017088090535253286\n",
      "epoch: 5 | 34272 / 114272 | training loss: 0.13728845119476318\n",
      "epoch: 5 | 34304 / 114272 | training loss: 0.00103110505733639\n",
      "epoch: 5 | 34336 / 114272 | training loss: 0.0007812515250407159\n",
      "epoch: 5 | 34368 / 114272 | training loss: 0.0003247952554374933\n",
      "epoch: 5 | 34400 / 114272 | training loss: 0.0014786365209147334\n",
      "epoch: 5 | 34432 / 114272 | training loss: 0.0008965323213487864\n",
      "epoch: 5 | 34464 / 114272 | training loss: 0.09492740780115128\n",
      "epoch: 5 | 34496 / 114272 | training loss: 0.0005129677592776716\n",
      "epoch: 5 | 34528 / 114272 | training loss: 0.14160926640033722\n",
      "epoch: 5 | 34560 / 114272 | training loss: 0.0007082952652126551\n",
      "epoch: 5 | 34592 / 114272 | training loss: 0.21294786036014557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 34624 / 114272 | training loss: 0.002011198317632079\n",
      "epoch: 5 | 34656 / 114272 | training loss: 0.0024905239697545767\n",
      "epoch: 5 | 34688 / 114272 | training loss: 0.0010289945639669895\n",
      "epoch: 5 | 34720 / 114272 | training loss: 0.0011747918324545026\n",
      "epoch: 5 | 34752 / 114272 | training loss: 0.0017581084975972772\n",
      "epoch: 5 | 34784 / 114272 | training loss: 0.0013389840023592114\n",
      "epoch: 5 | 34816 / 114272 | training loss: 0.08413717150688171\n",
      "epoch: 5 | 34848 / 114272 | training loss: 0.0010285797761753201\n",
      "epoch: 5 | 34880 / 114272 | training loss: 0.0050669340416789055\n",
      "epoch: 5 | 34912 / 114272 | training loss: 0.00062842812621966\n",
      "epoch: 5 | 34944 / 114272 | training loss: 0.0021016024984419346\n",
      "epoch: 5 | 34976 / 114272 | training loss: 0.1963283121585846\n",
      "epoch: 5 | 35008 / 114272 | training loss: 0.0005997529369778931\n",
      "epoch: 5 | 35040 / 114272 | training loss: 0.0007909320993348956\n",
      "epoch: 5 | 35072 / 114272 | training loss: 0.09457781910896301\n",
      "epoch: 5 | 35104 / 114272 | training loss: 0.001138690859079361\n",
      "epoch: 5 | 35136 / 114272 | training loss: 0.17451436817646027\n",
      "epoch: 5 | 35168 / 114272 | training loss: 0.001741637708619237\n",
      "epoch: 5 | 35200 / 114272 | training loss: 0.0005154716782271862\n",
      "epoch: 5 | 35232 / 114272 | training loss: 0.0016807053470984101\n",
      "epoch: 5 | 35264 / 114272 | training loss: 0.0004982741666026413\n",
      "epoch: 5 | 35296 / 114272 | training loss: 0.0018747906433418393\n",
      "epoch: 5 | 35328 / 114272 | training loss: 0.0017792105209082365\n",
      "epoch: 5 | 35360 / 114272 | training loss: 0.07116298377513885\n",
      "epoch: 5 | 35392 / 114272 | training loss: 0.20949456095695496\n",
      "epoch: 5 | 35424 / 114272 | training loss: 0.0008469587191939354\n",
      "epoch: 5 | 35456 / 114272 | training loss: 0.0007422639173455536\n",
      "epoch: 5 | 35488 / 114272 | training loss: 0.0016074402956292033\n",
      "epoch: 5 | 35520 / 114272 | training loss: 0.0017240834422409534\n",
      "epoch: 5 | 35552 / 114272 | training loss: 0.003385021351277828\n",
      "epoch: 5 | 35584 / 114272 | training loss: 0.010239148512482643\n",
      "epoch: 5 | 35616 / 114272 | training loss: 0.0044486867263913155\n",
      "epoch: 5 | 35648 / 114272 | training loss: 0.14594225585460663\n",
      "epoch: 5 | 35680 / 114272 | training loss: 0.1025242879986763\n",
      "epoch: 5 | 35712 / 114272 | training loss: 0.0014790375716984272\n",
      "epoch: 5 | 35744 / 114272 | training loss: 0.002824642928317189\n",
      "epoch: 5 | 35776 / 114272 | training loss: 0.0006467364146374166\n",
      "epoch: 5 | 35808 / 114272 | training loss: 0.0008324525551870465\n",
      "epoch: 5 | 35840 / 114272 | training loss: 0.003281588200479746\n",
      "epoch: 5 | 35872 / 114272 | training loss: 0.059634074568748474\n",
      "epoch: 5 | 35904 / 114272 | training loss: 0.0017699477029964328\n",
      "epoch: 5 | 35936 / 114272 | training loss: 0.007494197227060795\n",
      "epoch: 5 | 35968 / 114272 | training loss: 0.0027910659555345774\n",
      "epoch: 5 | 36000 / 114272 | training loss: 0.0008936672238633037\n",
      "epoch: 5 | 36032 / 114272 | training loss: 0.0015575221041217446\n",
      "epoch: 5 | 36064 / 114272 | training loss: 0.00046704921987839043\n",
      "epoch: 5 | 36096 / 114272 | training loss: 0.004470638930797577\n",
      "epoch: 5 | 36128 / 114272 | training loss: 0.002499569207429886\n",
      "epoch: 5 | 36160 / 114272 | training loss: 0.0015259691281244159\n",
      "epoch: 5 | 36192 / 114272 | training loss: 0.0015945413615554571\n",
      "epoch: 5 | 36224 / 114272 | training loss: 0.0016882214695215225\n",
      "epoch: 5 | 36256 / 114272 | training loss: 0.18165194988250732\n",
      "epoch: 5 | 36288 / 114272 | training loss: 0.0013766500633209944\n",
      "epoch: 5 | 36320 / 114272 | training loss: 0.0007114507025107741\n",
      "epoch: 5 | 36352 / 114272 | training loss: 0.021526917815208435\n",
      "epoch: 5 | 36384 / 114272 | training loss: 0.0006652074516750872\n",
      "epoch: 5 | 36416 / 114272 | training loss: 0.032689426094293594\n",
      "epoch: 5 | 36448 / 114272 | training loss: 0.0018617549212649465\n",
      "epoch: 5 | 36480 / 114272 | training loss: 0.0007776705315336585\n",
      "epoch: 5 | 36512 / 114272 | training loss: 0.0017916227225214243\n",
      "epoch: 5 | 36544 / 114272 | training loss: 0.0016571872401982546\n",
      "epoch: 5 | 36576 / 114272 | training loss: 0.0015280150109902024\n",
      "epoch: 5 | 36608 / 114272 | training loss: 0.0009798762621358037\n",
      "epoch: 5 | 36640 / 114272 | training loss: 0.0005949612823314965\n",
      "epoch: 5 | 36672 / 114272 | training loss: 0.22434554994106293\n",
      "epoch: 5 | 36704 / 114272 | training loss: 0.0006902744062244892\n",
      "epoch: 5 | 36736 / 114272 | training loss: 0.000499195244628936\n",
      "epoch: 5 | 36768 / 114272 | training loss: 0.0008575627580285072\n",
      "epoch: 5 | 36800 / 114272 | training loss: 0.0014778345357626677\n",
      "epoch: 5 | 36832 / 114272 | training loss: 0.0020131950732320547\n",
      "epoch: 5 | 36864 / 114272 | training loss: 0.0013209551107138395\n",
      "epoch: 5 | 36896 / 114272 | training loss: 0.01238651480525732\n",
      "epoch: 5 | 36928 / 114272 | training loss: 0.0003338786482345313\n",
      "epoch: 5 | 36960 / 114272 | training loss: 0.0007385612116195261\n",
      "epoch: 5 | 36992 / 114272 | training loss: 0.0038007732946425676\n",
      "epoch: 5 | 37024 / 114272 | training loss: 0.001574410474859178\n",
      "epoch: 5 | 37056 / 114272 | training loss: 0.0006062547327019274\n",
      "epoch: 5 | 37088 / 114272 | training loss: 0.0007472562720067799\n",
      "epoch: 5 | 37120 / 114272 | training loss: 0.000597210309933871\n",
      "epoch: 5 | 37152 / 114272 | training loss: 0.0023368073161691427\n",
      "epoch: 5 | 37184 / 114272 | training loss: 0.002218977315351367\n",
      "epoch: 5 | 37216 / 114272 | training loss: 0.0004545430128928274\n",
      "epoch: 5 | 37248 / 114272 | training loss: 0.002461513737216592\n",
      "epoch: 5 | 37280 / 114272 | training loss: 0.007161304354667664\n",
      "epoch: 5 | 37312 / 114272 | training loss: 0.0020870042499154806\n",
      "epoch: 5 | 37344 / 114272 | training loss: 0.0015032142400741577\n",
      "epoch: 5 | 37376 / 114272 | training loss: 0.00046803371515125036\n",
      "epoch: 5 | 37408 / 114272 | training loss: 0.0003223321109544486\n",
      "epoch: 5 | 37440 / 114272 | training loss: 0.003646902274340391\n",
      "epoch: 5 | 37472 / 114272 | training loss: 0.25167232751846313\n",
      "epoch: 5 | 37504 / 114272 | training loss: 0.001901135896332562\n",
      "epoch: 5 | 37536 / 114272 | training loss: 0.0030917245894670486\n",
      "epoch: 5 | 37568 / 114272 | training loss: 0.00306284730322659\n",
      "epoch: 5 | 37600 / 114272 | training loss: 0.0006120077450759709\n",
      "epoch: 5 | 37632 / 114272 | training loss: 0.0004622674605343491\n",
      "epoch: 5 | 37664 / 114272 | training loss: 0.005503917578607798\n",
      "epoch: 5 | 37696 / 114272 | training loss: 0.003606019075959921\n",
      "epoch: 5 | 37728 / 114272 | training loss: 0.0007351718959398568\n",
      "epoch: 5 | 37760 / 114272 | training loss: 0.0005524220177903771\n",
      "epoch: 5 | 37792 / 114272 | training loss: 0.000651788548566401\n",
      "epoch: 5 | 37824 / 114272 | training loss: 0.0005760221392847598\n",
      "epoch: 5 | 37856 / 114272 | training loss: 0.0006327775190584362\n",
      "epoch: 5 | 37888 / 114272 | training loss: 0.0006067198701202869\n",
      "epoch: 5 | 37920 / 114272 | training loss: 0.0011314626317471266\n",
      "epoch: 5 | 37952 / 114272 | training loss: 0.31080320477485657\n",
      "epoch: 5 | 37984 / 114272 | training loss: 0.000668383901938796\n",
      "epoch: 5 | 38016 / 114272 | training loss: 0.27773317694664\n",
      "epoch: 5 | 38048 / 114272 | training loss: 0.001726408489048481\n",
      "epoch: 5 | 38080 / 114272 | training loss: 0.0005480987019836903\n",
      "epoch: 5 | 38112 / 114272 | training loss: 0.0006729976739734411\n",
      "epoch: 5 | 38144 / 114272 | training loss: 0.001129254000261426\n",
      "epoch: 5 | 38176 / 114272 | training loss: 0.0009474584367126226\n",
      "epoch: 5 | 38208 / 114272 | training loss: 0.0009868894703686237\n",
      "epoch: 5 | 38240 / 114272 | training loss: 0.00043194228783249855\n",
      "epoch: 5 | 38272 / 114272 | training loss: 0.0005046873702667654\n",
      "epoch: 5 | 38304 / 114272 | training loss: 0.006320948712527752\n",
      "epoch: 5 | 38336 / 114272 | training loss: 0.0006004084134474397\n",
      "epoch: 5 | 38368 / 114272 | training loss: 0.07950763404369354\n",
      "epoch: 5 | 38400 / 114272 | training loss: 0.05079738423228264\n",
      "epoch: 5 | 38432 / 114272 | training loss: 0.1389436274766922\n",
      "epoch: 5 | 38464 / 114272 | training loss: 0.0012050789082422853\n",
      "epoch: 5 | 38496 / 114272 | training loss: 0.001046530669555068\n",
      "epoch: 5 | 38528 / 114272 | training loss: 0.0010543680982664227\n",
      "epoch: 5 | 38560 / 114272 | training loss: 0.14110109210014343\n",
      "epoch: 5 | 38592 / 114272 | training loss: 0.000594601035118103\n",
      "epoch: 5 | 38624 / 114272 | training loss: 0.000821074063424021\n",
      "epoch: 5 | 38656 / 114272 | training loss: 0.0008072208147495985\n",
      "epoch: 5 | 38688 / 114272 | training loss: 0.002150615444406867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 38720 / 114272 | training loss: 0.0007235070806927979\n",
      "epoch: 5 | 38752 / 114272 | training loss: 0.09357962012290955\n",
      "epoch: 5 | 38784 / 114272 | training loss: 0.2367236465215683\n",
      "epoch: 5 | 38816 / 114272 | training loss: 0.0006201599608175457\n",
      "epoch: 5 | 38848 / 114272 | training loss: 0.0008413661853410304\n",
      "epoch: 5 | 38880 / 114272 | training loss: 0.0009022193844430149\n",
      "epoch: 5 | 38912 / 114272 | training loss: 0.0006461025914177299\n",
      "epoch: 5 | 38944 / 114272 | training loss: 0.0017090700566768646\n",
      "epoch: 5 | 38976 / 114272 | training loss: 0.004223975818604231\n",
      "epoch: 5 | 39008 / 114272 | training loss: 0.0006740455864928663\n",
      "epoch: 5 | 39040 / 114272 | training loss: 0.0009501108434051275\n",
      "epoch: 5 | 39072 / 114272 | training loss: 0.19973056018352509\n",
      "epoch: 5 | 39104 / 114272 | training loss: 0.0003992370329797268\n",
      "epoch: 5 | 39136 / 114272 | training loss: 0.12331030517816544\n",
      "epoch: 5 | 39168 / 114272 | training loss: 0.00094296206953004\n",
      "epoch: 5 | 39200 / 114272 | training loss: 0.0008886310970410705\n",
      "epoch: 5 | 39232 / 114272 | training loss: 0.00047234343946911395\n",
      "epoch: 5 | 39264 / 114272 | training loss: 0.0005021925317123532\n",
      "epoch: 5 | 39296 / 114272 | training loss: 0.0005689393728971481\n",
      "epoch: 5 | 39328 / 114272 | training loss: 0.11108756810426712\n",
      "epoch: 5 | 39360 / 114272 | training loss: 0.003508802503347397\n",
      "epoch: 5 | 39392 / 114272 | training loss: 0.0025267871096730232\n",
      "epoch: 5 | 39424 / 114272 | training loss: 0.00047377156442962587\n",
      "epoch: 5 | 39456 / 114272 | training loss: 0.07121684402227402\n",
      "epoch: 5 | 39488 / 114272 | training loss: 0.11689546704292297\n",
      "epoch: 5 | 39520 / 114272 | training loss: 0.001722961780615151\n",
      "epoch: 5 | 39552 / 114272 | training loss: 0.0012828997569158673\n",
      "epoch: 5 | 39584 / 114272 | training loss: 0.19085215032100677\n",
      "epoch: 5 | 39616 / 114272 | training loss: 0.003340726485475898\n",
      "epoch: 5 | 39648 / 114272 | training loss: 0.0017547131283208728\n",
      "epoch: 5 | 39680 / 114272 | training loss: 0.0007887816755101085\n",
      "epoch: 5 | 39712 / 114272 | training loss: 0.0008037883671931922\n",
      "epoch: 5 | 39744 / 114272 | training loss: 0.001504056970588863\n",
      "epoch: 5 | 39776 / 114272 | training loss: 0.0017582509899511933\n",
      "epoch: 5 | 39808 / 114272 | training loss: 0.0013071037828922272\n",
      "epoch: 5 | 39840 / 114272 | training loss: 0.001237869611941278\n",
      "epoch: 5 | 39872 / 114272 | training loss: 0.0007549058645963669\n",
      "epoch: 5 | 39904 / 114272 | training loss: 0.0011149937054142356\n",
      "epoch: 5 | 39936 / 114272 | training loss: 0.0014445851556956768\n",
      "epoch: 5 | 39968 / 114272 | training loss: 0.001064229290932417\n",
      "epoch: 5 | 40000 / 114272 | training loss: 0.0005257561570033431\n",
      "epoch: 5 | 40032 / 114272 | training loss: 0.001284654252231121\n",
      "epoch: 5 | 40064 / 114272 | training loss: 0.0006993093411438167\n",
      "epoch: 5 | 40096 / 114272 | training loss: 0.001241150894202292\n",
      "epoch: 5 | 40128 / 114272 | training loss: 0.02703026309609413\n",
      "epoch: 5 | 40160 / 114272 | training loss: 0.0016584936529397964\n",
      "epoch: 5 | 40192 / 114272 | training loss: 0.13143345713615417\n",
      "epoch: 5 | 40224 / 114272 | training loss: 0.000835574755910784\n",
      "epoch: 5 | 40256 / 114272 | training loss: 0.058336205780506134\n",
      "epoch: 5 | 40288 / 114272 | training loss: 0.00055219343630597\n",
      "epoch: 5 | 40320 / 114272 | training loss: 0.0009616554016247392\n",
      "epoch: 5 | 40352 / 114272 | training loss: 0.00037448867806233466\n",
      "epoch: 5 | 40384 / 114272 | training loss: 0.0008017175714485347\n",
      "epoch: 5 | 40416 / 114272 | training loss: 0.0008077743696048856\n",
      "epoch: 5 | 40448 / 114272 | training loss: 0.0013564872788265347\n",
      "epoch: 5 | 40480 / 114272 | training loss: 0.0006831070641055703\n",
      "epoch: 5 | 40512 / 114272 | training loss: 0.0007642835844308138\n",
      "epoch: 5 | 40544 / 114272 | training loss: 0.0015232525765895844\n",
      "epoch: 5 | 40576 / 114272 | training loss: 0.11052341014146805\n",
      "epoch: 5 | 40608 / 114272 | training loss: 0.0005206020432524383\n",
      "epoch: 5 | 40640 / 114272 | training loss: 0.005435970611870289\n",
      "epoch: 5 | 40672 / 114272 | training loss: 0.0011115896049886942\n",
      "epoch: 5 | 40704 / 114272 | training loss: 0.002114446135237813\n",
      "epoch: 5 | 40736 / 114272 | training loss: 0.0007237993413582444\n",
      "epoch: 5 | 40768 / 114272 | training loss: 0.0009450727375224233\n",
      "epoch: 5 | 40800 / 114272 | training loss: 0.0005684287170879543\n",
      "epoch: 5 | 40832 / 114272 | training loss: 0.0005364370299503207\n",
      "epoch: 5 | 40864 / 114272 | training loss: 0.0007195196230895817\n",
      "epoch: 5 | 40896 / 114272 | training loss: 0.2044229805469513\n",
      "epoch: 5 | 40928 / 114272 | training loss: 0.002924711210653186\n",
      "epoch: 5 | 40960 / 114272 | training loss: 0.0005635596462525427\n",
      "epoch: 5 | 40992 / 114272 | training loss: 0.23928099870681763\n",
      "epoch: 5 | 41024 / 114272 | training loss: 0.0010600253008306026\n",
      "epoch: 5 | 41056 / 114272 | training loss: 0.0006979013560339808\n",
      "epoch: 5 | 41088 / 114272 | training loss: 0.2925613820552826\n",
      "epoch: 5 | 41120 / 114272 | training loss: 0.0018536070128902793\n",
      "epoch: 5 | 41152 / 114272 | training loss: 0.0005586522747762501\n",
      "epoch: 5 | 41184 / 114272 | training loss: 0.0006480270531028509\n",
      "epoch: 5 | 41216 / 114272 | training loss: 0.0016987688140943646\n",
      "epoch: 5 | 41248 / 114272 | training loss: 0.0010291443904861808\n",
      "epoch: 5 | 41280 / 114272 | training loss: 0.0196247398853302\n",
      "epoch: 5 | 41312 / 114272 | training loss: 0.002744329860433936\n",
      "epoch: 5 | 41344 / 114272 | training loss: 0.0009544556960463524\n",
      "epoch: 5 | 41376 / 114272 | training loss: 0.007196884136646986\n",
      "epoch: 5 | 41408 / 114272 | training loss: 0.0012960610911250114\n",
      "epoch: 5 | 41440 / 114272 | training loss: 0.2670742869377136\n",
      "epoch: 5 | 41472 / 114272 | training loss: 0.00784523505717516\n",
      "epoch: 5 | 41504 / 114272 | training loss: 0.05317635089159012\n",
      "epoch: 5 | 41536 / 114272 | training loss: 0.0012340855319052935\n",
      "epoch: 5 | 41568 / 114272 | training loss: 0.16213692724704742\n",
      "epoch: 5 | 41600 / 114272 | training loss: 0.025745170190930367\n",
      "epoch: 5 | 41632 / 114272 | training loss: 0.0012229601852595806\n",
      "epoch: 5 | 41664 / 114272 | training loss: 0.0009969897801056504\n",
      "epoch: 5 | 41696 / 114272 | training loss: 0.0016785692423582077\n",
      "epoch: 5 | 41728 / 114272 | training loss: 0.0016079061897471547\n",
      "epoch: 5 | 41760 / 114272 | training loss: 0.0023246868513524532\n",
      "epoch: 5 | 41792 / 114272 | training loss: 0.0025666451547294855\n",
      "epoch: 5 | 41824 / 114272 | training loss: 0.0008760956698097289\n",
      "epoch: 5 | 41856 / 114272 | training loss: 0.0008906942675821483\n",
      "epoch: 5 | 41888 / 114272 | training loss: 0.0011938901152461767\n",
      "epoch: 5 | 41920 / 114272 | training loss: 0.0032618886325508356\n",
      "epoch: 5 | 41952 / 114272 | training loss: 0.001129640731960535\n",
      "epoch: 5 | 41984 / 114272 | training loss: 0.0016551667358726263\n",
      "epoch: 5 | 42016 / 114272 | training loss: 0.0010103479726240039\n",
      "epoch: 5 | 42048 / 114272 | training loss: 0.0023417966440320015\n",
      "epoch: 5 | 42080 / 114272 | training loss: 0.03689149394631386\n",
      "epoch: 5 | 42112 / 114272 | training loss: 0.27216431498527527\n",
      "epoch: 5 | 42144 / 114272 | training loss: 0.16602887213230133\n",
      "epoch: 5 | 42176 / 114272 | training loss: 0.0014130020281299949\n",
      "epoch: 5 | 42208 / 114272 | training loss: 0.0006709168665111065\n",
      "epoch: 5 | 42240 / 114272 | training loss: 0.0005989793571643531\n",
      "epoch: 5 | 42272 / 114272 | training loss: 0.0008508533355779946\n",
      "epoch: 5 | 42304 / 114272 | training loss: 0.0032550659961998463\n",
      "epoch: 5 | 42336 / 114272 | training loss: 0.0011803314555436373\n",
      "epoch: 5 | 42368 / 114272 | training loss: 0.0029910411685705185\n",
      "epoch: 5 | 42400 / 114272 | training loss: 0.33040523529052734\n",
      "epoch: 5 | 42432 / 114272 | training loss: 0.0012297960929572582\n",
      "epoch: 5 | 42464 / 114272 | training loss: 0.13215267658233643\n",
      "epoch: 5 | 42496 / 114272 | training loss: 0.0009147312957793474\n",
      "epoch: 5 | 42528 / 114272 | training loss: 0.0015562853077426553\n",
      "epoch: 5 | 42560 / 114272 | training loss: 0.0016084740636870265\n",
      "epoch: 5 | 42592 / 114272 | training loss: 0.0011195653351023793\n",
      "epoch: 5 | 42624 / 114272 | training loss: 0.002218012697994709\n",
      "epoch: 5 | 42656 / 114272 | training loss: 0.0011094833025708795\n",
      "epoch: 5 | 42688 / 114272 | training loss: 0.0028516328893601894\n",
      "epoch: 5 | 42720 / 114272 | training loss: 0.014394775032997131\n",
      "epoch: 5 | 42752 / 114272 | training loss: 0.0014786081155762076\n",
      "epoch: 5 | 42784 / 114272 | training loss: 0.0009102952899411321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 42816 / 114272 | training loss: 0.002178552094846964\n",
      "epoch: 5 | 42848 / 114272 | training loss: 0.0021187267266213894\n",
      "epoch: 5 | 42880 / 114272 | training loss: 0.0009905813494697213\n",
      "epoch: 5 | 42912 / 114272 | training loss: 0.0013490671990439296\n",
      "epoch: 5 | 42944 / 114272 | training loss: 0.0006987046799622476\n",
      "epoch: 5 | 42976 / 114272 | training loss: 0.0015349133173003793\n",
      "epoch: 5 | 43008 / 114272 | training loss: 0.0012335787760093808\n",
      "epoch: 5 | 43040 / 114272 | training loss: 0.009034425020217896\n",
      "epoch: 5 | 43072 / 114272 | training loss: 0.00076202058698982\n",
      "epoch: 5 | 43104 / 114272 | training loss: 0.10422006994485855\n",
      "epoch: 5 | 43136 / 114272 | training loss: 0.13340814411640167\n",
      "epoch: 5 | 43168 / 114272 | training loss: 0.0009217570186592638\n",
      "epoch: 5 | 43200 / 114272 | training loss: 0.001687559182755649\n",
      "epoch: 5 | 43232 / 114272 | training loss: 0.0010809628292918205\n",
      "epoch: 5 | 43264 / 114272 | training loss: 0.0025266550946980715\n",
      "epoch: 5 | 43296 / 114272 | training loss: 0.0009468105854466558\n",
      "epoch: 5 | 43328 / 114272 | training loss: 0.0008498746319673955\n",
      "epoch: 5 | 43360 / 114272 | training loss: 0.001287523191422224\n",
      "epoch: 5 | 43392 / 114272 | training loss: 0.19156907498836517\n",
      "epoch: 5 | 43424 / 114272 | training loss: 0.0010158022632822394\n",
      "epoch: 5 | 43456 / 114272 | training loss: 0.0025964006781578064\n",
      "epoch: 5 | 43488 / 114272 | training loss: 0.16823680698871613\n",
      "epoch: 5 | 43520 / 114272 | training loss: 0.10014432668685913\n",
      "epoch: 5 | 43552 / 114272 | training loss: 0.07961084693670273\n",
      "epoch: 5 | 43584 / 114272 | training loss: 0.0011221222812309861\n",
      "epoch: 5 | 43616 / 114272 | training loss: 0.0008383833919651806\n",
      "epoch: 5 | 43648 / 114272 | training loss: 0.0019126279512420297\n",
      "epoch: 5 | 43680 / 114272 | training loss: 0.0012724671978503466\n",
      "epoch: 5 | 43712 / 114272 | training loss: 0.17993831634521484\n",
      "epoch: 5 | 43744 / 114272 | training loss: 0.0010810921667143703\n",
      "epoch: 5 | 43776 / 114272 | training loss: 0.026879562065005302\n",
      "epoch: 5 | 43808 / 114272 | training loss: 0.0009193142759613693\n",
      "epoch: 5 | 43840 / 114272 | training loss: 0.0027988734655082226\n",
      "epoch: 5 | 43872 / 114272 | training loss: 0.005271757487207651\n",
      "epoch: 5 | 43904 / 114272 | training loss: 0.11494839191436768\n",
      "epoch: 5 | 43936 / 114272 | training loss: 0.0012424009619280696\n",
      "epoch: 5 | 43968 / 114272 | training loss: 0.0014608886558562517\n",
      "epoch: 5 | 44000 / 114272 | training loss: 0.002591727301478386\n",
      "epoch: 5 | 44032 / 114272 | training loss: 0.1844840794801712\n",
      "epoch: 5 | 44064 / 114272 | training loss: 0.003182261949405074\n",
      "epoch: 5 | 44096 / 114272 | training loss: 0.000895948673132807\n",
      "epoch: 5 | 44128 / 114272 | training loss: 0.0005860484088771045\n",
      "epoch: 5 | 44160 / 114272 | training loss: 0.002788941143080592\n",
      "epoch: 5 | 44192 / 114272 | training loss: 0.0022856188006699085\n",
      "epoch: 5 | 44224 / 114272 | training loss: 0.0010509948479011655\n",
      "epoch: 5 | 44256 / 114272 | training loss: 0.007974471896886826\n",
      "epoch: 5 | 44288 / 114272 | training loss: 0.0008267659577541053\n",
      "epoch: 5 | 44320 / 114272 | training loss: 0.0027394737116992474\n",
      "epoch: 5 | 44352 / 114272 | training loss: 0.07344835996627808\n",
      "epoch: 5 | 44384 / 114272 | training loss: 0.0012429036432877183\n",
      "epoch: 5 | 44416 / 114272 | training loss: 0.1740851253271103\n",
      "epoch: 5 | 44448 / 114272 | training loss: 0.0030778180807828903\n",
      "epoch: 5 | 44480 / 114272 | training loss: 0.0011233895784243941\n",
      "epoch: 5 | 44512 / 114272 | training loss: 0.0012228203704580665\n",
      "epoch: 5 | 44544 / 114272 | training loss: 0.0010661312844604254\n",
      "epoch: 5 | 44576 / 114272 | training loss: 0.0007403165800496936\n",
      "epoch: 5 | 44608 / 114272 | training loss: 0.002089603105559945\n",
      "epoch: 5 | 44640 / 114272 | training loss: 0.0006394290248863399\n",
      "epoch: 5 | 44672 / 114272 | training loss: 0.19431686401367188\n",
      "epoch: 5 | 44704 / 114272 | training loss: 0.0008339515188708901\n",
      "epoch: 5 | 44736 / 114272 | training loss: 0.0005920454859733582\n",
      "epoch: 5 | 44768 / 114272 | training loss: 0.26596733927726746\n",
      "epoch: 5 | 44800 / 114272 | training loss: 0.0007196155493147671\n",
      "epoch: 5 | 44832 / 114272 | training loss: 0.0015233976300805807\n",
      "epoch: 5 | 44864 / 114272 | training loss: 0.10239842534065247\n",
      "epoch: 5 | 44896 / 114272 | training loss: 0.002003918867558241\n",
      "epoch: 5 | 44928 / 114272 | training loss: 0.0005367339472286403\n",
      "epoch: 5 | 44960 / 114272 | training loss: 0.0011039900127798319\n",
      "epoch: 5 | 44992 / 114272 | training loss: 0.0012268678983673453\n",
      "epoch: 5 | 45024 / 114272 | training loss: 0.0007595638744533062\n",
      "epoch: 5 | 45056 / 114272 | training loss: 0.0010945667745545506\n",
      "epoch: 5 | 45088 / 114272 | training loss: 0.03139518201351166\n",
      "epoch: 5 | 45120 / 114272 | training loss: 0.04015772417187691\n",
      "epoch: 5 | 45152 / 114272 | training loss: 0.0010840529575943947\n",
      "epoch: 5 | 45184 / 114272 | training loss: 0.0007091839797794819\n",
      "epoch: 5 | 45216 / 114272 | training loss: 0.0008578601409681141\n",
      "epoch: 5 | 45248 / 114272 | training loss: 0.01022249460220337\n",
      "epoch: 5 | 45280 / 114272 | training loss: 0.0016488231485709548\n",
      "epoch: 5 | 45312 / 114272 | training loss: 0.0006792494095861912\n",
      "epoch: 5 | 45344 / 114272 | training loss: 0.0028613039758056402\n",
      "epoch: 5 | 45376 / 114272 | training loss: 0.004122138489037752\n",
      "epoch: 5 | 45408 / 114272 | training loss: 0.0012234537862241268\n",
      "epoch: 5 | 45440 / 114272 | training loss: 0.1770240068435669\n",
      "epoch: 5 | 45472 / 114272 | training loss: 0.0015412409557029605\n",
      "epoch: 5 | 45504 / 114272 | training loss: 0.09334263950586319\n",
      "epoch: 5 | 45536 / 114272 | training loss: 0.0008603602764196694\n",
      "epoch: 5 | 45568 / 114272 | training loss: 0.005104938056319952\n",
      "epoch: 5 | 45600 / 114272 | training loss: 0.0011430272134020925\n",
      "epoch: 5 | 45632 / 114272 | training loss: 0.005674267653375864\n",
      "epoch: 5 | 45664 / 114272 | training loss: 0.004937178920954466\n",
      "epoch: 5 | 45696 / 114272 | training loss: 0.0009014790412038565\n",
      "epoch: 5 | 45728 / 114272 | training loss: 0.23885944485664368\n",
      "epoch: 5 | 45760 / 114272 | training loss: 0.11560282856225967\n",
      "epoch: 5 | 45792 / 114272 | training loss: 0.0006014484679326415\n",
      "epoch: 5 | 45824 / 114272 | training loss: 0.0022981418296694756\n",
      "epoch: 5 | 45856 / 114272 | training loss: 0.14656853675842285\n",
      "epoch: 5 | 45888 / 114272 | training loss: 0.0010178020456805825\n",
      "epoch: 5 | 45920 / 114272 | training loss: 0.29598766565322876\n",
      "epoch: 5 | 45952 / 114272 | training loss: 0.0011364478850737214\n",
      "epoch: 5 | 45984 / 114272 | training loss: 0.007126146927475929\n",
      "epoch: 5 | 46016 / 114272 | training loss: 0.0013589469017460942\n",
      "epoch: 5 | 46048 / 114272 | training loss: 0.0743945762515068\n",
      "epoch: 5 | 46080 / 114272 | training loss: 0.0012213237350806594\n",
      "epoch: 5 | 46112 / 114272 | training loss: 0.0005390614969655871\n",
      "epoch: 5 | 46144 / 114272 | training loss: 0.0005326534737832844\n",
      "epoch: 5 | 46176 / 114272 | training loss: 0.0007036174065433443\n",
      "epoch: 5 | 46208 / 114272 | training loss: 0.014152402989566326\n",
      "epoch: 5 | 46240 / 114272 | training loss: 0.00109810137655586\n",
      "epoch: 5 | 46272 / 114272 | training loss: 0.004674364812672138\n",
      "epoch: 5 | 46304 / 114272 | training loss: 0.000854804995469749\n",
      "epoch: 5 | 46336 / 114272 | training loss: 0.0016495861345902085\n",
      "epoch: 5 | 46368 / 114272 | training loss: 0.0010376264108344913\n",
      "epoch: 5 | 46400 / 114272 | training loss: 0.0027762828394770622\n",
      "epoch: 5 | 46432 / 114272 | training loss: 0.0009781303815543652\n",
      "epoch: 5 | 46464 / 114272 | training loss: 0.006253065541386604\n",
      "epoch: 5 | 46496 / 114272 | training loss: 0.0008806760306470096\n",
      "epoch: 5 | 46528 / 114272 | training loss: 0.0013799876905977726\n",
      "epoch: 5 | 46560 / 114272 | training loss: 0.0027328829746693373\n",
      "epoch: 5 | 46592 / 114272 | training loss: 0.006473149172961712\n",
      "epoch: 5 | 46624 / 114272 | training loss: 0.0015107359504327178\n",
      "epoch: 5 | 46656 / 114272 | training loss: 0.0012477741111069918\n",
      "epoch: 5 | 46688 / 114272 | training loss: 0.008962844498455524\n",
      "epoch: 5 | 46720 / 114272 | training loss: 0.32250478863716125\n",
      "epoch: 5 | 46752 / 114272 | training loss: 0.000709100509993732\n",
      "epoch: 5 | 46784 / 114272 | training loss: 0.0022301780991256237\n",
      "epoch: 5 | 46816 / 114272 | training loss: 0.0019131827866658568\n",
      "epoch: 5 | 46848 / 114272 | training loss: 0.0008599075954407454\n",
      "epoch: 5 | 46880 / 114272 | training loss: 0.31187108159065247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 46912 / 114272 | training loss: 0.0022906761150807142\n",
      "epoch: 5 | 46944 / 114272 | training loss: 0.001124007860198617\n",
      "epoch: 5 | 46976 / 114272 | training loss: 0.0027735820040106773\n",
      "epoch: 5 | 47008 / 114272 | training loss: 0.0008614197722636163\n",
      "epoch: 5 | 47040 / 114272 | training loss: 0.0006659528007730842\n",
      "epoch: 5 | 47072 / 114272 | training loss: 0.0006310638273134828\n",
      "epoch: 5 | 47104 / 114272 | training loss: 0.001229829271323979\n",
      "epoch: 5 | 47136 / 114272 | training loss: 0.0008526475867256522\n",
      "epoch: 5 | 47168 / 114272 | training loss: 0.0022108943667262793\n",
      "epoch: 5 | 47200 / 114272 | training loss: 0.00963013805449009\n",
      "epoch: 5 | 47232 / 114272 | training loss: 0.003383729374036193\n",
      "epoch: 5 | 47264 / 114272 | training loss: 0.004249954130500555\n",
      "epoch: 5 | 47296 / 114272 | training loss: 0.0013962272787466645\n",
      "epoch: 5 | 47328 / 114272 | training loss: 0.09739814698696136\n",
      "epoch: 5 | 47360 / 114272 | training loss: 0.0011076328810304403\n",
      "epoch: 5 | 47392 / 114272 | training loss: 0.0015677112387493253\n",
      "epoch: 5 | 47424 / 114272 | training loss: 0.002699261996895075\n",
      "epoch: 5 | 47456 / 114272 | training loss: 0.23590603470802307\n",
      "epoch: 5 | 47488 / 114272 | training loss: 0.0018856548704206944\n",
      "epoch: 5 | 47520 / 114272 | training loss: 0.0008369481074623764\n",
      "epoch: 5 | 47552 / 114272 | training loss: 0.0008971981005743146\n",
      "epoch: 5 | 47584 / 114272 | training loss: 0.0022414056584239006\n",
      "epoch: 5 | 47616 / 114272 | training loss: 0.0013097909977659583\n",
      "epoch: 5 | 47648 / 114272 | training loss: 0.0005459432140924037\n",
      "epoch: 5 | 47680 / 114272 | training loss: 0.0009306638385169208\n",
      "epoch: 5 | 47712 / 114272 | training loss: 0.000844173482619226\n",
      "epoch: 5 | 47744 / 114272 | training loss: 0.0012921966845169663\n",
      "epoch: 5 | 47776 / 114272 | training loss: 0.001375318504869938\n",
      "epoch: 5 | 47808 / 114272 | training loss: 0.00203339708968997\n",
      "epoch: 5 | 47840 / 114272 | training loss: 0.0014599687419831753\n",
      "epoch: 5 | 47872 / 114272 | training loss: 0.0011452685575932264\n",
      "epoch: 5 | 47904 / 114272 | training loss: 0.01876227557659149\n",
      "epoch: 5 | 47936 / 114272 | training loss: 0.10202980786561966\n",
      "epoch: 5 | 47968 / 114272 | training loss: 0.06525197625160217\n",
      "epoch: 5 | 48000 / 114272 | training loss: 0.0011242999462410808\n",
      "epoch: 5 | 48032 / 114272 | training loss: 0.0007943770615383983\n",
      "epoch: 5 | 48064 / 114272 | training loss: 0.11969900876283646\n",
      "epoch: 5 | 48096 / 114272 | training loss: 0.0007690876955166459\n",
      "epoch: 5 | 48128 / 114272 | training loss: 0.0006986509542912245\n",
      "epoch: 5 | 48160 / 114272 | training loss: 0.0010064529487863183\n",
      "epoch: 5 | 48192 / 114272 | training loss: 0.0015729495789855719\n",
      "epoch: 5 | 48224 / 114272 | training loss: 0.12957525253295898\n",
      "epoch: 5 | 48256 / 114272 | training loss: 0.0007140963571146131\n",
      "epoch: 5 | 48288 / 114272 | training loss: 0.0011795161990448833\n",
      "epoch: 5 | 48320 / 114272 | training loss: 0.2326267510652542\n",
      "epoch: 5 | 48352 / 114272 | training loss: 0.0008886303985491395\n",
      "epoch: 5 | 48384 / 114272 | training loss: 0.005897811148315668\n",
      "epoch: 5 | 48416 / 114272 | training loss: 0.0008897696388885379\n",
      "epoch: 5 | 48448 / 114272 | training loss: 0.0007317300769500434\n",
      "epoch: 5 | 48480 / 114272 | training loss: 0.016431894153356552\n",
      "epoch: 5 | 48512 / 114272 | training loss: 0.005011957138776779\n",
      "epoch: 5 | 48544 / 114272 | training loss: 0.2532792091369629\n",
      "epoch: 5 | 48576 / 114272 | training loss: 0.0020527043379843235\n",
      "epoch: 5 | 48608 / 114272 | training loss: 0.0008966477471403778\n",
      "epoch: 5 | 48640 / 114272 | training loss: 0.0010827145306393504\n",
      "epoch: 5 | 48672 / 114272 | training loss: 0.0030025369487702847\n",
      "epoch: 5 | 48704 / 114272 | training loss: 0.0007427048985846341\n",
      "epoch: 5 | 48736 / 114272 | training loss: 0.2941446900367737\n",
      "epoch: 5 | 48768 / 114272 | training loss: 0.0022948288824409246\n",
      "epoch: 5 | 48800 / 114272 | training loss: 0.16965122520923615\n",
      "epoch: 5 | 48832 / 114272 | training loss: 0.0008632379467599094\n",
      "epoch: 5 | 48864 / 114272 | training loss: 0.18024203181266785\n",
      "epoch: 5 | 48896 / 114272 | training loss: 0.0015130997635424137\n",
      "epoch: 5 | 48928 / 114272 | training loss: 0.0008826653356663883\n",
      "epoch: 5 | 48960 / 114272 | training loss: 0.19261540472507477\n",
      "epoch: 5 | 48992 / 114272 | training loss: 0.001519958721473813\n",
      "epoch: 5 | 49024 / 114272 | training loss: 0.0008354655583389103\n",
      "epoch: 5 | 49056 / 114272 | training loss: 0.0007952641462907195\n",
      "epoch: 5 | 49088 / 114272 | training loss: 0.0008209702791646123\n",
      "epoch: 5 | 49120 / 114272 | training loss: 0.0010283029405400157\n",
      "epoch: 5 | 49152 / 114272 | training loss: 0.0041818865574896336\n",
      "epoch: 5 | 49184 / 114272 | training loss: 0.19560961425304413\n",
      "epoch: 5 | 49216 / 114272 | training loss: 0.35469093918800354\n",
      "epoch: 5 | 49248 / 114272 | training loss: 0.000735461653675884\n",
      "epoch: 5 | 49280 / 114272 | training loss: 0.0020735410507768393\n",
      "epoch: 5 | 49312 / 114272 | training loss: 0.0013301554135978222\n",
      "epoch: 5 | 49344 / 114272 | training loss: 0.17030468583106995\n",
      "epoch: 5 | 49376 / 114272 | training loss: 0.0018829823238775134\n",
      "epoch: 5 | 49408 / 114272 | training loss: 0.006899905391037464\n",
      "epoch: 5 | 49440 / 114272 | training loss: 0.0013025605585426092\n",
      "epoch: 5 | 49472 / 114272 | training loss: 0.0014172764495015144\n",
      "epoch: 5 | 49504 / 114272 | training loss: 0.0010190852917730808\n",
      "epoch: 5 | 49536 / 114272 | training loss: 0.015203621238470078\n",
      "epoch: 5 | 49568 / 114272 | training loss: 0.0438772477209568\n",
      "epoch: 5 | 49600 / 114272 | training loss: 0.0014865968842059374\n",
      "epoch: 5 | 49632 / 114272 | training loss: 0.0013645377475768328\n",
      "epoch: 5 | 49664 / 114272 | training loss: 0.24763615429401398\n",
      "epoch: 5 | 49696 / 114272 | training loss: 0.09160304069519043\n",
      "epoch: 5 | 49728 / 114272 | training loss: 0.0022625839337706566\n",
      "epoch: 5 | 49760 / 114272 | training loss: 0.0005801087245345116\n",
      "epoch: 5 | 49792 / 114272 | training loss: 0.0015989469829946756\n",
      "epoch: 5 | 49824 / 114272 | training loss: 0.003897450864315033\n",
      "epoch: 5 | 49856 / 114272 | training loss: 0.002462502336129546\n",
      "epoch: 5 | 49888 / 114272 | training loss: 0.0010896449675783515\n",
      "epoch: 5 | 49920 / 114272 | training loss: 0.0020890571177005768\n",
      "epoch: 5 | 49952 / 114272 | training loss: 0.0018319544615224004\n",
      "epoch: 5 | 49984 / 114272 | training loss: 0.0012029442004859447\n",
      "epoch: 5 | 50016 / 114272 | training loss: 0.003459181636571884\n",
      "epoch: 5 | 50048 / 114272 | training loss: 0.001956729684025049\n",
      "epoch: 5 | 50080 / 114272 | training loss: 0.12302660197019577\n",
      "epoch: 5 | 50112 / 114272 | training loss: 0.0016722938744351268\n",
      "epoch: 5 | 50144 / 114272 | training loss: 0.000614382850471884\n",
      "epoch: 5 | 50176 / 114272 | training loss: 0.2801392078399658\n",
      "epoch: 5 | 50208 / 114272 | training loss: 0.005309329368174076\n",
      "epoch: 5 | 50240 / 114272 | training loss: 0.0014290991239249706\n",
      "epoch: 5 | 50272 / 114272 | training loss: 0.0010324595496058464\n",
      "epoch: 5 | 50304 / 114272 | training loss: 0.18304549157619476\n",
      "epoch: 5 | 50336 / 114272 | training loss: 0.06287878006696701\n",
      "epoch: 5 | 50368 / 114272 | training loss: 0.0007190932519733906\n",
      "epoch: 5 | 50400 / 114272 | training loss: 0.001960728084668517\n",
      "epoch: 5 | 50432 / 114272 | training loss: 0.0007797835278324783\n",
      "epoch: 5 | 50464 / 114272 | training loss: 0.0017757660243660212\n",
      "epoch: 5 | 50496 / 114272 | training loss: 0.001402071095071733\n",
      "epoch: 5 | 50528 / 114272 | training loss: 0.18124066293239594\n",
      "epoch: 5 | 50560 / 114272 | training loss: 0.0020065195858478546\n",
      "epoch: 5 | 50592 / 114272 | training loss: 0.0015520118176937103\n",
      "epoch: 5 | 50624 / 114272 | training loss: 0.22316837310791016\n",
      "epoch: 5 | 50656 / 114272 | training loss: 0.0012163210194557905\n",
      "epoch: 5 | 50688 / 114272 | training loss: 0.0018120302120223641\n",
      "epoch: 5 | 50720 / 114272 | training loss: 0.15620563924312592\n",
      "epoch: 5 | 50752 / 114272 | training loss: 0.0021190878469496965\n",
      "epoch: 5 | 50784 / 114272 | training loss: 0.0017797291511669755\n",
      "epoch: 5 | 50816 / 114272 | training loss: 0.0017957327654585242\n",
      "epoch: 5 | 50848 / 114272 | training loss: 0.018543250858783722\n",
      "epoch: 5 | 50880 / 114272 | training loss: 0.0035758495796471834\n",
      "epoch: 5 | 50912 / 114272 | training loss: 0.0010438107419759035\n",
      "epoch: 5 | 50944 / 114272 | training loss: 0.06845328211784363\n",
      "epoch: 5 | 50976 / 114272 | training loss: 0.13866111636161804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 51008 / 114272 | training loss: 0.3922196626663208\n",
      "epoch: 5 | 51040 / 114272 | training loss: 0.24205531179904938\n",
      "epoch: 5 | 51072 / 114272 | training loss: 0.00241032219491899\n",
      "epoch: 5 | 51104 / 114272 | training loss: 0.00291942129842937\n",
      "epoch: 5 | 51136 / 114272 | training loss: 0.002078799530863762\n",
      "epoch: 5 | 51168 / 114272 | training loss: 0.004060844425112009\n",
      "epoch: 5 | 51200 / 114272 | training loss: 0.0017031221650540829\n",
      "epoch: 5 | 51232 / 114272 | training loss: 0.18884536623954773\n",
      "epoch: 5 | 51264 / 114272 | training loss: 0.001441323896870017\n",
      "epoch: 5 | 51296 / 114272 | training loss: 0.002200620947405696\n",
      "epoch: 5 | 51328 / 114272 | training loss: 0.0012335303472355008\n",
      "epoch: 5 | 51360 / 114272 | training loss: 0.0013671867782250047\n",
      "epoch: 5 | 51392 / 114272 | training loss: 0.0009853843366727233\n",
      "epoch: 5 | 51424 / 114272 | training loss: 0.0023306026123464108\n",
      "epoch: 5 | 51456 / 114272 | training loss: 0.013733113184571266\n",
      "epoch: 5 | 51488 / 114272 | training loss: 0.0015197868924587965\n",
      "epoch: 5 | 51520 / 114272 | training loss: 0.0018764071865007281\n",
      "epoch: 5 | 51552 / 114272 | training loss: 0.0008575044339522719\n",
      "epoch: 5 | 51584 / 114272 | training loss: 0.0022261720150709152\n",
      "epoch: 5 | 51616 / 114272 | training loss: 0.18778574466705322\n",
      "epoch: 5 | 51648 / 114272 | training loss: 0.0021842005662620068\n",
      "epoch: 5 | 51680 / 114272 | training loss: 0.0017643088940531015\n",
      "epoch: 5 | 51712 / 114272 | training loss: 0.013655898161232471\n",
      "epoch: 5 | 51744 / 114272 | training loss: 0.0034955625887960196\n",
      "epoch: 5 | 51776 / 114272 | training loss: 0.0028626159764826298\n",
      "epoch: 5 | 51808 / 114272 | training loss: 0.11818171292543411\n",
      "epoch: 5 | 51840 / 114272 | training loss: 0.00247085839509964\n",
      "epoch: 5 | 51872 / 114272 | training loss: 0.15465350449085236\n",
      "epoch: 5 | 51904 / 114272 | training loss: 0.09087780117988586\n",
      "epoch: 5 | 51936 / 114272 | training loss: 0.0020904678385704756\n",
      "epoch: 5 | 51968 / 114272 | training loss: 0.0022489966358989477\n",
      "epoch: 5 | 52000 / 114272 | training loss: 0.0029983296990394592\n",
      "epoch: 5 | 52032 / 114272 | training loss: 0.004661206621676683\n",
      "epoch: 5 | 52064 / 114272 | training loss: 0.002501219743862748\n",
      "epoch: 5 | 52096 / 114272 | training loss: 0.0050806282088160515\n",
      "epoch: 5 | 52128 / 114272 | training loss: 0.001863884855993092\n",
      "epoch: 5 | 52160 / 114272 | training loss: 0.002382526407018304\n",
      "epoch: 5 | 52192 / 114272 | training loss: 0.002410135930404067\n",
      "epoch: 5 | 52224 / 114272 | training loss: 0.09234577417373657\n",
      "epoch: 5 | 52256 / 114272 | training loss: 0.013550602830946445\n",
      "epoch: 5 | 52288 / 114272 | training loss: 0.0031229204032570124\n",
      "epoch: 5 | 52320 / 114272 | training loss: 0.0913846343755722\n",
      "epoch: 5 | 52352 / 114272 | training loss: 0.028928270563483238\n",
      "epoch: 5 | 52384 / 114272 | training loss: 0.002861890010535717\n",
      "epoch: 5 | 52416 / 114272 | training loss: 0.00589562626555562\n",
      "epoch: 5 | 52448 / 114272 | training loss: 0.001417563296854496\n",
      "epoch: 5 | 52480 / 114272 | training loss: 0.002399744465947151\n",
      "epoch: 5 | 52512 / 114272 | training loss: 0.015682348981499672\n",
      "epoch: 5 | 52544 / 114272 | training loss: 0.002663067076355219\n",
      "epoch: 5 | 52576 / 114272 | training loss: 0.11169062554836273\n",
      "epoch: 5 | 52608 / 114272 | training loss: 0.003973915241658688\n",
      "epoch: 5 | 52640 / 114272 | training loss: 0.002627987414598465\n",
      "epoch: 5 | 52672 / 114272 | training loss: 0.0018553216941654682\n",
      "epoch: 5 | 52704 / 114272 | training loss: 0.3035949468612671\n",
      "epoch: 5 | 52736 / 114272 | training loss: 0.023412540555000305\n",
      "epoch: 5 | 52768 / 114272 | training loss: 0.0018951412057504058\n",
      "epoch: 5 | 52800 / 114272 | training loss: 0.011246280744671822\n",
      "epoch: 5 | 52832 / 114272 | training loss: 0.0030451109632849693\n",
      "epoch: 5 | 52864 / 114272 | training loss: 0.002163967816159129\n",
      "epoch: 5 | 52896 / 114272 | training loss: 0.0008736893651075661\n",
      "epoch: 5 | 52928 / 114272 | training loss: 0.00214095669798553\n",
      "epoch: 5 | 52960 / 114272 | training loss: 0.001537660020403564\n",
      "epoch: 5 | 52992 / 114272 | training loss: 0.002069709589704871\n",
      "epoch: 5 | 53024 / 114272 | training loss: 0.06001301109790802\n",
      "epoch: 5 | 53056 / 114272 | training loss: 0.002594830933958292\n",
      "epoch: 5 | 53088 / 114272 | training loss: 0.23471350967884064\n",
      "epoch: 5 | 53120 / 114272 | training loss: 0.0015094093978404999\n",
      "epoch: 5 | 53152 / 114272 | training loss: 0.0012332570040598512\n",
      "epoch: 5 | 53184 / 114272 | training loss: 0.0012955715646967292\n",
      "epoch: 5 | 53216 / 114272 | training loss: 0.0016089669661596417\n",
      "epoch: 5 | 53248 / 114272 | training loss: 0.003180868923664093\n",
      "epoch: 5 | 53280 / 114272 | training loss: 0.0022235875949263573\n",
      "epoch: 5 | 53312 / 114272 | training loss: 0.05143072083592415\n",
      "epoch: 5 | 53344 / 114272 | training loss: 0.01662582904100418\n",
      "epoch: 5 | 53376 / 114272 | training loss: 0.08807989209890366\n",
      "epoch: 5 | 53408 / 114272 | training loss: 0.002945105079561472\n",
      "epoch: 5 | 53440 / 114272 | training loss: 0.001120049855671823\n",
      "epoch: 5 | 53472 / 114272 | training loss: 0.0013620967511087656\n",
      "epoch: 5 | 53504 / 114272 | training loss: 0.0016214088536798954\n",
      "epoch: 5 | 53536 / 114272 | training loss: 0.0006888231146149337\n",
      "epoch: 5 | 53568 / 114272 | training loss: 0.008460002020001411\n",
      "epoch: 5 | 53600 / 114272 | training loss: 0.0017604484455659986\n",
      "epoch: 5 | 53632 / 114272 | training loss: 0.0010024855146184564\n",
      "epoch: 5 | 53664 / 114272 | training loss: 0.0014119199477136135\n",
      "epoch: 5 | 53696 / 114272 | training loss: 0.0015009514754638076\n",
      "epoch: 5 | 53728 / 114272 | training loss: 0.0012074406258761883\n",
      "epoch: 5 | 53760 / 114272 | training loss: 0.005242032930254936\n",
      "epoch: 5 | 53792 / 114272 | training loss: 0.0013764473842456937\n",
      "epoch: 5 | 53824 / 114272 | training loss: 0.003663914045318961\n",
      "epoch: 5 | 53856 / 114272 | training loss: 0.032157350331544876\n",
      "epoch: 5 | 53888 / 114272 | training loss: 0.0026642463635653257\n",
      "epoch: 5 | 53920 / 114272 | training loss: 0.003423988353461027\n",
      "epoch: 5 | 53952 / 114272 | training loss: 0.0007684946758672595\n",
      "epoch: 5 | 53984 / 114272 | training loss: 0.0016749505884945393\n",
      "epoch: 5 | 54016 / 114272 | training loss: 0.0006245867116376758\n",
      "epoch: 5 | 54048 / 114272 | training loss: 0.0011393462773412466\n",
      "epoch: 5 | 54080 / 114272 | training loss: 0.0013701310381293297\n",
      "epoch: 5 | 54112 / 114272 | training loss: 0.000894491036888212\n",
      "epoch: 5 | 54144 / 114272 | training loss: 0.008543440140783787\n",
      "epoch: 5 | 54176 / 114272 | training loss: 0.0006617318140342832\n",
      "epoch: 5 | 54208 / 114272 | training loss: 0.0809146910905838\n",
      "epoch: 5 | 54240 / 114272 | training loss: 0.0009835808305069804\n",
      "epoch: 5 | 54272 / 114272 | training loss: 0.18289045989513397\n",
      "epoch: 5 | 54304 / 114272 | training loss: 0.0038927639834582806\n",
      "epoch: 5 | 54336 / 114272 | training loss: 0.0013324872124940157\n",
      "epoch: 5 | 54368 / 114272 | training loss: 0.30870944261550903\n",
      "epoch: 5 | 54400 / 114272 | training loss: 0.14678552746772766\n",
      "epoch: 5 | 54432 / 114272 | training loss: 0.148490309715271\n",
      "epoch: 5 | 54464 / 114272 | training loss: 0.0011966763995587826\n",
      "epoch: 5 | 54496 / 114272 | training loss: 0.16671667993068695\n",
      "epoch: 5 | 54528 / 114272 | training loss: 0.0014816734474152327\n",
      "epoch: 5 | 54560 / 114272 | training loss: 0.013210025615990162\n",
      "epoch: 5 | 54592 / 114272 | training loss: 0.0008021067478694022\n",
      "epoch: 5 | 54624 / 114272 | training loss: 0.001508584013208747\n",
      "epoch: 5 | 54656 / 114272 | training loss: 0.003968032542616129\n",
      "epoch: 5 | 54688 / 114272 | training loss: 0.028153086081147194\n",
      "epoch: 5 | 54720 / 114272 | training loss: 0.004802549257874489\n",
      "epoch: 5 | 54752 / 114272 | training loss: 0.0010676525998860598\n",
      "epoch: 5 | 54784 / 114272 | training loss: 0.0033431537449359894\n",
      "epoch: 5 | 54816 / 114272 | training loss: 0.0017477943329140544\n",
      "epoch: 5 | 54848 / 114272 | training loss: 0.001519888755865395\n",
      "epoch: 5 | 54880 / 114272 | training loss: 0.09862759709358215\n",
      "epoch: 5 | 54912 / 114272 | training loss: 0.18088248372077942\n",
      "epoch: 5 | 54944 / 114272 | training loss: 0.002144878963008523\n",
      "epoch: 5 | 54976 / 114272 | training loss: 0.0022193677723407745\n",
      "epoch: 5 | 55008 / 114272 | training loss: 0.052475687116384506\n",
      "epoch: 5 | 55040 / 114272 | training loss: 0.0019518031040206552\n",
      "epoch: 5 | 55072 / 114272 | training loss: 0.0019675244111567736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 55104 / 114272 | training loss: 0.002002572640776634\n",
      "epoch: 5 | 55136 / 114272 | training loss: 0.003133920254185796\n",
      "epoch: 5 | 55168 / 114272 | training loss: 0.0015193258877843618\n",
      "epoch: 5 | 55200 / 114272 | training loss: 0.16000822186470032\n",
      "epoch: 5 | 55232 / 114272 | training loss: 0.0038797766901552677\n",
      "epoch: 5 | 55264 / 114272 | training loss: 0.0016956517938524485\n",
      "epoch: 5 | 55296 / 114272 | training loss: 0.001407718169502914\n",
      "epoch: 5 | 55328 / 114272 | training loss: 0.00362010789103806\n",
      "epoch: 5 | 55360 / 114272 | training loss: 0.0023021476808935404\n",
      "epoch: 5 | 55392 / 114272 | training loss: 0.0013496661558747292\n",
      "epoch: 5 | 55424 / 114272 | training loss: 0.09421952813863754\n",
      "epoch: 5 | 55456 / 114272 | training loss: 0.0027288857381790876\n",
      "epoch: 5 | 55488 / 114272 | training loss: 0.0037520723417401314\n",
      "epoch: 5 | 55520 / 114272 | training loss: 0.08507470041513443\n",
      "epoch: 5 | 55552 / 114272 | training loss: 0.00230220565572381\n",
      "epoch: 5 | 55584 / 114272 | training loss: 0.00296866986900568\n",
      "epoch: 5 | 55616 / 114272 | training loss: 0.0016388585790991783\n",
      "epoch: 5 | 55648 / 114272 | training loss: 0.1663999706506729\n",
      "epoch: 5 | 55680 / 114272 | training loss: 0.0012409535702317953\n",
      "epoch: 5 | 55712 / 114272 | training loss: 0.13312236964702606\n",
      "epoch: 5 | 55744 / 114272 | training loss: 0.0006859092391096056\n",
      "epoch: 5 | 55776 / 114272 | training loss: 0.0018327250145375729\n",
      "epoch: 5 | 55808 / 114272 | training loss: 0.07450464367866516\n",
      "epoch: 5 | 55840 / 114272 | training loss: 0.0011043786071240902\n",
      "epoch: 5 | 55872 / 114272 | training loss: 0.0010071403812617064\n",
      "epoch: 5 | 55904 / 114272 | training loss: 0.0015884583117440343\n",
      "epoch: 5 | 55936 / 114272 | training loss: 0.002649210626259446\n",
      "epoch: 5 | 55968 / 114272 | training loss: 0.012697190977633\n",
      "epoch: 5 | 56000 / 114272 | training loss: 0.002751072635874152\n",
      "epoch: 5 | 56032 / 114272 | training loss: 0.003345765871927142\n",
      "epoch: 5 | 56064 / 114272 | training loss: 0.03872765228152275\n",
      "epoch: 5 | 56096 / 114272 | training loss: 0.0016012181295081973\n",
      "epoch: 5 | 56128 / 114272 | training loss: 0.03559202328324318\n",
      "epoch: 5 | 56160 / 114272 | training loss: 0.00981524121016264\n",
      "epoch: 5 | 56192 / 114272 | training loss: 0.0011450893944129348\n",
      "epoch: 5 | 56224 / 114272 | training loss: 0.0016912888968363404\n",
      "epoch: 5 | 56256 / 114272 | training loss: 0.0033212602138519287\n",
      "epoch: 5 | 56288 / 114272 | training loss: 0.001832938869483769\n",
      "epoch: 5 | 56320 / 114272 | training loss: 0.0019644456915557384\n",
      "epoch: 5 | 56352 / 114272 | training loss: 0.10415054112672806\n",
      "epoch: 5 | 56384 / 114272 | training loss: 0.0009717155480757356\n",
      "epoch: 5 | 56416 / 114272 | training loss: 0.1999894380569458\n",
      "epoch: 5 | 56448 / 114272 | training loss: 0.003421585075557232\n",
      "epoch: 5 | 56480 / 114272 | training loss: 0.0006195878377184272\n",
      "epoch: 5 | 56512 / 114272 | training loss: 0.006404678802937269\n",
      "epoch: 5 | 56544 / 114272 | training loss: 0.001303918194025755\n",
      "epoch: 5 | 56576 / 114272 | training loss: 0.1531505584716797\n",
      "epoch: 5 | 56608 / 114272 | training loss: 0.0034085053484886885\n",
      "epoch: 5 | 56640 / 114272 | training loss: 0.001409525633789599\n",
      "epoch: 5 | 56672 / 114272 | training loss: 0.002308470197021961\n",
      "epoch: 5 | 56704 / 114272 | training loss: 0.017106691375374794\n",
      "epoch: 5 | 56736 / 114272 | training loss: 0.0007770053925924003\n",
      "epoch: 5 | 56768 / 114272 | training loss: 0.002593167359009385\n",
      "epoch: 5 | 56800 / 114272 | training loss: 0.006726683583110571\n",
      "epoch: 5 | 56832 / 114272 | training loss: 0.0016888946993276477\n",
      "epoch: 5 | 56864 / 114272 | training loss: 0.0017210433725267649\n",
      "epoch: 5 | 56896 / 114272 | training loss: 0.0014672718243673444\n",
      "epoch: 5 | 56928 / 114272 | training loss: 0.0018453337252140045\n",
      "epoch: 5 | 56960 / 114272 | training loss: 0.20255210995674133\n",
      "epoch: 5 | 56992 / 114272 | training loss: 0.21193863451480865\n",
      "epoch: 5 | 57024 / 114272 | training loss: 0.0006573493010364473\n",
      "epoch: 5 | 57056 / 114272 | training loss: 0.0008281699847429991\n",
      "epoch: 5 | 57088 / 114272 | training loss: 0.09622590243816376\n",
      "epoch: 5 | 57120 / 114272 | training loss: 0.13445624709129333\n",
      "epoch: 5 | 57152 / 114272 | training loss: 0.0010129512520506978\n",
      "epoch: 5 | 57184 / 114272 | training loss: 0.0015222616493701935\n",
      "epoch: 5 | 57216 / 114272 | training loss: 0.004377503413707018\n",
      "epoch: 5 | 57248 / 114272 | training loss: 0.0015342534752562642\n",
      "epoch: 5 | 57280 / 114272 | training loss: 0.00119946023914963\n",
      "epoch: 5 | 57312 / 114272 | training loss: 0.000913479074370116\n",
      "epoch: 5 | 57344 / 114272 | training loss: 0.0031500491313636303\n",
      "epoch: 5 | 57376 / 114272 | training loss: 0.0021262113004922867\n",
      "epoch: 5 | 57408 / 114272 | training loss: 0.0014465388376265764\n",
      "epoch: 5 | 57440 / 114272 | training loss: 0.007639680523425341\n",
      "epoch: 5 | 57472 / 114272 | training loss: 0.0022049034014344215\n",
      "epoch: 5 | 57504 / 114272 | training loss: 0.0013318436685949564\n",
      "epoch: 5 | 57536 / 114272 | training loss: 0.00045995734399184585\n",
      "epoch: 5 | 57568 / 114272 | training loss: 0.0014390285359695554\n",
      "epoch: 5 | 57600 / 114272 | training loss: 0.2856006920337677\n",
      "epoch: 5 | 57632 / 114272 | training loss: 0.1150282621383667\n",
      "epoch: 5 | 57664 / 114272 | training loss: 0.08368445932865143\n",
      "epoch: 5 | 57696 / 114272 | training loss: 0.0008728695102035999\n",
      "epoch: 5 | 57728 / 114272 | training loss: 0.0012261561350896955\n",
      "epoch: 5 | 57760 / 114272 | training loss: 0.2111072689294815\n",
      "epoch: 5 | 57792 / 114272 | training loss: 0.000955335795879364\n",
      "epoch: 5 | 57824 / 114272 | training loss: 0.0025111990980803967\n",
      "epoch: 5 | 57856 / 114272 | training loss: 0.0018442807486280799\n",
      "epoch: 5 | 57888 / 114272 | training loss: 0.1079258918762207\n",
      "epoch: 5 | 57920 / 114272 | training loss: 0.0009309338056482375\n",
      "epoch: 5 | 57952 / 114272 | training loss: 0.047326862812042236\n",
      "epoch: 5 | 57984 / 114272 | training loss: 0.0013401861069723964\n",
      "epoch: 5 | 58016 / 114272 | training loss: 0.0029320644680410624\n",
      "epoch: 5 | 58048 / 114272 | training loss: 0.0010909317061305046\n",
      "epoch: 5 | 58080 / 114272 | training loss: 0.001830792985856533\n",
      "epoch: 5 | 58112 / 114272 | training loss: 0.0026304752100259066\n",
      "epoch: 5 | 58144 / 114272 | training loss: 0.0010738784912973642\n",
      "epoch: 5 | 58176 / 114272 | training loss: 0.0008955983212217689\n",
      "epoch: 5 | 58208 / 114272 | training loss: 0.001590999192558229\n",
      "epoch: 5 | 58240 / 114272 | training loss: 0.0013140601804479957\n",
      "epoch: 5 | 58272 / 114272 | training loss: 0.001772542716935277\n",
      "epoch: 5 | 58304 / 114272 | training loss: 0.0019102283986285329\n",
      "epoch: 5 | 58336 / 114272 | training loss: 0.0017886575078591704\n",
      "epoch: 5 | 58368 / 114272 | training loss: 0.0026159926783293486\n",
      "epoch: 5 | 58400 / 114272 | training loss: 0.0010527055710554123\n",
      "epoch: 5 | 58432 / 114272 | training loss: 0.06183560565114021\n",
      "epoch: 5 | 58464 / 114272 | training loss: 0.2293235957622528\n",
      "epoch: 5 | 58496 / 114272 | training loss: 0.16627945005893707\n",
      "epoch: 5 | 58528 / 114272 | training loss: 0.001039427355863154\n",
      "epoch: 5 | 58560 / 114272 | training loss: 0.2851718068122864\n",
      "epoch: 5 | 58592 / 114272 | training loss: 0.0012625555973500013\n",
      "epoch: 5 | 58624 / 114272 | training loss: 0.0009578440804034472\n",
      "epoch: 5 | 58656 / 114272 | training loss: 0.002194135682657361\n",
      "epoch: 5 | 58688 / 114272 | training loss: 0.14502647519111633\n",
      "epoch: 5 | 58720 / 114272 | training loss: 0.006879102438688278\n",
      "epoch: 5 | 58752 / 114272 | training loss: 0.0010356262791901827\n",
      "epoch: 5 | 58784 / 114272 | training loss: 0.02109619602560997\n",
      "epoch: 5 | 58816 / 114272 | training loss: 0.0018785756547003984\n",
      "epoch: 5 | 58848 / 114272 | training loss: 0.0029042402748018503\n",
      "epoch: 5 | 58880 / 114272 | training loss: 0.0014434321783483028\n",
      "epoch: 5 | 58912 / 114272 | training loss: 0.20903553068637848\n",
      "epoch: 5 | 58944 / 114272 | training loss: 0.15763649344444275\n",
      "epoch: 5 | 58976 / 114272 | training loss: 0.0007399835158139467\n",
      "epoch: 5 | 59008 / 114272 | training loss: 0.01561726350337267\n",
      "epoch: 5 | 59040 / 114272 | training loss: 0.002181334886699915\n",
      "epoch: 5 | 59072 / 114272 | training loss: 0.035542842000722885\n",
      "epoch: 5 | 59104 / 114272 | training loss: 0.001487343804910779\n",
      "epoch: 5 | 59136 / 114272 | training loss: 0.002018493600189686\n",
      "epoch: 5 | 59168 / 114272 | training loss: 0.05130123347043991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 59200 / 114272 | training loss: 0.03899701312184334\n",
      "epoch: 5 | 59232 / 114272 | training loss: 0.10649389773607254\n",
      "epoch: 5 | 59264 / 114272 | training loss: 0.003295700065791607\n",
      "epoch: 5 | 59296 / 114272 | training loss: 0.0014693051343783736\n",
      "epoch: 5 | 59328 / 114272 | training loss: 0.0020227578934282064\n",
      "epoch: 5 | 59360 / 114272 | training loss: 0.0023773368448019028\n",
      "epoch: 5 | 59392 / 114272 | training loss: 0.0068643116392195225\n",
      "epoch: 5 | 59424 / 114272 | training loss: 0.002388023305684328\n",
      "epoch: 5 | 59456 / 114272 | training loss: 0.001730658463202417\n",
      "epoch: 5 | 59488 / 114272 | training loss: 0.03359416127204895\n",
      "epoch: 5 | 59520 / 114272 | training loss: 0.16001665592193604\n",
      "epoch: 5 | 59552 / 114272 | training loss: 0.00245847599580884\n",
      "epoch: 5 | 59584 / 114272 | training loss: 0.0031337363179773092\n",
      "epoch: 5 | 59616 / 114272 | training loss: 0.00497396569699049\n",
      "epoch: 5 | 59648 / 114272 | training loss: 0.002490437589585781\n",
      "epoch: 5 | 59680 / 114272 | training loss: 0.18318557739257812\n",
      "epoch: 5 | 59712 / 114272 | training loss: 0.0011321788188070059\n",
      "epoch: 5 | 59744 / 114272 | training loss: 0.16058088839054108\n",
      "epoch: 5 | 59776 / 114272 | training loss: 0.0022345916368067265\n",
      "epoch: 5 | 59808 / 114272 | training loss: 0.00203222269192338\n",
      "epoch: 5 | 59840 / 114272 | training loss: 0.12591391801834106\n",
      "epoch: 5 | 59872 / 114272 | training loss: 0.12762154638767242\n",
      "epoch: 5 | 59904 / 114272 | training loss: 0.006045065820217133\n",
      "epoch: 5 | 59936 / 114272 | training loss: 0.0022291666828095913\n",
      "epoch: 5 | 59968 / 114272 | training loss: 0.0015308423899114132\n",
      "epoch: 5 | 60000 / 114272 | training loss: 0.0021273097954690456\n",
      "epoch: 5 | 60032 / 114272 | training loss: 0.0028328553307801485\n",
      "epoch: 5 | 60064 / 114272 | training loss: 0.0014641841407865286\n",
      "epoch: 5 | 60096 / 114272 | training loss: 0.017041293904185295\n",
      "epoch: 5 | 60128 / 114272 | training loss: 0.0012688974384218454\n",
      "epoch: 5 | 60160 / 114272 | training loss: 0.0035045200493186712\n",
      "epoch: 5 | 60192 / 114272 | training loss: 0.002616708865389228\n",
      "epoch: 5 | 60224 / 114272 | training loss: 0.0032596103847026825\n",
      "epoch: 5 | 60256 / 114272 | training loss: 0.1554851531982422\n",
      "epoch: 5 | 60288 / 114272 | training loss: 0.11057357490062714\n",
      "epoch: 5 | 60320 / 114272 | training loss: 0.0013088798150420189\n",
      "epoch: 5 | 60352 / 114272 | training loss: 0.16014446318149567\n",
      "epoch: 5 | 60384 / 114272 | training loss: 0.0020768812391906977\n",
      "epoch: 5 | 60416 / 114272 | training loss: 0.0021178906317800283\n",
      "epoch: 5 | 60448 / 114272 | training loss: 0.02988276258111\n",
      "epoch: 5 | 60480 / 114272 | training loss: 0.0014839018695056438\n",
      "epoch: 5 | 60512 / 114272 | training loss: 0.00507917208597064\n",
      "epoch: 5 | 60544 / 114272 | training loss: 0.0018919478170573711\n",
      "epoch: 5 | 60576 / 114272 | training loss: 0.0687444657087326\n",
      "epoch: 5 | 60608 / 114272 | training loss: 0.0026111288461834192\n",
      "epoch: 5 | 60640 / 114272 | training loss: 0.2693771719932556\n",
      "epoch: 5 | 60672 / 114272 | training loss: 0.0015110403764992952\n",
      "epoch: 5 | 60704 / 114272 | training loss: 0.0033756468910723925\n",
      "epoch: 5 | 60736 / 114272 | training loss: 0.002777267247438431\n",
      "epoch: 5 | 60768 / 114272 | training loss: 0.11589743196964264\n",
      "epoch: 5 | 60800 / 114272 | training loss: 0.0021639124024659395\n",
      "epoch: 5 | 60832 / 114272 | training loss: 0.0046719457022845745\n",
      "epoch: 5 | 60864 / 114272 | training loss: 0.0007761393208056688\n",
      "epoch: 5 | 60896 / 114272 | training loss: 0.0029547205194830894\n",
      "epoch: 5 | 60928 / 114272 | training loss: 0.19882279634475708\n",
      "epoch: 5 | 60960 / 114272 | training loss: 0.002002054126933217\n",
      "epoch: 5 | 60992 / 114272 | training loss: 0.002767441561445594\n",
      "epoch: 5 | 61024 / 114272 | training loss: 0.002476312918588519\n",
      "epoch: 5 | 61056 / 114272 | training loss: 0.0025835870765149593\n",
      "epoch: 5 | 61088 / 114272 | training loss: 0.21010443568229675\n",
      "epoch: 5 | 61120 / 114272 | training loss: 0.006412861403077841\n",
      "epoch: 5 | 61152 / 114272 | training loss: 0.10393700748682022\n",
      "epoch: 5 | 61184 / 114272 | training loss: 0.0019080985803157091\n",
      "epoch: 5 | 61216 / 114272 | training loss: 0.003691768739372492\n",
      "epoch: 5 | 61248 / 114272 | training loss: 0.001555209280923009\n",
      "epoch: 5 | 61280 / 114272 | training loss: 0.0024387440644204617\n",
      "epoch: 5 | 61312 / 114272 | training loss: 0.0019080952042713761\n",
      "epoch: 5 | 61344 / 114272 | training loss: 0.018891144543886185\n",
      "epoch: 5 | 61376 / 114272 | training loss: 0.0035051293671131134\n",
      "epoch: 5 | 61408 / 114272 | training loss: 0.0013581528328359127\n",
      "epoch: 5 | 61440 / 114272 | training loss: 0.0025409560184925795\n",
      "epoch: 5 | 61472 / 114272 | training loss: 0.005793315824121237\n",
      "epoch: 5 | 61504 / 114272 | training loss: 0.0025673992931842804\n",
      "epoch: 5 | 61536 / 114272 | training loss: 0.0020744369830936193\n",
      "epoch: 5 | 61568 / 114272 | training loss: 0.0012483646860346198\n",
      "epoch: 5 | 61600 / 114272 | training loss: 0.0011477619409561157\n",
      "epoch: 5 | 61632 / 114272 | training loss: 0.0010986075503751636\n",
      "epoch: 5 | 61664 / 114272 | training loss: 0.27692711353302\n",
      "epoch: 5 | 61696 / 114272 | training loss: 0.006903447210788727\n",
      "epoch: 5 | 61728 / 114272 | training loss: 0.0009183514048345387\n",
      "epoch: 5 | 61760 / 114272 | training loss: 0.0034794395323842764\n",
      "epoch: 5 | 61792 / 114272 | training loss: 0.15710611641407013\n",
      "epoch: 5 | 61824 / 114272 | training loss: 0.19116801023483276\n",
      "epoch: 5 | 61856 / 114272 | training loss: 0.0019011074909940362\n",
      "epoch: 5 | 61888 / 114272 | training loss: 0.0008879259112291038\n",
      "epoch: 5 | 61920 / 114272 | training loss: 0.0018668783595785499\n",
      "epoch: 5 | 61952 / 114272 | training loss: 0.002242356538772583\n",
      "epoch: 5 | 61984 / 114272 | training loss: 0.0015212447615340352\n",
      "epoch: 5 | 62016 / 114272 | training loss: 0.15522868931293488\n",
      "epoch: 5 | 62048 / 114272 | training loss: 0.0015159694012254477\n",
      "epoch: 5 | 62080 / 114272 | training loss: 0.0010778201976791024\n",
      "epoch: 5 | 62112 / 114272 | training loss: 0.003850894281640649\n",
      "epoch: 5 | 62144 / 114272 | training loss: 0.17053136229515076\n",
      "epoch: 5 | 62176 / 114272 | training loss: 0.24590429663658142\n",
      "epoch: 5 | 62208 / 114272 | training loss: 0.01602987013757229\n",
      "epoch: 5 | 62240 / 114272 | training loss: 0.0016987825511023402\n",
      "epoch: 5 | 62272 / 114272 | training loss: 0.0012651674915105104\n",
      "epoch: 5 | 62304 / 114272 | training loss: 0.002626940608024597\n",
      "epoch: 5 | 62336 / 114272 | training loss: 0.0015790642937645316\n",
      "epoch: 5 | 62368 / 114272 | training loss: 0.0012084994232282043\n",
      "epoch: 5 | 62400 / 114272 | training loss: 0.0010064756497740746\n",
      "epoch: 5 | 62432 / 114272 | training loss: 0.0010551147861406207\n",
      "epoch: 5 | 62464 / 114272 | training loss: 0.0032979659736156464\n",
      "epoch: 5 | 62496 / 114272 | training loss: 0.1620682030916214\n",
      "epoch: 5 | 62528 / 114272 | training loss: 0.0007863182108849287\n",
      "epoch: 5 | 62560 / 114272 | training loss: 0.0022851931862533092\n",
      "epoch: 5 | 62592 / 114272 | training loss: 0.009776666760444641\n",
      "epoch: 5 | 62624 / 114272 | training loss: 0.0019431996624916792\n",
      "epoch: 5 | 62656 / 114272 | training loss: 0.0011555623495951295\n",
      "epoch: 5 | 62688 / 114272 | training loss: 0.004337874706834555\n",
      "epoch: 5 | 62720 / 114272 | training loss: 0.0012582718627527356\n",
      "epoch: 5 | 62752 / 114272 | training loss: 0.014709382317960262\n",
      "epoch: 5 | 62784 / 114272 | training loss: 0.13520285487174988\n",
      "epoch: 5 | 62816 / 114272 | training loss: 0.08177980035543442\n",
      "epoch: 5 | 62848 / 114272 | training loss: 0.011180531233549118\n",
      "epoch: 5 | 62880 / 114272 | training loss: 0.0016173407202586532\n",
      "epoch: 5 | 62912 / 114272 | training loss: 0.029142701998353004\n",
      "epoch: 5 | 62944 / 114272 | training loss: 0.004802901763468981\n",
      "epoch: 5 | 62976 / 114272 | training loss: 0.0019372004317119718\n",
      "epoch: 5 | 63008 / 114272 | training loss: 0.0019211319740861654\n",
      "epoch: 5 | 63040 / 114272 | training loss: 0.0009616206516511738\n",
      "epoch: 5 | 63072 / 114272 | training loss: 0.0005204313201829791\n",
      "epoch: 5 | 63104 / 114272 | training loss: 0.001440480351448059\n",
      "epoch: 5 | 63136 / 114272 | training loss: 0.0017029261216521263\n",
      "epoch: 5 | 63168 / 114272 | training loss: 0.0007393757114186883\n",
      "epoch: 5 | 63200 / 114272 | training loss: 0.1129525899887085\n",
      "epoch: 5 | 63232 / 114272 | training loss: 0.0015404715668410063\n",
      "epoch: 5 | 63264 / 114272 | training loss: 0.0007800472085364163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 63296 / 114272 | training loss: 0.028413232415914536\n",
      "epoch: 5 | 63328 / 114272 | training loss: 0.0892670750617981\n",
      "epoch: 5 | 63360 / 114272 | training loss: 0.1466299146413803\n",
      "epoch: 5 | 63392 / 114272 | training loss: 0.0020436602644622326\n",
      "epoch: 5 | 63424 / 114272 | training loss: 0.011422560550272465\n",
      "epoch: 5 | 63456 / 114272 | training loss: 0.0019020115723833442\n",
      "epoch: 5 | 63488 / 114272 | training loss: 0.0021060912404209375\n",
      "epoch: 5 | 63520 / 114272 | training loss: 0.00147044169716537\n",
      "epoch: 5 | 63552 / 114272 | training loss: 0.001209944486618042\n",
      "epoch: 5 | 63584 / 114272 | training loss: 0.0031837373971939087\n",
      "epoch: 5 | 63616 / 114272 | training loss: 0.16069279611110687\n",
      "epoch: 5 | 63648 / 114272 | training loss: 0.0007248815963976085\n",
      "epoch: 5 | 63680 / 114272 | training loss: 0.0021463935263454914\n",
      "epoch: 5 | 63712 / 114272 | training loss: 0.0008222584729082882\n",
      "epoch: 5 | 63744 / 114272 | training loss: 0.004023801069706678\n",
      "epoch: 5 | 63776 / 114272 | training loss: 0.0021055301185697317\n",
      "epoch: 5 | 63808 / 114272 | training loss: 0.0848197266459465\n",
      "epoch: 5 | 63840 / 114272 | training loss: 0.02401057258248329\n",
      "epoch: 5 | 63872 / 114272 | training loss: 0.001595828216522932\n",
      "epoch: 5 | 63904 / 114272 | training loss: 0.006524309981614351\n",
      "epoch: 5 | 63936 / 114272 | training loss: 0.0010824821656569839\n",
      "epoch: 5 | 63968 / 114272 | training loss: 0.0020507669541984797\n",
      "epoch: 5 | 64000 / 114272 | training loss: 0.004227503202855587\n",
      "epoch: 5 | 64032 / 114272 | training loss: 0.0013715794775635004\n",
      "epoch: 5 | 64064 / 114272 | training loss: 0.003373233135789633\n",
      "epoch: 5 | 64096 / 114272 | training loss: 0.15792319178581238\n",
      "epoch: 5 | 64128 / 114272 | training loss: 0.0012501119635999203\n",
      "epoch: 5 | 64160 / 114272 | training loss: 0.026426080614328384\n",
      "epoch: 5 | 64192 / 114272 | training loss: 0.20953764021396637\n",
      "epoch: 5 | 64224 / 114272 | training loss: 0.0007701743161305785\n",
      "epoch: 5 | 64256 / 114272 | training loss: 0.0039063068106770515\n",
      "epoch: 5 | 64288 / 114272 | training loss: 0.011019010096788406\n",
      "epoch: 5 | 64320 / 114272 | training loss: 0.001352298422716558\n",
      "epoch: 5 | 64352 / 114272 | training loss: 0.004589312709867954\n",
      "epoch: 5 | 64384 / 114272 | training loss: 0.001452364376746118\n",
      "epoch: 5 | 64416 / 114272 | training loss: 0.07871190458536148\n",
      "epoch: 5 | 64448 / 114272 | training loss: 0.0019717803224921227\n",
      "epoch: 5 | 64480 / 114272 | training loss: 0.0010780937736853957\n",
      "epoch: 5 | 64512 / 114272 | training loss: 0.0005845613777637482\n",
      "epoch: 5 | 64544 / 114272 | training loss: 0.10700750350952148\n",
      "epoch: 5 | 64576 / 114272 | training loss: 0.001822858932428062\n",
      "epoch: 5 | 64608 / 114272 | training loss: 0.15520600974559784\n",
      "epoch: 5 | 64640 / 114272 | training loss: 0.0009810898918658495\n",
      "epoch: 5 | 64672 / 114272 | training loss: 0.0009931382955983281\n",
      "epoch: 5 | 64704 / 114272 | training loss: 0.041816405951976776\n",
      "epoch: 5 | 64736 / 114272 | training loss: 0.0014240746386349201\n",
      "epoch: 5 | 64768 / 114272 | training loss: 0.0009501997265033424\n",
      "epoch: 5 | 64800 / 114272 | training loss: 0.002090506488457322\n",
      "epoch: 5 | 64832 / 114272 | training loss: 0.0056886509992182255\n",
      "epoch: 5 | 64864 / 114272 | training loss: 0.0009966968791559339\n",
      "epoch: 5 | 64896 / 114272 | training loss: 0.0006031840457580984\n",
      "epoch: 5 | 64928 / 114272 | training loss: 0.0013642031699419022\n",
      "epoch: 5 | 64960 / 114272 | training loss: 0.0010014744475483894\n",
      "epoch: 5 | 64992 / 114272 | training loss: 0.000648326997179538\n",
      "epoch: 5 | 65024 / 114272 | training loss: 0.0019328052876517177\n",
      "epoch: 5 | 65056 / 114272 | training loss: 0.014166529290378094\n",
      "epoch: 5 | 65088 / 114272 | training loss: 0.0024779499508440495\n",
      "epoch: 5 | 65120 / 114272 | training loss: 0.001071726088412106\n",
      "epoch: 5 | 65152 / 114272 | training loss: 0.0065449317917227745\n",
      "epoch: 5 | 65184 / 114272 | training loss: 0.0008618788560852408\n",
      "epoch: 5 | 65216 / 114272 | training loss: 0.001874496228992939\n",
      "epoch: 5 | 65248 / 114272 | training loss: 0.005663343705236912\n",
      "epoch: 5 | 65280 / 114272 | training loss: 0.0018478991696611047\n",
      "epoch: 5 | 65312 / 114272 | training loss: 0.0016293589724227786\n",
      "epoch: 5 | 65344 / 114272 | training loss: 0.01688450761139393\n",
      "epoch: 5 | 65376 / 114272 | training loss: 0.00126033800188452\n",
      "epoch: 5 | 65408 / 114272 | training loss: 0.0014706978108733892\n",
      "epoch: 5 | 65440 / 114272 | training loss: 0.23306907713413239\n",
      "epoch: 5 | 65472 / 114272 | training loss: 0.004626436624675989\n",
      "epoch: 5 | 65504 / 114272 | training loss: 0.12624025344848633\n",
      "epoch: 5 | 65536 / 114272 | training loss: 0.0033589128870517015\n",
      "epoch: 5 | 65568 / 114272 | training loss: 0.1555134356021881\n",
      "epoch: 5 | 65600 / 114272 | training loss: 0.0020766749512404203\n",
      "epoch: 5 | 65632 / 114272 | training loss: 0.0004423870996106416\n",
      "epoch: 5 | 65664 / 114272 | training loss: 0.1416555494070053\n",
      "epoch: 5 | 65696 / 114272 | training loss: 0.0030789412558078766\n",
      "epoch: 5 | 65728 / 114272 | training loss: 0.0006390858325175941\n",
      "epoch: 5 | 65760 / 114272 | training loss: 0.0018937818240374327\n",
      "epoch: 5 | 65792 / 114272 | training loss: 0.001295470050536096\n",
      "epoch: 5 | 65824 / 114272 | training loss: 0.0008644189219921827\n",
      "epoch: 5 | 65856 / 114272 | training loss: 0.001082914648577571\n",
      "epoch: 5 | 65888 / 114272 | training loss: 0.0011747382814064622\n",
      "epoch: 5 | 65920 / 114272 | training loss: 0.00391710689291358\n",
      "epoch: 5 | 65952 / 114272 | training loss: 0.0019513502484187484\n",
      "epoch: 5 | 65984 / 114272 | training loss: 0.13290083408355713\n",
      "epoch: 5 | 66016 / 114272 | training loss: 0.0008965969900600612\n",
      "epoch: 5 | 66048 / 114272 | training loss: 0.0019264545990154147\n",
      "epoch: 5 | 66080 / 114272 | training loss: 0.0015073817921802402\n",
      "epoch: 5 | 66112 / 114272 | training loss: 0.002621851395815611\n",
      "epoch: 5 | 66144 / 114272 | training loss: 0.004310455173254013\n",
      "epoch: 5 | 66176 / 114272 | training loss: 0.0016211785841733217\n",
      "epoch: 5 | 66208 / 114272 | training loss: 0.1890852451324463\n",
      "epoch: 5 | 66240 / 114272 | training loss: 0.0005476994556374848\n",
      "epoch: 5 | 66272 / 114272 | training loss: 0.018563635647296906\n",
      "epoch: 5 | 66304 / 114272 | training loss: 0.0015230566496029496\n",
      "epoch: 5 | 66336 / 114272 | training loss: 0.002730538370087743\n",
      "epoch: 5 | 66368 / 114272 | training loss: 0.0008728284738026559\n",
      "epoch: 5 | 66400 / 114272 | training loss: 0.0006925543420948088\n",
      "epoch: 5 | 66432 / 114272 | training loss: 0.15541671216487885\n",
      "epoch: 5 | 66464 / 114272 | training loss: 0.001225426560267806\n",
      "epoch: 5 | 66496 / 114272 | training loss: 0.000684468075633049\n",
      "epoch: 5 | 66528 / 114272 | training loss: 0.003540045116096735\n",
      "epoch: 5 | 66560 / 114272 | training loss: 0.0009717697976157069\n",
      "epoch: 5 | 66592 / 114272 | training loss: 0.12489478290081024\n",
      "epoch: 5 | 66624 / 114272 | training loss: 0.16255082190036774\n",
      "epoch: 5 | 66656 / 114272 | training loss: 0.0009962624171748757\n",
      "epoch: 5 | 66688 / 114272 | training loss: 0.0025448217056691647\n",
      "epoch: 5 | 66720 / 114272 | training loss: 0.0017830273136496544\n",
      "epoch: 5 | 66752 / 114272 | training loss: 0.0027456958778202534\n",
      "epoch: 5 | 66784 / 114272 | training loss: 0.0010595249477773905\n",
      "epoch: 5 | 66816 / 114272 | training loss: 0.00390962790697813\n",
      "epoch: 5 | 66848 / 114272 | training loss: 0.0022015196736902\n",
      "epoch: 5 | 66880 / 114272 | training loss: 0.0026252949610352516\n",
      "epoch: 5 | 66912 / 114272 | training loss: 0.002455037087202072\n",
      "epoch: 5 | 66944 / 114272 | training loss: 0.003739355830475688\n",
      "epoch: 5 | 66976 / 114272 | training loss: 0.005563318729400635\n",
      "epoch: 5 | 67008 / 114272 | training loss: 0.007174331229180098\n",
      "epoch: 5 | 67040 / 114272 | training loss: 0.0010052501456812024\n",
      "epoch: 5 | 67072 / 114272 | training loss: 0.2686281204223633\n",
      "epoch: 5 | 67104 / 114272 | training loss: 0.11160703748464584\n",
      "epoch: 5 | 67136 / 114272 | training loss: 0.0011367193656042218\n",
      "epoch: 5 | 67168 / 114272 | training loss: 0.22588640451431274\n",
      "epoch: 5 | 67200 / 114272 | training loss: 0.11094831675291061\n",
      "epoch: 5 | 67232 / 114272 | training loss: 0.08196360617876053\n",
      "epoch: 5 | 67264 / 114272 | training loss: 0.19841502606868744\n",
      "epoch: 5 | 67296 / 114272 | training loss: 0.0028842187020927668\n",
      "epoch: 5 | 67328 / 114272 | training loss: 0.0012261603260412812\n",
      "epoch: 5 | 67360 / 114272 | training loss: 0.003646764438599348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 67392 / 114272 | training loss: 0.058377787470817566\n",
      "epoch: 5 | 67424 / 114272 | training loss: 0.00305173359811306\n",
      "epoch: 5 | 67456 / 114272 | training loss: 0.0014900041278451681\n",
      "epoch: 5 | 67488 / 114272 | training loss: 0.0016412504483014345\n",
      "epoch: 5 | 67520 / 114272 | training loss: 0.0034283725544810295\n",
      "epoch: 5 | 67552 / 114272 | training loss: 0.10159546136856079\n",
      "epoch: 5 | 67584 / 114272 | training loss: 0.0013199286768212914\n",
      "epoch: 5 | 67616 / 114272 | training loss: 0.001523627550341189\n",
      "epoch: 5 | 67648 / 114272 | training loss: 0.12598967552185059\n",
      "epoch: 5 | 67680 / 114272 | training loss: 0.002122939098626375\n",
      "epoch: 5 | 67712 / 114272 | training loss: 0.001406027120538056\n",
      "epoch: 5 | 67744 / 114272 | training loss: 0.09346151351928711\n",
      "epoch: 5 | 67776 / 114272 | training loss: 0.003219741163775325\n",
      "epoch: 5 | 67808 / 114272 | training loss: 0.0014739639591425657\n",
      "epoch: 5 | 67840 / 114272 | training loss: 0.05284784361720085\n",
      "epoch: 5 | 67872 / 114272 | training loss: 0.002003192203119397\n",
      "epoch: 5 | 67904 / 114272 | training loss: 0.0028156707994639874\n",
      "epoch: 5 | 67936 / 114272 | training loss: 0.0017922207480296493\n",
      "epoch: 5 | 67968 / 114272 | training loss: 0.08100175112485886\n",
      "epoch: 5 | 68000 / 114272 | training loss: 0.0019527683034539223\n",
      "epoch: 5 | 68032 / 114272 | training loss: 0.00577519228681922\n",
      "epoch: 5 | 68064 / 114272 | training loss: 0.11385927349328995\n",
      "epoch: 5 | 68096 / 114272 | training loss: 0.004174789879471064\n",
      "epoch: 5 | 68128 / 114272 | training loss: 0.21071740984916687\n",
      "epoch: 5 | 68160 / 114272 | training loss: 0.00551937660202384\n",
      "epoch: 5 | 68192 / 114272 | training loss: 0.049484945833683014\n",
      "epoch: 5 | 68224 / 114272 | training loss: 0.0006389013142324984\n",
      "epoch: 5 | 68256 / 114272 | training loss: 0.0016808771761134267\n",
      "epoch: 5 | 68288 / 114272 | training loss: 0.0037868174258619547\n",
      "epoch: 5 | 68320 / 114272 | training loss: 0.0006030944059602916\n",
      "epoch: 5 | 68352 / 114272 | training loss: 0.010468692518770695\n",
      "epoch: 5 | 68384 / 114272 | training loss: 0.004519708454608917\n",
      "epoch: 5 | 68416 / 114272 | training loss: 0.11280886083841324\n",
      "epoch: 5 | 68448 / 114272 | training loss: 0.04023337364196777\n",
      "epoch: 5 | 68480 / 114272 | training loss: 0.0020006645936518908\n",
      "epoch: 5 | 68512 / 114272 | training loss: 0.004021571017801762\n",
      "epoch: 5 | 68544 / 114272 | training loss: 0.008268531411886215\n",
      "epoch: 5 | 68576 / 114272 | training loss: 0.0019297664985060692\n",
      "epoch: 5 | 68608 / 114272 | training loss: 0.003999765496701002\n",
      "epoch: 5 | 68640 / 114272 | training loss: 0.0027945039328187704\n",
      "epoch: 5 | 68672 / 114272 | training loss: 0.0043344516307115555\n",
      "epoch: 5 | 68704 / 114272 | training loss: 0.0030011991038918495\n",
      "epoch: 5 | 68736 / 114272 | training loss: 0.0006022948655299842\n",
      "epoch: 5 | 68768 / 114272 | training loss: 0.006627446040511131\n",
      "epoch: 5 | 68800 / 114272 | training loss: 0.0014618682907894254\n",
      "epoch: 5 | 68832 / 114272 | training loss: 0.0504029244184494\n",
      "epoch: 5 | 68864 / 114272 | training loss: 0.0009896572446450591\n",
      "epoch: 5 | 68896 / 114272 | training loss: 0.0025291419588029385\n",
      "epoch: 5 | 68928 / 114272 | training loss: 0.2160108983516693\n",
      "epoch: 5 | 68960 / 114272 | training loss: 0.00543470261618495\n",
      "epoch: 5 | 68992 / 114272 | training loss: 0.0013100942596793175\n",
      "epoch: 5 | 69024 / 114272 | training loss: 0.005844444967806339\n",
      "epoch: 5 | 69056 / 114272 | training loss: 0.0011360649950802326\n",
      "epoch: 5 | 69088 / 114272 | training loss: 0.004142905585467815\n",
      "epoch: 5 | 69120 / 114272 | training loss: 0.0006159324548207223\n",
      "epoch: 5 | 69152 / 114272 | training loss: 0.0018123141489923\n",
      "epoch: 5 | 69184 / 114272 | training loss: 0.0018757996149361134\n",
      "epoch: 5 | 69216 / 114272 | training loss: 0.0008362215012311935\n",
      "epoch: 5 | 69248 / 114272 | training loss: 0.0007466261740773916\n",
      "epoch: 5 | 69280 / 114272 | training loss: 0.2319815307855606\n",
      "epoch: 5 | 69312 / 114272 | training loss: 0.005725111346691847\n",
      "epoch: 5 | 69344 / 114272 | training loss: 0.0039852336049079895\n",
      "epoch: 5 | 69376 / 114272 | training loss: 0.11869487911462784\n",
      "epoch: 5 | 69408 / 114272 | training loss: 0.0022470089606940746\n",
      "epoch: 5 | 69440 / 114272 | training loss: 0.26876744627952576\n",
      "epoch: 5 | 69472 / 114272 | training loss: 0.0020221006125211716\n",
      "epoch: 5 | 69504 / 114272 | training loss: 0.00048796788905747235\n",
      "epoch: 5 | 69536 / 114272 | training loss: 0.0004899593186564744\n",
      "epoch: 5 | 69568 / 114272 | training loss: 0.001909534097649157\n",
      "epoch: 5 | 69600 / 114272 | training loss: 0.0009687021374702454\n",
      "epoch: 5 | 69632 / 114272 | training loss: 0.001684795250184834\n",
      "epoch: 5 | 69664 / 114272 | training loss: 0.0008903412381187081\n",
      "epoch: 5 | 69696 / 114272 | training loss: 0.002323646331205964\n",
      "epoch: 5 | 69728 / 114272 | training loss: 0.0009304648265242577\n",
      "epoch: 5 | 69760 / 114272 | training loss: 0.0015289095463231206\n",
      "epoch: 5 | 69792 / 114272 | training loss: 0.03294625133275986\n",
      "epoch: 5 | 69824 / 114272 | training loss: 0.0009545008069835603\n",
      "epoch: 5 | 69856 / 114272 | training loss: 0.010447616688907146\n",
      "epoch: 5 | 69888 / 114272 | training loss: 0.09620655328035355\n",
      "epoch: 5 | 69920 / 114272 | training loss: 0.1716201901435852\n",
      "epoch: 5 | 69952 / 114272 | training loss: 0.00144628353882581\n",
      "epoch: 5 | 69984 / 114272 | training loss: 0.0009546788642182946\n",
      "epoch: 5 | 70016 / 114272 | training loss: 0.0007410996477119625\n",
      "epoch: 5 | 70048 / 114272 | training loss: 0.0008644679328426719\n",
      "epoch: 5 | 70080 / 114272 | training loss: 0.0005678676534444094\n",
      "epoch: 5 | 70112 / 114272 | training loss: 0.000908038520719856\n",
      "epoch: 5 | 70144 / 114272 | training loss: 0.0011419898364692926\n",
      "epoch: 5 | 70176 / 114272 | training loss: 0.0007032914436422288\n",
      "epoch: 5 | 70208 / 114272 | training loss: 0.26588499546051025\n",
      "epoch: 5 | 70240 / 114272 | training loss: 0.0016202765982598066\n",
      "epoch: 5 | 70272 / 114272 | training loss: 0.08688197284936905\n",
      "epoch: 5 | 70304 / 114272 | training loss: 0.001954135484993458\n",
      "epoch: 5 | 70336 / 114272 | training loss: 0.0006163034704513848\n",
      "epoch: 5 | 70368 / 114272 | training loss: 0.0018504478503018618\n",
      "epoch: 5 | 70400 / 114272 | training loss: 0.006296960636973381\n",
      "epoch: 5 | 70432 / 114272 | training loss: 0.0012475787661969662\n",
      "epoch: 5 | 70464 / 114272 | training loss: 0.08147932589054108\n",
      "epoch: 5 | 70496 / 114272 | training loss: 0.0005971163045614958\n",
      "epoch: 5 | 70528 / 114272 | training loss: 0.0014870541635900736\n",
      "epoch: 5 | 70560 / 114272 | training loss: 0.0024692139122635126\n",
      "epoch: 5 | 70592 / 114272 | training loss: 0.002201266121119261\n",
      "epoch: 5 | 70624 / 114272 | training loss: 0.0006402794970199466\n",
      "epoch: 5 | 70656 / 114272 | training loss: 0.0015090354718267918\n",
      "epoch: 5 | 70688 / 114272 | training loss: 0.0015718296635895967\n",
      "epoch: 5 | 70720 / 114272 | training loss: 0.0006311308825388551\n",
      "epoch: 5 | 70752 / 114272 | training loss: 0.0015153256244957447\n",
      "epoch: 5 | 70784 / 114272 | training loss: 0.16667243838310242\n",
      "epoch: 5 | 70816 / 114272 | training loss: 0.0006784123834222555\n",
      "epoch: 5 | 70848 / 114272 | training loss: 0.22429834306240082\n",
      "epoch: 5 | 70880 / 114272 | training loss: 0.02997366152703762\n",
      "epoch: 5 | 70912 / 114272 | training loss: 0.0015907259657979012\n",
      "epoch: 5 | 70944 / 114272 | training loss: 0.0004319016879890114\n",
      "epoch: 5 | 70976 / 114272 | training loss: 0.28545820713043213\n",
      "epoch: 5 | 71008 / 114272 | training loss: 0.0024261546786874533\n",
      "epoch: 5 | 71040 / 114272 | training loss: 0.0009135781438089907\n",
      "epoch: 5 | 71072 / 114272 | training loss: 0.001345194992609322\n",
      "epoch: 5 | 71104 / 114272 | training loss: 0.0006837457185611129\n",
      "epoch: 5 | 71136 / 114272 | training loss: 0.008653979748487473\n",
      "epoch: 5 | 71168 / 114272 | training loss: 0.001020642346702516\n",
      "epoch: 5 | 71200 / 114272 | training loss: 0.003951061517000198\n",
      "epoch: 5 | 71232 / 114272 | training loss: 0.0009411281207576394\n",
      "epoch: 5 | 71264 / 114272 | training loss: 0.13658936321735382\n",
      "epoch: 5 | 71296 / 114272 | training loss: 0.0006953693809919059\n",
      "epoch: 5 | 71328 / 114272 | training loss: 0.0007634523790329695\n",
      "epoch: 5 | 71360 / 114272 | training loss: 0.0022187624126672745\n",
      "epoch: 5 | 71392 / 114272 | training loss: 0.0017070506000891328\n",
      "epoch: 5 | 71424 / 114272 | training loss: 0.001287279650568962\n",
      "epoch: 5 | 71456 / 114272 | training loss: 0.18139201402664185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 71488 / 114272 | training loss: 0.000846540555357933\n",
      "epoch: 5 | 71520 / 114272 | training loss: 0.10142461955547333\n",
      "epoch: 5 | 71552 / 114272 | training loss: 0.0011915804352611303\n",
      "epoch: 5 | 71584 / 114272 | training loss: 0.0005841792444698513\n",
      "epoch: 5 | 71616 / 114272 | training loss: 0.0947549045085907\n",
      "epoch: 5 | 71648 / 114272 | training loss: 0.0013151552993804216\n",
      "epoch: 5 | 71680 / 114272 | training loss: 0.004469520878046751\n",
      "epoch: 5 | 71712 / 114272 | training loss: 0.0031408327631652355\n",
      "epoch: 5 | 71744 / 114272 | training loss: 0.0011981700081378222\n",
      "epoch: 5 | 71776 / 114272 | training loss: 0.0038089226000010967\n",
      "epoch: 5 | 71808 / 114272 | training loss: 0.147142693400383\n",
      "epoch: 5 | 71840 / 114272 | training loss: 0.03702487424015999\n",
      "epoch: 5 | 71872 / 114272 | training loss: 0.009834875352680683\n",
      "epoch: 5 | 71904 / 114272 | training loss: 0.0011273985728621483\n",
      "epoch: 5 | 71936 / 114272 | training loss: 0.09784767776727676\n",
      "epoch: 5 | 71968 / 114272 | training loss: 0.0015508647775277495\n",
      "epoch: 5 | 72000 / 114272 | training loss: 0.02562614716589451\n",
      "epoch: 5 | 72032 / 114272 | training loss: 0.0019168477738276124\n",
      "epoch: 5 | 72064 / 114272 | training loss: 0.0007800986641086638\n",
      "epoch: 5 | 72096 / 114272 | training loss: 0.010331499390304089\n",
      "epoch: 5 | 72128 / 114272 | training loss: 0.00566550437361002\n",
      "epoch: 5 | 72160 / 114272 | training loss: 0.0029430680442601442\n",
      "epoch: 5 | 72192 / 114272 | training loss: 0.0004792243998963386\n",
      "epoch: 5 | 72224 / 114272 | training loss: 0.02314882166683674\n",
      "epoch: 5 | 72256 / 114272 | training loss: 0.007112480700016022\n",
      "epoch: 5 | 72288 / 114272 | training loss: 0.002512419829145074\n",
      "epoch: 5 | 72320 / 114272 | training loss: 0.00207888032309711\n",
      "epoch: 5 | 72352 / 114272 | training loss: 0.10270794481039047\n",
      "epoch: 5 | 72384 / 114272 | training loss: 0.0013945904793217778\n",
      "epoch: 5 | 72416 / 114272 | training loss: 0.0027320589870214462\n",
      "epoch: 5 | 72448 / 114272 | training loss: 0.005256859119981527\n",
      "epoch: 5 | 72480 / 114272 | training loss: 0.0030052822548896074\n",
      "epoch: 5 | 72512 / 114272 | training loss: 0.1574070155620575\n",
      "epoch: 5 | 72544 / 114272 | training loss: 0.0007804015185683966\n",
      "epoch: 5 | 72576 / 114272 | training loss: 0.005259064957499504\n",
      "epoch: 5 | 72608 / 114272 | training loss: 0.001177582424134016\n",
      "epoch: 5 | 72640 / 114272 | training loss: 0.0009502677712589502\n",
      "epoch: 5 | 72672 / 114272 | training loss: 0.0793866366147995\n",
      "epoch: 5 | 72704 / 114272 | training loss: 0.0019907744135707617\n",
      "epoch: 5 | 72736 / 114272 | training loss: 0.016863413155078888\n",
      "epoch: 5 | 72768 / 114272 | training loss: 0.001968124881386757\n",
      "epoch: 5 | 72800 / 114272 | training loss: 0.0011370949214324355\n",
      "epoch: 5 | 72832 / 114272 | training loss: 0.011488215997815132\n",
      "epoch: 5 | 72864 / 114272 | training loss: 0.004284234717488289\n",
      "epoch: 5 | 72896 / 114272 | training loss: 0.001454301760531962\n",
      "epoch: 5 | 72928 / 114272 | training loss: 0.006958343088626862\n",
      "epoch: 5 | 72960 / 114272 | training loss: 0.0020718465093523264\n",
      "epoch: 5 | 72992 / 114272 | training loss: 0.0005759501946158707\n",
      "epoch: 5 | 73024 / 114272 | training loss: 0.0019387094071134925\n",
      "epoch: 5 | 73056 / 114272 | training loss: 0.002578295301645994\n",
      "epoch: 5 | 73088 / 114272 | training loss: 0.07911151647567749\n",
      "epoch: 5 | 73120 / 114272 | training loss: 0.0009923239704221487\n",
      "epoch: 5 | 73152 / 114272 | training loss: 0.0012619707267731428\n",
      "epoch: 5 | 73184 / 114272 | training loss: 0.001558615011163056\n",
      "epoch: 5 | 73216 / 114272 | training loss: 0.18095237016677856\n",
      "epoch: 5 | 73248 / 114272 | training loss: 0.0006463852478191257\n",
      "epoch: 5 | 73280 / 114272 | training loss: 0.0058500864543020725\n",
      "epoch: 5 | 73312 / 114272 | training loss: 0.001023364719003439\n",
      "epoch: 5 | 73344 / 114272 | training loss: 0.0015242921654134989\n",
      "epoch: 5 | 73376 / 114272 | training loss: 0.21225997805595398\n",
      "epoch: 5 | 73408 / 114272 | training loss: 0.007294561248272657\n",
      "epoch: 5 | 73440 / 114272 | training loss: 0.0018000423442572355\n",
      "epoch: 5 | 73472 / 114272 | training loss: 0.0005741247441619635\n",
      "epoch: 5 | 73504 / 114272 | training loss: 0.00503947539255023\n",
      "epoch: 5 | 73536 / 114272 | training loss: 0.0713353306055069\n",
      "epoch: 5 | 73568 / 114272 | training loss: 0.03479306027293205\n",
      "epoch: 5 | 73600 / 114272 | training loss: 0.00039623049087822437\n",
      "epoch: 5 | 73632 / 114272 | training loss: 0.0009226517286151648\n",
      "epoch: 5 | 73664 / 114272 | training loss: 0.0010543486569076777\n",
      "epoch: 5 | 73696 / 114272 | training loss: 0.0003202797379344702\n",
      "epoch: 5 | 73728 / 114272 | training loss: 0.12002212554216385\n",
      "epoch: 5 | 73760 / 114272 | training loss: 0.0004903835360892117\n",
      "epoch: 5 | 73792 / 114272 | training loss: 0.0005007206927984953\n",
      "epoch: 5 | 73824 / 114272 | training loss: 0.2842688262462616\n",
      "epoch: 5 | 73856 / 114272 | training loss: 0.0009157690219581127\n",
      "epoch: 5 | 73888 / 114272 | training loss: 0.09190810471773148\n",
      "epoch: 5 | 73920 / 114272 | training loss: 0.003851852612569928\n",
      "epoch: 5 | 73952 / 114272 | training loss: 0.1283726990222931\n",
      "epoch: 5 | 73984 / 114272 | training loss: 0.0033871843479573727\n",
      "epoch: 5 | 74016 / 114272 | training loss: 0.00031824520556256175\n",
      "epoch: 5 | 74048 / 114272 | training loss: 0.20453821122646332\n",
      "epoch: 5 | 74080 / 114272 | training loss: 0.001230117049999535\n",
      "epoch: 5 | 74112 / 114272 | training loss: 0.000977060291916132\n",
      "epoch: 5 | 74144 / 114272 | training loss: 0.0010095249162986875\n",
      "epoch: 5 | 74176 / 114272 | training loss: 0.0008100263075903058\n",
      "epoch: 5 | 74208 / 114272 | training loss: 0.10786610841751099\n",
      "epoch: 5 | 74240 / 114272 | training loss: 0.003066442674025893\n",
      "epoch: 5 | 74272 / 114272 | training loss: 0.0012253361055627465\n",
      "epoch: 5 | 74304 / 114272 | training loss: 0.0004195844812784344\n",
      "epoch: 5 | 74336 / 114272 | training loss: 0.002790752099826932\n",
      "epoch: 5 | 74368 / 114272 | training loss: 0.001963521121069789\n",
      "epoch: 5 | 74400 / 114272 | training loss: 0.0011254835408180952\n",
      "epoch: 5 | 74432 / 114272 | training loss: 0.0013426137156784534\n",
      "epoch: 5 | 74464 / 114272 | training loss: 0.24034902453422546\n",
      "epoch: 5 | 74496 / 114272 | training loss: 0.0007075169123709202\n",
      "epoch: 5 | 74528 / 114272 | training loss: 0.0019317959668114781\n",
      "epoch: 5 | 74560 / 114272 | training loss: 0.0014746399829164147\n",
      "epoch: 5 | 74592 / 114272 | training loss: 0.007568244822323322\n",
      "epoch: 5 | 74624 / 114272 | training loss: 0.007952791638672352\n",
      "epoch: 5 | 74656 / 114272 | training loss: 0.07876842468976974\n",
      "epoch: 5 | 74688 / 114272 | training loss: 0.0004527772543951869\n",
      "epoch: 5 | 74720 / 114272 | training loss: 0.0006507556536234915\n",
      "epoch: 5 | 74752 / 114272 | training loss: 0.14451253414154053\n",
      "epoch: 5 | 74784 / 114272 | training loss: 0.03773907944560051\n",
      "epoch: 5 | 74816 / 114272 | training loss: 0.002721702679991722\n",
      "epoch: 5 | 74848 / 114272 | training loss: 0.004957274068146944\n",
      "epoch: 5 | 74880 / 114272 | training loss: 0.000766036449931562\n",
      "epoch: 5 | 74912 / 114272 | training loss: 0.0006568192038685083\n",
      "epoch: 5 | 74944 / 114272 | training loss: 0.0017139711417257786\n",
      "epoch: 5 | 74976 / 114272 | training loss: 0.0006173865986056626\n",
      "epoch: 5 | 75008 / 114272 | training loss: 0.0006151597481220961\n",
      "epoch: 5 | 75040 / 114272 | training loss: 0.0010349315125495195\n",
      "epoch: 5 | 75072 / 114272 | training loss: 0.0014456775970757008\n",
      "epoch: 5 | 75104 / 114272 | training loss: 0.0012297568609938025\n",
      "epoch: 5 | 75136 / 114272 | training loss: 0.1037713885307312\n",
      "epoch: 5 | 75168 / 114272 | training loss: 0.000918941164854914\n",
      "epoch: 5 | 75200 / 114272 | training loss: 0.001039775088429451\n",
      "epoch: 5 | 75232 / 114272 | training loss: 0.0005108196055516601\n",
      "epoch: 5 | 75264 / 114272 | training loss: 0.0009353765053674579\n",
      "epoch: 5 | 75296 / 114272 | training loss: 0.0030208691023290157\n",
      "epoch: 5 | 75328 / 114272 | training loss: 0.0009787158342078328\n",
      "epoch: 5 | 75360 / 114272 | training loss: 0.0003218691563233733\n",
      "epoch: 5 | 75392 / 114272 | training loss: 0.0004726685583591461\n",
      "epoch: 5 | 75424 / 114272 | training loss: 0.0013548790011554956\n",
      "epoch: 5 | 75456 / 114272 | training loss: 0.002438023453578353\n",
      "epoch: 5 | 75488 / 114272 | training loss: 0.000633113260846585\n",
      "epoch: 5 | 75520 / 114272 | training loss: 0.0006469134241342545\n",
      "epoch: 5 | 75552 / 114272 | training loss: 0.2275589406490326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 75584 / 114272 | training loss: 0.0010833394480869174\n",
      "epoch: 5 | 75616 / 114272 | training loss: 0.1674184799194336\n",
      "epoch: 5 | 75648 / 114272 | training loss: 0.13322113454341888\n",
      "epoch: 5 | 75680 / 114272 | training loss: 0.00627681240439415\n",
      "epoch: 5 | 75712 / 114272 | training loss: 0.0007184026180766523\n",
      "epoch: 5 | 75744 / 114272 | training loss: 0.2246386557817459\n",
      "epoch: 5 | 75776 / 114272 | training loss: 0.0012188274413347244\n",
      "epoch: 5 | 75808 / 114272 | training loss: 0.0005132262594997883\n",
      "epoch: 5 | 75840 / 114272 | training loss: 0.007125641219317913\n",
      "epoch: 5 | 75872 / 114272 | training loss: 0.0004001398046966642\n",
      "epoch: 5 | 75904 / 114272 | training loss: 0.0008265126962214708\n",
      "epoch: 5 | 75936 / 114272 | training loss: 0.31119677424430847\n",
      "epoch: 5 | 75968 / 114272 | training loss: 0.0016728428890928626\n",
      "epoch: 5 | 76000 / 114272 | training loss: 0.0011529780458658934\n",
      "epoch: 5 | 76032 / 114272 | training loss: 0.0011508517200127244\n",
      "epoch: 5 | 76064 / 114272 | training loss: 0.0005428050644695759\n",
      "epoch: 5 | 76096 / 114272 | training loss: 0.0015493638347834349\n",
      "epoch: 5 | 76128 / 114272 | training loss: 0.0008548838086426258\n",
      "epoch: 5 | 76160 / 114272 | training loss: 0.001056982553564012\n",
      "epoch: 5 | 76192 / 114272 | training loss: 0.05774965509772301\n",
      "epoch: 5 | 76224 / 114272 | training loss: 0.00300941476598382\n",
      "epoch: 5 | 76256 / 114272 | training loss: 0.10040868818759918\n",
      "epoch: 5 | 76288 / 114272 | training loss: 0.0009507157374173403\n",
      "epoch: 5 | 76320 / 114272 | training loss: 0.17537935078144073\n",
      "epoch: 5 | 76352 / 114272 | training loss: 0.0006985662621445954\n",
      "epoch: 5 | 76384 / 114272 | training loss: 0.0016649488825351\n",
      "epoch: 5 | 76416 / 114272 | training loss: 0.0008703980129212141\n",
      "epoch: 5 | 76448 / 114272 | training loss: 0.0006643388187512755\n",
      "epoch: 5 | 76480 / 114272 | training loss: 0.0005514273652806878\n",
      "epoch: 5 | 76512 / 114272 | training loss: 0.001887661055661738\n",
      "epoch: 5 | 76544 / 114272 | training loss: 0.001973299542441964\n",
      "epoch: 5 | 76576 / 114272 | training loss: 0.0009321126854047179\n",
      "epoch: 5 | 76608 / 114272 | training loss: 0.002069555688649416\n",
      "epoch: 5 | 76640 / 114272 | training loss: 0.0005676243454217911\n",
      "epoch: 5 | 76672 / 114272 | training loss: 0.0023551280610263348\n",
      "epoch: 5 | 76704 / 114272 | training loss: 0.01036346610635519\n",
      "epoch: 5 | 76736 / 114272 | training loss: 0.0013591169845312834\n",
      "epoch: 5 | 76768 / 114272 | training loss: 0.09199574589729309\n",
      "epoch: 5 | 76800 / 114272 | training loss: 0.009286445565521717\n",
      "epoch: 5 | 76832 / 114272 | training loss: 0.040004581212997437\n",
      "epoch: 5 | 76864 / 114272 | training loss: 0.0004349877417553216\n",
      "epoch: 5 | 76896 / 114272 | training loss: 0.22446885704994202\n",
      "epoch: 5 | 76928 / 114272 | training loss: 0.07021613419055939\n",
      "epoch: 5 | 76960 / 114272 | training loss: 0.00041808743844740093\n",
      "epoch: 5 | 76992 / 114272 | training loss: 0.0016119045903906226\n",
      "epoch: 5 | 77024 / 114272 | training loss: 0.001756463898345828\n",
      "epoch: 5 | 77056 / 114272 | training loss: 0.0011120762210339308\n",
      "epoch: 5 | 77088 / 114272 | training loss: 0.0022689069155603647\n",
      "epoch: 5 | 77120 / 114272 | training loss: 0.0014076457591727376\n",
      "epoch: 5 | 77152 / 114272 | training loss: 0.10527107119560242\n",
      "epoch: 5 | 77184 / 114272 | training loss: 0.0006803803262300789\n",
      "epoch: 5 | 77216 / 114272 | training loss: 0.009173013269901276\n",
      "epoch: 5 | 77248 / 114272 | training loss: 0.0008377225021831691\n",
      "epoch: 5 | 77280 / 114272 | training loss: 0.0012099497253075242\n",
      "epoch: 5 | 77312 / 114272 | training loss: 0.0023228966165333986\n",
      "epoch: 5 | 77344 / 114272 | training loss: 0.0017340555787086487\n",
      "epoch: 5 | 77376 / 114272 | training loss: 0.001947426120750606\n",
      "epoch: 5 | 77408 / 114272 | training loss: 0.0011456067441031337\n",
      "epoch: 5 | 77440 / 114272 | training loss: 0.002507172990590334\n",
      "epoch: 5 | 77472 / 114272 | training loss: 0.09948377311229706\n",
      "epoch: 5 | 77504 / 114272 | training loss: 0.004614733159542084\n",
      "epoch: 5 | 77536 / 114272 | training loss: 0.0034254100173711777\n",
      "epoch: 5 | 77568 / 114272 | training loss: 0.0021509509533643723\n",
      "epoch: 5 | 77600 / 114272 | training loss: 0.06162209436297417\n",
      "epoch: 5 | 77632 / 114272 | training loss: 0.13799752295017242\n",
      "epoch: 5 | 77664 / 114272 | training loss: 0.1652204543352127\n",
      "epoch: 5 | 77696 / 114272 | training loss: 0.08539113402366638\n",
      "epoch: 5 | 77728 / 114272 | training loss: 0.0019330973736941814\n",
      "epoch: 5 | 77760 / 114272 | training loss: 0.08550439774990082\n",
      "epoch: 5 | 77792 / 114272 | training loss: 0.025906212627887726\n",
      "epoch: 5 | 77824 / 114272 | training loss: 0.0017371426802128553\n",
      "epoch: 5 | 77856 / 114272 | training loss: 0.026680786162614822\n",
      "epoch: 5 | 77888 / 114272 | training loss: 0.005696801468729973\n",
      "epoch: 5 | 77920 / 114272 | training loss: 0.004654877819120884\n",
      "epoch: 5 | 77952 / 114272 | training loss: 0.0009938956936821342\n",
      "epoch: 5 | 77984 / 114272 | training loss: 0.005658041685819626\n",
      "epoch: 5 | 78016 / 114272 | training loss: 0.006545940414071083\n",
      "epoch: 5 | 78048 / 114272 | training loss: 0.13828422129154205\n",
      "epoch: 5 | 78080 / 114272 | training loss: 0.0008206411148421466\n",
      "epoch: 5 | 78112 / 114272 | training loss: 0.0028612862806767225\n",
      "epoch: 5 | 78144 / 114272 | training loss: 0.0010668406030163169\n",
      "epoch: 5 | 78176 / 114272 | training loss: 0.009801818057894707\n",
      "epoch: 5 | 78208 / 114272 | training loss: 0.47045576572418213\n",
      "epoch: 5 | 78240 / 114272 | training loss: 0.11618104577064514\n",
      "epoch: 5 | 78272 / 114272 | training loss: 0.1303972750902176\n",
      "epoch: 5 | 78304 / 114272 | training loss: 0.0018712112214416265\n",
      "epoch: 5 | 78336 / 114272 | training loss: 0.0009311970788985491\n",
      "epoch: 5 | 78368 / 114272 | training loss: 0.33353209495544434\n",
      "epoch: 5 | 78400 / 114272 | training loss: 0.3555334806442261\n",
      "epoch: 5 | 78432 / 114272 | training loss: 0.00540081225335598\n",
      "epoch: 5 | 78464 / 114272 | training loss: 0.0007955023320391774\n",
      "epoch: 5 | 78496 / 114272 | training loss: 0.0010716852266341448\n",
      "epoch: 5 | 78528 / 114272 | training loss: 0.0042596543207764626\n",
      "epoch: 5 | 78560 / 114272 | training loss: 0.0018251778092235327\n",
      "epoch: 5 | 78592 / 114272 | training loss: 0.19881610572338104\n",
      "epoch: 5 | 78624 / 114272 | training loss: 0.09675092995166779\n",
      "epoch: 5 | 78656 / 114272 | training loss: 0.007158597465604544\n",
      "epoch: 5 | 78688 / 114272 | training loss: 0.001481721643358469\n",
      "epoch: 5 | 78720 / 114272 | training loss: 0.1102980449795723\n",
      "epoch: 5 | 78752 / 114272 | training loss: 0.0014023870462551713\n",
      "epoch: 5 | 78784 / 114272 | training loss: 0.0019580451771616936\n",
      "epoch: 5 | 78816 / 114272 | training loss: 0.12060738354921341\n",
      "epoch: 5 | 78848 / 114272 | training loss: 0.0016089091077446938\n",
      "epoch: 5 | 78880 / 114272 | training loss: 0.0012025493197143078\n",
      "epoch: 5 | 78912 / 114272 | training loss: 0.0018820380792021751\n",
      "epoch: 5 | 78944 / 114272 | training loss: 0.005935541819781065\n",
      "epoch: 5 | 78976 / 114272 | training loss: 0.001961062429472804\n",
      "epoch: 5 | 79008 / 114272 | training loss: 0.002209097845479846\n",
      "epoch: 5 | 79040 / 114272 | training loss: 0.048241376876831055\n",
      "epoch: 5 | 79072 / 114272 | training loss: 0.07907221466302872\n",
      "epoch: 5 | 79104 / 114272 | training loss: 0.08013629168272018\n",
      "epoch: 5 | 79136 / 114272 | training loss: 0.003581062890589237\n",
      "epoch: 5 | 79168 / 114272 | training loss: 0.005904356017708778\n",
      "epoch: 5 | 79200 / 114272 | training loss: 0.0010093606542795897\n",
      "epoch: 5 | 79232 / 114272 | training loss: 0.01767941378057003\n",
      "epoch: 5 | 79264 / 114272 | training loss: 0.0044144317507743835\n",
      "epoch: 5 | 79296 / 114272 | training loss: 0.3128668963909149\n",
      "epoch: 5 | 79328 / 114272 | training loss: 0.0035279812291264534\n",
      "epoch: 5 | 79360 / 114272 | training loss: 0.0017234249971807003\n",
      "epoch: 5 | 79392 / 114272 | training loss: 0.20820336043834686\n",
      "epoch: 5 | 79424 / 114272 | training loss: 0.0006877239793539047\n",
      "epoch: 5 | 79456 / 114272 | training loss: 0.0062362803146243095\n",
      "epoch: 5 | 79488 / 114272 | training loss: 0.050586260855197906\n",
      "epoch: 5 | 79520 / 114272 | training loss: 0.0016524868551641703\n",
      "epoch: 5 | 79552 / 114272 | training loss: 0.00041726077324710786\n",
      "epoch: 5 | 79584 / 114272 | training loss: 0.0016902879578992724\n",
      "epoch: 5 | 79616 / 114272 | training loss: 0.06235538795590401\n",
      "epoch: 5 | 79648 / 114272 | training loss: 0.0014266170328482985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 79680 / 114272 | training loss: 0.0014851101441308856\n",
      "epoch: 5 | 79712 / 114272 | training loss: 0.00350860133767128\n",
      "epoch: 5 | 79744 / 114272 | training loss: 0.14320245385169983\n",
      "epoch: 5 | 79776 / 114272 | training loss: 0.0634278953075409\n",
      "epoch: 5 | 79808 / 114272 | training loss: 0.04262976720929146\n",
      "epoch: 5 | 79840 / 114272 | training loss: 0.10485756397247314\n",
      "epoch: 5 | 79872 / 114272 | training loss: 0.007270156405866146\n",
      "epoch: 5 | 79904 / 114272 | training loss: 0.009737732820212841\n",
      "epoch: 5 | 79936 / 114272 | training loss: 0.0011304147774353623\n",
      "epoch: 5 | 79968 / 114272 | training loss: 0.23217515647411346\n",
      "epoch: 5 | 80000 / 114272 | training loss: 0.0005893346969969571\n",
      "epoch: 5 | 80032 / 114272 | training loss: 0.0011788206174969673\n",
      "epoch: 5 | 80064 / 114272 | training loss: 0.10616468638181686\n",
      "epoch: 5 | 80096 / 114272 | training loss: 0.0012714553158730268\n",
      "epoch: 5 | 80128 / 114272 | training loss: 0.00281958538107574\n",
      "epoch: 5 | 80160 / 114272 | training loss: 0.00134767044801265\n",
      "epoch: 5 | 80192 / 114272 | training loss: 0.0013929191045463085\n",
      "epoch: 5 | 80224 / 114272 | training loss: 0.0004840377951040864\n",
      "epoch: 5 | 80256 / 114272 | training loss: 0.0013615468051284552\n",
      "epoch: 5 | 80288 / 114272 | training loss: 0.0009791580960154533\n",
      "epoch: 5 | 80320 / 114272 | training loss: 0.017279798164963722\n",
      "epoch: 5 | 80352 / 114272 | training loss: 0.002310733078047633\n",
      "epoch: 5 | 80384 / 114272 | training loss: 0.38289108872413635\n",
      "epoch: 5 | 80416 / 114272 | training loss: 0.0030770176090300083\n",
      "epoch: 5 | 80448 / 114272 | training loss: 0.17738766968250275\n",
      "epoch: 5 | 80480 / 114272 | training loss: 0.014137564226984978\n",
      "epoch: 5 | 80512 / 114272 | training loss: 0.004861017223447561\n",
      "epoch: 5 | 80544 / 114272 | training loss: 0.3129490911960602\n",
      "epoch: 5 | 80576 / 114272 | training loss: 0.000667876738589257\n",
      "epoch: 5 | 80608 / 114272 | training loss: 0.0014296332374215126\n",
      "epoch: 5 | 80640 / 114272 | training loss: 0.0015029453206807375\n",
      "epoch: 5 | 80672 / 114272 | training loss: 0.15463827550411224\n",
      "epoch: 5 | 80704 / 114272 | training loss: 0.005199894309043884\n",
      "epoch: 5 | 80736 / 114272 | training loss: 0.009280581958591938\n",
      "epoch: 5 | 80768 / 114272 | training loss: 0.0014829927822574973\n",
      "epoch: 5 | 80800 / 114272 | training loss: 0.0014704044442623854\n",
      "epoch: 5 | 80832 / 114272 | training loss: 0.003993852064013481\n",
      "epoch: 5 | 80864 / 114272 | training loss: 0.0017378419870510697\n",
      "epoch: 5 | 80896 / 114272 | training loss: 0.0045113274827599525\n",
      "epoch: 5 | 80928 / 114272 | training loss: 0.3656560480594635\n",
      "epoch: 5 | 80960 / 114272 | training loss: 0.0010504667880013585\n",
      "epoch: 5 | 80992 / 114272 | training loss: 0.14641498029232025\n",
      "epoch: 5 | 81024 / 114272 | training loss: 0.004680662881582975\n",
      "epoch: 5 | 81056 / 114272 | training loss: 0.003232604591175914\n",
      "epoch: 5 | 81088 / 114272 | training loss: 0.0042357840575277805\n",
      "epoch: 5 | 81120 / 114272 | training loss: 0.003037763526663184\n",
      "epoch: 5 | 81152 / 114272 | training loss: 0.0012857923284173012\n",
      "epoch: 5 | 81184 / 114272 | training loss: 0.0044267806224524975\n",
      "epoch: 5 | 81216 / 114272 | training loss: 0.0006334020290523767\n",
      "epoch: 5 | 81248 / 114272 | training loss: 0.00774983549490571\n",
      "epoch: 5 | 81280 / 114272 | training loss: 0.0014591303188353777\n",
      "epoch: 5 | 81312 / 114272 | training loss: 0.000702177407220006\n",
      "epoch: 5 | 81344 / 114272 | training loss: 0.0014337652828544378\n",
      "epoch: 5 | 81376 / 114272 | training loss: 0.002270192140713334\n",
      "epoch: 5 | 81408 / 114272 | training loss: 0.0031252210028469563\n",
      "epoch: 5 | 81440 / 114272 | training loss: 0.0020249676890671253\n",
      "epoch: 5 | 81472 / 114272 | training loss: 0.003192329313606024\n",
      "epoch: 5 | 81504 / 114272 | training loss: 0.00042821772512979805\n",
      "epoch: 5 | 81536 / 114272 | training loss: 0.0020508551970124245\n",
      "epoch: 5 | 81568 / 114272 | training loss: 0.0018045436590909958\n",
      "epoch: 5 | 81600 / 114272 | training loss: 0.06950628012418747\n",
      "epoch: 5 | 81632 / 114272 | training loss: 0.001710674143396318\n",
      "epoch: 5 | 81664 / 114272 | training loss: 0.0011928178137168288\n",
      "epoch: 5 | 81696 / 114272 | training loss: 0.0011025985004380345\n",
      "epoch: 5 | 81728 / 114272 | training loss: 0.004220614675432444\n",
      "epoch: 5 | 81760 / 114272 | training loss: 0.002846588846296072\n",
      "epoch: 5 | 81792 / 114272 | training loss: 0.125172957777977\n",
      "epoch: 5 | 81824 / 114272 | training loss: 0.0010642384877428412\n",
      "epoch: 5 | 81856 / 114272 | training loss: 0.0006779868854209781\n",
      "epoch: 5 | 81888 / 114272 | training loss: 0.0011125175515189767\n",
      "epoch: 5 | 81920 / 114272 | training loss: 0.0009886589832603931\n",
      "epoch: 5 | 81952 / 114272 | training loss: 0.08172204345464706\n",
      "epoch: 5 | 81984 / 114272 | training loss: 0.0015230518765747547\n",
      "epoch: 5 | 82016 / 114272 | training loss: 0.000619376776739955\n",
      "epoch: 5 | 82048 / 114272 | training loss: 0.0007646744488738477\n",
      "epoch: 5 | 82080 / 114272 | training loss: 0.0007647086749784648\n",
      "epoch: 5 | 82112 / 114272 | training loss: 0.012993314303457737\n",
      "epoch: 5 | 82144 / 114272 | training loss: 0.06775983422994614\n",
      "epoch: 5 | 82176 / 114272 | training loss: 0.17822489142417908\n",
      "epoch: 5 | 82208 / 114272 | training loss: 0.0021451772190630436\n",
      "epoch: 5 | 82240 / 114272 | training loss: 0.0006183520890772343\n",
      "epoch: 5 | 82272 / 114272 | training loss: 0.0003367787867318839\n",
      "epoch: 5 | 82304 / 114272 | training loss: 0.0015102766919881105\n",
      "epoch: 5 | 82336 / 114272 | training loss: 0.0010732648661360145\n",
      "epoch: 5 | 82368 / 114272 | training loss: 0.0033438403625041246\n",
      "epoch: 5 | 82400 / 114272 | training loss: 0.00628111744299531\n",
      "epoch: 5 | 82432 / 114272 | training loss: 0.030319251120090485\n",
      "epoch: 5 | 82464 / 114272 | training loss: 0.26169145107269287\n",
      "epoch: 5 | 82496 / 114272 | training loss: 0.0034562034998089075\n",
      "epoch: 5 | 82528 / 114272 | training loss: 0.003674205159768462\n",
      "epoch: 5 | 82560 / 114272 | training loss: 0.0006303271511569619\n",
      "epoch: 5 | 82592 / 114272 | training loss: 0.0010059127816930413\n",
      "epoch: 5 | 82624 / 114272 | training loss: 0.14843051135540009\n",
      "epoch: 5 | 82656 / 114272 | training loss: 0.17853976786136627\n",
      "epoch: 5 | 82688 / 114272 | training loss: 0.0011957238893955946\n",
      "epoch: 5 | 82720 / 114272 | training loss: 0.09513536095619202\n",
      "epoch: 5 | 82752 / 114272 | training loss: 0.0014184403698891401\n",
      "epoch: 5 | 82784 / 114272 | training loss: 0.18699562549591064\n",
      "epoch: 5 | 82816 / 114272 | training loss: 0.0006999612669460475\n",
      "epoch: 5 | 82848 / 114272 | training loss: 0.0017711265245452523\n",
      "epoch: 5 | 82880 / 114272 | training loss: 0.002039643470197916\n",
      "epoch: 5 | 82912 / 114272 | training loss: 0.023447833955287933\n",
      "epoch: 5 | 82944 / 114272 | training loss: 0.0009771131444722414\n",
      "epoch: 5 | 82976 / 114272 | training loss: 0.0016933815786615014\n",
      "epoch: 5 | 83008 / 114272 | training loss: 0.002368620131164789\n",
      "epoch: 5 | 83040 / 114272 | training loss: 0.0010832727421075106\n",
      "epoch: 5 | 83072 / 114272 | training loss: 0.0025154566392302513\n",
      "epoch: 5 | 83104 / 114272 | training loss: 0.09942267835140228\n",
      "epoch: 5 | 83136 / 114272 | training loss: 0.002554538892582059\n",
      "epoch: 5 | 83168 / 114272 | training loss: 0.14033567905426025\n",
      "epoch: 5 | 83200 / 114272 | training loss: 0.06274978071451187\n",
      "epoch: 5 | 83232 / 114272 | training loss: 0.00046437117271125317\n",
      "epoch: 5 | 83264 / 114272 | training loss: 0.004194919019937515\n",
      "epoch: 5 | 83296 / 114272 | training loss: 0.0006317192455753684\n",
      "epoch: 5 | 83328 / 114272 | training loss: 0.0010231563355773687\n",
      "epoch: 5 | 83360 / 114272 | training loss: 0.002375397365540266\n",
      "epoch: 5 | 83392 / 114272 | training loss: 0.0007398793241009116\n",
      "epoch: 5 | 83424 / 114272 | training loss: 0.0007995475898496807\n",
      "epoch: 5 | 83456 / 114272 | training loss: 0.04265875369310379\n",
      "epoch: 5 | 83488 / 114272 | training loss: 0.013094358146190643\n",
      "epoch: 5 | 83520 / 114272 | training loss: 0.029751988127827644\n",
      "epoch: 5 | 83552 / 114272 | training loss: 0.0006797249661758542\n",
      "epoch: 5 | 83584 / 114272 | training loss: 0.0028926434461027384\n",
      "epoch: 5 | 83616 / 114272 | training loss: 0.007648972794413567\n",
      "epoch: 5 | 83648 / 114272 | training loss: 0.0024622168857604265\n",
      "epoch: 5 | 83680 / 114272 | training loss: 0.0008095915545709431\n",
      "epoch: 5 | 83712 / 114272 | training loss: 0.0032596937380731106\n",
      "epoch: 5 | 83744 / 114272 | training loss: 0.0009131832630373538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 83776 / 114272 | training loss: 0.000722876749932766\n",
      "epoch: 5 | 83808 / 114272 | training loss: 0.0006220632931217551\n",
      "epoch: 5 | 83840 / 114272 | training loss: 0.0012373161735013127\n",
      "epoch: 5 | 83872 / 114272 | training loss: 0.0011705744545906782\n",
      "epoch: 5 | 83904 / 114272 | training loss: 0.00047008669935166836\n",
      "epoch: 5 | 83936 / 114272 | training loss: 0.27446067333221436\n",
      "epoch: 5 | 83968 / 114272 | training loss: 0.008994723670184612\n",
      "epoch: 5 | 84000 / 114272 | training loss: 0.0005275232251733541\n",
      "epoch: 5 | 84032 / 114272 | training loss: 0.0007624375866726041\n",
      "epoch: 5 | 84064 / 114272 | training loss: 0.0005980983260087669\n",
      "epoch: 5 | 84096 / 114272 | training loss: 0.0008882055408321321\n",
      "epoch: 5 | 84128 / 114272 | training loss: 0.0018857634859159589\n",
      "epoch: 5 | 84160 / 114272 | training loss: 0.0007632640190422535\n",
      "epoch: 5 | 84192 / 114272 | training loss: 0.0015560523606836796\n",
      "epoch: 5 | 84224 / 114272 | training loss: 0.0006623318186029792\n",
      "epoch: 5 | 84256 / 114272 | training loss: 0.0015082793543115258\n",
      "epoch: 5 | 84288 / 114272 | training loss: 0.18910780549049377\n",
      "epoch: 5 | 84320 / 114272 | training loss: 0.001410629483871162\n",
      "epoch: 5 | 84352 / 114272 | training loss: 0.0015994850546121597\n",
      "epoch: 5 | 84384 / 114272 | training loss: 0.000698253745213151\n",
      "epoch: 5 | 84416 / 114272 | training loss: 0.009969067759811878\n",
      "epoch: 5 | 84448 / 114272 | training loss: 0.06819110363721848\n",
      "epoch: 5 | 84480 / 114272 | training loss: 0.0009613970760256052\n",
      "epoch: 5 | 84512 / 114272 | training loss: 0.0009127198718488216\n",
      "epoch: 5 | 84544 / 114272 | training loss: 0.002046039793640375\n",
      "epoch: 5 | 84576 / 114272 | training loss: 0.0023971477057784796\n",
      "epoch: 5 | 84608 / 114272 | training loss: 0.0840827003121376\n",
      "epoch: 5 | 84640 / 114272 | training loss: 0.001275951275601983\n",
      "epoch: 5 | 84672 / 114272 | training loss: 0.0017578843981027603\n",
      "epoch: 5 | 84704 / 114272 | training loss: 0.0016284583834931254\n",
      "epoch: 5 | 84736 / 114272 | training loss: 0.003278833581134677\n",
      "epoch: 5 | 84768 / 114272 | training loss: 0.10963774472475052\n",
      "epoch: 5 | 84800 / 114272 | training loss: 0.001364401774480939\n",
      "epoch: 5 | 84832 / 114272 | training loss: 0.0010065545793622732\n",
      "epoch: 5 | 84864 / 114272 | training loss: 0.0016306535108014941\n",
      "epoch: 5 | 84896 / 114272 | training loss: 0.0031090143602341413\n",
      "epoch: 5 | 84928 / 114272 | training loss: 0.0012471729423850775\n",
      "epoch: 5 | 84960 / 114272 | training loss: 0.0028382509481161833\n",
      "epoch: 5 | 84992 / 114272 | training loss: 0.0010137096978724003\n",
      "epoch: 5 | 85024 / 114272 | training loss: 0.0014604124007746577\n",
      "epoch: 5 | 85056 / 114272 | training loss: 0.0011970852501690388\n",
      "epoch: 5 | 85088 / 114272 | training loss: 0.002276418264955282\n",
      "epoch: 5 | 85120 / 114272 | training loss: 0.013197190128266811\n",
      "epoch: 5 | 85152 / 114272 | training loss: 0.012027252465486526\n",
      "epoch: 5 | 85184 / 114272 | training loss: 0.08402438461780548\n",
      "epoch: 5 | 85216 / 114272 | training loss: 0.0024541933089494705\n",
      "epoch: 5 | 85248 / 114272 | training loss: 0.0018779709935188293\n",
      "epoch: 5 | 85280 / 114272 | training loss: 0.0374479740858078\n",
      "epoch: 5 | 85312 / 114272 | training loss: 0.15224391222000122\n",
      "epoch: 5 | 85344 / 114272 | training loss: 0.001335766864940524\n",
      "epoch: 5 | 85376 / 114272 | training loss: 0.002170596970245242\n",
      "epoch: 5 | 85408 / 114272 | training loss: 0.0008273303392343223\n",
      "epoch: 5 | 85440 / 114272 | training loss: 0.1808316707611084\n",
      "epoch: 5 | 85472 / 114272 | training loss: 0.0015175134176388383\n",
      "epoch: 5 | 85504 / 114272 | training loss: 0.0007684202282689512\n",
      "epoch: 5 | 85536 / 114272 | training loss: 0.000648086832370609\n",
      "epoch: 5 | 85568 / 114272 | training loss: 0.09290207922458649\n",
      "epoch: 5 | 85600 / 114272 | training loss: 0.0026329767424613237\n",
      "epoch: 5 | 85632 / 114272 | training loss: 0.0015333552146330476\n",
      "epoch: 5 | 85664 / 114272 | training loss: 0.002817818196490407\n",
      "epoch: 5 | 85696 / 114272 | training loss: 0.11641049385070801\n",
      "epoch: 5 | 85728 / 114272 | training loss: 0.000915100856218487\n",
      "epoch: 5 | 85760 / 114272 | training loss: 0.0013790922239422798\n",
      "epoch: 5 | 85792 / 114272 | training loss: 0.0026919024530798197\n",
      "epoch: 5 | 85824 / 114272 | training loss: 0.003767996095120907\n",
      "epoch: 5 | 85856 / 114272 | training loss: 0.0007424859795719385\n",
      "epoch: 5 | 85888 / 114272 | training loss: 0.0004168273590039462\n",
      "epoch: 5 | 85920 / 114272 | training loss: 0.0027375586796551943\n",
      "epoch: 5 | 85952 / 114272 | training loss: 0.0029765309300273657\n",
      "epoch: 5 | 85984 / 114272 | training loss: 0.0011554594384506345\n",
      "epoch: 5 | 86016 / 114272 | training loss: 0.21450240910053253\n",
      "epoch: 5 | 86048 / 114272 | training loss: 0.002308845752850175\n",
      "epoch: 5 | 86080 / 114272 | training loss: 0.0015184591757133603\n",
      "epoch: 5 | 86112 / 114272 | training loss: 0.001184782013297081\n",
      "epoch: 5 | 86144 / 114272 | training loss: 0.0037487405352294445\n",
      "epoch: 5 | 86176 / 114272 | training loss: 0.013707993552088737\n",
      "epoch: 5 | 86208 / 114272 | training loss: 0.0010451250709593296\n",
      "epoch: 5 | 86240 / 114272 | training loss: 0.0021395706571638584\n",
      "epoch: 5 | 86272 / 114272 | training loss: 0.001185756642371416\n",
      "epoch: 5 | 86304 / 114272 | training loss: 0.000971887435298413\n",
      "epoch: 5 | 86336 / 114272 | training loss: 0.0007274702074937522\n",
      "epoch: 5 | 86368 / 114272 | training loss: 0.0013353123795241117\n",
      "epoch: 5 | 86400 / 114272 | training loss: 0.0007899203919805586\n",
      "epoch: 5 | 86432 / 114272 | training loss: 0.0008965294109657407\n",
      "epoch: 5 | 86464 / 114272 | training loss: 0.0011978866532444954\n",
      "epoch: 5 | 86496 / 114272 | training loss: 0.0006457114941440523\n",
      "epoch: 5 | 86528 / 114272 | training loss: 0.0008939446997828782\n",
      "epoch: 5 | 86560 / 114272 | training loss: 0.0024107638746500015\n",
      "epoch: 5 | 86592 / 114272 | training loss: 0.0009982666233554482\n",
      "epoch: 5 | 86624 / 114272 | training loss: 0.17149654030799866\n",
      "epoch: 5 | 86656 / 114272 | training loss: 0.0005917988019064069\n",
      "epoch: 5 | 86688 / 114272 | training loss: 0.0005645683268085122\n",
      "epoch: 5 | 86720 / 114272 | training loss: 0.00047510265721939504\n",
      "epoch: 5 | 86752 / 114272 | training loss: 0.23769381642341614\n",
      "epoch: 5 | 86784 / 114272 | training loss: 0.0009776290971785784\n",
      "epoch: 5 | 86816 / 114272 | training loss: 0.1829717755317688\n",
      "epoch: 5 | 86848 / 114272 | training loss: 0.0015680420910939574\n",
      "epoch: 5 | 86880 / 114272 | training loss: 0.0009557419107295573\n",
      "epoch: 5 | 86912 / 114272 | training loss: 0.0017406048718839884\n",
      "epoch: 5 | 86944 / 114272 | training loss: 0.0007779992884024978\n",
      "epoch: 5 | 86976 / 114272 | training loss: 0.00657140277326107\n",
      "epoch: 5 | 87008 / 114272 | training loss: 0.127153679728508\n",
      "epoch: 5 | 87040 / 114272 | training loss: 0.002203729236498475\n",
      "epoch: 5 | 87072 / 114272 | training loss: 0.0010814545676112175\n",
      "epoch: 5 | 87104 / 114272 | training loss: 0.0007361759198829532\n",
      "epoch: 5 | 87136 / 114272 | training loss: 0.0011174465762451291\n",
      "epoch: 5 | 87168 / 114272 | training loss: 0.0014333876315504313\n",
      "epoch: 5 | 87200 / 114272 | training loss: 0.0012004106538370252\n",
      "epoch: 5 | 87232 / 114272 | training loss: 0.0015881452709436417\n",
      "epoch: 5 | 87264 / 114272 | training loss: 0.053389061242341995\n",
      "epoch: 5 | 87296 / 114272 | training loss: 0.002182027557864785\n",
      "epoch: 5 | 87328 / 114272 | training loss: 0.0984656885266304\n",
      "epoch: 5 | 87360 / 114272 | training loss: 0.001191688235849142\n",
      "epoch: 5 | 87392 / 114272 | training loss: 0.0015253719175234437\n",
      "epoch: 5 | 87424 / 114272 | training loss: 0.007198194973170757\n",
      "epoch: 5 | 87456 / 114272 | training loss: 0.23144182562828064\n",
      "epoch: 5 | 87488 / 114272 | training loss: 0.0007186637376435101\n",
      "epoch: 5 | 87520 / 114272 | training loss: 0.003179674269631505\n",
      "epoch: 5 | 87552 / 114272 | training loss: 0.01389558706432581\n",
      "epoch: 5 | 87584 / 114272 | training loss: 0.0015878300182521343\n",
      "epoch: 5 | 87616 / 114272 | training loss: 0.0023745440412312746\n",
      "epoch: 5 | 87648 / 114272 | training loss: 0.002112262649461627\n",
      "epoch: 5 | 87680 / 114272 | training loss: 0.0025868918746709824\n",
      "epoch: 5 | 87712 / 114272 | training loss: 0.0014091150369495153\n",
      "epoch: 5 | 87744 / 114272 | training loss: 0.0015709291910752654\n",
      "epoch: 5 | 87776 / 114272 | training loss: 0.0033576893620193005\n",
      "epoch: 5 | 87808 / 114272 | training loss: 0.005186387337744236\n",
      "epoch: 5 | 87840 / 114272 | training loss: 0.27605515718460083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 87872 / 114272 | training loss: 0.003664784599095583\n",
      "epoch: 5 | 87904 / 114272 | training loss: 0.0013722534058615565\n",
      "epoch: 5 | 87936 / 114272 | training loss: 0.0008312977734021842\n",
      "epoch: 5 | 87968 / 114272 | training loss: 0.0012197790201753378\n",
      "epoch: 5 | 88000 / 114272 | training loss: 0.0017617375124245882\n",
      "epoch: 5 | 88032 / 114272 | training loss: 0.136184424161911\n",
      "epoch: 5 | 88064 / 114272 | training loss: 0.0010704989545047283\n",
      "epoch: 5 | 88096 / 114272 | training loss: 0.009777627885341644\n",
      "epoch: 5 | 88128 / 114272 | training loss: 0.0011883267434313893\n",
      "epoch: 5 | 88160 / 114272 | training loss: 0.0006877583800815046\n",
      "epoch: 5 | 88192 / 114272 | training loss: 0.0009136400767602026\n",
      "epoch: 5 | 88224 / 114272 | training loss: 0.134870707988739\n",
      "epoch: 5 | 88256 / 114272 | training loss: 0.0013918906915932894\n",
      "epoch: 5 | 88288 / 114272 | training loss: 0.0007812832482159138\n",
      "epoch: 5 | 88320 / 114272 | training loss: 0.02134285308420658\n",
      "epoch: 5 | 88352 / 114272 | training loss: 0.17349621653556824\n",
      "epoch: 5 | 88384 / 114272 | training loss: 0.0007356415153481066\n",
      "epoch: 5 | 88416 / 114272 | training loss: 0.00399196520447731\n",
      "epoch: 5 | 88448 / 114272 | training loss: 0.0008125543245114386\n",
      "epoch: 5 | 88480 / 114272 | training loss: 0.0016464994987472892\n",
      "epoch: 5 | 88512 / 114272 | training loss: 0.0022893045097589493\n",
      "epoch: 5 | 88544 / 114272 | training loss: 0.0008911529439501464\n",
      "epoch: 5 | 88576 / 114272 | training loss: 0.04222569614648819\n",
      "epoch: 5 | 88608 / 114272 | training loss: 0.0005210652016103268\n",
      "epoch: 5 | 88640 / 114272 | training loss: 0.0010817613219842315\n",
      "epoch: 5 | 88672 / 114272 | training loss: 0.059467215090990067\n",
      "epoch: 5 | 88704 / 114272 | training loss: 0.0012626696843653917\n",
      "epoch: 5 | 88736 / 114272 | training loss: 0.0013944676611572504\n",
      "epoch: 5 | 88768 / 114272 | training loss: 0.001491522416472435\n",
      "epoch: 5 | 88800 / 114272 | training loss: 0.0031971894204616547\n",
      "epoch: 5 | 88832 / 114272 | training loss: 0.2313464879989624\n",
      "epoch: 5 | 88864 / 114272 | training loss: 0.0020743843633681536\n",
      "epoch: 5 | 88896 / 114272 | training loss: 0.0006683032843284309\n",
      "epoch: 5 | 88928 / 114272 | training loss: 0.000934636453166604\n",
      "epoch: 5 | 88960 / 114272 | training loss: 0.0006734690396115184\n",
      "epoch: 5 | 88992 / 114272 | training loss: 0.0007994215120561421\n",
      "epoch: 5 | 89024 / 114272 | training loss: 0.002857140265405178\n",
      "epoch: 5 | 89056 / 114272 | training loss: 0.20749403536319733\n",
      "epoch: 5 | 89088 / 114272 | training loss: 0.002340813633054495\n",
      "epoch: 5 | 89120 / 114272 | training loss: 0.0010731695219874382\n",
      "epoch: 5 | 89152 / 114272 | training loss: 0.15395934879779816\n",
      "epoch: 5 | 89184 / 114272 | training loss: 0.0021028961054980755\n",
      "epoch: 5 | 89216 / 114272 | training loss: 0.07226885855197906\n",
      "epoch: 5 | 89248 / 114272 | training loss: 0.002712899586185813\n",
      "epoch: 5 | 89280 / 114272 | training loss: 0.002904462395235896\n",
      "epoch: 5 | 89312 / 114272 | training loss: 0.0007053544977679849\n",
      "epoch: 5 | 89344 / 114272 | training loss: 0.0013715929817408323\n",
      "epoch: 5 | 89376 / 114272 | training loss: 0.018257172778248787\n",
      "epoch: 5 | 89408 / 114272 | training loss: 0.0013491063145920634\n",
      "epoch: 5 | 89440 / 114272 | training loss: 0.002904429566115141\n",
      "epoch: 5 | 89472 / 114272 | training loss: 0.0018929403740912676\n",
      "epoch: 5 | 89504 / 114272 | training loss: 0.14246056973934174\n",
      "epoch: 5 | 89536 / 114272 | training loss: 0.0016687067691236734\n",
      "epoch: 5 | 89568 / 114272 | training loss: 0.00033441398409195244\n",
      "epoch: 5 | 89600 / 114272 | training loss: 0.0005182644235901535\n",
      "epoch: 5 | 89632 / 114272 | training loss: 0.19662943482398987\n",
      "epoch: 5 | 89664 / 114272 | training loss: 0.004206872545182705\n",
      "epoch: 5 | 89696 / 114272 | training loss: 0.006532574072480202\n",
      "epoch: 5 | 89728 / 114272 | training loss: 0.003420604392886162\n",
      "epoch: 5 | 89760 / 114272 | training loss: 0.0009117888985201716\n",
      "epoch: 5 | 89792 / 114272 | training loss: 0.02763490192592144\n",
      "epoch: 5 | 89824 / 114272 | training loss: 0.0033829952590167522\n",
      "epoch: 5 | 89856 / 114272 | training loss: 0.0013083879603073\n",
      "epoch: 5 | 89888 / 114272 | training loss: 0.002325554611161351\n",
      "epoch: 5 | 89920 / 114272 | training loss: 0.002936688018962741\n",
      "epoch: 5 | 89952 / 114272 | training loss: 0.10648780316114426\n",
      "epoch: 5 | 89984 / 114272 | training loss: 0.029831578955054283\n",
      "epoch: 5 | 90016 / 114272 | training loss: 0.0006062326137907803\n",
      "epoch: 5 | 90048 / 114272 | training loss: 0.0005415853811427951\n",
      "epoch: 5 | 90080 / 114272 | training loss: 0.0009687440469861031\n",
      "epoch: 5 | 90112 / 114272 | training loss: 0.002156370086595416\n",
      "epoch: 5 | 90144 / 114272 | training loss: 0.20635110139846802\n",
      "epoch: 5 | 90176 / 114272 | training loss: 0.0022734268568456173\n",
      "epoch: 5 | 90208 / 114272 | training loss: 0.004579254891723394\n",
      "epoch: 5 | 90240 / 114272 | training loss: 0.11242543905973434\n",
      "epoch: 5 | 90272 / 114272 | training loss: 0.060446761548519135\n",
      "epoch: 5 | 90304 / 114272 | training loss: 0.001133812591433525\n",
      "epoch: 5 | 90336 / 114272 | training loss: 0.0017090511973947287\n",
      "epoch: 5 | 90368 / 114272 | training loss: 0.0020326822996139526\n",
      "epoch: 5 | 90400 / 114272 | training loss: 0.09040907025337219\n",
      "epoch: 5 | 90432 / 114272 | training loss: 0.0013070564018562436\n",
      "epoch: 5 | 90464 / 114272 | training loss: 0.0016162419924512506\n",
      "epoch: 5 | 90496 / 114272 | training loss: 0.0011326466919854283\n",
      "epoch: 5 | 90528 / 114272 | training loss: 0.0048534381203353405\n",
      "epoch: 5 | 90560 / 114272 | training loss: 0.0036341592203825712\n",
      "epoch: 5 | 90592 / 114272 | training loss: 0.0010545039549469948\n",
      "epoch: 5 | 90624 / 114272 | training loss: 0.0013864701613783836\n",
      "epoch: 5 | 90656 / 114272 | training loss: 0.0014726404333487153\n",
      "epoch: 5 | 90688 / 114272 | training loss: 0.0008529603946954012\n",
      "epoch: 5 | 90720 / 114272 | training loss: 0.0010184552520513535\n",
      "epoch: 5 | 90752 / 114272 | training loss: 0.0010315717663615942\n",
      "epoch: 5 | 90784 / 114272 | training loss: 0.0004927635891363025\n",
      "epoch: 5 | 90816 / 114272 | training loss: 0.0006443328456953168\n",
      "epoch: 5 | 90848 / 114272 | training loss: 0.0008344945963472128\n",
      "epoch: 5 | 90880 / 114272 | training loss: 0.001064781565219164\n",
      "epoch: 5 | 90912 / 114272 | training loss: 0.0007695218664593995\n",
      "epoch: 5 | 90944 / 114272 | training loss: 0.0006473592366091907\n",
      "epoch: 5 | 90976 / 114272 | training loss: 0.0005303253419697285\n",
      "epoch: 5 | 91008 / 114272 | training loss: 0.009520823135972023\n",
      "epoch: 5 | 91040 / 114272 | training loss: 0.003670186037197709\n",
      "epoch: 5 | 91072 / 114272 | training loss: 0.0009755338542163372\n",
      "epoch: 5 | 91104 / 114272 | training loss: 0.0007930410210974514\n",
      "epoch: 5 | 91136 / 114272 | training loss: 0.005499304737895727\n",
      "epoch: 5 | 91168 / 114272 | training loss: 0.0006416426622308791\n",
      "epoch: 5 | 91200 / 114272 | training loss: 0.000691652821842581\n",
      "epoch: 5 | 91232 / 114272 | training loss: 0.0006885803304612637\n",
      "epoch: 5 | 91264 / 114272 | training loss: 0.0011089348699897528\n",
      "epoch: 5 | 91296 / 114272 | training loss: 0.0004888811381533742\n",
      "epoch: 5 | 91328 / 114272 | training loss: 0.0005343020311556756\n",
      "epoch: 5 | 91360 / 114272 | training loss: 0.0008382530068047345\n",
      "epoch: 5 | 91392 / 114272 | training loss: 0.0007386313518509269\n",
      "epoch: 5 | 91424 / 114272 | training loss: 0.15201492607593536\n",
      "epoch: 5 | 91456 / 114272 | training loss: 0.001633523148484528\n",
      "epoch: 5 | 91488 / 114272 | training loss: 0.0006768028251826763\n",
      "epoch: 5 | 91520 / 114272 | training loss: 0.0005731699638999999\n",
      "epoch: 5 | 91552 / 114272 | training loss: 0.0007799644372425973\n",
      "epoch: 5 | 91584 / 114272 | training loss: 0.0005993878003209829\n",
      "epoch: 5 | 91616 / 114272 | training loss: 0.0010916243772953749\n",
      "epoch: 5 | 91648 / 114272 | training loss: 0.0010604610433802009\n",
      "epoch: 5 | 91680 / 114272 | training loss: 0.019646229222416878\n",
      "epoch: 5 | 91712 / 114272 | training loss: 0.0005794737953692675\n",
      "epoch: 5 | 91744 / 114272 | training loss: 0.11589459329843521\n",
      "epoch: 5 | 91776 / 114272 | training loss: 0.1250050663948059\n",
      "epoch: 5 | 91808 / 114272 | training loss: 0.0009799356339499354\n",
      "epoch: 5 | 91840 / 114272 | training loss: 0.0003118301974609494\n",
      "epoch: 5 | 91872 / 114272 | training loss: 0.22027575969696045\n",
      "epoch: 5 | 91904 / 114272 | training loss: 0.0012261923402547836\n",
      "epoch: 5 | 91936 / 114272 | training loss: 0.0048612141981720924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 91968 / 114272 | training loss: 0.0005782721564173698\n",
      "epoch: 5 | 92000 / 114272 | training loss: 0.0003939570742659271\n",
      "epoch: 5 | 92032 / 114272 | training loss: 0.0006695938063785434\n",
      "epoch: 5 | 92064 / 114272 | training loss: 0.0007899750489741564\n",
      "epoch: 5 | 92096 / 114272 | training loss: 0.0011333359871059656\n",
      "epoch: 5 | 92128 / 114272 | training loss: 0.0003717115323524922\n",
      "epoch: 5 | 92160 / 114272 | training loss: 0.0007018946344032884\n",
      "epoch: 5 | 92192 / 114272 | training loss: 0.0008965057204477489\n",
      "epoch: 5 | 92224 / 114272 | training loss: 0.0008348572882823646\n",
      "epoch: 5 | 92256 / 114272 | training loss: 0.0004551099264062941\n",
      "epoch: 5 | 92288 / 114272 | training loss: 0.0005517597310245037\n",
      "epoch: 5 | 92320 / 114272 | training loss: 0.0011045876890420914\n",
      "epoch: 5 | 92352 / 114272 | training loss: 0.0005003549158573151\n",
      "epoch: 5 | 92384 / 114272 | training loss: 0.46159911155700684\n",
      "epoch: 5 | 92416 / 114272 | training loss: 0.0007421633345074952\n",
      "epoch: 5 | 92448 / 114272 | training loss: 0.0016529458807781339\n",
      "epoch: 5 | 92480 / 114272 | training loss: 0.0021752198226749897\n",
      "epoch: 5 | 92512 / 114272 | training loss: 0.09438706934452057\n",
      "epoch: 5 | 92544 / 114272 | training loss: 0.3530469536781311\n",
      "epoch: 5 | 92576 / 114272 | training loss: 0.0007383891497738659\n",
      "epoch: 5 | 92608 / 114272 | training loss: 0.0005385197582654655\n",
      "epoch: 5 | 92640 / 114272 | training loss: 0.0008944476721808314\n",
      "epoch: 5 | 92672 / 114272 | training loss: 0.0012044054456055164\n",
      "epoch: 5 | 92704 / 114272 | training loss: 0.3960008919239044\n",
      "epoch: 5 | 92736 / 114272 | training loss: 0.0003399512788746506\n",
      "epoch: 5 | 92768 / 114272 | training loss: 0.0018210592679679394\n",
      "epoch: 5 | 92800 / 114272 | training loss: 0.0006406200700439513\n",
      "epoch: 5 | 92832 / 114272 | training loss: 0.05698637291789055\n",
      "epoch: 5 | 92864 / 114272 | training loss: 0.0026197247207164764\n",
      "epoch: 5 | 92896 / 114272 | training loss: 0.06354700028896332\n",
      "epoch: 5 | 92928 / 114272 | training loss: 0.0005761718493886292\n",
      "epoch: 5 | 92960 / 114272 | training loss: 0.10530253499746323\n",
      "epoch: 5 | 92992 / 114272 | training loss: 0.002336536766961217\n",
      "epoch: 5 | 93024 / 114272 | training loss: 0.00039630357059650123\n",
      "epoch: 5 | 93056 / 114272 | training loss: 0.24833302199840546\n",
      "epoch: 5 | 93088 / 114272 | training loss: 0.0019664252176880836\n",
      "epoch: 5 | 93120 / 114272 | training loss: 0.0018581798067316413\n",
      "epoch: 5 | 93152 / 114272 | training loss: 0.0004554499755613506\n",
      "epoch: 5 | 93184 / 114272 | training loss: 0.0005924231372773647\n",
      "epoch: 5 | 93216 / 114272 | training loss: 0.000560171261895448\n",
      "epoch: 5 | 93248 / 114272 | training loss: 0.0005714388098567724\n",
      "epoch: 5 | 93280 / 114272 | training loss: 0.0005633680848404765\n",
      "epoch: 5 | 93312 / 114272 | training loss: 0.0006666409899480641\n",
      "epoch: 5 | 93344 / 114272 | training loss: 0.0011067556915804744\n",
      "epoch: 5 | 93376 / 114272 | training loss: 0.0005298979813233018\n",
      "epoch: 5 | 93408 / 114272 | training loss: 0.001565544051118195\n",
      "epoch: 5 | 93440 / 114272 | training loss: 0.0005001771496608853\n",
      "epoch: 5 | 93472 / 114272 | training loss: 0.00303502450697124\n",
      "epoch: 5 | 93504 / 114272 | training loss: 0.004922040738165379\n",
      "epoch: 5 | 93536 / 114272 | training loss: 0.0004749504732899368\n",
      "epoch: 5 | 93568 / 114272 | training loss: 0.0007047122926451266\n",
      "epoch: 5 | 93600 / 114272 | training loss: 0.001849133288487792\n",
      "epoch: 5 | 93632 / 114272 | training loss: 0.0009360383846797049\n",
      "epoch: 5 | 93664 / 114272 | training loss: 0.0004843035130761564\n",
      "epoch: 5 | 93696 / 114272 | training loss: 0.0007104454562067986\n",
      "epoch: 5 | 93728 / 114272 | training loss: 0.05821121111512184\n",
      "epoch: 5 | 93760 / 114272 | training loss: 0.1976056545972824\n",
      "epoch: 5 | 93792 / 114272 | training loss: 0.0005564828752540052\n",
      "epoch: 5 | 93824 / 114272 | training loss: 0.0006597612518817186\n",
      "epoch: 5 | 93856 / 114272 | training loss: 0.0005918890819884837\n",
      "epoch: 5 | 93888 / 114272 | training loss: 0.0949539765715599\n",
      "epoch: 5 | 93920 / 114272 | training loss: 0.12298981100320816\n",
      "epoch: 5 | 93952 / 114272 | training loss: 0.00188423169311136\n",
      "epoch: 5 | 93984 / 114272 | training loss: 0.0005031533073633909\n",
      "epoch: 5 | 94016 / 114272 | training loss: 0.00041950936429202557\n",
      "epoch: 5 | 94048 / 114272 | training loss: 0.000485269702039659\n",
      "epoch: 5 | 94080 / 114272 | training loss: 0.00039961980655789375\n",
      "epoch: 5 | 94112 / 114272 | training loss: 0.0067380559630692005\n",
      "epoch: 5 | 94144 / 114272 | training loss: 0.0007871935376897454\n",
      "epoch: 5 | 94176 / 114272 | training loss: 0.06955865025520325\n",
      "epoch: 5 | 94208 / 114272 | training loss: 0.0030584344640374184\n",
      "epoch: 5 | 94240 / 114272 | training loss: 0.0010034089209511876\n",
      "epoch: 5 | 94272 / 114272 | training loss: 0.0007289350032806396\n",
      "epoch: 5 | 94304 / 114272 | training loss: 0.0005031421314924955\n",
      "epoch: 5 | 94336 / 114272 | training loss: 0.0009420111309736967\n",
      "epoch: 5 | 94368 / 114272 | training loss: 0.0007199249230325222\n",
      "epoch: 5 | 94400 / 114272 | training loss: 0.000300175859592855\n",
      "epoch: 5 | 94432 / 114272 | training loss: 0.0019246815936639905\n",
      "epoch: 5 | 94464 / 114272 | training loss: 0.0005461088730953634\n",
      "epoch: 5 | 94496 / 114272 | training loss: 0.11843881011009216\n",
      "epoch: 5 | 94528 / 114272 | training loss: 0.0006538555026054382\n",
      "epoch: 5 | 94560 / 114272 | training loss: 0.0004272174264770001\n",
      "epoch: 5 | 94592 / 114272 | training loss: 0.0008958298712968826\n",
      "epoch: 5 | 94624 / 114272 | training loss: 0.002891289535909891\n",
      "epoch: 5 | 94656 / 114272 | training loss: 0.11716246604919434\n",
      "epoch: 5 | 94688 / 114272 | training loss: 0.0023064047563821077\n",
      "epoch: 5 | 94720 / 114272 | training loss: 0.011467359960079193\n",
      "epoch: 5 | 94752 / 114272 | training loss: 0.0004158677184022963\n",
      "epoch: 5 | 94784 / 114272 | training loss: 0.0008378996863029897\n",
      "epoch: 5 | 94816 / 114272 | training loss: 0.06285770982503891\n",
      "epoch: 5 | 94848 / 114272 | training loss: 0.002638920908793807\n",
      "epoch: 5 | 94880 / 114272 | training loss: 0.13961218297481537\n",
      "epoch: 5 | 94912 / 114272 | training loss: 0.0006630429998040199\n",
      "epoch: 5 | 94944 / 114272 | training loss: 0.0005822145030833781\n",
      "epoch: 5 | 94976 / 114272 | training loss: 0.005588885862380266\n",
      "epoch: 5 | 95008 / 114272 | training loss: 0.000644085870590061\n",
      "epoch: 5 | 95040 / 114272 | training loss: 0.014299212023615837\n",
      "epoch: 5 | 95072 / 114272 | training loss: 0.0017738237511366606\n",
      "epoch: 5 | 95104 / 114272 | training loss: 0.0006933519034646451\n",
      "epoch: 5 | 95136 / 114272 | training loss: 0.0003212930459994823\n",
      "epoch: 5 | 95168 / 114272 | training loss: 0.00043303114944137633\n",
      "epoch: 5 | 95200 / 114272 | training loss: 0.2566388249397278\n",
      "epoch: 5 | 95232 / 114272 | training loss: 0.0005772947333753109\n",
      "epoch: 5 | 95264 / 114272 | training loss: 0.0004928840789943933\n",
      "epoch: 5 | 95296 / 114272 | training loss: 0.0016967571573331952\n",
      "epoch: 5 | 95328 / 114272 | training loss: 0.001321472693234682\n",
      "epoch: 5 | 95360 / 114272 | training loss: 0.0004931024159304798\n",
      "epoch: 5 | 95392 / 114272 | training loss: 0.013661632314324379\n",
      "epoch: 5 | 95424 / 114272 | training loss: 0.0014339641202241182\n",
      "epoch: 5 | 95456 / 114272 | training loss: 0.0004283426678739488\n",
      "epoch: 5 | 95488 / 114272 | training loss: 0.000872150412760675\n",
      "epoch: 5 | 95520 / 114272 | training loss: 0.005794861353933811\n",
      "epoch: 5 | 95552 / 114272 | training loss: 0.0006116188014857471\n",
      "epoch: 5 | 95584 / 114272 | training loss: 0.0008080254774540663\n",
      "epoch: 5 | 95616 / 114272 | training loss: 0.0010667870519682765\n",
      "epoch: 5 | 95648 / 114272 | training loss: 0.00039823041879571974\n",
      "epoch: 5 | 95680 / 114272 | training loss: 0.0005533126532100141\n",
      "epoch: 5 | 95712 / 114272 | training loss: 0.16770967841148376\n",
      "epoch: 5 | 95744 / 114272 | training loss: 0.001432788441888988\n",
      "epoch: 5 | 95776 / 114272 | training loss: 0.0005830136360600591\n",
      "epoch: 5 | 95808 / 114272 | training loss: 0.0015064955223351717\n",
      "epoch: 5 | 95840 / 114272 | training loss: 0.0005697401356883347\n",
      "epoch: 5 | 95872 / 114272 | training loss: 0.0005730206612497568\n",
      "epoch: 5 | 95904 / 114272 | training loss: 0.19448097050189972\n",
      "epoch: 5 | 95936 / 114272 | training loss: 0.0013343299506232142\n",
      "epoch: 5 | 95968 / 114272 | training loss: 0.0007692992803640664\n",
      "epoch: 5 | 96000 / 114272 | training loss: 0.00043578867916949093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 96032 / 114272 | training loss: 0.000746384437661618\n",
      "epoch: 5 | 96064 / 114272 | training loss: 0.0006573130958713591\n",
      "epoch: 5 | 96096 / 114272 | training loss: 0.0006672314484603703\n",
      "epoch: 5 | 96128 / 114272 | training loss: 0.059459950774908066\n",
      "epoch: 5 | 96160 / 114272 | training loss: 0.0007460517808794975\n",
      "epoch: 5 | 96192 / 114272 | training loss: 0.0007938108174130321\n",
      "epoch: 5 | 96224 / 114272 | training loss: 0.0009786575101315975\n",
      "epoch: 5 | 96256 / 114272 | training loss: 0.10985640436410904\n",
      "epoch: 5 | 96288 / 114272 | training loss: 0.0006873353850096464\n",
      "epoch: 5 | 96320 / 114272 | training loss: 0.0006244424148462713\n",
      "epoch: 5 | 96352 / 114272 | training loss: 0.0005016857176087797\n",
      "epoch: 5 | 96384 / 114272 | training loss: 0.23083728551864624\n",
      "epoch: 5 | 96416 / 114272 | training loss: 0.0009346972219645977\n",
      "epoch: 5 | 96448 / 114272 | training loss: 0.0010492362780496478\n",
      "epoch: 5 | 96480 / 114272 | training loss: 0.018379835411906242\n",
      "epoch: 5 | 96512 / 114272 | training loss: 0.000902718398720026\n",
      "epoch: 5 | 96544 / 114272 | training loss: 0.0006766291335225105\n",
      "epoch: 5 | 96576 / 114272 | training loss: 0.002762620570138097\n",
      "epoch: 5 | 96608 / 114272 | training loss: 0.0010910282144322991\n",
      "epoch: 5 | 96640 / 114272 | training loss: 0.21563208103179932\n",
      "epoch: 5 | 96672 / 114272 | training loss: 0.0003995759761892259\n",
      "epoch: 5 | 96704 / 114272 | training loss: 0.0006538974703289568\n",
      "epoch: 5 | 96736 / 114272 | training loss: 0.10614092648029327\n",
      "epoch: 5 | 96768 / 114272 | training loss: 0.21382437646389008\n",
      "epoch: 5 | 96800 / 114272 | training loss: 0.21554142236709595\n",
      "epoch: 5 | 96832 / 114272 | training loss: 0.21140938997268677\n",
      "epoch: 5 | 96864 / 114272 | training loss: 0.000489767815452069\n",
      "epoch: 5 | 96896 / 114272 | training loss: 0.000653724477160722\n",
      "epoch: 5 | 96928 / 114272 | training loss: 0.0006671171868219972\n",
      "epoch: 5 | 96960 / 114272 | training loss: 0.0014789223205298185\n",
      "epoch: 5 | 96992 / 114272 | training loss: 0.0038696567062288523\n",
      "epoch: 5 | 97024 / 114272 | training loss: 0.23369041085243225\n",
      "epoch: 5 | 97056 / 114272 | training loss: 0.0011005363194271922\n",
      "epoch: 5 | 97088 / 114272 | training loss: 0.001877865637652576\n",
      "epoch: 5 | 97120 / 114272 | training loss: 0.0009611841524019837\n",
      "epoch: 5 | 97152 / 114272 | training loss: 0.001242471975274384\n",
      "epoch: 5 | 97184 / 114272 | training loss: 0.14068914949893951\n",
      "epoch: 5 | 97216 / 114272 | training loss: 0.001539641642011702\n",
      "epoch: 5 | 97248 / 114272 | training loss: 0.0017626399639993906\n",
      "epoch: 5 | 97280 / 114272 | training loss: 0.0012353224446997046\n",
      "epoch: 5 | 97312 / 114272 | training loss: 0.25950467586517334\n",
      "epoch: 5 | 97344 / 114272 | training loss: 0.002333801006898284\n",
      "epoch: 5 | 97376 / 114272 | training loss: 0.0026991383638232946\n",
      "epoch: 5 | 97408 / 114272 | training loss: 0.0037312947679311037\n",
      "epoch: 5 | 97440 / 114272 | training loss: 0.13343824446201324\n",
      "epoch: 5 | 97472 / 114272 | training loss: 0.001379945664666593\n",
      "epoch: 5 | 97504 / 114272 | training loss: 0.0015753835905343294\n",
      "epoch: 5 | 97536 / 114272 | training loss: 0.0033690391574054956\n",
      "epoch: 5 | 97568 / 114272 | training loss: 0.0009814146906137466\n",
      "epoch: 5 | 97600 / 114272 | training loss: 0.24060776829719543\n",
      "epoch: 5 | 97632 / 114272 | training loss: 0.0031689705792814493\n",
      "epoch: 5 | 97664 / 114272 | training loss: 0.005509005859494209\n",
      "epoch: 5 | 97696 / 114272 | training loss: 0.002055116929113865\n",
      "epoch: 5 | 97728 / 114272 | training loss: 0.0012615727027878165\n",
      "epoch: 5 | 97760 / 114272 | training loss: 0.00134842109400779\n",
      "epoch: 5 | 97792 / 114272 | training loss: 0.0013746526092290878\n",
      "epoch: 5 | 97824 / 114272 | training loss: 0.0035418428014963865\n",
      "epoch: 5 | 97856 / 114272 | training loss: 0.003468617796897888\n",
      "epoch: 5 | 97888 / 114272 | training loss: 0.0019344715401530266\n",
      "epoch: 5 | 97920 / 114272 | training loss: 0.0017214438412338495\n",
      "epoch: 5 | 97952 / 114272 | training loss: 0.003292272100225091\n",
      "epoch: 5 | 97984 / 114272 | training loss: 0.16542133688926697\n",
      "epoch: 5 | 98016 / 114272 | training loss: 0.03417148068547249\n",
      "epoch: 5 | 98048 / 114272 | training loss: 0.0015444268938153982\n",
      "epoch: 5 | 98080 / 114272 | training loss: 0.12430918961763382\n",
      "epoch: 5 | 98112 / 114272 | training loss: 0.001130281132645905\n",
      "epoch: 5 | 98144 / 114272 | training loss: 0.0031833485700190067\n",
      "epoch: 5 | 98176 / 114272 | training loss: 0.0018456534016877413\n",
      "epoch: 5 | 98208 / 114272 | training loss: 0.002466688398271799\n",
      "epoch: 5 | 98240 / 114272 | training loss: 0.001395441242493689\n",
      "epoch: 5 | 98272 / 114272 | training loss: 0.001212996430695057\n",
      "epoch: 5 | 98304 / 114272 | training loss: 0.004572046920657158\n",
      "epoch: 5 | 98336 / 114272 | training loss: 0.0029400994535535574\n",
      "epoch: 5 | 98368 / 114272 | training loss: 0.0009456863044761121\n",
      "epoch: 5 | 98400 / 114272 | training loss: 0.0016277272952720523\n",
      "epoch: 5 | 98432 / 114272 | training loss: 0.005389176309108734\n",
      "epoch: 5 | 98464 / 114272 | training loss: 0.0010229091858491302\n",
      "epoch: 5 | 98496 / 114272 | training loss: 0.001953618135303259\n",
      "epoch: 5 | 98528 / 114272 | training loss: 0.0020825585816055536\n",
      "epoch: 5 | 98560 / 114272 | training loss: 0.0026250279042869806\n",
      "epoch: 5 | 98592 / 114272 | training loss: 0.0009808528702706099\n",
      "epoch: 5 | 98624 / 114272 | training loss: 0.0021417071111500263\n",
      "epoch: 5 | 98656 / 114272 | training loss: 0.01977432891726494\n",
      "epoch: 5 | 98688 / 114272 | training loss: 0.001525567495264113\n",
      "epoch: 5 | 98720 / 114272 | training loss: 0.0016272658249363303\n",
      "epoch: 5 | 98752 / 114272 | training loss: 0.0020370781421661377\n",
      "epoch: 5 | 98784 / 114272 | training loss: 0.0017576803220435977\n",
      "epoch: 5 | 98816 / 114272 | training loss: 0.23339961469173431\n",
      "epoch: 5 | 98848 / 114272 | training loss: 0.2420513927936554\n",
      "epoch: 5 | 98880 / 114272 | training loss: 0.0009377499809488654\n",
      "epoch: 5 | 98912 / 114272 | training loss: 0.025984490290284157\n",
      "epoch: 5 | 98944 / 114272 | training loss: 0.022193172946572304\n",
      "epoch: 5 | 98976 / 114272 | training loss: 0.01851644739508629\n",
      "epoch: 5 | 99008 / 114272 | training loss: 0.0012062740279361606\n",
      "epoch: 5 | 99040 / 114272 | training loss: 0.0030000831466168165\n",
      "epoch: 5 | 99072 / 114272 | training loss: 0.22970618307590485\n",
      "epoch: 5 | 99104 / 114272 | training loss: 0.030360201373696327\n",
      "epoch: 5 | 99136 / 114272 | training loss: 0.0035683317109942436\n",
      "epoch: 5 | 99168 / 114272 | training loss: 0.0025550867430865765\n",
      "epoch: 5 | 99200 / 114272 | training loss: 0.20651941001415253\n",
      "epoch: 5 | 99232 / 114272 | training loss: 0.008751966059207916\n",
      "epoch: 5 | 99264 / 114272 | training loss: 0.0015211778227239847\n",
      "epoch: 5 | 99296 / 114272 | training loss: 0.002930573420599103\n",
      "epoch: 5 | 99328 / 114272 | training loss: 0.01567676104605198\n",
      "epoch: 5 | 99360 / 114272 | training loss: 0.09006951004266739\n",
      "epoch: 5 | 99392 / 114272 | training loss: 0.001765281311236322\n",
      "epoch: 5 | 99424 / 114272 | training loss: 0.0015487285563722253\n",
      "epoch: 5 | 99456 / 114272 | training loss: 0.004700364079326391\n",
      "epoch: 5 | 99488 / 114272 | training loss: 0.02627057209610939\n",
      "epoch: 5 | 99520 / 114272 | training loss: 0.0015910471556708217\n",
      "epoch: 5 | 99552 / 114272 | training loss: 0.0013459513429552317\n",
      "epoch: 5 | 99584 / 114272 | training loss: 0.0023159109987318516\n",
      "epoch: 5 | 99616 / 114272 | training loss: 0.0016807408537715673\n",
      "epoch: 5 | 99648 / 114272 | training loss: 0.0012852586805820465\n",
      "epoch: 5 | 99680 / 114272 | training loss: 0.002633030992001295\n",
      "epoch: 5 | 99712 / 114272 | training loss: 0.0017780722118914127\n",
      "epoch: 5 | 99744 / 114272 | training loss: 0.1362389624118805\n",
      "epoch: 5 | 99776 / 114272 | training loss: 0.0013409964740276337\n",
      "epoch: 5 | 99808 / 114272 | training loss: 0.001567029976285994\n",
      "epoch: 5 | 99840 / 114272 | training loss: 0.0010345103219151497\n",
      "epoch: 5 | 99872 / 114272 | training loss: 0.002443206263706088\n",
      "epoch: 5 | 99904 / 114272 | training loss: 0.0021039480343461037\n",
      "epoch: 5 | 99936 / 114272 | training loss: 0.001540640601888299\n",
      "epoch: 5 | 99968 / 114272 | training loss: 0.3652438819408417\n",
      "epoch: 5 | 100000 / 114272 | training loss: 0.00225074402987957\n",
      "epoch: 5 | 100032 / 114272 | training loss: 0.001418832689523697\n",
      "epoch: 5 | 100064 / 114272 | training loss: 0.10881627351045609\n",
      "epoch: 5 | 100096 / 114272 | training loss: 0.1986449807882309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 100128 / 114272 | training loss: 0.0019303897861391306\n",
      "epoch: 5 | 100160 / 114272 | training loss: 0.1254296600818634\n",
      "epoch: 5 | 100192 / 114272 | training loss: 0.0009762005647644401\n",
      "epoch: 5 | 100224 / 114272 | training loss: 0.0015020361170172691\n",
      "epoch: 5 | 100256 / 114272 | training loss: 0.0015030731447041035\n",
      "epoch: 5 | 100288 / 114272 | training loss: 0.0019095420138910413\n",
      "epoch: 5 | 100320 / 114272 | training loss: 0.0015175646403804421\n",
      "epoch: 5 | 100352 / 114272 | training loss: 0.0047889286652207375\n",
      "epoch: 5 | 100384 / 114272 | training loss: 0.001307332655414939\n",
      "epoch: 5 | 100416 / 114272 | training loss: 0.001444231253117323\n",
      "epoch: 5 | 100448 / 114272 | training loss: 0.0013487056130543351\n",
      "epoch: 5 | 100480 / 114272 | training loss: 0.19618721306324005\n",
      "epoch: 5 | 100512 / 114272 | training loss: 0.0019025790970772505\n",
      "epoch: 5 | 100544 / 114272 | training loss: 0.22371289134025574\n",
      "epoch: 5 | 100576 / 114272 | training loss: 0.0022686428856104612\n",
      "epoch: 5 | 100608 / 114272 | training loss: 0.0634969100356102\n",
      "epoch: 5 | 100640 / 114272 | training loss: 0.002163634868338704\n",
      "epoch: 5 | 100672 / 114272 | training loss: 0.0032007540576159954\n",
      "epoch: 5 | 100704 / 114272 | training loss: 0.004087378270924091\n",
      "epoch: 5 | 100736 / 114272 | training loss: 0.19829675555229187\n",
      "epoch: 5 | 100768 / 114272 | training loss: 0.003442364977672696\n",
      "epoch: 5 | 100800 / 114272 | training loss: 0.001462840591557324\n",
      "epoch: 5 | 100832 / 114272 | training loss: 0.0024068544153124094\n",
      "epoch: 5 | 100864 / 114272 | training loss: 0.005633787717670202\n",
      "epoch: 5 | 100896 / 114272 | training loss: 0.003444438800215721\n",
      "epoch: 5 | 100928 / 114272 | training loss: 0.001341800088994205\n",
      "epoch: 5 | 100960 / 114272 | training loss: 0.003880287753418088\n",
      "epoch: 5 | 100992 / 114272 | training loss: 0.0046228887513279915\n",
      "epoch: 5 | 101024 / 114272 | training loss: 0.0030184939969331026\n",
      "epoch: 5 | 101056 / 114272 | training loss: 0.08900141716003418\n",
      "epoch: 5 | 101088 / 114272 | training loss: 0.0021514398977160454\n",
      "epoch: 5 | 101120 / 114272 | training loss: 0.00249619223177433\n",
      "epoch: 5 | 101152 / 114272 | training loss: 0.0016012730775400996\n",
      "epoch: 5 | 101184 / 114272 | training loss: 0.0026732617989182472\n",
      "epoch: 5 | 101216 / 114272 | training loss: 0.0026957280933856964\n",
      "epoch: 5 | 101248 / 114272 | training loss: 0.002816359279677272\n",
      "epoch: 5 | 101280 / 114272 | training loss: 0.057738594710826874\n",
      "epoch: 5 | 101312 / 114272 | training loss: 0.0020656047854572535\n",
      "epoch: 5 | 101344 / 114272 | training loss: 0.006464146077632904\n",
      "epoch: 5 | 101376 / 114272 | training loss: 0.17852896451950073\n",
      "epoch: 5 | 101408 / 114272 | training loss: 0.003078257432207465\n",
      "epoch: 5 | 101440 / 114272 | training loss: 0.0009197478648275137\n",
      "epoch: 5 | 101472 / 114272 | training loss: 0.0016331208171322942\n",
      "epoch: 5 | 101504 / 114272 | training loss: 0.005166939925402403\n",
      "epoch: 5 | 101536 / 114272 | training loss: 0.0028844333719462156\n",
      "epoch: 5 | 101568 / 114272 | training loss: 0.013576098717749119\n",
      "epoch: 5 | 101600 / 114272 | training loss: 0.001283008954487741\n",
      "epoch: 5 | 101632 / 114272 | training loss: 0.0011983164586126804\n",
      "epoch: 5 | 101664 / 114272 | training loss: 0.002180687617510557\n",
      "epoch: 5 | 101696 / 114272 | training loss: 0.003309941617771983\n",
      "epoch: 5 | 101728 / 114272 | training loss: 0.10130809247493744\n",
      "epoch: 5 | 101760 / 114272 | training loss: 0.003485970664769411\n",
      "epoch: 5 | 101792 / 114272 | training loss: 0.0025767923798412085\n",
      "epoch: 5 | 101824 / 114272 | training loss: 0.0018517919816076756\n",
      "epoch: 5 | 101856 / 114272 | training loss: 0.005544722080230713\n",
      "epoch: 5 | 101888 / 114272 | training loss: 0.0011850794544443488\n",
      "epoch: 5 | 101920 / 114272 | training loss: 0.004098356701433659\n",
      "epoch: 5 | 101952 / 114272 | training loss: 0.0017893094336614013\n",
      "epoch: 5 | 101984 / 114272 | training loss: 0.006250868085771799\n",
      "epoch: 5 | 102016 / 114272 | training loss: 0.00438498193398118\n",
      "epoch: 5 | 102048 / 114272 | training loss: 0.005276484414935112\n",
      "epoch: 5 | 102080 / 114272 | training loss: 0.002806612988933921\n",
      "epoch: 5 | 102112 / 114272 | training loss: 0.0007036594906821847\n",
      "epoch: 5 | 102144 / 114272 | training loss: 0.11926709115505219\n",
      "epoch: 5 | 102176 / 114272 | training loss: 0.0015163534553721547\n",
      "epoch: 5 | 102208 / 114272 | training loss: 0.001346561941318214\n",
      "epoch: 5 | 102240 / 114272 | training loss: 0.0015114169800654054\n",
      "epoch: 5 | 102272 / 114272 | training loss: 0.0025689213071018457\n",
      "epoch: 5 | 102304 / 114272 | training loss: 0.004297942854464054\n",
      "epoch: 5 | 102336 / 114272 | training loss: 0.001443287474103272\n",
      "epoch: 5 | 102368 / 114272 | training loss: 0.0017744809156283736\n",
      "epoch: 5 | 102400 / 114272 | training loss: 0.0014232347020879388\n",
      "epoch: 5 | 102432 / 114272 | training loss: 0.0009043331374414265\n",
      "epoch: 5 | 102464 / 114272 | training loss: 0.0012535224668681622\n",
      "epoch: 5 | 102496 / 114272 | training loss: 0.0023256137501448393\n",
      "epoch: 5 | 102528 / 114272 | training loss: 0.0030803459230810404\n",
      "epoch: 5 | 102560 / 114272 | training loss: 0.0025502434000372887\n",
      "epoch: 5 | 102592 / 114272 | training loss: 0.0020148777402937412\n",
      "epoch: 5 | 102624 / 114272 | training loss: 0.0010050361743196845\n",
      "epoch: 5 | 102656 / 114272 | training loss: 0.0012535526184365153\n",
      "epoch: 5 | 102688 / 114272 | training loss: 0.004256250336766243\n",
      "epoch: 5 | 102720 / 114272 | training loss: 0.0009481097804382443\n",
      "epoch: 5 | 102752 / 114272 | training loss: 0.0012795297661796212\n",
      "epoch: 5 | 102784 / 114272 | training loss: 0.0006027600029483438\n",
      "epoch: 5 | 102816 / 114272 | training loss: 0.0011875153286382556\n",
      "epoch: 5 | 102848 / 114272 | training loss: 0.08501801639795303\n",
      "epoch: 5 | 102880 / 114272 | training loss: 0.0015940541634336114\n",
      "epoch: 5 | 102912 / 114272 | training loss: 0.006010790821164846\n",
      "epoch: 5 | 102944 / 114272 | training loss: 0.0011173451784998178\n",
      "epoch: 5 | 102976 / 114272 | training loss: 0.0008944431319832802\n",
      "epoch: 5 | 103008 / 114272 | training loss: 0.0016122038941830397\n",
      "epoch: 5 | 103040 / 114272 | training loss: 0.0010303315939381719\n",
      "epoch: 5 | 103072 / 114272 | training loss: 0.001012292574159801\n",
      "epoch: 5 | 103104 / 114272 | training loss: 0.16762763261795044\n",
      "epoch: 5 | 103136 / 114272 | training loss: 0.091483935713768\n",
      "epoch: 5 | 103168 / 114272 | training loss: 0.0010670581832528114\n",
      "epoch: 5 | 103200 / 114272 | training loss: 0.000556102953851223\n",
      "epoch: 5 | 103232 / 114272 | training loss: 0.0007731822552159429\n",
      "epoch: 5 | 103264 / 114272 | training loss: 0.0009578801691532135\n",
      "epoch: 5 | 103296 / 114272 | training loss: 0.0009398838737979531\n",
      "epoch: 5 | 103328 / 114272 | training loss: 0.06848982721567154\n",
      "epoch: 5 | 103360 / 114272 | training loss: 0.001166385831311345\n",
      "epoch: 5 | 103392 / 114272 | training loss: 0.000989460153505206\n",
      "epoch: 5 | 103424 / 114272 | training loss: 0.018797123804688454\n",
      "epoch: 5 | 103456 / 114272 | training loss: 0.0006967763765715063\n",
      "epoch: 5 | 103488 / 114272 | training loss: 0.0009894781978800893\n",
      "epoch: 5 | 103520 / 114272 | training loss: 0.0008955660741776228\n",
      "epoch: 5 | 103552 / 114272 | training loss: 0.026543481275439262\n",
      "epoch: 5 | 103584 / 114272 | training loss: 0.0022888591047376394\n",
      "epoch: 5 | 103616 / 114272 | training loss: 0.2006995528936386\n",
      "epoch: 5 | 103648 / 114272 | training loss: 0.0010319948196411133\n",
      "epoch: 5 | 103680 / 114272 | training loss: 0.19864621758460999\n",
      "epoch: 5 | 103712 / 114272 | training loss: 0.0019157790811732411\n",
      "epoch: 5 | 103744 / 114272 | training loss: 0.0007097638444975019\n",
      "epoch: 5 | 103776 / 114272 | training loss: 0.00108666333835572\n",
      "epoch: 5 | 103808 / 114272 | training loss: 0.0010730911744758487\n",
      "epoch: 5 | 103840 / 114272 | training loss: 0.0011781826615333557\n",
      "epoch: 5 | 103872 / 114272 | training loss: 0.11051870137453079\n",
      "epoch: 5 | 103904 / 114272 | training loss: 0.10175453126430511\n",
      "epoch: 5 | 103936 / 114272 | training loss: 0.0010262664873152971\n",
      "epoch: 5 | 103968 / 114272 | training loss: 0.0023102376144379377\n",
      "epoch: 5 | 104000 / 114272 | training loss: 0.0007633710629306734\n",
      "epoch: 5 | 104032 / 114272 | training loss: 0.0019139668438583612\n",
      "epoch: 5 | 104064 / 114272 | training loss: 0.009220915846526623\n",
      "epoch: 5 | 104096 / 114272 | training loss: 0.0014594238018617034\n",
      "epoch: 5 | 104128 / 114272 | training loss: 0.0010343551402911544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 104160 / 114272 | training loss: 0.13680538535118103\n",
      "epoch: 5 | 104192 / 114272 | training loss: 0.0019145181868225336\n",
      "epoch: 5 | 104224 / 114272 | training loss: 0.0015199331101030111\n",
      "epoch: 5 | 104256 / 114272 | training loss: 0.1007116287946701\n",
      "epoch: 5 | 104288 / 114272 | training loss: 0.0010072296718135476\n",
      "epoch: 5 | 104320 / 114272 | training loss: 0.06885407119989395\n",
      "epoch: 5 | 104352 / 114272 | training loss: 0.23255929350852966\n",
      "epoch: 5 | 104384 / 114272 | training loss: 0.1435493677854538\n",
      "epoch: 5 | 104416 / 114272 | training loss: 0.001595218083821237\n",
      "epoch: 5 | 104448 / 114272 | training loss: 0.0005521079292520881\n",
      "epoch: 5 | 104480 / 114272 | training loss: 0.0016789664514362812\n",
      "epoch: 5 | 104512 / 114272 | training loss: 0.0012353017227724195\n",
      "epoch: 5 | 104544 / 114272 | training loss: 0.0023595348466187716\n",
      "epoch: 5 | 104576 / 114272 | training loss: 0.02412879653275013\n",
      "epoch: 5 | 104608 / 114272 | training loss: 0.0009803349385038018\n",
      "epoch: 5 | 104640 / 114272 | training loss: 0.0010375682031735778\n",
      "epoch: 5 | 104672 / 114272 | training loss: 0.14031055569648743\n",
      "epoch: 5 | 104704 / 114272 | training loss: 0.0013739621499553323\n",
      "epoch: 5 | 104736 / 114272 | training loss: 0.0011841689702123404\n",
      "epoch: 5 | 104768 / 114272 | training loss: 0.11007025092840195\n",
      "epoch: 5 | 104800 / 114272 | training loss: 0.13297192752361298\n",
      "epoch: 5 | 104832 / 114272 | training loss: 0.0012176641030237079\n",
      "epoch: 5 | 104864 / 114272 | training loss: 0.0010385236237198114\n",
      "epoch: 5 | 104896 / 114272 | training loss: 0.0008063421119004488\n",
      "epoch: 5 | 104928 / 114272 | training loss: 0.001281068311072886\n",
      "epoch: 5 | 104960 / 114272 | training loss: 0.5506467819213867\n",
      "epoch: 5 | 104992 / 114272 | training loss: 0.006279580760747194\n",
      "epoch: 5 | 105024 / 114272 | training loss: 0.062294282019138336\n",
      "epoch: 5 | 105056 / 114272 | training loss: 0.05010291934013367\n",
      "epoch: 5 | 105088 / 114272 | training loss: 0.021439000964164734\n",
      "epoch: 5 | 105120 / 114272 | training loss: 0.001633527223020792\n",
      "epoch: 5 | 105152 / 114272 | training loss: 0.002335599157959223\n",
      "epoch: 5 | 105184 / 114272 | training loss: 0.0033708217088133097\n",
      "epoch: 5 | 105216 / 114272 | training loss: 0.0013391695683822036\n",
      "epoch: 5 | 105248 / 114272 | training loss: 0.001752723939716816\n",
      "epoch: 5 | 105280 / 114272 | training loss: 0.001429305411875248\n",
      "epoch: 5 | 105312 / 114272 | training loss: 0.10980181396007538\n",
      "epoch: 5 | 105344 / 114272 | training loss: 0.001203367137350142\n",
      "epoch: 5 | 105376 / 114272 | training loss: 0.0017594635719433427\n",
      "epoch: 5 | 105408 / 114272 | training loss: 0.12456776201725006\n",
      "epoch: 5 | 105440 / 114272 | training loss: 0.022756079211831093\n",
      "epoch: 5 | 105472 / 114272 | training loss: 0.002808375284075737\n",
      "epoch: 5 | 105504 / 114272 | training loss: 0.0013749743811786175\n",
      "epoch: 5 | 105536 / 114272 | training loss: 0.006576301530003548\n",
      "epoch: 5 | 105568 / 114272 | training loss: 0.017037408426404\n",
      "epoch: 5 | 105600 / 114272 | training loss: 0.01037693489342928\n",
      "epoch: 5 | 105632 / 114272 | training loss: 0.0010401703184470534\n",
      "epoch: 5 | 105664 / 114272 | training loss: 0.020564286038279533\n",
      "epoch: 5 | 105696 / 114272 | training loss: 0.0024215918965637684\n",
      "epoch: 5 | 105728 / 114272 | training loss: 0.0033228148240596056\n",
      "epoch: 5 | 105760 / 114272 | training loss: 0.004231203347444534\n",
      "epoch: 5 | 105792 / 114272 | training loss: 0.1651468724012375\n",
      "epoch: 5 | 105824 / 114272 | training loss: 0.002110355067998171\n",
      "epoch: 5 | 105856 / 114272 | training loss: 0.005647625774145126\n",
      "epoch: 5 | 105888 / 114272 | training loss: 0.00466447789222002\n",
      "epoch: 5 | 105920 / 114272 | training loss: 0.0018625152297317982\n",
      "epoch: 5 | 105952 / 114272 | training loss: 0.0044001745991408825\n",
      "epoch: 5 | 105984 / 114272 | training loss: 0.0018901029834523797\n",
      "epoch: 5 | 106016 / 114272 | training loss: 0.15895670652389526\n",
      "epoch: 5 | 106048 / 114272 | training loss: 0.0035961787216365337\n",
      "epoch: 5 | 106080 / 114272 | training loss: 0.001030760002322495\n",
      "epoch: 5 | 106112 / 114272 | training loss: 0.001840889686718583\n",
      "epoch: 5 | 106144 / 114272 | training loss: 0.0013986305566504598\n",
      "epoch: 5 | 106176 / 114272 | training loss: 0.002945795888081193\n",
      "epoch: 5 | 106208 / 114272 | training loss: 0.0013105670223012567\n",
      "epoch: 5 | 106240 / 114272 | training loss: 0.0013073936570435762\n",
      "epoch: 5 | 106272 / 114272 | training loss: 0.002086515771225095\n",
      "epoch: 5 | 106304 / 114272 | training loss: 0.0018634767038747668\n",
      "epoch: 5 | 106336 / 114272 | training loss: 0.0013522052904590964\n",
      "epoch: 5 | 106368 / 114272 | training loss: 0.0019909231923520565\n",
      "epoch: 5 | 106400 / 114272 | training loss: 0.0024244324304163456\n",
      "epoch: 5 | 106432 / 114272 | training loss: 0.0013859859900549054\n",
      "epoch: 5 | 106464 / 114272 | training loss: 0.001587478443980217\n",
      "epoch: 5 | 106496 / 114272 | training loss: 0.10375767946243286\n",
      "epoch: 5 | 106528 / 114272 | training loss: 0.09103357046842575\n",
      "epoch: 5 | 106560 / 114272 | training loss: 0.0014801326906308532\n",
      "epoch: 5 | 106592 / 114272 | training loss: 0.01022147573530674\n",
      "epoch: 5 | 106624 / 114272 | training loss: 0.0024531802628189325\n",
      "epoch: 5 | 106656 / 114272 | training loss: 0.0017852451419457793\n",
      "epoch: 5 | 106688 / 114272 | training loss: 0.002368279965594411\n",
      "epoch: 5 | 106720 / 114272 | training loss: 0.002024884568527341\n",
      "epoch: 5 | 106752 / 114272 | training loss: 0.0023880130611360073\n",
      "epoch: 5 | 106784 / 114272 | training loss: 0.13697022199630737\n",
      "epoch: 5 | 106816 / 114272 | training loss: 0.0010789699153974652\n",
      "epoch: 5 | 106848 / 114272 | training loss: 0.0014106641756370664\n",
      "epoch: 5 | 106880 / 114272 | training loss: 0.14663879573345184\n",
      "epoch: 5 | 106912 / 114272 | training loss: 0.0013804011978209019\n",
      "epoch: 5 | 106944 / 114272 | training loss: 0.09062763303518295\n",
      "epoch: 5 | 106976 / 114272 | training loss: 0.007842231541872025\n",
      "epoch: 5 | 107008 / 114272 | training loss: 0.018255705013871193\n",
      "epoch: 5 | 107040 / 114272 | training loss: 0.0014411380980163813\n",
      "epoch: 5 | 107072 / 114272 | training loss: 0.002708629472181201\n",
      "epoch: 5 | 107104 / 114272 | training loss: 0.3705703616142273\n",
      "epoch: 5 | 107136 / 114272 | training loss: 0.0018188529647886753\n",
      "epoch: 5 | 107168 / 114272 | training loss: 0.0014911970356479287\n",
      "epoch: 5 | 107200 / 114272 | training loss: 0.0014057663502171636\n",
      "epoch: 5 | 107232 / 114272 | training loss: 0.002698000520467758\n",
      "epoch: 5 | 107264 / 114272 | training loss: 0.13513140380382538\n",
      "epoch: 5 | 107296 / 114272 | training loss: 0.0018571875989437103\n",
      "epoch: 5 | 107328 / 114272 | training loss: 0.08451606333255768\n",
      "epoch: 5 | 107360 / 114272 | training loss: 0.0023014014586806297\n",
      "epoch: 5 | 107392 / 114272 | training loss: 0.001252878224477172\n",
      "epoch: 5 | 107424 / 114272 | training loss: 0.0006280855159275234\n",
      "epoch: 5 | 107456 / 114272 | training loss: 0.003183677326887846\n",
      "epoch: 5 | 107488 / 114272 | training loss: 0.2002839297056198\n",
      "epoch: 5 | 107520 / 114272 | training loss: 0.1675664335489273\n",
      "epoch: 5 | 107552 / 114272 | training loss: 0.021406790241599083\n",
      "epoch: 5 | 107584 / 114272 | training loss: 0.0020928282756358385\n",
      "epoch: 5 | 107616 / 114272 | training loss: 0.009966070763766766\n",
      "epoch: 5 | 107648 / 114272 | training loss: 0.0017635207623243332\n",
      "epoch: 5 | 107680 / 114272 | training loss: 0.0018178423633798957\n",
      "epoch: 5 | 107712 / 114272 | training loss: 0.007658450864255428\n",
      "epoch: 5 | 107744 / 114272 | training loss: 0.010313425213098526\n",
      "epoch: 5 | 107776 / 114272 | training loss: 0.0015455876709893346\n",
      "epoch: 5 | 107808 / 114272 | training loss: 0.001372653292492032\n",
      "epoch: 5 | 107840 / 114272 | training loss: 0.0020450283773243427\n",
      "epoch: 5 | 107872 / 114272 | training loss: 0.001351594808511436\n",
      "epoch: 5 | 107904 / 114272 | training loss: 0.0014312643324956298\n",
      "epoch: 5 | 107936 / 114272 | training loss: 0.15700635313987732\n",
      "epoch: 5 | 107968 / 114272 | training loss: 0.0010819823946803808\n",
      "epoch: 5 | 108000 / 114272 | training loss: 0.06735428422689438\n",
      "epoch: 5 | 108032 / 114272 | training loss: 0.002419075695797801\n",
      "epoch: 5 | 108064 / 114272 | training loss: 0.10566486418247223\n",
      "epoch: 5 | 108096 / 114272 | training loss: 0.163862943649292\n",
      "epoch: 5 | 108128 / 114272 | training loss: 0.002192083979025483\n",
      "epoch: 5 | 108160 / 114272 | training loss: 0.1676521748304367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 108192 / 114272 | training loss: 0.07108079642057419\n",
      "epoch: 5 | 108224 / 114272 | training loss: 0.0008496216614730656\n",
      "epoch: 5 | 108256 / 114272 | training loss: 0.0029723241459578276\n",
      "epoch: 5 | 108288 / 114272 | training loss: 0.001112094847485423\n",
      "epoch: 5 | 108320 / 114272 | training loss: 0.015512975864112377\n",
      "epoch: 5 | 108352 / 114272 | training loss: 0.10066205263137817\n",
      "epoch: 5 | 108384 / 114272 | training loss: 0.0016132080927491188\n",
      "epoch: 5 | 108416 / 114272 | training loss: 0.13542141020298004\n",
      "epoch: 5 | 108448 / 114272 | training loss: 0.0014596442924812436\n",
      "epoch: 5 | 108480 / 114272 | training loss: 0.00328684039413929\n",
      "epoch: 5 | 108512 / 114272 | training loss: 0.0016583106480538845\n",
      "epoch: 5 | 108544 / 114272 | training loss: 0.01125836931169033\n",
      "epoch: 5 | 108576 / 114272 | training loss: 0.003204686101526022\n",
      "epoch: 5 | 108608 / 114272 | training loss: 0.0021074097603559494\n",
      "epoch: 5 | 108640 / 114272 | training loss: 0.0022531715221703053\n",
      "epoch: 5 | 108672 / 114272 | training loss: 0.0012484475737437606\n",
      "epoch: 5 | 108704 / 114272 | training loss: 0.16131232678890228\n",
      "epoch: 5 | 108736 / 114272 | training loss: 0.0014705986250191927\n",
      "epoch: 5 | 108768 / 114272 | training loss: 0.0025300192646682262\n",
      "epoch: 5 | 108800 / 114272 | training loss: 0.00349472276866436\n",
      "epoch: 5 | 108832 / 114272 | training loss: 0.000853203993756324\n",
      "epoch: 5 | 108864 / 114272 | training loss: 0.0008308388642035425\n",
      "epoch: 5 | 108896 / 114272 | training loss: 0.0065684071741998196\n",
      "epoch: 5 | 108928 / 114272 | training loss: 0.1333964765071869\n",
      "epoch: 5 | 108960 / 114272 | training loss: 0.001394330058246851\n",
      "epoch: 5 | 108992 / 114272 | training loss: 0.002520123263821006\n",
      "epoch: 5 | 109024 / 114272 | training loss: 0.010628962889313698\n",
      "epoch: 5 | 109056 / 114272 | training loss: 0.0029291617684066296\n",
      "epoch: 5 | 109088 / 114272 | training loss: 0.0023786253295838833\n",
      "epoch: 5 | 109120 / 114272 | training loss: 0.0010434066643938422\n",
      "epoch: 5 | 109152 / 114272 | training loss: 0.0015061557060107589\n",
      "epoch: 5 | 109184 / 114272 | training loss: 0.0016170176677405834\n",
      "epoch: 5 | 109216 / 114272 | training loss: 0.0038384816143661737\n",
      "epoch: 5 | 109248 / 114272 | training loss: 0.08818017691373825\n",
      "epoch: 5 | 109280 / 114272 | training loss: 0.13763222098350525\n",
      "epoch: 5 | 109312 / 114272 | training loss: 0.0023139873519539833\n",
      "epoch: 5 | 109344 / 114272 | training loss: 0.0030313334427773952\n",
      "epoch: 5 | 109376 / 114272 | training loss: 0.001847843173891306\n",
      "epoch: 5 | 109408 / 114272 | training loss: 0.10148893296718597\n",
      "epoch: 5 | 109440 / 114272 | training loss: 0.001730319345369935\n",
      "epoch: 5 | 109472 / 114272 | training loss: 0.0044823926873505116\n",
      "epoch: 5 | 109504 / 114272 | training loss: 0.004250683821737766\n",
      "epoch: 5 | 109536 / 114272 | training loss: 0.0024493117816746235\n",
      "epoch: 5 | 109568 / 114272 | training loss: 0.0012900575529783964\n",
      "epoch: 5 | 109600 / 114272 | training loss: 0.003489857539534569\n",
      "epoch: 5 | 109632 / 114272 | training loss: 0.003357542213052511\n",
      "epoch: 5 | 109664 / 114272 | training loss: 0.006151313427835703\n",
      "epoch: 5 | 109696 / 114272 | training loss: 0.02877119369804859\n",
      "epoch: 5 | 109728 / 114272 | training loss: 0.004691334441304207\n",
      "epoch: 5 | 109760 / 114272 | training loss: 0.0017448756843805313\n",
      "epoch: 5 | 109792 / 114272 | training loss: 0.006946959998458624\n",
      "epoch: 5 | 109824 / 114272 | training loss: 0.002118533942848444\n",
      "epoch: 5 | 109856 / 114272 | training loss: 0.00111016898881644\n",
      "epoch: 5 | 109888 / 114272 | training loss: 0.0032496661879122257\n",
      "epoch: 5 | 109920 / 114272 | training loss: 0.0015481822192668915\n",
      "epoch: 5 | 109952 / 114272 | training loss: 0.0027618997264653444\n",
      "epoch: 5 | 109984 / 114272 | training loss: 0.0033296244218945503\n",
      "epoch: 5 | 110016 / 114272 | training loss: 0.0016657429514452815\n",
      "epoch: 5 | 110048 / 114272 | training loss: 0.004525769967585802\n",
      "epoch: 5 | 110080 / 114272 | training loss: 0.001942739705555141\n",
      "epoch: 5 | 110112 / 114272 | training loss: 0.003955786116421223\n",
      "epoch: 5 | 110144 / 114272 | training loss: 0.12067092955112457\n",
      "epoch: 5 | 110176 / 114272 | training loss: 0.004000237677246332\n",
      "epoch: 5 | 110208 / 114272 | training loss: 0.005654325243085623\n",
      "epoch: 5 | 110240 / 114272 | training loss: 0.004926086403429508\n",
      "epoch: 5 | 110272 / 114272 | training loss: 0.0017547575989738107\n",
      "epoch: 5 | 110304 / 114272 | training loss: 0.0015079444274306297\n",
      "epoch: 5 | 110336 / 114272 | training loss: 0.0013713939115405083\n",
      "epoch: 5 | 110368 / 114272 | training loss: 0.0013296094257384539\n",
      "epoch: 5 | 110400 / 114272 | training loss: 0.003966018091887236\n",
      "epoch: 5 | 110432 / 114272 | training loss: 0.10866023600101471\n",
      "epoch: 5 | 110464 / 114272 | training loss: 0.0020517294760793447\n",
      "epoch: 5 | 110496 / 114272 | training loss: 0.003755745477974415\n",
      "epoch: 5 | 110528 / 114272 | training loss: 0.10360554605722427\n",
      "epoch: 5 | 110560 / 114272 | training loss: 0.07218775153160095\n",
      "epoch: 5 | 110592 / 114272 | training loss: 0.0013997765490785241\n",
      "epoch: 5 | 110624 / 114272 | training loss: 0.003466474125161767\n",
      "epoch: 5 | 110656 / 114272 | training loss: 0.0016396800056099892\n",
      "epoch: 5 | 110688 / 114272 | training loss: 0.0025040667969733477\n",
      "epoch: 5 | 110720 / 114272 | training loss: 0.050459280610084534\n",
      "epoch: 5 | 110752 / 114272 | training loss: 0.0011935608927160501\n",
      "epoch: 5 | 110784 / 114272 | training loss: 0.005689864046871662\n",
      "epoch: 5 | 110816 / 114272 | training loss: 0.11310373246669769\n",
      "epoch: 5 | 110848 / 114272 | training loss: 0.0015584906795993447\n",
      "epoch: 5 | 110880 / 114272 | training loss: 0.0019291266798973083\n",
      "epoch: 5 | 110912 / 114272 | training loss: 0.003519124584272504\n",
      "epoch: 5 | 110944 / 114272 | training loss: 0.011909493245184422\n",
      "epoch: 5 | 110976 / 114272 | training loss: 0.0014550265623256564\n",
      "epoch: 5 | 111008 / 114272 | training loss: 0.07305356115102768\n",
      "epoch: 5 | 111040 / 114272 | training loss: 0.011133963242173195\n",
      "epoch: 5 | 111072 / 114272 | training loss: 0.17677564918994904\n",
      "epoch: 5 | 111104 / 114272 | training loss: 0.0026881671510636806\n",
      "epoch: 5 | 111136 / 114272 | training loss: 0.0014399247011169791\n",
      "epoch: 5 | 111168 / 114272 | training loss: 0.00224245130084455\n",
      "epoch: 5 | 111200 / 114272 | training loss: 0.0006980461184866726\n",
      "epoch: 5 | 111232 / 114272 | training loss: 0.005414607003331184\n",
      "epoch: 5 | 111264 / 114272 | training loss: 0.0022862101905047894\n",
      "epoch: 5 | 111296 / 114272 | training loss: 0.0013708589831367135\n",
      "epoch: 5 | 111328 / 114272 | training loss: 0.008694875054061413\n",
      "epoch: 5 | 111360 / 114272 | training loss: 0.0011323736980557442\n",
      "epoch: 5 | 111392 / 114272 | training loss: 0.06535150855779648\n",
      "epoch: 5 | 111424 / 114272 | training loss: 0.0037523265928030014\n",
      "epoch: 5 | 111456 / 114272 | training loss: 0.02334832400083542\n",
      "epoch: 5 | 111488 / 114272 | training loss: 0.18446096777915955\n",
      "epoch: 5 | 111520 / 114272 | training loss: 0.09706567227840424\n",
      "epoch: 5 | 111552 / 114272 | training loss: 0.002648470690473914\n",
      "epoch: 5 | 111584 / 114272 | training loss: 0.001124039408750832\n",
      "epoch: 5 | 111616 / 114272 | training loss: 0.1112123504281044\n",
      "epoch: 5 | 111648 / 114272 | training loss: 0.0010847777593880892\n",
      "epoch: 5 | 111680 / 114272 | training loss: 0.0015447811456397176\n",
      "epoch: 5 | 111712 / 114272 | training loss: 0.002080594189465046\n",
      "epoch: 5 | 111744 / 114272 | training loss: 0.36799460649490356\n",
      "epoch: 5 | 111776 / 114272 | training loss: 0.0014118198305368423\n",
      "epoch: 5 | 111808 / 114272 | training loss: 0.0009697495843283832\n",
      "epoch: 5 | 111840 / 114272 | training loss: 0.003035470377653837\n",
      "epoch: 5 | 111872 / 114272 | training loss: 0.08057581633329391\n",
      "epoch: 5 | 111904 / 114272 | training loss: 0.0027612412814050913\n",
      "epoch: 5 | 111936 / 114272 | training loss: 0.0009626513929106295\n",
      "epoch: 5 | 111968 / 114272 | training loss: 0.0007071009604260325\n",
      "epoch: 5 | 112000 / 114272 | training loss: 0.0012114832643419504\n",
      "epoch: 5 | 112032 / 114272 | training loss: 0.12031424790620804\n",
      "epoch: 5 | 112064 / 114272 | training loss: 0.00970575213432312\n",
      "epoch: 5 | 112096 / 114272 | training loss: 0.003958066925406456\n",
      "epoch: 5 | 112128 / 114272 | training loss: 0.0017492326442152262\n",
      "epoch: 5 | 112160 / 114272 | training loss: 0.003547928063198924\n",
      "epoch: 5 | 112192 / 114272 | training loss: 0.012002512812614441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | 112224 / 114272 | training loss: 0.0070738703943789005\n",
      "epoch: 5 | 112256 / 114272 | training loss: 0.011053386144340038\n",
      "epoch: 5 | 112288 / 114272 | training loss: 0.0006142278434708714\n",
      "epoch: 5 | 112320 / 114272 | training loss: 0.0028317722026258707\n",
      "epoch: 5 | 112352 / 114272 | training loss: 0.1816495954990387\n",
      "epoch: 5 | 112384 / 114272 | training loss: 0.0015743252588436007\n",
      "epoch: 5 | 112416 / 114272 | training loss: 0.1365346610546112\n",
      "epoch: 5 | 112448 / 114272 | training loss: 0.011908633634448051\n",
      "epoch: 5 | 112480 / 114272 | training loss: 0.008731533773243427\n",
      "epoch: 5 | 112512 / 114272 | training loss: 0.0011038045631721616\n",
      "epoch: 5 | 112544 / 114272 | training loss: 0.2580929398536682\n",
      "epoch: 5 | 112576 / 114272 | training loss: 0.019082393497228622\n",
      "epoch: 5 | 112608 / 114272 | training loss: 0.0022412666585296392\n",
      "epoch: 5 | 112640 / 114272 | training loss: 0.0029328290838748217\n",
      "epoch: 5 | 112672 / 114272 | training loss: 0.004031764809042215\n",
      "epoch: 5 | 112704 / 114272 | training loss: 0.0039635528810322285\n",
      "epoch: 5 | 112736 / 114272 | training loss: 0.000807739794254303\n",
      "epoch: 5 | 112768 / 114272 | training loss: 0.001807174296118319\n",
      "epoch: 5 | 112800 / 114272 | training loss: 0.0012451686197891831\n",
      "epoch: 5 | 112832 / 114272 | training loss: 0.0037679029628634453\n",
      "epoch: 5 | 112864 / 114272 | training loss: 0.005051661282777786\n",
      "epoch: 5 | 112896 / 114272 | training loss: 0.0005659749731421471\n",
      "epoch: 5 | 112928 / 114272 | training loss: 0.0025234215427190065\n",
      "epoch: 5 | 112960 / 114272 | training loss: 0.0072184111922979355\n",
      "epoch: 5 | 112992 / 114272 | training loss: 0.0014806909020990133\n",
      "epoch: 5 | 113024 / 114272 | training loss: 0.0009104635682888329\n",
      "epoch: 5 | 113056 / 114272 | training loss: 0.0012363633140921593\n",
      "epoch: 5 | 113088 / 114272 | training loss: 0.00106116256210953\n",
      "epoch: 5 | 113120 / 114272 | training loss: 0.0019088441040366888\n",
      "epoch: 5 | 113152 / 114272 | training loss: 0.0015272835735231638\n",
      "epoch: 5 | 113184 / 114272 | training loss: 0.25652700662612915\n",
      "epoch: 5 | 113216 / 114272 | training loss: 0.18013766407966614\n",
      "epoch: 5 | 113248 / 114272 | training loss: 0.0007418214809149504\n",
      "epoch: 5 | 113280 / 114272 | training loss: 0.01264276821166277\n",
      "epoch: 5 | 113312 / 114272 | training loss: 0.000508524477481842\n",
      "epoch: 5 | 113344 / 114272 | training loss: 0.001191719900816679\n",
      "epoch: 5 | 113376 / 114272 | training loss: 0.0013951294822618365\n",
      "epoch: 5 | 113408 / 114272 | training loss: 0.0026290572714060545\n",
      "epoch: 5 | 113440 / 114272 | training loss: 0.0010511218570172787\n",
      "epoch: 5 | 113472 / 114272 | training loss: 0.000757798261474818\n",
      "epoch: 5 | 113504 / 114272 | training loss: 0.16578105092048645\n",
      "epoch: 5 | 113536 / 114272 | training loss: 0.17687106132507324\n",
      "epoch: 5 | 113568 / 114272 | training loss: 0.08578086644411087\n",
      "epoch: 5 | 113600 / 114272 | training loss: 0.08416914939880371\n",
      "epoch: 5 | 113632 / 114272 | training loss: 0.07733958214521408\n",
      "epoch: 5 | 113664 / 114272 | training loss: 0.002099494682624936\n",
      "epoch: 5 | 113696 / 114272 | training loss: 0.0012024156749248505\n",
      "epoch: 5 | 113728 / 114272 | training loss: 0.0016558407805860043\n",
      "epoch: 5 | 113760 / 114272 | training loss: 0.008269516751170158\n",
      "epoch: 5 | 113792 / 114272 | training loss: 0.1583217978477478\n",
      "epoch: 5 | 113824 / 114272 | training loss: 0.11699514091014862\n",
      "epoch: 5 | 113856 / 114272 | training loss: 0.11221558600664139\n",
      "epoch: 5 | 113888 / 114272 | training loss: 0.0018011145293712616\n",
      "epoch: 5 | 113920 / 114272 | training loss: 0.15349626541137695\n",
      "epoch: 5 | 113952 / 114272 | training loss: 0.0017968951724469662\n",
      "epoch: 5 | 113984 / 114272 | training loss: 0.0015563869383186102\n",
      "epoch: 5 | 114016 / 114272 | training loss: 0.0013220011023804545\n",
      "epoch: 5 | 114048 / 114272 | training loss: 0.009759403765201569\n",
      "epoch: 5 | 114080 / 114272 | training loss: 0.004571754951030016\n",
      "epoch: 5 | 114112 / 114272 | training loss: 0.15205781161785126\n",
      "epoch: 5 | 114144 / 114272 | training loss: 0.001362050068564713\n",
      "epoch: 5 | 114176 / 114272 | training loss: 0.09642501175403595\n",
      "epoch: 5 | 114208 / 114272 | training loss: 0.006929535418748856\n",
      "epoch: 5 | 114240 / 114272 | training loss: 0.0011713244020938873\n",
      "Training epoch 5 done! Average loss: 0.03028207239987643. Accuracy: 0.9927103752450295\n",
      "Validation epoch 5 done! Average loss: 0.21920872175447764. Accurage: 0.9544183445190156\n",
      "Epoch 7 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 6 | 0 / 114272 | training loss: 0.19917365908622742\n",
      "epoch: 6 | 32 / 114272 | training loss: 0.003067095996811986\n",
      "epoch: 6 | 64 / 114272 | training loss: 0.005366452503949404\n",
      "epoch: 6 | 96 / 114272 | training loss: 0.0037342256400734186\n",
      "epoch: 6 | 128 / 114272 | training loss: 0.015371902845799923\n",
      "epoch: 6 | 160 / 114272 | training loss: 0.0013899956829845905\n",
      "epoch: 6 | 192 / 114272 | training loss: 0.04696664214134216\n",
      "epoch: 6 | 224 / 114272 | training loss: 0.0028066057711839676\n",
      "epoch: 6 | 256 / 114272 | training loss: 0.003872219007462263\n",
      "epoch: 6 | 288 / 114272 | training loss: 0.0018365801079198718\n",
      "epoch: 6 | 320 / 114272 | training loss: 0.0021445804741233587\n",
      "epoch: 6 | 352 / 114272 | training loss: 0.0024329915177077055\n",
      "epoch: 6 | 384 / 114272 | training loss: 0.001517490134574473\n",
      "epoch: 6 | 416 / 114272 | training loss: 0.0030947797931730747\n",
      "epoch: 6 | 448 / 114272 | training loss: 0.017585469409823418\n",
      "epoch: 6 | 480 / 114272 | training loss: 0.0029780797194689512\n",
      "epoch: 6 | 512 / 114272 | training loss: 0.0027164279017597437\n",
      "epoch: 6 | 544 / 114272 | training loss: 0.0674474686384201\n",
      "epoch: 6 | 576 / 114272 | training loss: 0.0010541798546910286\n",
      "epoch: 6 | 608 / 114272 | training loss: 0.002175417263060808\n",
      "epoch: 6 | 640 / 114272 | training loss: 0.0015517291612923145\n",
      "epoch: 6 | 672 / 114272 | training loss: 0.001436356920748949\n",
      "epoch: 6 | 704 / 114272 | training loss: 0.0009896806441247463\n",
      "epoch: 6 | 736 / 114272 | training loss: 0.0010328693315386772\n",
      "epoch: 6 | 768 / 114272 | training loss: 0.0010936640901491046\n",
      "epoch: 6 | 800 / 114272 | training loss: 0.0008697201265022159\n",
      "epoch: 6 | 832 / 114272 | training loss: 0.0015706574777141213\n",
      "epoch: 6 | 864 / 114272 | training loss: 0.0016850227257236838\n",
      "epoch: 6 | 896 / 114272 | training loss: 0.0014130619820207357\n",
      "epoch: 6 | 928 / 114272 | training loss: 0.14260907471179962\n",
      "epoch: 6 | 960 / 114272 | training loss: 0.0013870721450075507\n",
      "epoch: 6 | 992 / 114272 | training loss: 0.0014727863017469645\n",
      "epoch: 6 | 1024 / 114272 | training loss: 0.001315969624556601\n",
      "epoch: 6 | 1056 / 114272 | training loss: 0.0010203913552686572\n",
      "epoch: 6 | 1088 / 114272 | training loss: 0.0009088784572668374\n",
      "epoch: 6 | 1120 / 114272 | training loss: 0.114639513194561\n",
      "epoch: 6 | 1152 / 114272 | training loss: 0.0011707963421940804\n",
      "epoch: 6 | 1184 / 114272 | training loss: 0.0012587327510118484\n",
      "epoch: 6 | 1216 / 114272 | training loss: 0.0010742469457909465\n",
      "epoch: 6 | 1248 / 114272 | training loss: 0.0014788798289373517\n",
      "epoch: 6 | 1280 / 114272 | training loss: 0.0013080734061077237\n",
      "epoch: 6 | 1312 / 114272 | training loss: 0.0009471636149100959\n",
      "epoch: 6 | 1344 / 114272 | training loss: 0.0008873050683178008\n",
      "epoch: 6 | 1376 / 114272 | training loss: 0.0009550240356475115\n",
      "epoch: 6 | 1408 / 114272 | training loss: 0.16537977755069733\n",
      "epoch: 6 | 1440 / 114272 | training loss: 0.017198927700519562\n",
      "epoch: 6 | 1472 / 114272 | training loss: 0.001399447675794363\n",
      "epoch: 6 | 1504 / 114272 | training loss: 0.0015105714555829763\n",
      "epoch: 6 | 1536 / 114272 | training loss: 0.0015110999811440706\n",
      "epoch: 6 | 1568 / 114272 | training loss: 0.001847336650826037\n",
      "epoch: 6 | 1600 / 114272 | training loss: 0.06947246938943863\n",
      "epoch: 6 | 1632 / 114272 | training loss: 0.08288679271936417\n",
      "epoch: 6 | 1664 / 114272 | training loss: 0.0009763695416040719\n",
      "epoch: 6 | 1696 / 114272 | training loss: 0.001284323283471167\n",
      "epoch: 6 | 1728 / 114272 | training loss: 0.002110433764755726\n",
      "epoch: 6 | 1760 / 114272 | training loss: 0.0018515655538067222\n",
      "epoch: 6 | 1792 / 114272 | training loss: 0.006313420366495848\n",
      "epoch: 6 | 1824 / 114272 | training loss: 0.0025403073523193598\n",
      "epoch: 6 | 1856 / 114272 | training loss: 0.0006412097136490047\n",
      "epoch: 6 | 1888 / 114272 | training loss: 0.002582387300208211\n",
      "epoch: 6 | 1920 / 114272 | training loss: 0.0020851867739111185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 1952 / 114272 | training loss: 0.0884111225605011\n",
      "epoch: 6 | 1984 / 114272 | training loss: 0.0007875576266087592\n",
      "epoch: 6 | 2016 / 114272 | training loss: 0.11214202642440796\n",
      "epoch: 6 | 2048 / 114272 | training loss: 0.001622559386305511\n",
      "epoch: 6 | 2080 / 114272 | training loss: 0.0007801356259733438\n",
      "epoch: 6 | 2112 / 114272 | training loss: 0.004436114337295294\n",
      "epoch: 6 | 2144 / 114272 | training loss: 0.0011323309736326337\n",
      "epoch: 6 | 2176 / 114272 | training loss: 0.0020720921456813812\n",
      "epoch: 6 | 2208 / 114272 | training loss: 0.06597345322370529\n",
      "epoch: 6 | 2240 / 114272 | training loss: 0.00510387821123004\n",
      "epoch: 6 | 2272 / 114272 | training loss: 0.001297707436606288\n",
      "epoch: 6 | 2304 / 114272 | training loss: 0.17955975234508514\n",
      "epoch: 6 | 2336 / 114272 | training loss: 0.0014127368340268731\n",
      "epoch: 6 | 2368 / 114272 | training loss: 0.0035409622360020876\n",
      "epoch: 6 | 2400 / 114272 | training loss: 0.0011628669453784823\n",
      "epoch: 6 | 2432 / 114272 | training loss: 0.0020134979858994484\n",
      "epoch: 6 | 2464 / 114272 | training loss: 0.001116492203436792\n",
      "epoch: 6 | 2496 / 114272 | training loss: 0.0009014036040753126\n",
      "epoch: 6 | 2528 / 114272 | training loss: 0.0013824242632836103\n",
      "epoch: 6 | 2560 / 114272 | training loss: 0.0013231378979980946\n",
      "epoch: 6 | 2592 / 114272 | training loss: 0.01499597541987896\n",
      "epoch: 6 | 2624 / 114272 | training loss: 0.0013837043661624193\n",
      "epoch: 6 | 2656 / 114272 | training loss: 0.001139974221587181\n",
      "epoch: 6 | 2688 / 114272 | training loss: 0.0010983587708324194\n",
      "epoch: 6 | 2720 / 114272 | training loss: 0.0043832832016050816\n",
      "epoch: 6 | 2752 / 114272 | training loss: 0.10334780812263489\n",
      "epoch: 6 | 2784 / 114272 | training loss: 0.18266402184963226\n",
      "epoch: 6 | 2816 / 114272 | training loss: 0.000948195462115109\n",
      "epoch: 6 | 2848 / 114272 | training loss: 0.007148520089685917\n",
      "epoch: 6 | 2880 / 114272 | training loss: 0.004798463080078363\n",
      "epoch: 6 | 2912 / 114272 | training loss: 0.0023037174250930548\n",
      "epoch: 6 | 2944 / 114272 | training loss: 0.0023828407283872366\n",
      "epoch: 6 | 2976 / 114272 | training loss: 0.0029164450243115425\n",
      "epoch: 6 | 3008 / 114272 | training loss: 0.0016208679880946875\n",
      "epoch: 6 | 3040 / 114272 | training loss: 0.0059215836226940155\n",
      "epoch: 6 | 3072 / 114272 | training loss: 0.0040832688100636005\n",
      "epoch: 6 | 3104 / 114272 | training loss: 0.0013118099886924028\n",
      "epoch: 6 | 3136 / 114272 | training loss: 0.000971547095105052\n",
      "epoch: 6 | 3168 / 114272 | training loss: 0.0012563831405714154\n",
      "epoch: 6 | 3200 / 114272 | training loss: 0.0012497229035943747\n",
      "epoch: 6 | 3232 / 114272 | training loss: 0.0006371646304614842\n",
      "epoch: 6 | 3264 / 114272 | training loss: 0.001503223436884582\n",
      "epoch: 6 | 3296 / 114272 | training loss: 0.0006303114350885153\n",
      "epoch: 6 | 3328 / 114272 | training loss: 0.0020504833664745092\n",
      "epoch: 6 | 3360 / 114272 | training loss: 0.0011023934930562973\n",
      "epoch: 6 | 3392 / 114272 | training loss: 0.0037406375631690025\n",
      "epoch: 6 | 3424 / 114272 | training loss: 0.0018656039610505104\n",
      "epoch: 6 | 3456 / 114272 | training loss: 0.009951924905180931\n",
      "epoch: 6 | 3488 / 114272 | training loss: 0.001464159693568945\n",
      "epoch: 6 | 3520 / 114272 | training loss: 0.0007821467006579041\n",
      "epoch: 6 | 3552 / 114272 | training loss: 0.0006736227078363299\n",
      "epoch: 6 | 3584 / 114272 | training loss: 0.0022698112297803164\n",
      "epoch: 6 | 3616 / 114272 | training loss: 0.0017335168085992336\n",
      "epoch: 6 | 3648 / 114272 | training loss: 0.00030140485614538193\n",
      "epoch: 6 | 3680 / 114272 | training loss: 0.003635068889707327\n",
      "epoch: 6 | 3712 / 114272 | training loss: 0.0007953298045322299\n",
      "epoch: 6 | 3744 / 114272 | training loss: 0.21103434264659882\n",
      "epoch: 6 | 3776 / 114272 | training loss: 0.10893692076206207\n",
      "epoch: 6 | 3808 / 114272 | training loss: 0.0006583002395927906\n",
      "epoch: 6 | 3840 / 114272 | training loss: 0.0005940130213275552\n",
      "epoch: 6 | 3872 / 114272 | training loss: 0.0022251675836741924\n",
      "epoch: 6 | 3904 / 114272 | training loss: 0.0008603897294960916\n",
      "epoch: 6 | 3936 / 114272 | training loss: 0.0008732991991564631\n",
      "epoch: 6 | 3968 / 114272 | training loss: 0.002180592156946659\n",
      "epoch: 6 | 4000 / 114272 | training loss: 0.0007451548590324819\n",
      "epoch: 6 | 4032 / 114272 | training loss: 0.0005269672255963087\n",
      "epoch: 6 | 4064 / 114272 | training loss: 0.001191325020045042\n",
      "epoch: 6 | 4096 / 114272 | training loss: 0.000697476090863347\n",
      "epoch: 6 | 4128 / 114272 | training loss: 0.1723150759935379\n",
      "epoch: 6 | 4160 / 114272 | training loss: 0.0023325628135353327\n",
      "epoch: 6 | 4192 / 114272 | training loss: 0.0006781109841540456\n",
      "epoch: 6 | 4224 / 114272 | training loss: 0.0007331824745051563\n",
      "epoch: 6 | 4256 / 114272 | training loss: 0.0010449507972225547\n",
      "epoch: 6 | 4288 / 114272 | training loss: 0.00265717227011919\n",
      "epoch: 6 | 4320 / 114272 | training loss: 0.0011178572895005345\n",
      "epoch: 6 | 4352 / 114272 | training loss: 0.0008911348413676023\n",
      "epoch: 6 | 4384 / 114272 | training loss: 0.001056498265825212\n",
      "epoch: 6 | 4416 / 114272 | training loss: 0.2472638189792633\n",
      "epoch: 6 | 4448 / 114272 | training loss: 0.0006282786489464343\n",
      "epoch: 6 | 4480 / 114272 | training loss: 0.08559917658567429\n",
      "epoch: 6 | 4512 / 114272 | training loss: 0.0005021712277084589\n",
      "epoch: 6 | 4544 / 114272 | training loss: 0.000720182724762708\n",
      "epoch: 6 | 4576 / 114272 | training loss: 0.0010175766656175256\n",
      "epoch: 6 | 4608 / 114272 | training loss: 0.0007979566580615938\n",
      "epoch: 6 | 4640 / 114272 | training loss: 0.0006404827581718564\n",
      "epoch: 6 | 4672 / 114272 | training loss: 0.0025606255512684584\n",
      "epoch: 6 | 4704 / 114272 | training loss: 0.003443084191530943\n",
      "epoch: 6 | 4736 / 114272 | training loss: 0.0010543388780206442\n",
      "epoch: 6 | 4768 / 114272 | training loss: 0.0036031443160027266\n",
      "epoch: 6 | 4800 / 114272 | training loss: 0.0007401456241495907\n",
      "epoch: 6 | 4832 / 114272 | training loss: 0.0011024258565157652\n",
      "epoch: 6 | 4864 / 114272 | training loss: 0.0007589781889691949\n",
      "epoch: 6 | 4896 / 114272 | training loss: 0.00040773145155981183\n",
      "epoch: 6 | 4928 / 114272 | training loss: 0.3623725473880768\n",
      "epoch: 6 | 4960 / 114272 | training loss: 0.0059312186203897\n",
      "epoch: 6 | 4992 / 114272 | training loss: 0.0011411532759666443\n",
      "epoch: 6 | 5024 / 114272 | training loss: 0.0012047174386680126\n",
      "epoch: 6 | 5056 / 114272 | training loss: 0.0011064301943406463\n",
      "epoch: 6 | 5088 / 114272 | training loss: 0.002173386048525572\n",
      "epoch: 6 | 5120 / 114272 | training loss: 0.18128471076488495\n",
      "epoch: 6 | 5152 / 114272 | training loss: 0.0007246676832437515\n",
      "epoch: 6 | 5184 / 114272 | training loss: 0.0006740938406437635\n",
      "epoch: 6 | 5216 / 114272 | training loss: 0.006984172388911247\n",
      "epoch: 6 | 5248 / 114272 | training loss: 0.0005062806885689497\n",
      "epoch: 6 | 5280 / 114272 | training loss: 0.0008220972958952188\n",
      "epoch: 6 | 5312 / 114272 | training loss: 0.005591307766735554\n",
      "epoch: 6 | 5344 / 114272 | training loss: 0.001010446809232235\n",
      "epoch: 6 | 5376 / 114272 | training loss: 0.002266530180349946\n",
      "epoch: 6 | 5408 / 114272 | training loss: 0.0027265397366136312\n",
      "epoch: 6 | 5440 / 114272 | training loss: 0.0010424136416986585\n",
      "epoch: 6 | 5472 / 114272 | training loss: 0.001676394953392446\n",
      "epoch: 6 | 5504 / 114272 | training loss: 0.0009748943848535419\n",
      "epoch: 6 | 5536 / 114272 | training loss: 0.0025662160478532314\n",
      "epoch: 6 | 5568 / 114272 | training loss: 0.0011305076768621802\n",
      "epoch: 6 | 5600 / 114272 | training loss: 0.001083626295439899\n",
      "epoch: 6 | 5632 / 114272 | training loss: 0.0008375808247365057\n",
      "epoch: 6 | 5664 / 114272 | training loss: 0.0018596745794638991\n",
      "epoch: 6 | 5696 / 114272 | training loss: 0.0007386173238046467\n",
      "epoch: 6 | 5728 / 114272 | training loss: 0.001428219024091959\n",
      "epoch: 6 | 5760 / 114272 | training loss: 0.026725128293037415\n",
      "epoch: 6 | 5792 / 114272 | training loss: 0.0007542394450865686\n",
      "epoch: 6 | 5824 / 114272 | training loss: 0.001571023603901267\n",
      "epoch: 6 | 5856 / 114272 | training loss: 0.0012938182335346937\n",
      "epoch: 6 | 5888 / 114272 | training loss: 0.0007388310041278601\n",
      "epoch: 6 | 5920 / 114272 | training loss: 0.0014100783737376332\n",
      "epoch: 6 | 5952 / 114272 | training loss: 0.0009321728139184415\n",
      "epoch: 6 | 5984 / 114272 | training loss: 0.0008213496766984463\n",
      "epoch: 6 | 6016 / 114272 | training loss: 0.0014647324569523335\n",
      "epoch: 6 | 6048 / 114272 | training loss: 0.0013532567536458373\n",
      "epoch: 6 | 6080 / 114272 | training loss: 0.0010642731795087457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 6112 / 114272 | training loss: 0.0010820281459018588\n",
      "epoch: 6 | 6144 / 114272 | training loss: 0.0010761015582829714\n",
      "epoch: 6 | 6176 / 114272 | training loss: 0.0009552392875775695\n",
      "epoch: 6 | 6208 / 114272 | training loss: 0.0022370254155248404\n",
      "epoch: 6 | 6240 / 114272 | training loss: 0.0012077133869752288\n",
      "epoch: 6 | 6272 / 114272 | training loss: 0.0010342012392356992\n",
      "epoch: 6 | 6304 / 114272 | training loss: 0.0008766366518102586\n",
      "epoch: 6 | 6336 / 114272 | training loss: 0.0015432313084602356\n",
      "epoch: 6 | 6368 / 114272 | training loss: 0.0017734956927597523\n",
      "epoch: 6 | 6400 / 114272 | training loss: 0.002025078749284148\n",
      "epoch: 6 | 6432 / 114272 | training loss: 0.0008084754808805883\n",
      "epoch: 6 | 6464 / 114272 | training loss: 0.0015575821744278073\n",
      "epoch: 6 | 6496 / 114272 | training loss: 0.06478119641542435\n",
      "epoch: 6 | 6528 / 114272 | training loss: 0.0007557316566817462\n",
      "epoch: 6 | 6560 / 114272 | training loss: 0.09161726385354996\n",
      "epoch: 6 | 6592 / 114272 | training loss: 0.0007784372428432107\n",
      "epoch: 6 | 6624 / 114272 | training loss: 0.0015490067889913917\n",
      "epoch: 6 | 6656 / 114272 | training loss: 0.0014017756329849362\n",
      "epoch: 6 | 6688 / 114272 | training loss: 0.0912066251039505\n",
      "epoch: 6 | 6720 / 114272 | training loss: 0.0005608603241853416\n",
      "epoch: 6 | 6752 / 114272 | training loss: 0.0009316593059338629\n",
      "epoch: 6 | 6784 / 114272 | training loss: 0.0015742955729365349\n",
      "epoch: 6 | 6816 / 114272 | training loss: 0.0009383989381603897\n",
      "epoch: 6 | 6848 / 114272 | training loss: 0.001281260745599866\n",
      "epoch: 6 | 6880 / 114272 | training loss: 0.0005145439063198864\n",
      "epoch: 6 | 6912 / 114272 | training loss: 0.0007919238414615393\n",
      "epoch: 6 | 6944 / 114272 | training loss: 0.006598881911486387\n",
      "epoch: 6 | 6976 / 114272 | training loss: 0.0009660069481469691\n",
      "epoch: 6 | 7008 / 114272 | training loss: 0.0014838323695585132\n",
      "epoch: 6 | 7040 / 114272 | training loss: 0.03231276944279671\n",
      "epoch: 6 | 7072 / 114272 | training loss: 0.0004938915953971446\n",
      "epoch: 6 | 7104 / 114272 | training loss: 0.000902597326785326\n",
      "epoch: 6 | 7136 / 114272 | training loss: 0.22137239575386047\n",
      "epoch: 6 | 7168 / 114272 | training loss: 0.0008986903703771532\n",
      "epoch: 6 | 7200 / 114272 | training loss: 0.0005755660240538418\n",
      "epoch: 6 | 7232 / 114272 | training loss: 0.000392251560697332\n",
      "epoch: 6 | 7264 / 114272 | training loss: 0.0016651193145662546\n",
      "epoch: 6 | 7296 / 114272 | training loss: 0.0006697880453430116\n",
      "epoch: 6 | 7328 / 114272 | training loss: 0.0012081197928637266\n",
      "epoch: 6 | 7360 / 114272 | training loss: 0.0005992474616505206\n",
      "epoch: 6 | 7392 / 114272 | training loss: 0.000733305118046701\n",
      "epoch: 6 | 7424 / 114272 | training loss: 0.0013684318400919437\n",
      "epoch: 6 | 7456 / 114272 | training loss: 0.18460968136787415\n",
      "epoch: 6 | 7488 / 114272 | training loss: 0.0011461334070190787\n",
      "epoch: 6 | 7520 / 114272 | training loss: 0.00040168024133890867\n",
      "epoch: 6 | 7552 / 114272 | training loss: 0.000783933384809643\n",
      "epoch: 6 | 7584 / 114272 | training loss: 0.0008702227496542037\n",
      "epoch: 6 | 7616 / 114272 | training loss: 0.0007808248046785593\n",
      "epoch: 6 | 7648 / 114272 | training loss: 0.0013999822549521923\n",
      "epoch: 6 | 7680 / 114272 | training loss: 0.0012154337018728256\n",
      "epoch: 6 | 7712 / 114272 | training loss: 0.0011560897110030055\n",
      "epoch: 6 | 7744 / 114272 | training loss: 0.0005216507124714553\n",
      "epoch: 6 | 7776 / 114272 | training loss: 0.19279393553733826\n",
      "epoch: 6 | 7808 / 114272 | training loss: 0.0010522110387682915\n",
      "epoch: 6 | 7840 / 114272 | training loss: 0.007919488474726677\n",
      "epoch: 6 | 7872 / 114272 | training loss: 0.0008015202474780381\n",
      "epoch: 6 | 7904 / 114272 | training loss: 0.17042797803878784\n",
      "epoch: 6 | 7936 / 114272 | training loss: 0.0010768980719149113\n",
      "epoch: 6 | 7968 / 114272 | training loss: 0.0005708662210963666\n",
      "epoch: 6 | 8000 / 114272 | training loss: 0.08746643364429474\n",
      "epoch: 6 | 8032 / 114272 | training loss: 0.0005431073368526995\n",
      "epoch: 6 | 8064 / 114272 | training loss: 0.0015013758093118668\n",
      "epoch: 6 | 8096 / 114272 | training loss: 0.0005007391446270049\n",
      "epoch: 6 | 8128 / 114272 | training loss: 0.0007567165303044021\n",
      "epoch: 6 | 8160 / 114272 | training loss: 0.0007584303966723382\n",
      "epoch: 6 | 8192 / 114272 | training loss: 0.12102670967578888\n",
      "epoch: 6 | 8224 / 114272 | training loss: 0.0005947526078671217\n",
      "epoch: 6 | 8256 / 114272 | training loss: 0.000635716540273279\n",
      "epoch: 6 | 8288 / 114272 | training loss: 0.0013777009444311261\n",
      "epoch: 6 | 8320 / 114272 | training loss: 0.0008284524665214121\n",
      "epoch: 6 | 8352 / 114272 | training loss: 0.003536021336913109\n",
      "epoch: 6 | 8384 / 114272 | training loss: 0.0009295980562455952\n",
      "epoch: 6 | 8416 / 114272 | training loss: 0.002241093898192048\n",
      "epoch: 6 | 8448 / 114272 | training loss: 0.0012247220147401094\n",
      "epoch: 6 | 8480 / 114272 | training loss: 0.0008970648050308228\n",
      "epoch: 6 | 8512 / 114272 | training loss: 0.000367847882444039\n",
      "epoch: 6 | 8544 / 114272 | training loss: 0.0006716599455103278\n",
      "epoch: 6 | 8576 / 114272 | training loss: 0.0010786120546981692\n",
      "epoch: 6 | 8608 / 114272 | training loss: 0.0011949660256505013\n",
      "epoch: 6 | 8640 / 114272 | training loss: 0.061160773038864136\n",
      "epoch: 6 | 8672 / 114272 | training loss: 0.0010154879419133067\n",
      "epoch: 6 | 8704 / 114272 | training loss: 0.0007984552066773176\n",
      "epoch: 6 | 8736 / 114272 | training loss: 0.0010853486601263285\n",
      "epoch: 6 | 8768 / 114272 | training loss: 0.0010312801459804177\n",
      "epoch: 6 | 8800 / 114272 | training loss: 0.000520000874530524\n",
      "epoch: 6 | 8832 / 114272 | training loss: 0.0008501691045239568\n",
      "epoch: 6 | 8864 / 114272 | training loss: 0.0025483774952590466\n",
      "epoch: 6 | 8896 / 114272 | training loss: 0.0005670011159963906\n",
      "epoch: 6 | 8928 / 114272 | training loss: 0.0007911836146377027\n",
      "epoch: 6 | 8960 / 114272 | training loss: 0.0014651846140623093\n",
      "epoch: 6 | 8992 / 114272 | training loss: 0.0017788095865398645\n",
      "epoch: 6 | 9024 / 114272 | training loss: 0.0006229935679584742\n",
      "epoch: 6 | 9056 / 114272 | training loss: 0.0015545125352218747\n",
      "epoch: 6 | 9088 / 114272 | training loss: 0.001101409550756216\n",
      "epoch: 6 | 9120 / 114272 | training loss: 0.15466444194316864\n",
      "epoch: 6 | 9152 / 114272 | training loss: 0.0010678862454369664\n",
      "epoch: 6 | 9184 / 114272 | training loss: 0.0010328146163374186\n",
      "epoch: 6 | 9216 / 114272 | training loss: 0.0009628148400224745\n",
      "epoch: 6 | 9248 / 114272 | training loss: 0.0005058800452388823\n",
      "epoch: 6 | 9280 / 114272 | training loss: 0.00885947234928608\n",
      "epoch: 6 | 9312 / 114272 | training loss: 0.000859884254168719\n",
      "epoch: 6 | 9344 / 114272 | training loss: 0.06079233065247536\n",
      "epoch: 6 | 9376 / 114272 | training loss: 0.003086528740823269\n",
      "epoch: 6 | 9408 / 114272 | training loss: 0.0009607162792235613\n",
      "epoch: 6 | 9440 / 114272 | training loss: 0.0005932197091169655\n",
      "epoch: 6 | 9472 / 114272 | training loss: 0.0008118588593788445\n",
      "epoch: 6 | 9504 / 114272 | training loss: 0.0023918962106108665\n",
      "epoch: 6 | 9536 / 114272 | training loss: 0.0003612790605984628\n",
      "epoch: 6 | 9568 / 114272 | training loss: 0.4272100627422333\n",
      "epoch: 6 | 9600 / 114272 | training loss: 0.0006724967388436198\n",
      "epoch: 6 | 9632 / 114272 | training loss: 0.05511731654405594\n",
      "epoch: 6 | 9664 / 114272 | training loss: 0.005024514626711607\n",
      "epoch: 6 | 9696 / 114272 | training loss: 0.15593452751636505\n",
      "epoch: 6 | 9728 / 114272 | training loss: 0.08622843027114868\n",
      "epoch: 6 | 9760 / 114272 | training loss: 0.0019512808648869395\n",
      "epoch: 6 | 9792 / 114272 | training loss: 0.07385338097810745\n",
      "epoch: 6 | 9824 / 114272 | training loss: 0.007398643996566534\n",
      "epoch: 6 | 9856 / 114272 | training loss: 0.0006240320508368313\n",
      "epoch: 6 | 9888 / 114272 | training loss: 0.0005148586351424456\n",
      "epoch: 6 | 9920 / 114272 | training loss: 0.0005182646564207971\n",
      "epoch: 6 | 9952 / 114272 | training loss: 0.004155798815190792\n",
      "epoch: 6 | 9984 / 114272 | training loss: 0.0008692744886502624\n",
      "epoch: 6 | 10016 / 114272 | training loss: 0.0006238571950234473\n",
      "epoch: 6 | 10048 / 114272 | training loss: 0.0007713573868386447\n",
      "epoch: 6 | 10080 / 114272 | training loss: 0.0005126123433001339\n",
      "epoch: 6 | 10112 / 114272 | training loss: 0.0006324196583591402\n",
      "epoch: 6 | 10144 / 114272 | training loss: 0.004515666048973799\n",
      "epoch: 6 | 10176 / 114272 | training loss: 0.001157540362328291\n",
      "epoch: 6 | 10208 / 114272 | training loss: 0.0012361707631498575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 10240 / 114272 | training loss: 0.0003940407477784902\n",
      "epoch: 6 | 10272 / 114272 | training loss: 0.0007019963813945651\n",
      "epoch: 6 | 10304 / 114272 | training loss: 0.002147934166714549\n",
      "epoch: 6 | 10336 / 114272 | training loss: 0.0005181226297281682\n",
      "epoch: 6 | 10368 / 114272 | training loss: 0.0006776071386411786\n",
      "epoch: 6 | 10400 / 114272 | training loss: 0.0015716407215222716\n",
      "epoch: 6 | 10432 / 114272 | training loss: 0.14768509566783905\n",
      "epoch: 6 | 10464 / 114272 | training loss: 0.01326058991253376\n",
      "epoch: 6 | 10496 / 114272 | training loss: 0.0003508872468955815\n",
      "epoch: 6 | 10528 / 114272 | training loss: 0.0009055145201273263\n",
      "epoch: 6 | 10560 / 114272 | training loss: 0.0012492486275732517\n",
      "epoch: 6 | 10592 / 114272 | training loss: 0.0009126344812102616\n",
      "epoch: 6 | 10624 / 114272 | training loss: 0.050684139132499695\n",
      "epoch: 6 | 10656 / 114272 | training loss: 0.0007384602795355022\n",
      "epoch: 6 | 10688 / 114272 | training loss: 0.000536846462637186\n",
      "epoch: 6 | 10720 / 114272 | training loss: 0.0007335476111620665\n",
      "epoch: 6 | 10752 / 114272 | training loss: 0.0006708437576889992\n",
      "epoch: 6 | 10784 / 114272 | training loss: 0.0007767155766487122\n",
      "epoch: 6 | 10816 / 114272 | training loss: 0.00043499749153852463\n",
      "epoch: 6 | 10848 / 114272 | training loss: 0.0003324852732475847\n",
      "epoch: 6 | 10880 / 114272 | training loss: 0.0009288464789278805\n",
      "epoch: 6 | 10912 / 114272 | training loss: 0.02905980870127678\n",
      "epoch: 6 | 10944 / 114272 | training loss: 0.000538802589289844\n",
      "epoch: 6 | 10976 / 114272 | training loss: 0.0033561515156179667\n",
      "epoch: 6 | 11008 / 114272 | training loss: 0.0004627247981261462\n",
      "epoch: 6 | 11040 / 114272 | training loss: 0.0004907805705443025\n",
      "epoch: 6 | 11072 / 114272 | training loss: 0.0010096356272697449\n",
      "epoch: 6 | 11104 / 114272 | training loss: 0.0013015239965170622\n",
      "epoch: 6 | 11136 / 114272 | training loss: 0.001749437302350998\n",
      "epoch: 6 | 11168 / 114272 | training loss: 0.0005604906473308802\n",
      "epoch: 6 | 11200 / 114272 | training loss: 0.00042236543959006667\n",
      "epoch: 6 | 11232 / 114272 | training loss: 0.010526291094720364\n",
      "epoch: 6 | 11264 / 114272 | training loss: 0.022113030776381493\n",
      "epoch: 6 | 11296 / 114272 | training loss: 0.0015006428584456444\n",
      "epoch: 6 | 11328 / 114272 | training loss: 0.0007888286490924656\n",
      "epoch: 6 | 11360 / 114272 | training loss: 0.0006057964637875557\n",
      "epoch: 6 | 11392 / 114272 | training loss: 0.00025153375463560224\n",
      "epoch: 6 | 11424 / 114272 | training loss: 0.0005470775067806244\n",
      "epoch: 6 | 11456 / 114272 | training loss: 0.000347136112395674\n",
      "epoch: 6 | 11488 / 114272 | training loss: 0.0006465785554610193\n",
      "epoch: 6 | 11520 / 114272 | training loss: 0.0005678387242369354\n",
      "epoch: 6 | 11552 / 114272 | training loss: 0.000900421931874007\n",
      "epoch: 6 | 11584 / 114272 | training loss: 0.0005749649135395885\n",
      "epoch: 6 | 11616 / 114272 | training loss: 0.00029394187731668353\n",
      "epoch: 6 | 11648 / 114272 | training loss: 0.007892858237028122\n",
      "epoch: 6 | 11680 / 114272 | training loss: 0.0004861193592660129\n",
      "epoch: 6 | 11712 / 114272 | training loss: 0.002311858581379056\n",
      "epoch: 6 | 11744 / 114272 | training loss: 0.00022156324121169746\n",
      "epoch: 6 | 11776 / 114272 | training loss: 0.0005684216739609838\n",
      "epoch: 6 | 11808 / 114272 | training loss: 0.00169303675647825\n",
      "epoch: 6 | 11840 / 114272 | training loss: 0.041566673666238785\n",
      "epoch: 6 | 11872 / 114272 | training loss: 0.009991923347115517\n",
      "epoch: 6 | 11904 / 114272 | training loss: 0.0006804406875744462\n",
      "epoch: 6 | 11936 / 114272 | training loss: 0.0003238662611693144\n",
      "epoch: 6 | 11968 / 114272 | training loss: 0.0006147441454231739\n",
      "epoch: 6 | 12000 / 114272 | training loss: 0.00043790327617898583\n",
      "epoch: 6 | 12032 / 114272 | training loss: 0.0002221834147349\n",
      "epoch: 6 | 12064 / 114272 | training loss: 0.00038570474134758115\n",
      "epoch: 6 | 12096 / 114272 | training loss: 0.00045236205914989114\n",
      "epoch: 6 | 12128 / 114272 | training loss: 0.0005943874712102115\n",
      "epoch: 6 | 12160 / 114272 | training loss: 0.0004646070010494441\n",
      "epoch: 6 | 12192 / 114272 | training loss: 0.0006211376748979092\n",
      "epoch: 6 | 12224 / 114272 | training loss: 0.025257473811507225\n",
      "epoch: 6 | 12256 / 114272 | training loss: 0.0005230739479884505\n",
      "epoch: 6 | 12288 / 114272 | training loss: 0.0005485497531481087\n",
      "epoch: 6 | 12320 / 114272 | training loss: 0.0005463626584969461\n",
      "epoch: 6 | 12352 / 114272 | training loss: 0.0005358479684218764\n",
      "epoch: 6 | 12384 / 114272 | training loss: 0.0004257730906829238\n",
      "epoch: 6 | 12416 / 114272 | training loss: 0.0003730402095243335\n",
      "epoch: 6 | 12448 / 114272 | training loss: 0.0012209007982164621\n",
      "epoch: 6 | 12480 / 114272 | training loss: 0.0007854712312109768\n",
      "epoch: 6 | 12512 / 114272 | training loss: 0.0006160843768157065\n",
      "epoch: 6 | 12544 / 114272 | training loss: 0.0009377364185638726\n",
      "epoch: 6 | 12576 / 114272 | training loss: 0.0007019010372459888\n",
      "epoch: 6 | 12608 / 114272 | training loss: 0.0004876922757830471\n",
      "epoch: 6 | 12640 / 114272 | training loss: 0.0009524248889647424\n",
      "epoch: 6 | 12672 / 114272 | training loss: 0.0004979814984835684\n",
      "epoch: 6 | 12704 / 114272 | training loss: 0.00040158178308047354\n",
      "epoch: 6 | 12736 / 114272 | training loss: 0.00033822061959654093\n",
      "epoch: 6 | 12768 / 114272 | training loss: 0.00020824759849347174\n",
      "epoch: 6 | 12800 / 114272 | training loss: 0.0004511833540163934\n",
      "epoch: 6 | 12832 / 114272 | training loss: 0.0007210012408904731\n",
      "epoch: 6 | 12864 / 114272 | training loss: 0.00040101067861542106\n",
      "epoch: 6 | 12896 / 114272 | training loss: 0.0004313180106692016\n",
      "epoch: 6 | 12928 / 114272 | training loss: 0.0003696379717439413\n",
      "epoch: 6 | 12960 / 114272 | training loss: 0.16648198664188385\n",
      "epoch: 6 | 12992 / 114272 | training loss: 0.000356020696926862\n",
      "epoch: 6 | 13024 / 114272 | training loss: 0.20080934464931488\n",
      "epoch: 6 | 13056 / 114272 | training loss: 0.12574651837348938\n",
      "epoch: 6 | 13088 / 114272 | training loss: 0.0003186684916727245\n",
      "epoch: 6 | 13120 / 114272 | training loss: 0.09748382866382599\n",
      "epoch: 6 | 13152 / 114272 | training loss: 0.09477419406175613\n",
      "epoch: 6 | 13184 / 114272 | training loss: 0.000473208783660084\n",
      "epoch: 6 | 13216 / 114272 | training loss: 0.10161425918340683\n",
      "epoch: 6 | 13248 / 114272 | training loss: 0.00038010560092516243\n",
      "epoch: 6 | 13280 / 114272 | training loss: 0.00045157744898460805\n",
      "epoch: 6 | 13312 / 114272 | training loss: 0.0007632546476088464\n",
      "epoch: 6 | 13344 / 114272 | training loss: 0.00045479528489522636\n",
      "epoch: 6 | 13376 / 114272 | training loss: 0.0006949928938411176\n",
      "epoch: 6 | 13408 / 114272 | training loss: 0.17859859764575958\n",
      "epoch: 6 | 13440 / 114272 | training loss: 0.00046429657959379256\n",
      "epoch: 6 | 13472 / 114272 | training loss: 0.0008229706436395645\n",
      "epoch: 6 | 13504 / 114272 | training loss: 0.0008728124666959047\n",
      "epoch: 6 | 13536 / 114272 | training loss: 0.000557393825147301\n",
      "epoch: 6 | 13568 / 114272 | training loss: 0.005924887023866177\n",
      "epoch: 6 | 13600 / 114272 | training loss: 0.0004103759420104325\n",
      "epoch: 6 | 13632 / 114272 | training loss: 0.0012088087387382984\n",
      "epoch: 6 | 13664 / 114272 | training loss: 0.0007431417470797896\n",
      "epoch: 6 | 13696 / 114272 | training loss: 0.005743257235735655\n",
      "epoch: 6 | 13728 / 114272 | training loss: 0.17968879640102386\n",
      "epoch: 6 | 13760 / 114272 | training loss: 0.0008153292001225054\n",
      "epoch: 6 | 13792 / 114272 | training loss: 0.003990135621279478\n",
      "epoch: 6 | 13824 / 114272 | training loss: 0.0006183012155815959\n",
      "epoch: 6 | 13856 / 114272 | training loss: 0.0006064327317290008\n",
      "epoch: 6 | 13888 / 114272 | training loss: 0.003514912910759449\n",
      "epoch: 6 | 13920 / 114272 | training loss: 0.0005555070820264518\n",
      "epoch: 6 | 13952 / 114272 | training loss: 0.0004268520278856158\n",
      "epoch: 6 | 13984 / 114272 | training loss: 0.000520927831530571\n",
      "epoch: 6 | 14016 / 114272 | training loss: 0.00046210631262511015\n",
      "epoch: 6 | 14048 / 114272 | training loss: 0.0005215335404500365\n",
      "epoch: 6 | 14080 / 114272 | training loss: 0.0004523025418166071\n",
      "epoch: 6 | 14112 / 114272 | training loss: 0.0009038888965733349\n",
      "epoch: 6 | 14144 / 114272 | training loss: 0.0019615511409938335\n",
      "epoch: 6 | 14176 / 114272 | training loss: 0.0011294290889054537\n",
      "epoch: 6 | 14208 / 114272 | training loss: 0.00031047521042637527\n",
      "epoch: 6 | 14240 / 114272 | training loss: 0.0010865306248888373\n",
      "epoch: 6 | 14272 / 114272 | training loss: 0.09420078247785568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 14304 / 114272 | training loss: 0.0005511533236131072\n",
      "epoch: 6 | 14336 / 114272 | training loss: 0.00102298054844141\n",
      "epoch: 6 | 14368 / 114272 | training loss: 0.001067852834239602\n",
      "epoch: 6 | 14400 / 114272 | training loss: 0.00047796862781979144\n",
      "epoch: 6 | 14432 / 114272 | training loss: 0.08417214453220367\n",
      "epoch: 6 | 14464 / 114272 | training loss: 0.00041388781392015517\n",
      "epoch: 6 | 14496 / 114272 | training loss: 0.0005557160475291312\n",
      "epoch: 6 | 14528 / 114272 | training loss: 0.0007890055421739817\n",
      "epoch: 6 | 14560 / 114272 | training loss: 0.0004565732670016587\n",
      "epoch: 6 | 14592 / 114272 | training loss: 0.0005128410994075239\n",
      "epoch: 6 | 14624 / 114272 | training loss: 0.00044150103349238634\n",
      "epoch: 6 | 14656 / 114272 | training loss: 0.0008285714429803193\n",
      "epoch: 6 | 14688 / 114272 | training loss: 0.0005947554600425065\n",
      "epoch: 6 | 14720 / 114272 | training loss: 0.0008195345290005207\n",
      "epoch: 6 | 14752 / 114272 | training loss: 0.000443712662672624\n",
      "epoch: 6 | 14784 / 114272 | training loss: 0.0006078114965930581\n",
      "epoch: 6 | 14816 / 114272 | training loss: 0.0004829425015486777\n",
      "epoch: 6 | 14848 / 114272 | training loss: 0.1409768909215927\n",
      "epoch: 6 | 14880 / 114272 | training loss: 0.2140844762325287\n",
      "epoch: 6 | 14912 / 114272 | training loss: 0.0004546711570583284\n",
      "epoch: 6 | 14944 / 114272 | training loss: 0.0005830462905578315\n",
      "epoch: 6 | 14976 / 114272 | training loss: 0.0009849974885582924\n",
      "epoch: 6 | 15008 / 114272 | training loss: 0.0009091765969060361\n",
      "epoch: 6 | 15040 / 114272 | training loss: 0.0006802687421441078\n",
      "epoch: 6 | 15072 / 114272 | training loss: 0.0007476835744455457\n",
      "epoch: 6 | 15104 / 114272 | training loss: 0.0005739727057516575\n",
      "epoch: 6 | 15136 / 114272 | training loss: 0.0013827091315761209\n",
      "epoch: 6 | 15168 / 114272 | training loss: 0.0005520464619621634\n",
      "epoch: 6 | 15200 / 114272 | training loss: 0.0018386966548860073\n",
      "epoch: 6 | 15232 / 114272 | training loss: 0.000662727456074208\n",
      "epoch: 6 | 15264 / 114272 | training loss: 0.0009769018506631255\n",
      "epoch: 6 | 15296 / 114272 | training loss: 0.0012141595361754298\n",
      "epoch: 6 | 15328 / 114272 | training loss: 0.001001268276013434\n",
      "epoch: 6 | 15360 / 114272 | training loss: 0.0006089721573516726\n",
      "epoch: 6 | 15392 / 114272 | training loss: 0.0005165896727703512\n",
      "epoch: 6 | 15424 / 114272 | training loss: 0.0021706577390432358\n",
      "epoch: 6 | 15456 / 114272 | training loss: 0.0006369958282448351\n",
      "epoch: 6 | 15488 / 114272 | training loss: 0.0006983369239605963\n",
      "epoch: 6 | 15520 / 114272 | training loss: 0.0018325913697481155\n",
      "epoch: 6 | 15552 / 114272 | training loss: 0.17150768637657166\n",
      "epoch: 6 | 15584 / 114272 | training loss: 0.1865406334400177\n",
      "epoch: 6 | 15616 / 114272 | training loss: 0.00039482390275225043\n",
      "epoch: 6 | 15648 / 114272 | training loss: 0.0005687951925210655\n",
      "epoch: 6 | 15680 / 114272 | training loss: 0.04959581047296524\n",
      "epoch: 6 | 15712 / 114272 | training loss: 0.00029943036497570574\n",
      "epoch: 6 | 15744 / 114272 | training loss: 0.00031913339626044035\n",
      "epoch: 6 | 15776 / 114272 | training loss: 0.0010253404034301639\n",
      "epoch: 6 | 15808 / 114272 | training loss: 0.4424512982368469\n",
      "epoch: 6 | 15840 / 114272 | training loss: 0.0024836715310811996\n",
      "epoch: 6 | 15872 / 114272 | training loss: 0.0007791147800162435\n",
      "epoch: 6 | 15904 / 114272 | training loss: 0.0007029307307675481\n",
      "epoch: 6 | 15936 / 114272 | training loss: 0.14050307869911194\n",
      "epoch: 6 | 15968 / 114272 | training loss: 0.0006221667863428593\n",
      "epoch: 6 | 16000 / 114272 | training loss: 0.0008854587795212865\n",
      "epoch: 6 | 16032 / 114272 | training loss: 0.0018191331764683127\n",
      "epoch: 6 | 16064 / 114272 | training loss: 0.0007126976270228624\n",
      "epoch: 6 | 16096 / 114272 | training loss: 0.0016996975755319\n",
      "epoch: 6 | 16128 / 114272 | training loss: 0.17659364640712738\n",
      "epoch: 6 | 16160 / 114272 | training loss: 0.0003556236915756017\n",
      "epoch: 6 | 16192 / 114272 | training loss: 0.0010317296255379915\n",
      "epoch: 6 | 16224 / 114272 | training loss: 0.0007660561241209507\n",
      "epoch: 6 | 16256 / 114272 | training loss: 0.0012051808880642056\n",
      "epoch: 6 | 16288 / 114272 | training loss: 0.0012253947788849473\n",
      "epoch: 6 | 16320 / 114272 | training loss: 0.0012094273697584867\n",
      "epoch: 6 | 16352 / 114272 | training loss: 0.0010438987519592047\n",
      "epoch: 6 | 16384 / 114272 | training loss: 0.0015061558224260807\n",
      "epoch: 6 | 16416 / 114272 | training loss: 0.0008266205550171435\n",
      "epoch: 6 | 16448 / 114272 | training loss: 0.0005236996803432703\n",
      "epoch: 6 | 16480 / 114272 | training loss: 0.0037171056028455496\n",
      "epoch: 6 | 16512 / 114272 | training loss: 0.0009120379691012204\n",
      "epoch: 6 | 16544 / 114272 | training loss: 0.0009900631848722696\n",
      "epoch: 6 | 16576 / 114272 | training loss: 0.11684543639421463\n",
      "epoch: 6 | 16608 / 114272 | training loss: 0.0009412438375875354\n",
      "epoch: 6 | 16640 / 114272 | training loss: 0.0015599919715896249\n",
      "epoch: 6 | 16672 / 114272 | training loss: 0.002878249157220125\n",
      "epoch: 6 | 16704 / 114272 | training loss: 0.09773504734039307\n",
      "epoch: 6 | 16736 / 114272 | training loss: 0.0008468683809041977\n",
      "epoch: 6 | 16768 / 114272 | training loss: 0.0005765990354120731\n",
      "epoch: 6 | 16800 / 114272 | training loss: 0.0005413212929852307\n",
      "epoch: 6 | 16832 / 114272 | training loss: 0.0007424221839755774\n",
      "epoch: 6 | 16864 / 114272 | training loss: 0.07827099412679672\n",
      "epoch: 6 | 16896 / 114272 | training loss: 0.0009951821994036436\n",
      "epoch: 6 | 16928 / 114272 | training loss: 0.045203372836112976\n",
      "epoch: 6 | 16960 / 114272 | training loss: 0.001416932325810194\n",
      "epoch: 6 | 16992 / 114272 | training loss: 0.000683703226968646\n",
      "epoch: 6 | 17024 / 114272 | training loss: 0.0006048112409189343\n",
      "epoch: 6 | 17056 / 114272 | training loss: 0.0007737550185993314\n",
      "epoch: 6 | 17088 / 114272 | training loss: 0.0019071296555921435\n",
      "epoch: 6 | 17120 / 114272 | training loss: 0.0009731645113788545\n",
      "epoch: 6 | 17152 / 114272 | training loss: 0.00041613526991568506\n",
      "epoch: 6 | 17184 / 114272 | training loss: 0.027717774733901024\n",
      "epoch: 6 | 17216 / 114272 | training loss: 0.0025579396169632673\n",
      "epoch: 6 | 17248 / 114272 | training loss: 0.0005092125502415001\n",
      "epoch: 6 | 17280 / 114272 | training loss: 0.0008540406124666333\n",
      "epoch: 6 | 17312 / 114272 | training loss: 0.00044541546958498657\n",
      "epoch: 6 | 17344 / 114272 | training loss: 0.0008535188972018659\n",
      "epoch: 6 | 17376 / 114272 | training loss: 0.0004207013116683811\n",
      "epoch: 6 | 17408 / 114272 | training loss: 0.0007920192438177764\n",
      "epoch: 6 | 17440 / 114272 | training loss: 0.14407916367053986\n",
      "epoch: 6 | 17472 / 114272 | training loss: 0.0007931467262096703\n",
      "epoch: 6 | 17504 / 114272 | training loss: 0.001249695080332458\n",
      "epoch: 6 | 17536 / 114272 | training loss: 0.0006424720631912351\n",
      "epoch: 6 | 17568 / 114272 | training loss: 0.021754996851086617\n",
      "epoch: 6 | 17600 / 114272 | training loss: 0.001561959506943822\n",
      "epoch: 6 | 17632 / 114272 | training loss: 0.000287225324427709\n",
      "epoch: 6 | 17664 / 114272 | training loss: 0.22414720058441162\n",
      "epoch: 6 | 17696 / 114272 | training loss: 0.00024515134282410145\n",
      "epoch: 6 | 17728 / 114272 | training loss: 0.0023537122178822756\n",
      "epoch: 6 | 17760 / 114272 | training loss: 0.0011557104298844934\n",
      "epoch: 6 | 17792 / 114272 | training loss: 0.13954998552799225\n",
      "epoch: 6 | 17824 / 114272 | training loss: 0.001199087593704462\n",
      "epoch: 6 | 17856 / 114272 | training loss: 0.0005705261137336493\n",
      "epoch: 6 | 17888 / 114272 | training loss: 0.00039953942177817225\n",
      "epoch: 6 | 17920 / 114272 | training loss: 0.0009217492770403624\n",
      "epoch: 6 | 17952 / 114272 | training loss: 0.0009736667852848768\n",
      "epoch: 6 | 17984 / 114272 | training loss: 0.000425884296419099\n",
      "epoch: 6 | 18016 / 114272 | training loss: 0.0005345617537386715\n",
      "epoch: 6 | 18048 / 114272 | training loss: 0.26907020807266235\n",
      "epoch: 6 | 18080 / 114272 | training loss: 0.180252805352211\n",
      "epoch: 6 | 18112 / 114272 | training loss: 0.0006267561111599207\n",
      "epoch: 6 | 18144 / 114272 | training loss: 0.001011743675917387\n",
      "epoch: 6 | 18176 / 114272 | training loss: 0.03191421180963516\n",
      "epoch: 6 | 18208 / 114272 | training loss: 0.0024931312073022127\n",
      "epoch: 6 | 18240 / 114272 | training loss: 0.1697336733341217\n",
      "epoch: 6 | 18272 / 114272 | training loss: 0.09338919073343277\n",
      "epoch: 6 | 18304 / 114272 | training loss: 0.0012356556253507733\n",
      "epoch: 6 | 18336 / 114272 | training loss: 0.0014426617417484522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 18368 / 114272 | training loss: 0.0007336360868066549\n",
      "epoch: 6 | 18400 / 114272 | training loss: 0.0015535790007561445\n",
      "epoch: 6 | 18432 / 114272 | training loss: 0.0006074989796616137\n",
      "epoch: 6 | 18464 / 114272 | training loss: 0.0005904158460907638\n",
      "epoch: 6 | 18496 / 114272 | training loss: 0.002374122617766261\n",
      "epoch: 6 | 18528 / 114272 | training loss: 0.08084964752197266\n",
      "epoch: 6 | 18560 / 114272 | training loss: 0.0007877728785388172\n",
      "epoch: 6 | 18592 / 114272 | training loss: 0.004440910182893276\n",
      "epoch: 6 | 18624 / 114272 | training loss: 0.0007932629087008536\n",
      "epoch: 6 | 18656 / 114272 | training loss: 0.0016088619595393538\n",
      "epoch: 6 | 18688 / 114272 | training loss: 0.12678839266300201\n",
      "epoch: 6 | 18720 / 114272 | training loss: 0.0010683631990104914\n",
      "epoch: 6 | 18752 / 114272 | training loss: 0.00034527445677667856\n",
      "epoch: 6 | 18784 / 114272 | training loss: 0.05978022515773773\n",
      "epoch: 6 | 18816 / 114272 | training loss: 0.0015243629459291697\n",
      "epoch: 6 | 18848 / 114272 | training loss: 0.0016232812777161598\n",
      "epoch: 6 | 18880 / 114272 | training loss: 0.26702284812927246\n",
      "epoch: 6 | 18912 / 114272 | training loss: 0.0010361018357798457\n",
      "epoch: 6 | 18944 / 114272 | training loss: 0.024695197120308876\n",
      "epoch: 6 | 18976 / 114272 | training loss: 0.0009151870617642999\n",
      "epoch: 6 | 19008 / 114272 | training loss: 0.0015968126244843006\n",
      "epoch: 6 | 19040 / 114272 | training loss: 0.2598591148853302\n",
      "epoch: 6 | 19072 / 114272 | training loss: 0.00042761024087667465\n",
      "epoch: 6 | 19104 / 114272 | training loss: 0.002577797044068575\n",
      "epoch: 6 | 19136 / 114272 | training loss: 0.0011572417570278049\n",
      "epoch: 6 | 19168 / 114272 | training loss: 0.0026003308594226837\n",
      "epoch: 6 | 19200 / 114272 | training loss: 0.0023274787236005068\n",
      "epoch: 6 | 19232 / 114272 | training loss: 0.0019813715480268\n",
      "epoch: 6 | 19264 / 114272 | training loss: 0.0008255677530542016\n",
      "epoch: 6 | 19296 / 114272 | training loss: 0.2439863532781601\n",
      "epoch: 6 | 19328 / 114272 | training loss: 0.0008017265354283154\n",
      "epoch: 6 | 19360 / 114272 | training loss: 0.002384646562859416\n",
      "epoch: 6 | 19392 / 114272 | training loss: 0.004145388025790453\n",
      "epoch: 6 | 19424 / 114272 | training loss: 0.13704900443553925\n",
      "epoch: 6 | 19456 / 114272 | training loss: 0.0027688131667673588\n",
      "epoch: 6 | 19488 / 114272 | training loss: 0.0014364970847964287\n",
      "epoch: 6 | 19520 / 114272 | training loss: 0.26589125394821167\n",
      "epoch: 6 | 19552 / 114272 | training loss: 0.0005831244634464383\n",
      "epoch: 6 | 19584 / 114272 | training loss: 0.0017123905709013343\n",
      "epoch: 6 | 19616 / 114272 | training loss: 0.0024521842133253813\n",
      "epoch: 6 | 19648 / 114272 | training loss: 0.0020639742724597454\n",
      "epoch: 6 | 19680 / 114272 | training loss: 0.0008694644784554839\n",
      "epoch: 6 | 19712 / 114272 | training loss: 0.0011093742214143276\n",
      "epoch: 6 | 19744 / 114272 | training loss: 0.020169107243418694\n",
      "epoch: 6 | 19776 / 114272 | training loss: 0.004798633512109518\n",
      "epoch: 6 | 19808 / 114272 | training loss: 0.001612733700312674\n",
      "epoch: 6 | 19840 / 114272 | training loss: 0.0009403787553310394\n",
      "epoch: 6 | 19872 / 114272 | training loss: 0.0014609170611947775\n",
      "epoch: 6 | 19904 / 114272 | training loss: 0.0012829573825001717\n",
      "epoch: 6 | 19936 / 114272 | training loss: 0.0013045785017311573\n",
      "epoch: 6 | 19968 / 114272 | training loss: 0.000676710857078433\n",
      "epoch: 6 | 20000 / 114272 | training loss: 0.008236200548708439\n",
      "epoch: 6 | 20032 / 114272 | training loss: 0.0011885720305144787\n",
      "epoch: 6 | 20064 / 114272 | training loss: 0.0007818142767064273\n",
      "epoch: 6 | 20096 / 114272 | training loss: 0.0013480630004778504\n",
      "epoch: 6 | 20128 / 114272 | training loss: 0.0017866704147309065\n",
      "epoch: 6 | 20160 / 114272 | training loss: 0.0016189435264095664\n",
      "epoch: 6 | 20192 / 114272 | training loss: 0.000801375659648329\n",
      "epoch: 6 | 20224 / 114272 | training loss: 0.0007614016649313271\n",
      "epoch: 6 | 20256 / 114272 | training loss: 0.005834721028804779\n",
      "epoch: 6 | 20288 / 114272 | training loss: 0.0009036152041517198\n",
      "epoch: 6 | 20320 / 114272 | training loss: 0.0010708202607929707\n",
      "epoch: 6 | 20352 / 114272 | training loss: 0.001042327145114541\n",
      "epoch: 6 | 20384 / 114272 | training loss: 0.002025185851380229\n",
      "epoch: 6 | 20416 / 114272 | training loss: 0.016595065593719482\n",
      "epoch: 6 | 20448 / 114272 | training loss: 0.002149899024516344\n",
      "epoch: 6 | 20480 / 114272 | training loss: 0.10166198760271072\n",
      "epoch: 6 | 20512 / 114272 | training loss: 0.0005528964102268219\n",
      "epoch: 6 | 20544 / 114272 | training loss: 0.0009993152925744653\n",
      "epoch: 6 | 20576 / 114272 | training loss: 0.0011231841053813696\n",
      "epoch: 6 | 20608 / 114272 | training loss: 0.0015844051958993077\n",
      "epoch: 6 | 20640 / 114272 | training loss: 0.0010362549219280481\n",
      "epoch: 6 | 20672 / 114272 | training loss: 0.001349458354525268\n",
      "epoch: 6 | 20704 / 114272 | training loss: 0.0009330365573987365\n",
      "epoch: 6 | 20736 / 114272 | training loss: 0.0014815881149843335\n",
      "epoch: 6 | 20768 / 114272 | training loss: 0.0009321403340436518\n",
      "epoch: 6 | 20800 / 114272 | training loss: 0.0009581646881997585\n",
      "epoch: 6 | 20832 / 114272 | training loss: 0.0010151462629437447\n",
      "epoch: 6 | 20864 / 114272 | training loss: 0.05566130205988884\n",
      "epoch: 6 | 20896 / 114272 | training loss: 0.0014319202164188027\n",
      "epoch: 6 | 20928 / 114272 | training loss: 0.0007314348476938903\n",
      "epoch: 6 | 20960 / 114272 | training loss: 0.0011330604320392013\n",
      "epoch: 6 | 20992 / 114272 | training loss: 0.00046583940275013447\n",
      "epoch: 6 | 21024 / 114272 | training loss: 0.0005807771231047809\n",
      "epoch: 6 | 21056 / 114272 | training loss: 0.17739805579185486\n",
      "epoch: 6 | 21088 / 114272 | training loss: 0.0006152024143375456\n",
      "epoch: 6 | 21120 / 114272 | training loss: 0.0012851355131715536\n",
      "epoch: 6 | 21152 / 114272 | training loss: 0.0007262876606546342\n",
      "epoch: 6 | 21184 / 114272 | training loss: 0.014067420735955238\n",
      "epoch: 6 | 21216 / 114272 | training loss: 0.000554046593606472\n",
      "epoch: 6 | 21248 / 114272 | training loss: 0.05387045815587044\n",
      "epoch: 6 | 21280 / 114272 | training loss: 0.003352529602125287\n",
      "epoch: 6 | 21312 / 114272 | training loss: 0.0009596130694262683\n",
      "epoch: 6 | 21344 / 114272 | training loss: 0.002134050242602825\n",
      "epoch: 6 | 21376 / 114272 | training loss: 0.0011557163670659065\n",
      "epoch: 6 | 21408 / 114272 | training loss: 0.0012549099046736956\n",
      "epoch: 6 | 21440 / 114272 | training loss: 0.0011271026451140642\n",
      "epoch: 6 | 21472 / 114272 | training loss: 0.0025713350623846054\n",
      "epoch: 6 | 21504 / 114272 | training loss: 0.0008880282985046506\n",
      "epoch: 6 | 21536 / 114272 | training loss: 0.0007003059145063162\n",
      "epoch: 6 | 21568 / 114272 | training loss: 0.0009325766004621983\n",
      "epoch: 6 | 21600 / 114272 | training loss: 0.0004993045004084706\n",
      "epoch: 6 | 21632 / 114272 | training loss: 0.0008938491228036582\n",
      "epoch: 6 | 21664 / 114272 | training loss: 0.07057825475931168\n",
      "epoch: 6 | 21696 / 114272 | training loss: 0.00043796325917355716\n",
      "epoch: 6 | 21728 / 114272 | training loss: 0.0009031519875861704\n",
      "epoch: 6 | 21760 / 114272 | training loss: 0.0009471963858231902\n",
      "epoch: 6 | 21792 / 114272 | training loss: 0.0009244832908734679\n",
      "epoch: 6 | 21824 / 114272 | training loss: 0.001800076337531209\n",
      "epoch: 6 | 21856 / 114272 | training loss: 0.0008406681590713561\n",
      "epoch: 6 | 21888 / 114272 | training loss: 0.0006568327080458403\n",
      "epoch: 6 | 21920 / 114272 | training loss: 0.0007814273121766746\n",
      "epoch: 6 | 21952 / 114272 | training loss: 0.001475157798267901\n",
      "epoch: 6 | 21984 / 114272 | training loss: 0.16551294922828674\n",
      "epoch: 6 | 22016 / 114272 | training loss: 0.010234193876385689\n",
      "epoch: 6 | 22048 / 114272 | training loss: 0.0010887503158301115\n",
      "epoch: 6 | 22080 / 114272 | training loss: 0.0009687668643891811\n",
      "epoch: 6 | 22112 / 114272 | training loss: 0.16753238439559937\n",
      "epoch: 6 | 22144 / 114272 | training loss: 0.0036441562697291374\n",
      "epoch: 6 | 22176 / 114272 | training loss: 0.06933707743883133\n",
      "epoch: 6 | 22208 / 114272 | training loss: 0.0010114888427779078\n",
      "epoch: 6 | 22240 / 114272 | training loss: 0.0007545888656750321\n",
      "epoch: 6 | 22272 / 114272 | training loss: 0.0031166698317974806\n",
      "epoch: 6 | 22304 / 114272 | training loss: 0.0009254916803911328\n",
      "epoch: 6 | 22336 / 114272 | training loss: 0.0014144658343866467\n",
      "epoch: 6 | 22368 / 114272 | training loss: 0.0015526777133345604\n",
      "epoch: 6 | 22400 / 114272 | training loss: 0.000865728419739753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 22432 / 114272 | training loss: 0.000872082426212728\n",
      "epoch: 6 | 22464 / 114272 | training loss: 0.0028057857416570187\n",
      "epoch: 6 | 22496 / 114272 | training loss: 0.004637429025024176\n",
      "epoch: 6 | 22528 / 114272 | training loss: 0.0008443320984952152\n",
      "epoch: 6 | 22560 / 114272 | training loss: 0.0009126149816438556\n",
      "epoch: 6 | 22592 / 114272 | training loss: 0.0016381905879825354\n",
      "epoch: 6 | 22624 / 114272 | training loss: 0.0007132358732633293\n",
      "epoch: 6 | 22656 / 114272 | training loss: 0.0006424665334634483\n",
      "epoch: 6 | 22688 / 114272 | training loss: 0.001632695086300373\n",
      "epoch: 6 | 22720 / 114272 | training loss: 0.001214783638715744\n",
      "epoch: 6 | 22752 / 114272 | training loss: 0.0004866757954005152\n",
      "epoch: 6 | 22784 / 114272 | training loss: 0.0010996608762070537\n",
      "epoch: 6 | 22816 / 114272 | training loss: 0.0009230305440723896\n",
      "epoch: 6 | 22848 / 114272 | training loss: 0.0009113898850046098\n",
      "epoch: 6 | 22880 / 114272 | training loss: 0.0007981092785485089\n",
      "epoch: 6 | 22912 / 114272 | training loss: 0.0005351583240553737\n",
      "epoch: 6 | 22944 / 114272 | training loss: 0.006164160557091236\n",
      "epoch: 6 | 22976 / 114272 | training loss: 0.0009891402442008257\n",
      "epoch: 6 | 23008 / 114272 | training loss: 0.0012301354436203837\n",
      "epoch: 6 | 23040 / 114272 | training loss: 0.00046362626017071307\n",
      "epoch: 6 | 23072 / 114272 | training loss: 0.0010484461672604084\n",
      "epoch: 6 | 23104 / 114272 | training loss: 0.2442481815814972\n",
      "epoch: 6 | 23136 / 114272 | training loss: 0.0006976802833378315\n",
      "epoch: 6 | 23168 / 114272 | training loss: 0.004026062786579132\n",
      "epoch: 6 | 23200 / 114272 | training loss: 0.0009640358039177954\n",
      "epoch: 6 | 23232 / 114272 | training loss: 0.0018982854671776295\n",
      "epoch: 6 | 23264 / 114272 | training loss: 0.0009569200919941068\n",
      "epoch: 6 | 23296 / 114272 | training loss: 0.0011851947056129575\n",
      "epoch: 6 | 23328 / 114272 | training loss: 0.001013565226458013\n",
      "epoch: 6 | 23360 / 114272 | training loss: 0.1184212937951088\n",
      "epoch: 6 | 23392 / 114272 | training loss: 0.0006513274856843054\n",
      "epoch: 6 | 23424 / 114272 | training loss: 0.2874954640865326\n",
      "epoch: 6 | 23456 / 114272 | training loss: 0.14813151955604553\n",
      "epoch: 6 | 23488 / 114272 | training loss: 0.0008810202707536519\n",
      "epoch: 6 | 23520 / 114272 | training loss: 0.0007667289464734495\n",
      "epoch: 6 | 23552 / 114272 | training loss: 0.0006622080109082162\n",
      "epoch: 6 | 23584 / 114272 | training loss: 0.001124230562709272\n",
      "epoch: 6 | 23616 / 114272 | training loss: 0.1003955751657486\n",
      "epoch: 6 | 23648 / 114272 | training loss: 0.0036413283087313175\n",
      "epoch: 6 | 23680 / 114272 | training loss: 0.0021066407207399607\n",
      "epoch: 6 | 23712 / 114272 | training loss: 0.001390513963997364\n",
      "epoch: 6 | 23744 / 114272 | training loss: 0.0008338112384080887\n",
      "epoch: 6 | 23776 / 114272 | training loss: 0.0019197335932403803\n",
      "epoch: 6 | 23808 / 114272 | training loss: 0.0016558729112148285\n",
      "epoch: 6 | 23840 / 114272 | training loss: 0.26960691809654236\n",
      "epoch: 6 | 23872 / 114272 | training loss: 0.10223552584648132\n",
      "epoch: 6 | 23904 / 114272 | training loss: 0.0011937088565900922\n",
      "epoch: 6 | 23936 / 114272 | training loss: 0.18190255761146545\n",
      "epoch: 6 | 23968 / 114272 | training loss: 0.0009743893751874566\n",
      "epoch: 6 | 24000 / 114272 | training loss: 0.0011580672580748796\n",
      "epoch: 6 | 24032 / 114272 | training loss: 0.10714859515428543\n",
      "epoch: 6 | 24064 / 114272 | training loss: 0.0015980774769559503\n",
      "epoch: 6 | 24096 / 114272 | training loss: 0.0011859445367008448\n",
      "epoch: 6 | 24128 / 114272 | training loss: 0.0011964298319071531\n",
      "epoch: 6 | 24160 / 114272 | training loss: 0.17080754041671753\n",
      "epoch: 6 | 24192 / 114272 | training loss: 0.0017224304610863328\n",
      "epoch: 6 | 24224 / 114272 | training loss: 0.0007019459153525531\n",
      "epoch: 6 | 24256 / 114272 | training loss: 0.0015530482633039355\n",
      "epoch: 6 | 24288 / 114272 | training loss: 0.0020983268041163683\n",
      "epoch: 6 | 24320 / 114272 | training loss: 0.10360733419656754\n",
      "epoch: 6 | 24352 / 114272 | training loss: 0.0009102384792640805\n",
      "epoch: 6 | 24384 / 114272 | training loss: 0.002659683348610997\n",
      "epoch: 6 | 24416 / 114272 | training loss: 0.0005473711644299328\n",
      "epoch: 6 | 24448 / 114272 | training loss: 0.001375337247736752\n",
      "epoch: 6 | 24480 / 114272 | training loss: 0.000808555691037327\n",
      "epoch: 6 | 24512 / 114272 | training loss: 0.0010961279040202498\n",
      "epoch: 6 | 24544 / 114272 | training loss: 0.20771965384483337\n",
      "epoch: 6 | 24576 / 114272 | training loss: 0.0008507727761752903\n",
      "epoch: 6 | 24608 / 114272 | training loss: 0.001562341582030058\n",
      "epoch: 6 | 24640 / 114272 | training loss: 0.001025347039103508\n",
      "epoch: 6 | 24672 / 114272 | training loss: 0.0008908849558793008\n",
      "epoch: 6 | 24704 / 114272 | training loss: 0.0012297672219574451\n",
      "epoch: 6 | 24736 / 114272 | training loss: 0.0007271290523931384\n",
      "epoch: 6 | 24768 / 114272 | training loss: 0.001974755432456732\n",
      "epoch: 6 | 24800 / 114272 | training loss: 0.000817703315988183\n",
      "epoch: 6 | 24832 / 114272 | training loss: 0.0006415880052372813\n",
      "epoch: 6 | 24864 / 114272 | training loss: 0.0006946698413230479\n",
      "epoch: 6 | 24896 / 114272 | training loss: 0.007091600447893143\n",
      "epoch: 6 | 24928 / 114272 | training loss: 0.0007667067693546414\n",
      "epoch: 6 | 24960 / 114272 | training loss: 0.0013272925280034542\n",
      "epoch: 6 | 24992 / 114272 | training loss: 0.0009782336419448256\n",
      "epoch: 6 | 25024 / 114272 | training loss: 0.0017300208564847708\n",
      "epoch: 6 | 25056 / 114272 | training loss: 0.0005206348723731935\n",
      "epoch: 6 | 25088 / 114272 | training loss: 0.0011082623386755586\n",
      "epoch: 6 | 25120 / 114272 | training loss: 0.24256837368011475\n",
      "epoch: 6 | 25152 / 114272 | training loss: 0.0008620030712336302\n",
      "epoch: 6 | 25184 / 114272 | training loss: 0.0005568943452090025\n",
      "epoch: 6 | 25216 / 114272 | training loss: 0.000685392355080694\n",
      "epoch: 6 | 25248 / 114272 | training loss: 0.0010200169635936618\n",
      "epoch: 6 | 25280 / 114272 | training loss: 0.0008047005394473672\n",
      "epoch: 6 | 25312 / 114272 | training loss: 0.001215407159179449\n",
      "epoch: 6 | 25344 / 114272 | training loss: 0.0009100564057007432\n",
      "epoch: 6 | 25376 / 114272 | training loss: 0.0013503856025636196\n",
      "epoch: 6 | 25408 / 114272 | training loss: 0.00039466359885409474\n",
      "epoch: 6 | 25440 / 114272 | training loss: 0.0013358338037505746\n",
      "epoch: 6 | 25472 / 114272 | training loss: 0.0008089623297564685\n",
      "epoch: 6 | 25504 / 114272 | training loss: 0.0005097634857520461\n",
      "epoch: 6 | 25536 / 114272 | training loss: 0.11194904893636703\n",
      "epoch: 6 | 25568 / 114272 | training loss: 0.0036905372980982065\n",
      "epoch: 6 | 25600 / 114272 | training loss: 0.23763148486614227\n",
      "epoch: 6 | 25632 / 114272 | training loss: 0.0007248208275996149\n",
      "epoch: 6 | 25664 / 114272 | training loss: 0.0014336435124278069\n",
      "epoch: 6 | 25696 / 114272 | training loss: 0.0006100506288930774\n",
      "epoch: 6 | 25728 / 114272 | training loss: 0.0003933646366931498\n",
      "epoch: 6 | 25760 / 114272 | training loss: 0.0008385690161958337\n",
      "epoch: 6 | 25792 / 114272 | training loss: 0.0012386706657707691\n",
      "epoch: 6 | 25824 / 114272 | training loss: 0.0022174238692969084\n",
      "epoch: 6 | 25856 / 114272 | training loss: 0.0008368780836462975\n",
      "epoch: 6 | 25888 / 114272 | training loss: 0.000613643613178283\n",
      "epoch: 6 | 25920 / 114272 | training loss: 0.0006243885727599263\n",
      "epoch: 6 | 25952 / 114272 | training loss: 0.0007398126181215048\n",
      "epoch: 6 | 25984 / 114272 | training loss: 0.04030768200755119\n",
      "epoch: 6 | 26016 / 114272 | training loss: 0.000889271090272814\n",
      "epoch: 6 | 26048 / 114272 | training loss: 0.18074485659599304\n",
      "epoch: 6 | 26080 / 114272 | training loss: 0.0005325500969775021\n",
      "epoch: 6 | 26112 / 114272 | training loss: 0.00077030248939991\n",
      "epoch: 6 | 26144 / 114272 | training loss: 0.007342570461332798\n",
      "epoch: 6 | 26176 / 114272 | training loss: 0.001134539837948978\n",
      "epoch: 6 | 26208 / 114272 | training loss: 0.0011114703956991434\n",
      "epoch: 6 | 26240 / 114272 | training loss: 0.0013652127236127853\n",
      "epoch: 6 | 26272 / 114272 | training loss: 0.0007668529287911952\n",
      "epoch: 6 | 26304 / 114272 | training loss: 0.004680638201534748\n",
      "epoch: 6 | 26336 / 114272 | training loss: 0.0016453280113637447\n",
      "epoch: 6 | 26368 / 114272 | training loss: 0.0010057527106255293\n",
      "epoch: 6 | 26400 / 114272 | training loss: 0.0006164876976981759\n",
      "epoch: 6 | 26432 / 114272 | training loss: 0.0007510192808695138\n",
      "epoch: 6 | 26464 / 114272 | training loss: 0.013334047980606556\n",
      "epoch: 6 | 26496 / 114272 | training loss: 0.0013008774258196354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 26528 / 114272 | training loss: 0.0015789035242050886\n",
      "epoch: 6 | 26560 / 114272 | training loss: 0.000577887985855341\n",
      "epoch: 6 | 26592 / 114272 | training loss: 0.0007478186162188649\n",
      "epoch: 6 | 26624 / 114272 | training loss: 0.000566481554415077\n",
      "epoch: 6 | 26656 / 114272 | training loss: 0.17240597307682037\n",
      "epoch: 6 | 26688 / 114272 | training loss: 0.0009638369083404541\n",
      "epoch: 6 | 26720 / 114272 | training loss: 0.0006938090082257986\n",
      "epoch: 6 | 26752 / 114272 | training loss: 0.18847514688968658\n",
      "epoch: 6 | 26784 / 114272 | training loss: 0.0006687474669888616\n",
      "epoch: 6 | 26816 / 114272 | training loss: 0.00049794209189713\n",
      "epoch: 6 | 26848 / 114272 | training loss: 0.16978050768375397\n",
      "epoch: 6 | 26880 / 114272 | training loss: 0.000983034260571003\n",
      "epoch: 6 | 26912 / 114272 | training loss: 0.0014183380408212543\n",
      "epoch: 6 | 26944 / 114272 | training loss: 0.002195566426962614\n",
      "epoch: 6 | 26976 / 114272 | training loss: 0.002435927512124181\n",
      "epoch: 6 | 27008 / 114272 | training loss: 0.19614450633525848\n",
      "epoch: 6 | 27040 / 114272 | training loss: 0.15781907737255096\n",
      "epoch: 6 | 27072 / 114272 | training loss: 0.0006561115151271224\n",
      "epoch: 6 | 27104 / 114272 | training loss: 0.1385437548160553\n",
      "epoch: 6 | 27136 / 114272 | training loss: 0.0015614451840519905\n",
      "epoch: 6 | 27168 / 114272 | training loss: 0.062072474509477615\n",
      "epoch: 6 | 27200 / 114272 | training loss: 0.018284769728779793\n",
      "epoch: 6 | 27232 / 114272 | training loss: 0.0013598866062238812\n",
      "epoch: 6 | 27264 / 114272 | training loss: 0.002509809099137783\n",
      "epoch: 6 | 27296 / 114272 | training loss: 0.0018336786888539791\n",
      "epoch: 6 | 27328 / 114272 | training loss: 0.003125411458313465\n",
      "epoch: 6 | 27360 / 114272 | training loss: 0.004011473618447781\n",
      "epoch: 6 | 27392 / 114272 | training loss: 0.0016542599769309163\n",
      "epoch: 6 | 27424 / 114272 | training loss: 0.00950626190751791\n",
      "epoch: 6 | 27456 / 114272 | training loss: 0.0026402194052934647\n",
      "epoch: 6 | 27488 / 114272 | training loss: 0.0009217380429618061\n",
      "epoch: 6 | 27520 / 114272 | training loss: 0.002598660998046398\n",
      "epoch: 6 | 27552 / 114272 | training loss: 0.0015656999312341213\n",
      "epoch: 6 | 27584 / 114272 | training loss: 0.0016712386859580874\n",
      "epoch: 6 | 27616 / 114272 | training loss: 0.0015276854392141104\n",
      "epoch: 6 | 27648 / 114272 | training loss: 0.0008241230971179903\n",
      "epoch: 6 | 27680 / 114272 | training loss: 0.008107589557766914\n",
      "epoch: 6 | 27712 / 114272 | training loss: 0.0010082203662022948\n",
      "epoch: 6 | 27744 / 114272 | training loss: 0.002029102062806487\n",
      "epoch: 6 | 27776 / 114272 | training loss: 0.12431848049163818\n",
      "epoch: 6 | 27808 / 114272 | training loss: 0.0013109578285366297\n",
      "epoch: 6 | 27840 / 114272 | training loss: 0.0013750768266618252\n",
      "epoch: 6 | 27872 / 114272 | training loss: 0.002796335844323039\n",
      "epoch: 6 | 27904 / 114272 | training loss: 0.001186812180094421\n",
      "epoch: 6 | 27936 / 114272 | training loss: 0.0006795870140194893\n",
      "epoch: 6 | 27968 / 114272 | training loss: 0.005976565182209015\n",
      "epoch: 6 | 28000 / 114272 | training loss: 0.0011704752687364817\n",
      "epoch: 6 | 28032 / 114272 | training loss: 0.001982851419597864\n",
      "epoch: 6 | 28064 / 114272 | training loss: 0.1431005299091339\n",
      "epoch: 6 | 28096 / 114272 | training loss: 0.0010396052384749055\n",
      "epoch: 6 | 28128 / 114272 | training loss: 0.003289683721959591\n",
      "epoch: 6 | 28160 / 114272 | training loss: 0.001129880896769464\n",
      "epoch: 6 | 28192 / 114272 | training loss: 0.0012529859086498618\n",
      "epoch: 6 | 28224 / 114272 | training loss: 0.001525221741758287\n",
      "epoch: 6 | 28256 / 114272 | training loss: 0.002238485962152481\n",
      "epoch: 6 | 28288 / 114272 | training loss: 0.0011672539403662086\n",
      "epoch: 6 | 28320 / 114272 | training loss: 0.0006172825233079493\n",
      "epoch: 6 | 28352 / 114272 | training loss: 0.001138987485319376\n",
      "epoch: 6 | 28384 / 114272 | training loss: 0.0014771242858842015\n",
      "epoch: 6 | 28416 / 114272 | training loss: 0.002556326100602746\n",
      "epoch: 6 | 28448 / 114272 | training loss: 0.0010627912124618888\n",
      "epoch: 6 | 28480 / 114272 | training loss: 0.0005194360273890197\n",
      "epoch: 6 | 28512 / 114272 | training loss: 0.0007355723064392805\n",
      "epoch: 6 | 28544 / 114272 | training loss: 0.000722847762517631\n",
      "epoch: 6 | 28576 / 114272 | training loss: 0.10850303620100021\n",
      "epoch: 6 | 28608 / 114272 | training loss: 0.0020779911428689957\n",
      "epoch: 6 | 28640 / 114272 | training loss: 0.0025298153050243855\n",
      "epoch: 6 | 28672 / 114272 | training loss: 0.00076729228021577\n",
      "epoch: 6 | 28704 / 114272 | training loss: 0.004078210331499577\n",
      "epoch: 6 | 28736 / 114272 | training loss: 0.06108296290040016\n",
      "epoch: 6 | 28768 / 114272 | training loss: 0.0005977310938760638\n",
      "epoch: 6 | 28800 / 114272 | training loss: 0.000970677298028022\n",
      "epoch: 6 | 28832 / 114272 | training loss: 0.15478090941905975\n",
      "epoch: 6 | 28864 / 114272 | training loss: 0.0005055826622992754\n",
      "epoch: 6 | 28896 / 114272 | training loss: 0.0009195570019073784\n",
      "epoch: 6 | 28928 / 114272 | training loss: 0.0011923679849132895\n",
      "epoch: 6 | 28960 / 114272 | training loss: 0.0006918834988027811\n",
      "epoch: 6 | 28992 / 114272 | training loss: 0.0014661405002698302\n",
      "epoch: 6 | 29024 / 114272 | training loss: 0.000975003291387111\n",
      "epoch: 6 | 29056 / 114272 | training loss: 0.0003388954501133412\n",
      "epoch: 6 | 29088 / 114272 | training loss: 0.0009079418960027397\n",
      "epoch: 6 | 29120 / 114272 | training loss: 0.0025945715606212616\n",
      "epoch: 6 | 29152 / 114272 | training loss: 0.006686733569949865\n",
      "epoch: 6 | 29184 / 114272 | training loss: 0.0009738720254972577\n",
      "epoch: 6 | 29216 / 114272 | training loss: 0.0008458093507215381\n",
      "epoch: 6 | 29248 / 114272 | training loss: 0.00039835029747337103\n",
      "epoch: 6 | 29280 / 114272 | training loss: 0.0014053438790142536\n",
      "epoch: 6 | 29312 / 114272 | training loss: 0.0014146255562081933\n",
      "epoch: 6 | 29344 / 114272 | training loss: 0.0009709225269034505\n",
      "epoch: 6 | 29376 / 114272 | training loss: 0.000912408868316561\n",
      "epoch: 6 | 29408 / 114272 | training loss: 0.002802223199978471\n",
      "epoch: 6 | 29440 / 114272 | training loss: 0.04814055934548378\n",
      "epoch: 6 | 29472 / 114272 | training loss: 0.0011308378307148814\n",
      "epoch: 6 | 29504 / 114272 | training loss: 0.0008147775661200285\n",
      "epoch: 6 | 29536 / 114272 | training loss: 0.0008858294459059834\n",
      "epoch: 6 | 29568 / 114272 | training loss: 0.0010462014470249414\n",
      "epoch: 6 | 29600 / 114272 | training loss: 0.0008286606171168387\n",
      "epoch: 6 | 29632 / 114272 | training loss: 0.0019052011193707585\n",
      "epoch: 6 | 29664 / 114272 | training loss: 0.0014651917153969407\n",
      "epoch: 6 | 29696 / 114272 | training loss: 0.0014430200681090355\n",
      "epoch: 6 | 29728 / 114272 | training loss: 0.000994589296169579\n",
      "epoch: 6 | 29760 / 114272 | training loss: 0.0005969107733108103\n",
      "epoch: 6 | 29792 / 114272 | training loss: 0.0006558506283909082\n",
      "epoch: 6 | 29824 / 114272 | training loss: 0.00037169415736570954\n",
      "epoch: 6 | 29856 / 114272 | training loss: 0.0007768867071717978\n",
      "epoch: 6 | 29888 / 114272 | training loss: 0.00047592094051651657\n",
      "epoch: 6 | 29920 / 114272 | training loss: 0.0014616308035328984\n",
      "epoch: 6 | 29952 / 114272 | training loss: 0.0008419989026151597\n",
      "epoch: 6 | 29984 / 114272 | training loss: 0.0006374962395057082\n",
      "epoch: 6 | 30016 / 114272 | training loss: 0.0012132880510762334\n",
      "epoch: 6 | 30048 / 114272 | training loss: 0.0003098772431258112\n",
      "epoch: 6 | 30080 / 114272 | training loss: 0.0008004932897165418\n",
      "epoch: 6 | 30112 / 114272 | training loss: 0.0011655830312520266\n",
      "epoch: 6 | 30144 / 114272 | training loss: 0.0013563240645453334\n",
      "epoch: 6 | 30176 / 114272 | training loss: 0.0004048490955028683\n",
      "epoch: 6 | 30208 / 114272 | training loss: 0.11014404892921448\n",
      "epoch: 6 | 30240 / 114272 | training loss: 0.0005908315652050078\n",
      "epoch: 6 | 30272 / 114272 | training loss: 0.0005903070559725165\n",
      "epoch: 6 | 30304 / 114272 | training loss: 0.0007824508938938379\n",
      "epoch: 6 | 30336 / 114272 | training loss: 0.018850447610020638\n",
      "epoch: 6 | 30368 / 114272 | training loss: 0.0008324209484271705\n",
      "epoch: 6 | 30400 / 114272 | training loss: 0.0011393866734579206\n",
      "epoch: 6 | 30432 / 114272 | training loss: 0.0009651723667047918\n",
      "epoch: 6 | 30464 / 114272 | training loss: 0.0005215604905970395\n",
      "epoch: 6 | 30496 / 114272 | training loss: 0.13998331129550934\n",
      "epoch: 6 | 30528 / 114272 | training loss: 0.0009507731883786619\n",
      "epoch: 6 | 30560 / 114272 | training loss: 0.004032332915812731\n",
      "epoch: 6 | 30592 / 114272 | training loss: 0.00857087317854166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 30624 / 114272 | training loss: 0.0005216777208261192\n",
      "epoch: 6 | 30656 / 114272 | training loss: 0.0004369341186247766\n",
      "epoch: 6 | 30688 / 114272 | training loss: 0.0008001279784366488\n",
      "epoch: 6 | 30720 / 114272 | training loss: 0.0006095223943702877\n",
      "epoch: 6 | 30752 / 114272 | training loss: 0.0006294528720900416\n",
      "epoch: 6 | 30784 / 114272 | training loss: 0.0016200130339711905\n",
      "epoch: 6 | 30816 / 114272 | training loss: 0.0007278355187736452\n",
      "epoch: 6 | 30848 / 114272 | training loss: 0.1882040649652481\n",
      "epoch: 6 | 30880 / 114272 | training loss: 0.0004728648054879159\n",
      "epoch: 6 | 30912 / 114272 | training loss: 0.0006615732563659549\n",
      "epoch: 6 | 30944 / 114272 | training loss: 0.13005605340003967\n",
      "epoch: 6 | 30976 / 114272 | training loss: 0.0006883494206704199\n",
      "epoch: 6 | 31008 / 114272 | training loss: 0.0019243689021095634\n",
      "epoch: 6 | 31040 / 114272 | training loss: 0.002838385757058859\n",
      "epoch: 6 | 31072 / 114272 | training loss: 0.0005316848983056843\n",
      "epoch: 6 | 31104 / 114272 | training loss: 0.0003644531825557351\n",
      "epoch: 6 | 31136 / 114272 | training loss: 0.013070032931864262\n",
      "epoch: 6 | 31168 / 114272 | training loss: 0.0003604938101489097\n",
      "epoch: 6 | 31200 / 114272 | training loss: 0.0005480621475726366\n",
      "epoch: 6 | 31232 / 114272 | training loss: 0.001631307415664196\n",
      "epoch: 6 | 31264 / 114272 | training loss: 0.00123521126806736\n",
      "epoch: 6 | 31296 / 114272 | training loss: 0.0019854491110891104\n",
      "epoch: 6 | 31328 / 114272 | training loss: 0.0008433816256001592\n",
      "epoch: 6 | 31360 / 114272 | training loss: 0.0007071639993228018\n",
      "epoch: 6 | 31392 / 114272 | training loss: 0.002861172193661332\n",
      "epoch: 6 | 31424 / 114272 | training loss: 0.0008246018551290035\n",
      "epoch: 6 | 31456 / 114272 | training loss: 0.17062027752399445\n",
      "epoch: 6 | 31488 / 114272 | training loss: 0.0008054070058278739\n",
      "epoch: 6 | 31520 / 114272 | training loss: 0.06505283713340759\n",
      "epoch: 6 | 31552 / 114272 | training loss: 0.0003495094133540988\n",
      "epoch: 6 | 31584 / 114272 | training loss: 0.0003818982222583145\n",
      "epoch: 6 | 31616 / 114272 | training loss: 0.26498642563819885\n",
      "epoch: 6 | 31648 / 114272 | training loss: 0.0006382131832651794\n",
      "epoch: 6 | 31680 / 114272 | training loss: 0.0006444945465773344\n",
      "epoch: 6 | 31712 / 114272 | training loss: 0.0002687040832825005\n",
      "epoch: 6 | 31744 / 114272 | training loss: 0.004473687615245581\n",
      "epoch: 6 | 31776 / 114272 | training loss: 0.0009069781517609954\n",
      "epoch: 6 | 31808 / 114272 | training loss: 0.14296753704547882\n",
      "epoch: 6 | 31840 / 114272 | training loss: 0.11050163209438324\n",
      "epoch: 6 | 31872 / 114272 | training loss: 0.0004343034524936229\n",
      "epoch: 6 | 31904 / 114272 | training loss: 0.0014511083718389273\n",
      "epoch: 6 | 31936 / 114272 | training loss: 0.0007754277903586626\n",
      "epoch: 6 | 31968 / 114272 | training loss: 0.0008188486099243164\n",
      "epoch: 6 | 32000 / 114272 | training loss: 0.0011451919563114643\n",
      "epoch: 6 | 32032 / 114272 | training loss: 0.002373959170654416\n",
      "epoch: 6 | 32064 / 114272 | training loss: 0.002562157344073057\n",
      "epoch: 6 | 32096 / 114272 | training loss: 0.0006692569004371762\n",
      "epoch: 6 | 32128 / 114272 | training loss: 0.002415880560874939\n",
      "epoch: 6 | 32160 / 114272 | training loss: 0.0006402076105587184\n",
      "epoch: 6 | 32192 / 114272 | training loss: 0.0012633607257157564\n",
      "epoch: 6 | 32224 / 114272 | training loss: 0.001686024945229292\n",
      "epoch: 6 | 32256 / 114272 | training loss: 0.0015487360069528222\n",
      "epoch: 6 | 32288 / 114272 | training loss: 0.0006519907619804144\n",
      "epoch: 6 | 32320 / 114272 | training loss: 0.009469350799918175\n",
      "epoch: 6 | 32352 / 114272 | training loss: 0.0005428214790299535\n",
      "epoch: 6 | 32384 / 114272 | training loss: 0.0010332951787859201\n",
      "epoch: 6 | 32416 / 114272 | training loss: 0.0007723799208179116\n",
      "epoch: 6 | 32448 / 114272 | training loss: 0.00072823790833354\n",
      "epoch: 6 | 32480 / 114272 | training loss: 0.4022929072380066\n",
      "epoch: 6 | 32512 / 114272 | training loss: 0.0010561810340732336\n",
      "epoch: 6 | 32544 / 114272 | training loss: 0.0008154164534062147\n",
      "epoch: 6 | 32576 / 114272 | training loss: 0.1327240765094757\n",
      "epoch: 6 | 32608 / 114272 | training loss: 0.0006379609112627804\n",
      "epoch: 6 | 32640 / 114272 | training loss: 0.0007418552413582802\n",
      "epoch: 6 | 32672 / 114272 | training loss: 0.0020935642533004284\n",
      "epoch: 6 | 32704 / 114272 | training loss: 0.0007399593596346676\n",
      "epoch: 6 | 32736 / 114272 | training loss: 0.0008218871080316603\n",
      "epoch: 6 | 32768 / 114272 | training loss: 0.217612162232399\n",
      "epoch: 6 | 32800 / 114272 | training loss: 0.0008004914852790534\n",
      "epoch: 6 | 32832 / 114272 | training loss: 0.0014119441621005535\n",
      "epoch: 6 | 32864 / 114272 | training loss: 0.25284329056739807\n",
      "epoch: 6 | 32896 / 114272 | training loss: 0.002142401644960046\n",
      "epoch: 6 | 32928 / 114272 | training loss: 0.0008998488774523139\n",
      "epoch: 6 | 32960 / 114272 | training loss: 0.08425398916006088\n",
      "epoch: 6 | 32992 / 114272 | training loss: 0.0008635512785986066\n",
      "epoch: 6 | 33024 / 114272 | training loss: 0.001003882847726345\n",
      "epoch: 6 | 33056 / 114272 | training loss: 0.0016210731118917465\n",
      "epoch: 6 | 33088 / 114272 | training loss: 0.001680790912359953\n",
      "epoch: 6 | 33120 / 114272 | training loss: 0.00045841257087886333\n",
      "epoch: 6 | 33152 / 114272 | training loss: 0.0008426716085523367\n",
      "epoch: 6 | 33184 / 114272 | training loss: 0.0016766702756285667\n",
      "epoch: 6 | 33216 / 114272 | training loss: 0.0006540731410495937\n",
      "epoch: 6 | 33248 / 114272 | training loss: 0.0011134460801258683\n",
      "epoch: 6 | 33280 / 114272 | training loss: 0.10659106820821762\n",
      "epoch: 6 | 33312 / 114272 | training loss: 0.0013719943817704916\n",
      "epoch: 6 | 33344 / 114272 | training loss: 0.16620875895023346\n",
      "epoch: 6 | 33376 / 114272 | training loss: 0.0008145708707161248\n",
      "epoch: 6 | 33408 / 114272 | training loss: 0.0009345211437903345\n",
      "epoch: 6 | 33440 / 114272 | training loss: 0.0007581228855997324\n",
      "epoch: 6 | 33472 / 114272 | training loss: 0.0008538571419194341\n",
      "epoch: 6 | 33504 / 114272 | training loss: 0.00103874655906111\n",
      "epoch: 6 | 33536 / 114272 | training loss: 0.0007894427981227636\n",
      "epoch: 6 | 33568 / 114272 | training loss: 0.00289723789319396\n",
      "epoch: 6 | 33600 / 114272 | training loss: 0.0008724381914362311\n",
      "epoch: 6 | 33632 / 114272 | training loss: 0.0008482733392156661\n",
      "epoch: 6 | 33664 / 114272 | training loss: 0.0007225084118545055\n",
      "epoch: 6 | 33696 / 114272 | training loss: 0.0005532305222004652\n",
      "epoch: 6 | 33728 / 114272 | training loss: 0.0006057230639271438\n",
      "epoch: 6 | 33760 / 114272 | training loss: 0.0009013885864987969\n",
      "epoch: 6 | 33792 / 114272 | training loss: 0.000786887074355036\n",
      "epoch: 6 | 33824 / 114272 | training loss: 0.0022904537618160248\n",
      "epoch: 6 | 33856 / 114272 | training loss: 0.0008712862036190927\n",
      "epoch: 6 | 33888 / 114272 | training loss: 0.005027583334594965\n",
      "epoch: 6 | 33920 / 114272 | training loss: 0.0287054181098938\n",
      "epoch: 6 | 33952 / 114272 | training loss: 0.0004395382711663842\n",
      "epoch: 6 | 33984 / 114272 | training loss: 0.0006378812249749899\n",
      "epoch: 6 | 34016 / 114272 | training loss: 0.0008177912095561624\n",
      "epoch: 6 | 34048 / 114272 | training loss: 0.0008676658617332578\n",
      "epoch: 6 | 34080 / 114272 | training loss: 0.0008730806293897331\n",
      "epoch: 6 | 34112 / 114272 | training loss: 0.0011391141451895237\n",
      "epoch: 6 | 34144 / 114272 | training loss: 0.0004948115674778819\n",
      "epoch: 6 | 34176 / 114272 | training loss: 0.0036519954446703196\n",
      "epoch: 6 | 34208 / 114272 | training loss: 0.0003516011929605156\n",
      "epoch: 6 | 34240 / 114272 | training loss: 0.0008510409970767796\n",
      "epoch: 6 | 34272 / 114272 | training loss: 0.0007985350675880909\n",
      "epoch: 6 | 34304 / 114272 | training loss: 0.0011306621599942446\n",
      "epoch: 6 | 34336 / 114272 | training loss: 0.0010915335733443499\n",
      "epoch: 6 | 34368 / 114272 | training loss: 0.0009285714477300644\n",
      "epoch: 6 | 34400 / 114272 | training loss: 0.050720181316137314\n",
      "epoch: 6 | 34432 / 114272 | training loss: 0.0006170870619826019\n",
      "epoch: 6 | 34464 / 114272 | training loss: 0.0004926588153466582\n",
      "epoch: 6 | 34496 / 114272 | training loss: 0.0005866094725206494\n",
      "epoch: 6 | 34528 / 114272 | training loss: 0.0005957410903647542\n",
      "epoch: 6 | 34560 / 114272 | training loss: 0.04258483648300171\n",
      "epoch: 6 | 34592 / 114272 | training loss: 0.00043854385148733854\n",
      "epoch: 6 | 34624 / 114272 | training loss: 0.004367908462882042\n",
      "epoch: 6 | 34656 / 114272 | training loss: 0.0008863405673764646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 34688 / 114272 | training loss: 0.0007199781248345971\n",
      "epoch: 6 | 34720 / 114272 | training loss: 0.004287541378289461\n",
      "epoch: 6 | 34752 / 114272 | training loss: 0.0006066226633265615\n",
      "epoch: 6 | 34784 / 114272 | training loss: 0.0004640316474251449\n",
      "epoch: 6 | 34816 / 114272 | training loss: 0.0018041945295408368\n",
      "epoch: 6 | 34848 / 114272 | training loss: 0.0003607104590628296\n",
      "epoch: 6 | 34880 / 114272 | training loss: 0.0006247623823583126\n",
      "epoch: 6 | 34912 / 114272 | training loss: 0.20468537509441376\n",
      "epoch: 6 | 34944 / 114272 | training loss: 0.0022286386229097843\n",
      "epoch: 6 | 34976 / 114272 | training loss: 0.0007391827530227602\n",
      "epoch: 6 | 35008 / 114272 | training loss: 0.0009210487478412688\n",
      "epoch: 6 | 35040 / 114272 | training loss: 0.0007927261176519096\n",
      "epoch: 6 | 35072 / 114272 | training loss: 0.004043923690915108\n",
      "epoch: 6 | 35104 / 114272 | training loss: 0.0011540682753548026\n",
      "epoch: 6 | 35136 / 114272 | training loss: 0.1859225630760193\n",
      "epoch: 6 | 35168 / 114272 | training loss: 0.0014790934510529041\n",
      "epoch: 6 | 35200 / 114272 | training loss: 0.0705508440732956\n",
      "epoch: 6 | 35232 / 114272 | training loss: 0.025297481566667557\n",
      "epoch: 6 | 35264 / 114272 | training loss: 0.028677495196461678\n",
      "epoch: 6 | 35296 / 114272 | training loss: 0.0016121946973726153\n",
      "epoch: 6 | 35328 / 114272 | training loss: 0.0016784598119556904\n",
      "epoch: 6 | 35360 / 114272 | training loss: 0.0005378058413043618\n",
      "epoch: 6 | 35392 / 114272 | training loss: 0.0006779085379093885\n",
      "epoch: 6 | 35424 / 114272 | training loss: 0.08208096772432327\n",
      "epoch: 6 | 35456 / 114272 | training loss: 0.0005106673925183713\n",
      "epoch: 6 | 35488 / 114272 | training loss: 0.00040808969060890377\n",
      "epoch: 6 | 35520 / 114272 | training loss: 0.22575856745243073\n",
      "epoch: 6 | 35552 / 114272 | training loss: 0.00048380056978203356\n",
      "epoch: 6 | 35584 / 114272 | training loss: 0.18461719155311584\n",
      "epoch: 6 | 35616 / 114272 | training loss: 0.01505013182759285\n",
      "epoch: 6 | 35648 / 114272 | training loss: 0.00036911515053361654\n",
      "epoch: 6 | 35680 / 114272 | training loss: 0.0006043245084583759\n",
      "epoch: 6 | 35712 / 114272 | training loss: 0.0008575160754844546\n",
      "epoch: 6 | 35744 / 114272 | training loss: 0.0007470078417100012\n",
      "epoch: 6 | 35776 / 114272 | training loss: 0.002952679293230176\n",
      "epoch: 6 | 35808 / 114272 | training loss: 0.19861318171024323\n",
      "epoch: 6 | 35840 / 114272 | training loss: 0.0019889865070581436\n",
      "epoch: 6 | 35872 / 114272 | training loss: 0.0004807896330021322\n",
      "epoch: 6 | 35904 / 114272 | training loss: 0.0005944902077317238\n",
      "epoch: 6 | 35936 / 114272 | training loss: 0.18217754364013672\n",
      "epoch: 6 | 35968 / 114272 | training loss: 0.0008347496623173356\n",
      "epoch: 6 | 36000 / 114272 | training loss: 0.0011752451537176967\n",
      "epoch: 6 | 36032 / 114272 | training loss: 0.021013686433434486\n",
      "epoch: 6 | 36064 / 114272 | training loss: 0.0009174162405543029\n",
      "epoch: 6 | 36096 / 114272 | training loss: 0.13754777610301971\n",
      "epoch: 6 | 36128 / 114272 | training loss: 0.0010153116891160607\n",
      "epoch: 6 | 36160 / 114272 | training loss: 0.0008210623636841774\n",
      "epoch: 6 | 36192 / 114272 | training loss: 0.001242533209733665\n",
      "epoch: 6 | 36224 / 114272 | training loss: 0.012717929668724537\n",
      "epoch: 6 | 36256 / 114272 | training loss: 0.004552722442895174\n",
      "epoch: 6 | 36288 / 114272 | training loss: 0.00185558688826859\n",
      "epoch: 6 | 36320 / 114272 | training loss: 0.1360122263431549\n",
      "epoch: 6 | 36352 / 114272 | training loss: 0.053794506937265396\n",
      "epoch: 6 | 36384 / 114272 | training loss: 0.005090170539915562\n",
      "epoch: 6 | 36416 / 114272 | training loss: 0.0015324738342314959\n",
      "epoch: 6 | 36448 / 114272 | training loss: 0.0011596106924116611\n",
      "epoch: 6 | 36480 / 114272 | training loss: 0.10954664647579193\n",
      "epoch: 6 | 36512 / 114272 | training loss: 0.0006818596157245338\n",
      "epoch: 6 | 36544 / 114272 | training loss: 0.0015245504910126328\n",
      "epoch: 6 | 36576 / 114272 | training loss: 0.0013832553522661328\n",
      "epoch: 6 | 36608 / 114272 | training loss: 0.22703410685062408\n",
      "epoch: 6 | 36640 / 114272 | training loss: 0.0025668367743492126\n",
      "epoch: 6 | 36672 / 114272 | training loss: 0.0018226318061351776\n",
      "epoch: 6 | 36704 / 114272 | training loss: 0.007258929777890444\n",
      "epoch: 6 | 36736 / 114272 | training loss: 0.0009991778060793877\n",
      "epoch: 6 | 36768 / 114272 | training loss: 0.0015172671992331743\n",
      "epoch: 6 | 36800 / 114272 | training loss: 0.0015928728971630335\n",
      "epoch: 6 | 36832 / 114272 | training loss: 0.0016076506581157446\n",
      "epoch: 6 | 36864 / 114272 | training loss: 0.001632165047340095\n",
      "epoch: 6 | 36896 / 114272 | training loss: 0.0012226426042616367\n",
      "epoch: 6 | 36928 / 114272 | training loss: 0.0006237868219614029\n",
      "epoch: 6 | 36960 / 114272 | training loss: 0.0030312391463667154\n",
      "epoch: 6 | 36992 / 114272 | training loss: 0.001232194947078824\n",
      "epoch: 6 | 37024 / 114272 | training loss: 0.0013023077044636011\n",
      "epoch: 6 | 37056 / 114272 | training loss: 0.0010452850256115198\n",
      "epoch: 6 | 37088 / 114272 | training loss: 0.001327655976638198\n",
      "epoch: 6 | 37120 / 114272 | training loss: 0.0014248398365452886\n",
      "epoch: 6 | 37152 / 114272 | training loss: 0.0012294730404391885\n",
      "epoch: 6 | 37184 / 114272 | training loss: 0.0015233376761898398\n",
      "epoch: 6 | 37216 / 114272 | training loss: 0.0017698958981782198\n",
      "epoch: 6 | 37248 / 114272 | training loss: 0.0012993786949664354\n",
      "epoch: 6 | 37280 / 114272 | training loss: 0.1622915416955948\n",
      "epoch: 6 | 37312 / 114272 | training loss: 0.0003893272369168699\n",
      "epoch: 6 | 37344 / 114272 | training loss: 0.0016648451564833522\n",
      "epoch: 6 | 37376 / 114272 | training loss: 0.0009184764930978417\n",
      "epoch: 6 | 37408 / 114272 | training loss: 0.0015509686199948192\n",
      "epoch: 6 | 37440 / 114272 | training loss: 0.07234930992126465\n",
      "epoch: 6 | 37472 / 114272 | training loss: 0.000606144720222801\n",
      "epoch: 6 | 37504 / 114272 | training loss: 0.00240375567227602\n",
      "epoch: 6 | 37536 / 114272 | training loss: 0.0016403582412749529\n",
      "epoch: 6 | 37568 / 114272 | training loss: 0.002218065783381462\n",
      "epoch: 6 | 37600 / 114272 | training loss: 0.0014984799781814218\n",
      "epoch: 6 | 37632 / 114272 | training loss: 0.00033852181513793766\n",
      "epoch: 6 | 37664 / 114272 | training loss: 0.00973749440163374\n",
      "epoch: 6 | 37696 / 114272 | training loss: 0.0005954921361990273\n",
      "epoch: 6 | 37728 / 114272 | training loss: 0.0011667661601677537\n",
      "epoch: 6 | 37760 / 114272 | training loss: 0.0017182636074721813\n",
      "epoch: 6 | 37792 / 114272 | training loss: 0.016543149948120117\n",
      "epoch: 6 | 37824 / 114272 | training loss: 0.0013661631383001804\n",
      "epoch: 6 | 37856 / 114272 | training loss: 0.0013400537427514791\n",
      "epoch: 6 | 37888 / 114272 | training loss: 0.00046893570106476545\n",
      "epoch: 6 | 37920 / 114272 | training loss: 0.0010906581301242113\n",
      "epoch: 6 | 37952 / 114272 | training loss: 0.001499500940553844\n",
      "epoch: 6 | 37984 / 114272 | training loss: 0.001319164177402854\n",
      "epoch: 6 | 38016 / 114272 | training loss: 0.2498161792755127\n",
      "epoch: 6 | 38048 / 114272 | training loss: 0.001189082395285368\n",
      "epoch: 6 | 38080 / 114272 | training loss: 0.0008615863625891507\n",
      "epoch: 6 | 38112 / 114272 | training loss: 0.13076281547546387\n",
      "epoch: 6 | 38144 / 114272 | training loss: 0.09005826711654663\n",
      "epoch: 6 | 38176 / 114272 | training loss: 0.0008744022925384343\n",
      "epoch: 6 | 38208 / 114272 | training loss: 0.0012584164505824447\n",
      "epoch: 6 | 38240 / 114272 | training loss: 0.001430820906534791\n",
      "epoch: 6 | 38272 / 114272 | training loss: 0.0005015063215978444\n",
      "epoch: 6 | 38304 / 114272 | training loss: 0.0010179356904700398\n",
      "epoch: 6 | 38336 / 114272 | training loss: 0.0037432897370308638\n",
      "epoch: 6 | 38368 / 114272 | training loss: 0.0009450596990063787\n",
      "epoch: 6 | 38400 / 114272 | training loss: 0.0010072642471641302\n",
      "epoch: 6 | 38432 / 114272 | training loss: 0.0011644953629001975\n",
      "epoch: 6 | 38464 / 114272 | training loss: 0.23245182633399963\n",
      "epoch: 6 | 38496 / 114272 | training loss: 0.0005765471723861992\n",
      "epoch: 6 | 38528 / 114272 | training loss: 0.07249899953603745\n",
      "epoch: 6 | 38560 / 114272 | training loss: 0.0008006605785340071\n",
      "epoch: 6 | 38592 / 114272 | training loss: 0.002178929513320327\n",
      "epoch: 6 | 38624 / 114272 | training loss: 0.27703264355659485\n",
      "epoch: 6 | 38656 / 114272 | training loss: 0.0010385859059169888\n",
      "epoch: 6 | 38688 / 114272 | training loss: 0.0006364313885569572\n",
      "epoch: 6 | 38720 / 114272 | training loss: 0.0003511846880428493\n",
      "epoch: 6 | 38752 / 114272 | training loss: 0.0012978296726942062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 38784 / 114272 | training loss: 0.0008770817657932639\n",
      "epoch: 6 | 38816 / 114272 | training loss: 0.004812062252312899\n",
      "epoch: 6 | 38848 / 114272 | training loss: 0.0005517036188393831\n",
      "epoch: 6 | 38880 / 114272 | training loss: 0.001515671145170927\n",
      "epoch: 6 | 38912 / 114272 | training loss: 0.0016345218755304813\n",
      "epoch: 6 | 38944 / 114272 | training loss: 0.006195941008627415\n",
      "epoch: 6 | 38976 / 114272 | training loss: 0.0012520382879301906\n",
      "epoch: 6 | 39008 / 114272 | training loss: 0.000922028033528477\n",
      "epoch: 6 | 39040 / 114272 | training loss: 0.1139460951089859\n",
      "epoch: 6 | 39072 / 114272 | training loss: 0.001665147254243493\n",
      "epoch: 6 | 39104 / 114272 | training loss: 0.0008505308069288731\n",
      "epoch: 6 | 39136 / 114272 | training loss: 0.0009488556534051895\n",
      "epoch: 6 | 39168 / 114272 | training loss: 0.0013301866129040718\n",
      "epoch: 6 | 39200 / 114272 | training loss: 0.000665605126414448\n",
      "epoch: 6 | 39232 / 114272 | training loss: 0.0006913783145137131\n",
      "epoch: 6 | 39264 / 114272 | training loss: 0.0009170377161353827\n",
      "epoch: 6 | 39296 / 114272 | training loss: 0.00047982812975533307\n",
      "epoch: 6 | 39328 / 114272 | training loss: 0.0007760691223666072\n",
      "epoch: 6 | 39360 / 114272 | training loss: 0.000934081501327455\n",
      "epoch: 6 | 39392 / 114272 | training loss: 0.0724698156118393\n",
      "epoch: 6 | 39424 / 114272 | training loss: 0.0010988835711032152\n",
      "epoch: 6 | 39456 / 114272 | training loss: 0.0023370906710624695\n",
      "epoch: 6 | 39488 / 114272 | training loss: 0.0013601458631455898\n",
      "epoch: 6 | 39520 / 114272 | training loss: 0.0016180068487301469\n",
      "epoch: 6 | 39552 / 114272 | training loss: 0.18861432373523712\n",
      "epoch: 6 | 39584 / 114272 | training loss: 0.0011245820205658674\n",
      "epoch: 6 | 39616 / 114272 | training loss: 0.0008644816698506474\n",
      "epoch: 6 | 39648 / 114272 | training loss: 0.0011132583022117615\n",
      "epoch: 6 | 39680 / 114272 | training loss: 0.0004925092216581106\n",
      "epoch: 6 | 39712 / 114272 | training loss: 0.0005502100684680045\n",
      "epoch: 6 | 39744 / 114272 | training loss: 0.0007229153416119516\n",
      "epoch: 6 | 39776 / 114272 | training loss: 0.0006791120395064354\n",
      "epoch: 6 | 39808 / 114272 | training loss: 0.0017300482140854\n",
      "epoch: 6 | 39840 / 114272 | training loss: 0.0011535058729350567\n",
      "epoch: 6 | 39872 / 114272 | training loss: 0.0010967967100441456\n",
      "epoch: 6 | 39904 / 114272 | training loss: 0.0009332909248769283\n",
      "epoch: 6 | 39936 / 114272 | training loss: 0.0006887070485390723\n",
      "epoch: 6 | 39968 / 114272 | training loss: 0.002962534548714757\n",
      "epoch: 6 | 40000 / 114272 | training loss: 0.0010609137825667858\n",
      "epoch: 6 | 40032 / 114272 | training loss: 0.0007114389445632696\n",
      "epoch: 6 | 40064 / 114272 | training loss: 0.0038346280343830585\n",
      "epoch: 6 | 40096 / 114272 | training loss: 0.044691573828458786\n",
      "epoch: 6 | 40128 / 114272 | training loss: 0.000688048719894141\n",
      "epoch: 6 | 40160 / 114272 | training loss: 0.0005045445868745446\n",
      "epoch: 6 | 40192 / 114272 | training loss: 0.0018107382347807288\n",
      "epoch: 6 | 40224 / 114272 | training loss: 0.23571473360061646\n",
      "epoch: 6 | 40256 / 114272 | training loss: 0.13270314037799835\n",
      "epoch: 6 | 40288 / 114272 | training loss: 0.0017812115838751197\n",
      "epoch: 6 | 40320 / 114272 | training loss: 0.0020192014053463936\n",
      "epoch: 6 | 40352 / 114272 | training loss: 0.023159407079219818\n",
      "epoch: 6 | 40384 / 114272 | training loss: 0.0013401705073192716\n",
      "epoch: 6 | 40416 / 114272 | training loss: 0.001496494747698307\n",
      "epoch: 6 | 40448 / 114272 | training loss: 0.001128845033235848\n",
      "epoch: 6 | 40480 / 114272 | training loss: 0.0015731214080005884\n",
      "epoch: 6 | 40512 / 114272 | training loss: 0.0015260191867128015\n",
      "epoch: 6 | 40544 / 114272 | training loss: 0.0007109519792720675\n",
      "epoch: 6 | 40576 / 114272 | training loss: 0.0011686522047966719\n",
      "epoch: 6 | 40608 / 114272 | training loss: 0.0015837247483432293\n",
      "epoch: 6 | 40640 / 114272 | training loss: 0.0008926299633458257\n",
      "epoch: 6 | 40672 / 114272 | training loss: 0.0011994090164080262\n",
      "epoch: 6 | 40704 / 114272 | training loss: 0.20913881063461304\n",
      "epoch: 6 | 40736 / 114272 | training loss: 0.000943319289945066\n",
      "epoch: 6 | 40768 / 114272 | training loss: 0.0011491981567814946\n",
      "epoch: 6 | 40800 / 114272 | training loss: 0.16385012865066528\n",
      "epoch: 6 | 40832 / 114272 | training loss: 0.0014326819218695164\n",
      "epoch: 6 | 40864 / 114272 | training loss: 0.2204897105693817\n",
      "epoch: 6 | 40896 / 114272 | training loss: 0.0017044282285496593\n",
      "epoch: 6 | 40928 / 114272 | training loss: 0.001436680555343628\n",
      "epoch: 6 | 40960 / 114272 | training loss: 0.00117150554433465\n",
      "epoch: 6 | 40992 / 114272 | training loss: 0.001185345696285367\n",
      "epoch: 6 | 41024 / 114272 | training loss: 0.0017071467591449618\n",
      "epoch: 6 | 41056 / 114272 | training loss: 0.002586489077657461\n",
      "epoch: 6 | 41088 / 114272 | training loss: 0.004070131573826075\n",
      "epoch: 6 | 41120 / 114272 | training loss: 0.0020066313445568085\n",
      "epoch: 6 | 41152 / 114272 | training loss: 0.2430020570755005\n",
      "epoch: 6 | 41184 / 114272 | training loss: 0.001962411915883422\n",
      "epoch: 6 | 41216 / 114272 | training loss: 0.08513922989368439\n",
      "epoch: 6 | 41248 / 114272 | training loss: 0.001101036206819117\n",
      "epoch: 6 | 41280 / 114272 | training loss: 0.0009697237983345985\n",
      "epoch: 6 | 41312 / 114272 | training loss: 0.0018131290562450886\n",
      "epoch: 6 | 41344 / 114272 | training loss: 0.0019396670395508409\n",
      "epoch: 6 | 41376 / 114272 | training loss: 0.0059272353537380695\n",
      "epoch: 6 | 41408 / 114272 | training loss: 0.0028142614755779505\n",
      "epoch: 6 | 41440 / 114272 | training loss: 0.002059879247099161\n",
      "epoch: 6 | 41472 / 114272 | training loss: 0.00111517368350178\n",
      "epoch: 6 | 41504 / 114272 | training loss: 0.002785222604870796\n",
      "epoch: 6 | 41536 / 114272 | training loss: 0.001461569219827652\n",
      "epoch: 6 | 41568 / 114272 | training loss: 0.16778910160064697\n",
      "epoch: 6 | 41600 / 114272 | training loss: 0.001719651510939002\n",
      "epoch: 6 | 41632 / 114272 | training loss: 0.1513284295797348\n",
      "epoch: 6 | 41664 / 114272 | training loss: 0.0014279921306297183\n",
      "epoch: 6 | 41696 / 114272 | training loss: 0.0038434418383985758\n",
      "epoch: 6 | 41728 / 114272 | training loss: 0.0015698897186666727\n",
      "epoch: 6 | 41760 / 114272 | training loss: 0.0036327273119241\n",
      "epoch: 6 | 41792 / 114272 | training loss: 0.0014419798972085118\n",
      "epoch: 6 | 41824 / 114272 | training loss: 0.0014750752598047256\n",
      "epoch: 6 | 41856 / 114272 | training loss: 0.0021849903278052807\n",
      "epoch: 6 | 41888 / 114272 | training loss: 0.0013505148235708475\n",
      "epoch: 6 | 41920 / 114272 | training loss: 0.003663304029032588\n",
      "epoch: 6 | 41952 / 114272 | training loss: 0.002169850980862975\n",
      "epoch: 6 | 41984 / 114272 | training loss: 0.00797728169709444\n",
      "epoch: 6 | 42016 / 114272 | training loss: 0.0021919263526797295\n",
      "epoch: 6 | 42048 / 114272 | training loss: 0.0009178225300274789\n",
      "epoch: 6 | 42080 / 114272 | training loss: 0.0011546085588634014\n",
      "epoch: 6 | 42112 / 114272 | training loss: 0.0007466398528777063\n",
      "epoch: 6 | 42144 / 114272 | training loss: 0.0018954990664497018\n",
      "epoch: 6 | 42176 / 114272 | training loss: 0.0016473938012495637\n",
      "epoch: 6 | 42208 / 114272 | training loss: 0.0022526464890688658\n",
      "epoch: 6 | 42240 / 114272 | training loss: 0.002148388884961605\n",
      "epoch: 6 | 42272 / 114272 | training loss: 0.0020703880582004786\n",
      "epoch: 6 | 42304 / 114272 | training loss: 0.0016303861048072577\n",
      "epoch: 6 | 42336 / 114272 | training loss: 0.1832931488752365\n",
      "epoch: 6 | 42368 / 114272 | training loss: 0.0013226605951786041\n",
      "epoch: 6 | 42400 / 114272 | training loss: 0.001504265470430255\n",
      "epoch: 6 | 42432 / 114272 | training loss: 0.0017412612214684486\n",
      "epoch: 6 | 42464 / 114272 | training loss: 0.0015033511444926262\n",
      "epoch: 6 | 42496 / 114272 | training loss: 0.0011396226473152637\n",
      "epoch: 6 | 42528 / 114272 | training loss: 0.3573635220527649\n",
      "epoch: 6 | 42560 / 114272 | training loss: 0.07063259929418564\n",
      "epoch: 6 | 42592 / 114272 | training loss: 0.0009811834897845984\n",
      "epoch: 6 | 42624 / 114272 | training loss: 0.0016398116713389754\n",
      "epoch: 6 | 42656 / 114272 | training loss: 0.0011732574785128236\n",
      "epoch: 6 | 42688 / 114272 | training loss: 0.001827295171096921\n",
      "epoch: 6 | 42720 / 114272 | training loss: 0.0006963906926102936\n",
      "epoch: 6 | 42752 / 114272 | training loss: 0.0008461027173325419\n",
      "epoch: 6 | 42784 / 114272 | training loss: 0.001186477136798203\n",
      "epoch: 6 | 42816 / 114272 | training loss: 0.015547412447631359\n",
      "epoch: 6 | 42848 / 114272 | training loss: 0.001219449331983924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 42880 / 114272 | training loss: 0.001004581805318594\n",
      "epoch: 6 | 42912 / 114272 | training loss: 0.001019276212900877\n",
      "epoch: 6 | 42944 / 114272 | training loss: 0.08275092393159866\n",
      "epoch: 6 | 42976 / 114272 | training loss: 0.0012051139492541552\n",
      "epoch: 6 | 43008 / 114272 | training loss: 0.0011162023292854428\n",
      "epoch: 6 | 43040 / 114272 | training loss: 0.0016863025957718492\n",
      "epoch: 6 | 43072 / 114272 | training loss: 0.0009785055881366134\n",
      "epoch: 6 | 43104 / 114272 | training loss: 0.0013163121184334159\n",
      "epoch: 6 | 43136 / 114272 | training loss: 0.001064444542862475\n",
      "epoch: 6 | 43168 / 114272 | training loss: 0.0022522583603858948\n",
      "epoch: 6 | 43200 / 114272 | training loss: 0.0009480107109993696\n",
      "epoch: 6 | 43232 / 114272 | training loss: 0.1488153636455536\n",
      "epoch: 6 | 43264 / 114272 | training loss: 0.07850142568349838\n",
      "epoch: 6 | 43296 / 114272 | training loss: 0.10053903609514236\n",
      "epoch: 6 | 43328 / 114272 | training loss: 0.006242632400244474\n",
      "epoch: 6 | 43360 / 114272 | training loss: 0.0014514579670503736\n",
      "epoch: 6 | 43392 / 114272 | training loss: 0.003511737333610654\n",
      "epoch: 6 | 43424 / 114272 | training loss: 0.0012908221688121557\n",
      "epoch: 6 | 43456 / 114272 | training loss: 0.0010694483062252402\n",
      "epoch: 6 | 43488 / 114272 | training loss: 0.0019405103521421552\n",
      "epoch: 6 | 43520 / 114272 | training loss: 0.0025627240538597107\n",
      "epoch: 6 | 43552 / 114272 | training loss: 0.007532910443842411\n",
      "epoch: 6 | 43584 / 114272 | training loss: 0.0015122528420761228\n",
      "epoch: 6 | 43616 / 114272 | training loss: 0.001487213303335011\n",
      "epoch: 6 | 43648 / 114272 | training loss: 0.09008314460515976\n",
      "epoch: 6 | 43680 / 114272 | training loss: 0.0017427554121240973\n",
      "epoch: 6 | 43712 / 114272 | training loss: 0.001547746011056006\n",
      "epoch: 6 | 43744 / 114272 | training loss: 0.0009716861532069743\n",
      "epoch: 6 | 43776 / 114272 | training loss: 0.0016337792621925473\n",
      "epoch: 6 | 43808 / 114272 | training loss: 0.01841939426958561\n",
      "epoch: 6 | 43840 / 114272 | training loss: 0.0014444986591115594\n",
      "epoch: 6 | 43872 / 114272 | training loss: 0.0013014860451221466\n",
      "epoch: 6 | 43904 / 114272 | training loss: 0.10790141671895981\n",
      "epoch: 6 | 43936 / 114272 | training loss: 0.002657210687175393\n",
      "epoch: 6 | 43968 / 114272 | training loss: 0.0018730737501755357\n",
      "epoch: 6 | 44000 / 114272 | training loss: 0.1232762262225151\n",
      "epoch: 6 | 44032 / 114272 | training loss: 0.0016027132514864206\n",
      "epoch: 6 | 44064 / 114272 | training loss: 0.0017798551125451922\n",
      "epoch: 6 | 44096 / 114272 | training loss: 0.0021017268300056458\n",
      "epoch: 6 | 44128 / 114272 | training loss: 0.0010276249377056956\n",
      "epoch: 6 | 44160 / 114272 | training loss: 0.0009419381385669112\n",
      "epoch: 6 | 44192 / 114272 | training loss: 0.00101865129545331\n",
      "epoch: 6 | 44224 / 114272 | training loss: 0.0007659195689484477\n",
      "epoch: 6 | 44256 / 114272 | training loss: 0.144780233502388\n",
      "epoch: 6 | 44288 / 114272 | training loss: 0.002010629279538989\n",
      "epoch: 6 | 44320 / 114272 | training loss: 0.0013418718008324504\n",
      "epoch: 6 | 44352 / 114272 | training loss: 0.14851169288158417\n",
      "epoch: 6 | 44384 / 114272 | training loss: 0.000654134841170162\n",
      "epoch: 6 | 44416 / 114272 | training loss: 0.06900592893362045\n",
      "epoch: 6 | 44448 / 114272 | training loss: 0.002266300143674016\n",
      "epoch: 6 | 44480 / 114272 | training loss: 0.0013887977693229914\n",
      "epoch: 6 | 44512 / 114272 | training loss: 0.0008457875228486955\n",
      "epoch: 6 | 44544 / 114272 | training loss: 0.0016940576024353504\n",
      "epoch: 6 | 44576 / 114272 | training loss: 0.0009151793201453984\n",
      "epoch: 6 | 44608 / 114272 | training loss: 0.001268949476070702\n",
      "epoch: 6 | 44640 / 114272 | training loss: 0.004692611750215292\n",
      "epoch: 6 | 44672 / 114272 | training loss: 0.0012128666276112199\n",
      "epoch: 6 | 44704 / 114272 | training loss: 0.001355456537567079\n",
      "epoch: 6 | 44736 / 114272 | training loss: 0.5227083563804626\n",
      "epoch: 6 | 44768 / 114272 | training loss: 0.0018865709425881505\n",
      "epoch: 6 | 44800 / 114272 | training loss: 0.0017791276331990957\n",
      "epoch: 6 | 44832 / 114272 | training loss: 0.0015652419533580542\n",
      "epoch: 6 | 44864 / 114272 | training loss: 0.01784227229654789\n",
      "epoch: 6 | 44896 / 114272 | training loss: 0.241669163107872\n",
      "epoch: 6 | 44928 / 114272 | training loss: 0.0007457368774339557\n",
      "epoch: 6 | 44960 / 114272 | training loss: 0.0009085078490898013\n",
      "epoch: 6 | 44992 / 114272 | training loss: 0.0018230124842375517\n",
      "epoch: 6 | 45024 / 114272 | training loss: 0.2195548713207245\n",
      "epoch: 6 | 45056 / 114272 | training loss: 0.002199132228270173\n",
      "epoch: 6 | 45088 / 114272 | training loss: 0.001713041216135025\n",
      "epoch: 6 | 45120 / 114272 | training loss: 0.001243098871782422\n",
      "epoch: 6 | 45152 / 114272 | training loss: 0.039634451270103455\n",
      "epoch: 6 | 45184 / 114272 | training loss: 0.000946290441788733\n",
      "epoch: 6 | 45216 / 114272 | training loss: 0.0006597544415853918\n",
      "epoch: 6 | 45248 / 114272 | training loss: 0.0015321619575843215\n",
      "epoch: 6 | 45280 / 114272 | training loss: 0.1854574829339981\n",
      "epoch: 6 | 45312 / 114272 | training loss: 0.0011453705374151468\n",
      "epoch: 6 | 45344 / 114272 | training loss: 0.002026735804975033\n",
      "epoch: 6 | 45376 / 114272 | training loss: 0.0011948341270908713\n",
      "epoch: 6 | 45408 / 114272 | training loss: 0.0008355833706445992\n",
      "epoch: 6 | 45440 / 114272 | training loss: 0.0668155699968338\n",
      "epoch: 6 | 45472 / 114272 | training loss: 0.19840139150619507\n",
      "epoch: 6 | 45504 / 114272 | training loss: 0.0019344297470524907\n",
      "epoch: 6 | 45536 / 114272 | training loss: 0.0015699254581704736\n",
      "epoch: 6 | 45568 / 114272 | training loss: 0.0008082109852693975\n",
      "epoch: 6 | 45600 / 114272 | training loss: 0.0019122199155390263\n",
      "epoch: 6 | 45632 / 114272 | training loss: 0.1368035227060318\n",
      "epoch: 6 | 45664 / 114272 | training loss: 0.00184822257142514\n",
      "epoch: 6 | 45696 / 114272 | training loss: 0.338734894990921\n",
      "epoch: 6 | 45728 / 114272 | training loss: 0.13828478753566742\n",
      "epoch: 6 | 45760 / 114272 | training loss: 0.0668448656797409\n",
      "epoch: 6 | 45792 / 114272 | training loss: 0.001932355109602213\n",
      "epoch: 6 | 45824 / 114272 | training loss: 0.060587115585803986\n",
      "epoch: 6 | 45856 / 114272 | training loss: 0.0013200920075178146\n",
      "epoch: 6 | 45888 / 114272 | training loss: 0.0021786808501929045\n",
      "epoch: 6 | 45920 / 114272 | training loss: 0.0657789409160614\n",
      "epoch: 6 | 45952 / 114272 | training loss: 0.0015286628622561693\n",
      "epoch: 6 | 45984 / 114272 | training loss: 0.026224976405501366\n",
      "epoch: 6 | 46016 / 114272 | training loss: 0.0035604785662144423\n",
      "epoch: 6 | 46048 / 114272 | training loss: 0.0018404178554192185\n",
      "epoch: 6 | 46080 / 114272 | training loss: 0.047014012932777405\n",
      "epoch: 6 | 46112 / 114272 | training loss: 0.0015727340942248702\n",
      "epoch: 6 | 46144 / 114272 | training loss: 0.0026357334572821856\n",
      "epoch: 6 | 46176 / 114272 | training loss: 0.0021598869934678078\n",
      "epoch: 6 | 46208 / 114272 | training loss: 0.0023311669938266277\n",
      "epoch: 6 | 46240 / 114272 | training loss: 0.001178415142931044\n",
      "epoch: 6 | 46272 / 114272 | training loss: 0.001479333033785224\n",
      "epoch: 6 | 46304 / 114272 | training loss: 0.0013114464236423373\n",
      "epoch: 6 | 46336 / 114272 | training loss: 0.001793488278053701\n",
      "epoch: 6 | 46368 / 114272 | training loss: 0.0012596389278769493\n",
      "epoch: 6 | 46400 / 114272 | training loss: 0.001607749960385263\n",
      "epoch: 6 | 46432 / 114272 | training loss: 0.001646665157750249\n",
      "epoch: 6 | 46464 / 114272 | training loss: 0.232539564371109\n",
      "epoch: 6 | 46496 / 114272 | training loss: 0.0015874214004725218\n",
      "epoch: 6 | 46528 / 114272 | training loss: 0.0012315005296841264\n",
      "epoch: 6 | 46560 / 114272 | training loss: 0.001124661066569388\n",
      "epoch: 6 | 46592 / 114272 | training loss: 0.0019323049345985055\n",
      "epoch: 6 | 46624 / 114272 | training loss: 0.005404366180300713\n",
      "epoch: 6 | 46656 / 114272 | training loss: 0.0007391449180431664\n",
      "epoch: 6 | 46688 / 114272 | training loss: 0.0027231737039983273\n",
      "epoch: 6 | 46720 / 114272 | training loss: 0.0023642918094992638\n",
      "epoch: 6 | 46752 / 114272 | training loss: 0.002220709342509508\n",
      "epoch: 6 | 46784 / 114272 | training loss: 0.0012108554365113378\n",
      "epoch: 6 | 46816 / 114272 | training loss: 0.0017638489371165633\n",
      "epoch: 6 | 46848 / 114272 | training loss: 0.0008550250786356628\n",
      "epoch: 6 | 46880 / 114272 | training loss: 0.15877816081047058\n",
      "epoch: 6 | 46912 / 114272 | training loss: 0.0009280901867896318\n",
      "epoch: 6 | 46944 / 114272 | training loss: 0.0009296360658481717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 46976 / 114272 | training loss: 0.0032153157517313957\n",
      "epoch: 6 | 47008 / 114272 | training loss: 0.0018775220960378647\n",
      "epoch: 6 | 47040 / 114272 | training loss: 0.0020111866760998964\n",
      "epoch: 6 | 47072 / 114272 | training loss: 0.11348222941160202\n",
      "epoch: 6 | 47104 / 114272 | training loss: 0.0014143262524157763\n",
      "epoch: 6 | 47136 / 114272 | training loss: 0.21016927063465118\n",
      "epoch: 6 | 47168 / 114272 | training loss: 0.0014972916105762124\n",
      "epoch: 6 | 47200 / 114272 | training loss: 0.09306813031435013\n",
      "epoch: 6 | 47232 / 114272 | training loss: 0.0028264953289180994\n",
      "epoch: 6 | 47264 / 114272 | training loss: 0.0008718050085008144\n",
      "epoch: 6 | 47296 / 114272 | training loss: 0.0009866305626928806\n",
      "epoch: 6 | 47328 / 114272 | training loss: 0.0013016187585890293\n",
      "epoch: 6 | 47360 / 114272 | training loss: 0.0013358796713873744\n",
      "epoch: 6 | 47392 / 114272 | training loss: 0.000770917278714478\n",
      "epoch: 6 | 47424 / 114272 | training loss: 0.0011009149020537734\n",
      "epoch: 6 | 47456 / 114272 | training loss: 0.00999409332871437\n",
      "epoch: 6 | 47488 / 114272 | training loss: 0.0021166109945625067\n",
      "epoch: 6 | 47520 / 114272 | training loss: 0.002086634747684002\n",
      "epoch: 6 | 47552 / 114272 | training loss: 0.0010768398642539978\n",
      "epoch: 6 | 47584 / 114272 | training loss: 0.004176664631813765\n",
      "epoch: 6 | 47616 / 114272 | training loss: 0.0016225301660597324\n",
      "epoch: 6 | 47648 / 114272 | training loss: 0.0013022280763834715\n",
      "epoch: 6 | 47680 / 114272 | training loss: 0.05166930705308914\n",
      "epoch: 6 | 47712 / 114272 | training loss: 0.0008273734711110592\n",
      "epoch: 6 | 47744 / 114272 | training loss: 0.001217440702021122\n",
      "epoch: 6 | 47776 / 114272 | training loss: 0.0008417341159656644\n",
      "epoch: 6 | 47808 / 114272 | training loss: 0.08570870012044907\n",
      "epoch: 6 | 47840 / 114272 | training loss: 0.0007252625655382872\n",
      "epoch: 6 | 47872 / 114272 | training loss: 0.0009988006204366684\n",
      "epoch: 6 | 47904 / 114272 | training loss: 0.0038459901697933674\n",
      "epoch: 6 | 47936 / 114272 | training loss: 0.0007211544434539974\n",
      "epoch: 6 | 47968 / 114272 | training loss: 0.0016247242456302047\n",
      "epoch: 6 | 48000 / 114272 | training loss: 0.0006952306721359491\n",
      "epoch: 6 | 48032 / 114272 | training loss: 0.07523258775472641\n",
      "epoch: 6 | 48064 / 114272 | training loss: 0.0010508963605389\n",
      "epoch: 6 | 48096 / 114272 | training loss: 0.0006587413954548538\n",
      "epoch: 6 | 48128 / 114272 | training loss: 0.0016048206016421318\n",
      "epoch: 6 | 48160 / 114272 | training loss: 0.0010230994084849954\n",
      "epoch: 6 | 48192 / 114272 | training loss: 0.0028905970975756645\n",
      "epoch: 6 | 48224 / 114272 | training loss: 0.0007707133190706372\n",
      "epoch: 6 | 48256 / 114272 | training loss: 0.000571971875615418\n",
      "epoch: 6 | 48288 / 114272 | training loss: 0.0016235595103353262\n",
      "epoch: 6 | 48320 / 114272 | training loss: 0.0006334013887681067\n",
      "epoch: 6 | 48352 / 114272 | training loss: 0.002141243778169155\n",
      "epoch: 6 | 48384 / 114272 | training loss: 0.011392736807465553\n",
      "epoch: 6 | 48416 / 114272 | training loss: 0.000993599882349372\n",
      "epoch: 6 | 48448 / 114272 | training loss: 0.0006181616336107254\n",
      "epoch: 6 | 48480 / 114272 | training loss: 0.0010662964778020978\n",
      "epoch: 6 | 48512 / 114272 | training loss: 0.04423949122428894\n",
      "epoch: 6 | 48544 / 114272 | training loss: 0.001421434455551207\n",
      "epoch: 6 | 48576 / 114272 | training loss: 0.0011157292174175382\n",
      "epoch: 6 | 48608 / 114272 | training loss: 0.0007423608913086355\n",
      "epoch: 6 | 48640 / 114272 | training loss: 0.0009476488921791315\n",
      "epoch: 6 | 48672 / 114272 | training loss: 0.0004584636481013149\n",
      "epoch: 6 | 48704 / 114272 | training loss: 0.009253581054508686\n",
      "epoch: 6 | 48736 / 114272 | training loss: 0.001010688254609704\n",
      "epoch: 6 | 48768 / 114272 | training loss: 0.0009623139631003141\n",
      "epoch: 6 | 48800 / 114272 | training loss: 0.0008163331658579409\n",
      "epoch: 6 | 48832 / 114272 | training loss: 0.1413739174604416\n",
      "epoch: 6 | 48864 / 114272 | training loss: 0.0006878039566799998\n",
      "epoch: 6 | 48896 / 114272 | training loss: 0.000699576223269105\n",
      "epoch: 6 | 48928 / 114272 | training loss: 0.000998391187749803\n",
      "epoch: 6 | 48960 / 114272 | training loss: 0.0033434478100389242\n",
      "epoch: 6 | 48992 / 114272 | training loss: 0.07928459346294403\n",
      "epoch: 6 | 49024 / 114272 | training loss: 0.0008905826834961772\n",
      "epoch: 6 | 49056 / 114272 | training loss: 0.0007455283775925636\n",
      "epoch: 6 | 49088 / 114272 | training loss: 0.0007034750888124108\n",
      "epoch: 6 | 49120 / 114272 | training loss: 0.0012181170750409365\n",
      "epoch: 6 | 49152 / 114272 | training loss: 0.0013178234221413732\n",
      "epoch: 6 | 49184 / 114272 | training loss: 0.0005770412972196937\n",
      "epoch: 6 | 49216 / 114272 | training loss: 0.001215763739310205\n",
      "epoch: 6 | 49248 / 114272 | training loss: 0.001376928761601448\n",
      "epoch: 6 | 49280 / 114272 | training loss: 0.0013701235875487328\n",
      "epoch: 6 | 49312 / 114272 | training loss: 0.0011267189402133226\n",
      "epoch: 6 | 49344 / 114272 | training loss: 0.04369308799505234\n",
      "epoch: 6 | 49376 / 114272 | training loss: 0.0015922938473522663\n",
      "epoch: 6 | 49408 / 114272 | training loss: 0.0005089417682029307\n",
      "epoch: 6 | 49440 / 114272 | training loss: 0.005971436854451895\n",
      "epoch: 6 | 49472 / 114272 | training loss: 0.002691943198442459\n",
      "epoch: 6 | 49504 / 114272 | training loss: 0.0008977874531410635\n",
      "epoch: 6 | 49536 / 114272 | training loss: 0.0006620044587180018\n",
      "epoch: 6 | 49568 / 114272 | training loss: 0.001557658426463604\n",
      "epoch: 6 | 49600 / 114272 | training loss: 0.0014030467718839645\n",
      "epoch: 6 | 49632 / 114272 | training loss: 0.0011716214939951897\n",
      "epoch: 6 | 49664 / 114272 | training loss: 0.0010102043161168694\n",
      "epoch: 6 | 49696 / 114272 | training loss: 0.0008581120055168867\n",
      "epoch: 6 | 49728 / 114272 | training loss: 0.06522329896688461\n",
      "epoch: 6 | 49760 / 114272 | training loss: 0.0010965076508000493\n",
      "epoch: 6 | 49792 / 114272 | training loss: 0.0034713302738964558\n",
      "epoch: 6 | 49824 / 114272 | training loss: 0.019446689635515213\n",
      "epoch: 6 | 49856 / 114272 | training loss: 0.01448342576622963\n",
      "epoch: 6 | 49888 / 114272 | training loss: 0.0019140467047691345\n",
      "epoch: 6 | 49920 / 114272 | training loss: 0.0010226096492260695\n",
      "epoch: 6 | 49952 / 114272 | training loss: 0.0011777260806411505\n",
      "epoch: 6 | 49984 / 114272 | training loss: 0.2351066768169403\n",
      "epoch: 6 | 50016 / 114272 | training loss: 0.004356765188276768\n",
      "epoch: 6 | 50048 / 114272 | training loss: 0.18431124091148376\n",
      "epoch: 6 | 50080 / 114272 | training loss: 0.0009230584255419672\n",
      "epoch: 6 | 50112 / 114272 | training loss: 0.0010399742750450969\n",
      "epoch: 6 | 50144 / 114272 | training loss: 0.0009706383571028709\n",
      "epoch: 6 | 50176 / 114272 | training loss: 0.0014690094394609332\n",
      "epoch: 6 | 50208 / 114272 | training loss: 0.19067338109016418\n",
      "epoch: 6 | 50240 / 114272 | training loss: 0.0052737221121788025\n",
      "epoch: 6 | 50272 / 114272 | training loss: 0.000967155210673809\n",
      "epoch: 6 | 50304 / 114272 | training loss: 0.0012790390755981207\n",
      "epoch: 6 | 50336 / 114272 | training loss: 0.002900456776842475\n",
      "epoch: 6 | 50368 / 114272 | training loss: 0.001699970685876906\n",
      "epoch: 6 | 50400 / 114272 | training loss: 0.003231035079807043\n",
      "epoch: 6 | 50432 / 114272 | training loss: 0.003523311112076044\n",
      "epoch: 6 | 50464 / 114272 | training loss: 0.0011917110532522202\n",
      "epoch: 6 | 50496 / 114272 | training loss: 0.0010193553753197193\n",
      "epoch: 6 | 50528 / 114272 | training loss: 0.0018341095419600606\n",
      "epoch: 6 | 50560 / 114272 | training loss: 0.0011850426672026515\n",
      "epoch: 6 | 50592 / 114272 | training loss: 0.0007732469239272177\n",
      "epoch: 6 | 50624 / 114272 | training loss: 0.1303662210702896\n",
      "epoch: 6 | 50656 / 114272 | training loss: 0.0013473359867930412\n",
      "epoch: 6 | 50688 / 114272 | training loss: 0.005148167256265879\n",
      "epoch: 6 | 50720 / 114272 | training loss: 0.251422643661499\n",
      "epoch: 6 | 50752 / 114272 | training loss: 0.0011810055002570152\n",
      "epoch: 6 | 50784 / 114272 | training loss: 0.0010874016443267465\n",
      "epoch: 6 | 50816 / 114272 | training loss: 0.0009734468185342848\n",
      "epoch: 6 | 50848 / 114272 | training loss: 0.0007493082084693015\n",
      "epoch: 6 | 50880 / 114272 | training loss: 0.23967260122299194\n",
      "epoch: 6 | 50912 / 114272 | training loss: 0.0011034755734726787\n",
      "epoch: 6 | 50944 / 114272 | training loss: 0.0010913558071479201\n",
      "epoch: 6 | 50976 / 114272 | training loss: 0.0016968874260783195\n",
      "epoch: 6 | 51008 / 114272 | training loss: 0.0012885918840765953\n",
      "epoch: 6 | 51040 / 114272 | training loss: 0.0018720254302024841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 51072 / 114272 | training loss: 0.001597398309968412\n",
      "epoch: 6 | 51104 / 114272 | training loss: 0.0011035487987101078\n",
      "epoch: 6 | 51136 / 114272 | training loss: 0.0013174331979826093\n",
      "epoch: 6 | 51168 / 114272 | training loss: 0.0013877865858376026\n",
      "epoch: 6 | 51200 / 114272 | training loss: 0.00103412673342973\n",
      "epoch: 6 | 51232 / 114272 | training loss: 0.032217495143413544\n",
      "epoch: 6 | 51264 / 114272 | training loss: 0.0005812582676298916\n",
      "epoch: 6 | 51296 / 114272 | training loss: 0.06244490668177605\n",
      "epoch: 6 | 51328 / 114272 | training loss: 0.0035882340744137764\n",
      "epoch: 6 | 51360 / 114272 | training loss: 0.0016365931369364262\n",
      "epoch: 6 | 51392 / 114272 | training loss: 0.0015040271682664752\n",
      "epoch: 6 | 51424 / 114272 | training loss: 0.000859825115185231\n",
      "epoch: 6 | 51456 / 114272 | training loss: 0.005109906662255526\n",
      "epoch: 6 | 51488 / 114272 | training loss: 0.000962123682256788\n",
      "epoch: 6 | 51520 / 114272 | training loss: 0.13944639265537262\n",
      "epoch: 6 | 51552 / 114272 | training loss: 0.00207611545920372\n",
      "epoch: 6 | 51584 / 114272 | training loss: 0.0013911380665376782\n",
      "epoch: 6 | 51616 / 114272 | training loss: 0.0009052506648004055\n",
      "epoch: 6 | 51648 / 114272 | training loss: 0.0011800338979810476\n",
      "epoch: 6 | 51680 / 114272 | training loss: 0.0008566190372221172\n",
      "epoch: 6 | 51712 / 114272 | training loss: 0.0012203060323372483\n",
      "epoch: 6 | 51744 / 114272 | training loss: 0.0005922373966313899\n",
      "epoch: 6 | 51776 / 114272 | training loss: 0.0011846277629956603\n",
      "epoch: 6 | 51808 / 114272 | training loss: 0.007083244156092405\n",
      "epoch: 6 | 51840 / 114272 | training loss: 0.001156529295258224\n",
      "epoch: 6 | 51872 / 114272 | training loss: 0.0008737760945223272\n",
      "epoch: 6 | 51904 / 114272 | training loss: 0.03194389119744301\n",
      "epoch: 6 | 51936 / 114272 | training loss: 0.0007628775201737881\n",
      "epoch: 6 | 51968 / 114272 | training loss: 0.0007736659026704729\n",
      "epoch: 6 | 52000 / 114272 | training loss: 0.0010368546936661005\n",
      "epoch: 6 | 52032 / 114272 | training loss: 0.0026780443731695414\n",
      "epoch: 6 | 52064 / 114272 | training loss: 0.0011410280130803585\n",
      "epoch: 6 | 52096 / 114272 | training loss: 0.0014408777933567762\n",
      "epoch: 6 | 52128 / 114272 | training loss: 0.0014344288501888514\n",
      "epoch: 6 | 52160 / 114272 | training loss: 0.1874162256717682\n",
      "epoch: 6 | 52192 / 114272 | training loss: 0.0018213808070868254\n",
      "epoch: 6 | 52224 / 114272 | training loss: 0.010420137085020542\n",
      "epoch: 6 | 52256 / 114272 | training loss: 0.0007052531000226736\n",
      "epoch: 6 | 52288 / 114272 | training loss: 0.0013142162933945656\n",
      "epoch: 6 | 52320 / 114272 | training loss: 0.0007958753267303109\n",
      "epoch: 6 | 52352 / 114272 | training loss: 0.0010817080037668347\n",
      "epoch: 6 | 52384 / 114272 | training loss: 0.000720999960321933\n",
      "epoch: 6 | 52416 / 114272 | training loss: 0.0012065208284184337\n",
      "epoch: 6 | 52448 / 114272 | training loss: 0.0010549838189035654\n",
      "epoch: 6 | 52480 / 114272 | training loss: 0.013387912884354591\n",
      "epoch: 6 | 52512 / 114272 | training loss: 0.09785891324281693\n",
      "epoch: 6 | 52544 / 114272 | training loss: 0.0005440845852717757\n",
      "epoch: 6 | 52576 / 114272 | training loss: 0.0007072424632497132\n",
      "epoch: 6 | 52608 / 114272 | training loss: 0.0006804088479839265\n",
      "epoch: 6 | 52640 / 114272 | training loss: 0.0032923880498856306\n",
      "epoch: 6 | 52672 / 114272 | training loss: 0.05689581483602524\n",
      "epoch: 6 | 52704 / 114272 | training loss: 0.002205346245318651\n",
      "epoch: 6 | 52736 / 114272 | training loss: 0.0009160943445749581\n",
      "epoch: 6 | 52768 / 114272 | training loss: 0.0009786785813048482\n",
      "epoch: 6 | 52800 / 114272 | training loss: 0.0014317897148430347\n",
      "epoch: 6 | 52832 / 114272 | training loss: 0.0006827209144830704\n",
      "epoch: 6 | 52864 / 114272 | training loss: 0.0011012534378096461\n",
      "epoch: 6 | 52896 / 114272 | training loss: 0.0009250226430594921\n",
      "epoch: 6 | 52928 / 114272 | training loss: 0.0007546086562797427\n",
      "epoch: 6 | 52960 / 114272 | training loss: 0.014843854121863842\n",
      "epoch: 6 | 52992 / 114272 | training loss: 0.0009607080137357116\n",
      "epoch: 6 | 53024 / 114272 | training loss: 0.0011807939736172557\n",
      "epoch: 6 | 53056 / 114272 | training loss: 0.0005834491457790136\n",
      "epoch: 6 | 53088 / 114272 | training loss: 0.001012782333418727\n",
      "epoch: 6 | 53120 / 114272 | training loss: 0.0006076209829188883\n",
      "epoch: 6 | 53152 / 114272 | training loss: 0.0008355145109817386\n",
      "epoch: 6 | 53184 / 114272 | training loss: 0.0005420675151981413\n",
      "epoch: 6 | 53216 / 114272 | training loss: 0.0004901271895505488\n",
      "epoch: 6 | 53248 / 114272 | training loss: 0.0007679631235077977\n",
      "epoch: 6 | 53280 / 114272 | training loss: 0.0006783801945857704\n",
      "epoch: 6 | 53312 / 114272 | training loss: 0.0005810438888147473\n",
      "epoch: 6 | 53344 / 114272 | training loss: 0.0005832297611050308\n",
      "epoch: 6 | 53376 / 114272 | training loss: 0.0007508386042900383\n",
      "epoch: 6 | 53408 / 114272 | training loss: 0.0005471218610182405\n",
      "epoch: 6 | 53440 / 114272 | training loss: 0.0009677260532043874\n",
      "epoch: 6 | 53472 / 114272 | training loss: 0.0005651253159157932\n",
      "epoch: 6 | 53504 / 114272 | training loss: 0.0010089139686897397\n",
      "epoch: 6 | 53536 / 114272 | training loss: 0.0008951490162871778\n",
      "epoch: 6 | 53568 / 114272 | training loss: 0.0017779528861865401\n",
      "epoch: 6 | 53600 / 114272 | training loss: 0.0009736298816278577\n",
      "epoch: 6 | 53632 / 114272 | training loss: 0.0004067871777806431\n",
      "epoch: 6 | 53664 / 114272 | training loss: 0.13178056478500366\n",
      "epoch: 6 | 53696 / 114272 | training loss: 0.0005680210306309164\n",
      "epoch: 6 | 53728 / 114272 | training loss: 0.0007676083478145301\n",
      "epoch: 6 | 53760 / 114272 | training loss: 0.0008585071773268282\n",
      "epoch: 6 | 53792 / 114272 | training loss: 0.001346298959106207\n",
      "epoch: 6 | 53824 / 114272 | training loss: 0.18155591189861298\n",
      "epoch: 6 | 53856 / 114272 | training loss: 0.00034532364225015044\n",
      "epoch: 6 | 53888 / 114272 | training loss: 0.18739980459213257\n",
      "epoch: 6 | 53920 / 114272 | training loss: 0.061778560280799866\n",
      "epoch: 6 | 53952 / 114272 | training loss: 0.012086308561265469\n",
      "epoch: 6 | 53984 / 114272 | training loss: 0.0006434473907575011\n",
      "epoch: 6 | 54016 / 114272 | training loss: 0.000726753962226212\n",
      "epoch: 6 | 54048 / 114272 | training loss: 0.0007093297317624092\n",
      "epoch: 6 | 54080 / 114272 | training loss: 0.000659748911857605\n",
      "epoch: 6 | 54112 / 114272 | training loss: 0.02505018189549446\n",
      "epoch: 6 | 54144 / 114272 | training loss: 0.00055840949062258\n",
      "epoch: 6 | 54176 / 114272 | training loss: 0.0005070998449809849\n",
      "epoch: 6 | 54208 / 114272 | training loss: 0.10695026069879532\n",
      "epoch: 6 | 54240 / 114272 | training loss: 0.12517784535884857\n",
      "epoch: 6 | 54272 / 114272 | training loss: 0.0007570256711915135\n",
      "epoch: 6 | 54304 / 114272 | training loss: 0.0009300870588049293\n",
      "epoch: 6 | 54336 / 114272 | training loss: 0.0010943628149107099\n",
      "epoch: 6 | 54368 / 114272 | training loss: 0.0004887963295914233\n",
      "epoch: 6 | 54400 / 114272 | training loss: 0.0011935899965465069\n",
      "epoch: 6 | 54432 / 114272 | training loss: 0.0008327727555297315\n",
      "epoch: 6 | 54464 / 114272 | training loss: 0.0010696491226553917\n",
      "epoch: 6 | 54496 / 114272 | training loss: 0.0006530905375257134\n",
      "epoch: 6 | 54528 / 114272 | training loss: 0.0005510153714567423\n",
      "epoch: 6 | 54560 / 114272 | training loss: 0.0008435235940851271\n",
      "epoch: 6 | 54592 / 114272 | training loss: 0.0008211986860260367\n",
      "epoch: 6 | 54624 / 114272 | training loss: 0.0006863547605462372\n",
      "epoch: 6 | 54656 / 114272 | training loss: 0.0005886646104045212\n",
      "epoch: 6 | 54688 / 114272 | training loss: 0.0012520033633336425\n",
      "epoch: 6 | 54720 / 114272 | training loss: 0.0007491029100492597\n",
      "epoch: 6 | 54752 / 114272 | training loss: 0.0005593540845438838\n",
      "epoch: 6 | 54784 / 114272 | training loss: 0.0015828015748411417\n",
      "epoch: 6 | 54816 / 114272 | training loss: 0.0014415240148082376\n",
      "epoch: 6 | 54848 / 114272 | training loss: 0.0008620779844932258\n",
      "epoch: 6 | 54880 / 114272 | training loss: 0.0007239264668896794\n",
      "epoch: 6 | 54912 / 114272 | training loss: 0.08520890772342682\n",
      "epoch: 6 | 54944 / 114272 | training loss: 0.000633472518529743\n",
      "epoch: 6 | 54976 / 114272 | training loss: 0.0006757874507457018\n",
      "epoch: 6 | 55008 / 114272 | training loss: 0.0008237233269028366\n",
      "epoch: 6 | 55040 / 114272 | training loss: 0.0006722956313751638\n",
      "epoch: 6 | 55072 / 114272 | training loss: 0.0009530464885756373\n",
      "epoch: 6 | 55104 / 114272 | training loss: 0.0007555021438747644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 55136 / 114272 | training loss: 0.0006384301232174039\n",
      "epoch: 6 | 55168 / 114272 | training loss: 0.0005497671663761139\n",
      "epoch: 6 | 55200 / 114272 | training loss: 0.001150869415141642\n",
      "epoch: 6 | 55232 / 114272 | training loss: 0.0005151170189492404\n",
      "epoch: 6 | 55264 / 114272 | training loss: 0.1397116482257843\n",
      "epoch: 6 | 55296 / 114272 | training loss: 0.00048401919775642455\n",
      "epoch: 6 | 55328 / 114272 | training loss: 0.0007484222878701985\n",
      "epoch: 6 | 55360 / 114272 | training loss: 0.000371762114809826\n",
      "epoch: 6 | 55392 / 114272 | training loss: 0.11670590192079544\n",
      "epoch: 6 | 55424 / 114272 | training loss: 0.2616083323955536\n",
      "epoch: 6 | 55456 / 114272 | training loss: 0.0006278075161390007\n",
      "epoch: 6 | 55488 / 114272 | training loss: 0.0009897398995235562\n",
      "epoch: 6 | 55520 / 114272 | training loss: 0.0007702159346081316\n",
      "epoch: 6 | 55552 / 114272 | training loss: 0.000369713525287807\n",
      "epoch: 6 | 55584 / 114272 | training loss: 0.0006036580307409167\n",
      "epoch: 6 | 55616 / 114272 | training loss: 0.0009240315412171185\n",
      "epoch: 6 | 55648 / 114272 | training loss: 0.0007905823295004666\n",
      "epoch: 6 | 55680 / 114272 | training loss: 0.0004453695728443563\n",
      "epoch: 6 | 55712 / 114272 | training loss: 0.0007809491362422705\n",
      "epoch: 6 | 55744 / 114272 | training loss: 0.11755141615867615\n",
      "epoch: 6 | 55776 / 114272 | training loss: 0.0007006700616329908\n",
      "epoch: 6 | 55808 / 114272 | training loss: 0.0009191837161779404\n",
      "epoch: 6 | 55840 / 114272 | training loss: 0.0008461014949716628\n",
      "epoch: 6 | 55872 / 114272 | training loss: 0.00066803686786443\n",
      "epoch: 6 | 55904 / 114272 | training loss: 0.0005424831761047244\n",
      "epoch: 6 | 55936 / 114272 | training loss: 0.0035356194712221622\n",
      "epoch: 6 | 55968 / 114272 | training loss: 0.0008442341350018978\n",
      "epoch: 6 | 56000 / 114272 | training loss: 0.002165356883779168\n",
      "epoch: 6 | 56032 / 114272 | training loss: 0.0010663782013580203\n",
      "epoch: 6 | 56064 / 114272 | training loss: 0.0006739901727996767\n",
      "epoch: 6 | 56096 / 114272 | training loss: 0.0007131300517357886\n",
      "epoch: 6 | 56128 / 114272 | training loss: 0.0005078100948594511\n",
      "epoch: 6 | 56160 / 114272 | training loss: 0.0007355007692240179\n",
      "epoch: 6 | 56192 / 114272 | training loss: 0.13039591908454895\n",
      "epoch: 6 | 56224 / 114272 | training loss: 0.000616734498180449\n",
      "epoch: 6 | 56256 / 114272 | training loss: 0.0007120168302208185\n",
      "epoch: 6 | 56288 / 114272 | training loss: 0.0010111851152032614\n",
      "epoch: 6 | 56320 / 114272 | training loss: 0.17198814451694489\n",
      "epoch: 6 | 56352 / 114272 | training loss: 0.000547866802662611\n",
      "epoch: 6 | 56384 / 114272 | training loss: 0.0009461414301767945\n",
      "epoch: 6 | 56416 / 114272 | training loss: 0.0007422984344884753\n",
      "epoch: 6 | 56448 / 114272 | training loss: 0.001174597186036408\n",
      "epoch: 6 | 56480 / 114272 | training loss: 0.0009807661408558488\n",
      "epoch: 6 | 56512 / 114272 | training loss: 0.0019062196370214224\n",
      "epoch: 6 | 56544 / 114272 | training loss: 0.006568544544279575\n",
      "epoch: 6 | 56576 / 114272 | training loss: 0.0006323368288576603\n",
      "epoch: 6 | 56608 / 114272 | training loss: 0.0007259467383846641\n",
      "epoch: 6 | 56640 / 114272 | training loss: 0.19249726831912994\n",
      "epoch: 6 | 56672 / 114272 | training loss: 0.00041256824624724686\n",
      "epoch: 6 | 56704 / 114272 | training loss: 0.0005531407077796757\n",
      "epoch: 6 | 56736 / 114272 | training loss: 0.13324587047100067\n",
      "epoch: 6 | 56768 / 114272 | training loss: 0.001220206031575799\n",
      "epoch: 6 | 56800 / 114272 | training loss: 0.22690124809741974\n",
      "epoch: 6 | 56832 / 114272 | training loss: 0.0023807797115296125\n",
      "epoch: 6 | 56864 / 114272 | training loss: 0.0008684947970323265\n",
      "epoch: 6 | 56896 / 114272 | training loss: 0.0008172327652573586\n",
      "epoch: 6 | 56928 / 114272 | training loss: 0.00045253397547639906\n",
      "epoch: 6 | 56960 / 114272 | training loss: 0.0005947203608229756\n",
      "epoch: 6 | 56992 / 114272 | training loss: 0.0010213122004643083\n",
      "epoch: 6 | 57024 / 114272 | training loss: 0.0008277046727016568\n",
      "epoch: 6 | 57056 / 114272 | training loss: 0.0006156976451165974\n",
      "epoch: 6 | 57088 / 114272 | training loss: 0.0783098116517067\n",
      "epoch: 6 | 57120 / 114272 | training loss: 0.0008939005201682448\n",
      "epoch: 6 | 57152 / 114272 | training loss: 0.0008034823113121092\n",
      "epoch: 6 | 57184 / 114272 | training loss: 0.0015877523692324758\n",
      "epoch: 6 | 57216 / 114272 | training loss: 0.0005650673410855234\n",
      "epoch: 6 | 57248 / 114272 | training loss: 0.123783178627491\n",
      "epoch: 6 | 57280 / 114272 | training loss: 0.000928314053453505\n",
      "epoch: 6 | 57312 / 114272 | training loss: 0.0006864784518256783\n",
      "epoch: 6 | 57344 / 114272 | training loss: 0.0006296071223914623\n",
      "epoch: 6 | 57376 / 114272 | training loss: 0.0007853280985727906\n",
      "epoch: 6 | 57408 / 114272 | training loss: 0.0009769464377313852\n",
      "epoch: 6 | 57440 / 114272 | training loss: 0.0006084425840526819\n",
      "epoch: 6 | 57472 / 114272 | training loss: 0.0007839563186280429\n",
      "epoch: 6 | 57504 / 114272 | training loss: 0.0057485587894916534\n",
      "epoch: 6 | 57536 / 114272 | training loss: 0.0007472945144400001\n",
      "epoch: 6 | 57568 / 114272 | training loss: 0.0005486368318088353\n",
      "epoch: 6 | 57600 / 114272 | training loss: 0.0006575554143637419\n",
      "epoch: 6 | 57632 / 114272 | training loss: 0.0009149401448667049\n",
      "epoch: 6 | 57664 / 114272 | training loss: 0.0008553809602744877\n",
      "epoch: 6 | 57696 / 114272 | training loss: 0.0005549846682697535\n",
      "epoch: 6 | 57728 / 114272 | training loss: 0.00044219798292033374\n",
      "epoch: 6 | 57760 / 114272 | training loss: 0.005268580745905638\n",
      "epoch: 6 | 57792 / 114272 | training loss: 0.0006947375368326902\n",
      "epoch: 6 | 57824 / 114272 | training loss: 0.0015358907403424382\n",
      "epoch: 6 | 57856 / 114272 | training loss: 0.0007872268324717879\n",
      "epoch: 6 | 57888 / 114272 | training loss: 0.0004912301083095372\n",
      "epoch: 6 | 57920 / 114272 | training loss: 0.04560907930135727\n",
      "epoch: 6 | 57952 / 114272 | training loss: 0.0018365711439400911\n",
      "epoch: 6 | 57984 / 114272 | training loss: 0.0007149343146011233\n",
      "epoch: 6 | 58016 / 114272 | training loss: 0.0007364582852460444\n",
      "epoch: 6 | 58048 / 114272 | training loss: 0.0010532699525356293\n",
      "epoch: 6 | 58080 / 114272 | training loss: 0.0008366117253899574\n",
      "epoch: 6 | 58112 / 114272 | training loss: 0.002638810547068715\n",
      "epoch: 6 | 58144 / 114272 | training loss: 0.13525432348251343\n",
      "epoch: 6 | 58176 / 114272 | training loss: 0.001100915833376348\n",
      "epoch: 6 | 58208 / 114272 | training loss: 0.001245005987584591\n",
      "epoch: 6 | 58240 / 114272 | training loss: 0.0013927265536040068\n",
      "epoch: 6 | 58272 / 114272 | training loss: 0.0010483248624950647\n",
      "epoch: 6 | 58304 / 114272 | training loss: 0.0013527261326089501\n",
      "epoch: 6 | 58336 / 114272 | training loss: 0.1839408129453659\n",
      "epoch: 6 | 58368 / 114272 | training loss: 0.06973836570978165\n",
      "epoch: 6 | 58400 / 114272 | training loss: 0.001146168215200305\n",
      "epoch: 6 | 58432 / 114272 | training loss: 0.0009114809799939394\n",
      "epoch: 6 | 58464 / 114272 | training loss: 0.0016519048949703574\n",
      "epoch: 6 | 58496 / 114272 | training loss: 0.009724782779812813\n",
      "epoch: 6 | 58528 / 114272 | training loss: 0.000956545292865485\n",
      "epoch: 6 | 58560 / 114272 | training loss: 0.0006487103528343141\n",
      "epoch: 6 | 58592 / 114272 | training loss: 0.0008138398989103734\n",
      "epoch: 6 | 58624 / 114272 | training loss: 0.10466843098402023\n",
      "epoch: 6 | 58656 / 114272 | training loss: 0.0006365762674249709\n",
      "epoch: 6 | 58688 / 114272 | training loss: 0.0011015079217031598\n",
      "epoch: 6 | 58720 / 114272 | training loss: 0.006216649431735277\n",
      "epoch: 6 | 58752 / 114272 | training loss: 0.0012362333945930004\n",
      "epoch: 6 | 58784 / 114272 | training loss: 0.00038684497121721506\n",
      "epoch: 6 | 58816 / 114272 | training loss: 0.0006156392628327012\n",
      "epoch: 6 | 58848 / 114272 | training loss: 0.0008477320661768317\n",
      "epoch: 6 | 58880 / 114272 | training loss: 0.0005544640007428825\n",
      "epoch: 6 | 58912 / 114272 | training loss: 0.0009327961015515029\n",
      "epoch: 6 | 58944 / 114272 | training loss: 0.00059346086345613\n",
      "epoch: 6 | 58976 / 114272 | training loss: 0.002079279161989689\n",
      "epoch: 6 | 59008 / 114272 | training loss: 0.23706018924713135\n",
      "epoch: 6 | 59040 / 114272 | training loss: 0.0007015038281679153\n",
      "epoch: 6 | 59072 / 114272 | training loss: 0.0008905548602342606\n",
      "epoch: 6 | 59104 / 114272 | training loss: 0.0008819554350338876\n",
      "epoch: 6 | 59136 / 114272 | training loss: 0.0012940671294927597\n",
      "epoch: 6 | 59168 / 114272 | training loss: 0.0005989730125293136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 59200 / 114272 | training loss: 0.0071461619809269905\n",
      "epoch: 6 | 59232 / 114272 | training loss: 0.2248733788728714\n",
      "epoch: 6 | 59264 / 114272 | training loss: 0.0008602861780673265\n",
      "epoch: 6 | 59296 / 114272 | training loss: 0.0016719558043405414\n",
      "epoch: 6 | 59328 / 114272 | training loss: 0.1632225513458252\n",
      "epoch: 6 | 59360 / 114272 | training loss: 0.0013664698926731944\n",
      "epoch: 6 | 59392 / 114272 | training loss: 0.0006742029800079763\n",
      "epoch: 6 | 59424 / 114272 | training loss: 0.020183347165584564\n",
      "epoch: 6 | 59456 / 114272 | training loss: 0.0004255681415088475\n",
      "epoch: 6 | 59488 / 114272 | training loss: 0.0006268636789172888\n",
      "epoch: 6 | 59520 / 114272 | training loss: 0.0008938008104451001\n",
      "epoch: 6 | 59552 / 114272 | training loss: 0.0014081026893109083\n",
      "epoch: 6 | 59584 / 114272 | training loss: 0.0038519459776580334\n",
      "epoch: 6 | 59616 / 114272 | training loss: 0.00043618219206109643\n",
      "epoch: 6 | 59648 / 114272 | training loss: 0.0008414606563746929\n",
      "epoch: 6 | 59680 / 114272 | training loss: 0.0006601905333809555\n",
      "epoch: 6 | 59712 / 114272 | training loss: 0.0007301106816157699\n",
      "epoch: 6 | 59744 / 114272 | training loss: 0.0007747909985482693\n",
      "epoch: 6 | 59776 / 114272 | training loss: 0.0007746969349682331\n",
      "epoch: 6 | 59808 / 114272 | training loss: 0.0007666198071092367\n",
      "epoch: 6 | 59840 / 114272 | training loss: 0.0899403765797615\n",
      "epoch: 6 | 59872 / 114272 | training loss: 0.02055877260863781\n",
      "epoch: 6 | 59904 / 114272 | training loss: 0.0006795318331569433\n",
      "epoch: 6 | 59936 / 114272 | training loss: 0.08571421355009079\n",
      "epoch: 6 | 59968 / 114272 | training loss: 0.000550225202459842\n",
      "epoch: 6 | 60000 / 114272 | training loss: 0.0018472516676411033\n",
      "epoch: 6 | 60032 / 114272 | training loss: 0.0010542570380493999\n",
      "epoch: 6 | 60064 / 114272 | training loss: 0.003108200617134571\n",
      "epoch: 6 | 60096 / 114272 | training loss: 0.0005438412772491574\n",
      "epoch: 6 | 60128 / 114272 | training loss: 0.0011232973774895072\n",
      "epoch: 6 | 60160 / 114272 | training loss: 0.0006003553862683475\n",
      "epoch: 6 | 60192 / 114272 | training loss: 0.0010536350309848785\n",
      "epoch: 6 | 60224 / 114272 | training loss: 0.0007920757634565234\n",
      "epoch: 6 | 60256 / 114272 | training loss: 0.000540734501555562\n",
      "epoch: 6 | 60288 / 114272 | training loss: 0.0010590336751192808\n",
      "epoch: 6 | 60320 / 114272 | training loss: 0.007836377248167992\n",
      "epoch: 6 | 60352 / 114272 | training loss: 0.0005645729834213853\n",
      "epoch: 6 | 60384 / 114272 | training loss: 0.1508289873600006\n",
      "epoch: 6 | 60416 / 114272 | training loss: 0.0004937332705594599\n",
      "epoch: 6 | 60448 / 114272 | training loss: 0.0008815694600343704\n",
      "epoch: 6 | 60480 / 114272 | training loss: 0.0007422344642691314\n",
      "epoch: 6 | 60512 / 114272 | training loss: 0.00045563094317913055\n",
      "epoch: 6 | 60544 / 114272 | training loss: 0.000987010425888002\n",
      "epoch: 6 | 60576 / 114272 | training loss: 0.0007568919099867344\n",
      "epoch: 6 | 60608 / 114272 | training loss: 0.005203222390264273\n",
      "epoch: 6 | 60640 / 114272 | training loss: 0.0008676167344674468\n",
      "epoch: 6 | 60672 / 114272 | training loss: 0.0006374642834998667\n",
      "epoch: 6 | 60704 / 114272 | training loss: 0.0010272131767123938\n",
      "epoch: 6 | 60736 / 114272 | training loss: 0.0037536723539233208\n",
      "epoch: 6 | 60768 / 114272 | training loss: 0.001069506979547441\n",
      "epoch: 6 | 60800 / 114272 | training loss: 0.05804756283760071\n",
      "epoch: 6 | 60832 / 114272 | training loss: 0.0005177211714908481\n",
      "epoch: 6 | 60864 / 114272 | training loss: 0.0007189813768491149\n",
      "epoch: 6 | 60896 / 114272 | training loss: 0.0012470813235267997\n",
      "epoch: 6 | 60928 / 114272 | training loss: 0.0006633835146203637\n",
      "epoch: 6 | 60960 / 114272 | training loss: 0.1500670462846756\n",
      "epoch: 6 | 60992 / 114272 | training loss: 0.0005124029121361673\n",
      "epoch: 6 | 61024 / 114272 | training loss: 0.0009315602947026491\n",
      "epoch: 6 | 61056 / 114272 | training loss: 0.15031570196151733\n",
      "epoch: 6 | 61088 / 114272 | training loss: 0.0007746354676783085\n",
      "epoch: 6 | 61120 / 114272 | training loss: 0.08653060346841812\n",
      "epoch: 6 | 61152 / 114272 | training loss: 0.0009407420293428004\n",
      "epoch: 6 | 61184 / 114272 | training loss: 0.0009336458169855177\n",
      "epoch: 6 | 61216 / 114272 | training loss: 0.001267526764422655\n",
      "epoch: 6 | 61248 / 114272 | training loss: 0.0007550748414359987\n",
      "epoch: 6 | 61280 / 114272 | training loss: 0.0009180969791486859\n",
      "epoch: 6 | 61312 / 114272 | training loss: 0.0012291132006794214\n",
      "epoch: 6 | 61344 / 114272 | training loss: 0.0008902836707420647\n",
      "epoch: 6 | 61376 / 114272 | training loss: 0.0007181655964814126\n",
      "epoch: 6 | 61408 / 114272 | training loss: 0.0008045170689001679\n",
      "epoch: 6 | 61440 / 114272 | training loss: 0.0010110302828252316\n",
      "epoch: 6 | 61472 / 114272 | training loss: 0.0006282264366745949\n",
      "epoch: 6 | 61504 / 114272 | training loss: 0.00043107764213345945\n",
      "epoch: 6 | 61536 / 114272 | training loss: 0.0009640806820243597\n",
      "epoch: 6 | 61568 / 114272 | training loss: 0.000680911703966558\n",
      "epoch: 6 | 61600 / 114272 | training loss: 0.17097555100917816\n",
      "epoch: 6 | 61632 / 114272 | training loss: 0.0006741339457221329\n",
      "epoch: 6 | 61664 / 114272 | training loss: 0.0005825812695547938\n",
      "epoch: 6 | 61696 / 114272 | training loss: 0.0005751174176111817\n",
      "epoch: 6 | 61728 / 114272 | training loss: 0.13848602771759033\n",
      "epoch: 6 | 61760 / 114272 | training loss: 0.0004882944631390274\n",
      "epoch: 6 | 61792 / 114272 | training loss: 0.019111648201942444\n",
      "epoch: 6 | 61824 / 114272 | training loss: 0.001958744367584586\n",
      "epoch: 6 | 61856 / 114272 | training loss: 0.18458572030067444\n",
      "epoch: 6 | 61888 / 114272 | training loss: 0.04718996211886406\n",
      "epoch: 6 | 61920 / 114272 | training loss: 0.0007784737972542644\n",
      "epoch: 6 | 61952 / 114272 | training loss: 0.0007554171606898308\n",
      "epoch: 6 | 61984 / 114272 | training loss: 0.0007373173721134663\n",
      "epoch: 6 | 62016 / 114272 | training loss: 0.015725960955023766\n",
      "epoch: 6 | 62048 / 114272 | training loss: 0.0007402002811431885\n",
      "epoch: 6 | 62080 / 114272 | training loss: 0.18777501583099365\n",
      "epoch: 6 | 62112 / 114272 | training loss: 0.0006521147442981601\n",
      "epoch: 6 | 62144 / 114272 | training loss: 0.0007638675742782652\n",
      "epoch: 6 | 62176 / 114272 | training loss: 0.00041975046042352915\n",
      "epoch: 6 | 62208 / 114272 | training loss: 0.001047732774168253\n",
      "epoch: 6 | 62240 / 114272 | training loss: 0.0005571555229835212\n",
      "epoch: 6 | 62272 / 114272 | training loss: 0.0008554611704312265\n",
      "epoch: 6 | 62304 / 114272 | training loss: 0.001085126306861639\n",
      "epoch: 6 | 62336 / 114272 | training loss: 0.19697816669940948\n",
      "epoch: 6 | 62368 / 114272 | training loss: 0.0019967788830399513\n",
      "epoch: 6 | 62400 / 114272 | training loss: 0.0007654235232621431\n",
      "epoch: 6 | 62432 / 114272 | training loss: 0.0009804846486076713\n",
      "epoch: 6 | 62464 / 114272 | training loss: 0.002628769027069211\n",
      "epoch: 6 | 62496 / 114272 | training loss: 0.23481516540050507\n",
      "epoch: 6 | 62528 / 114272 | training loss: 0.0009514375124126673\n",
      "epoch: 6 | 62560 / 114272 | training loss: 0.16579429805278778\n",
      "epoch: 6 | 62592 / 114272 | training loss: 0.0014710896648466587\n",
      "epoch: 6 | 62624 / 114272 | training loss: 0.004409071058034897\n",
      "epoch: 6 | 62656 / 114272 | training loss: 0.0008084737346507609\n",
      "epoch: 6 | 62688 / 114272 | training loss: 0.0008392943418584764\n",
      "epoch: 6 | 62720 / 114272 | training loss: 0.0011888983426615596\n",
      "epoch: 6 | 62752 / 114272 | training loss: 0.00045829772716388106\n",
      "epoch: 6 | 62784 / 114272 | training loss: 0.0010818260489031672\n",
      "epoch: 6 | 62816 / 114272 | training loss: 0.11208690702915192\n",
      "epoch: 6 | 62848 / 114272 | training loss: 0.1368972212076187\n",
      "epoch: 6 | 62880 / 114272 | training loss: 0.0005281108897179365\n",
      "epoch: 6 | 62912 / 114272 | training loss: 0.14018146693706512\n",
      "epoch: 6 | 62944 / 114272 | training loss: 0.0011724212672561407\n",
      "epoch: 6 | 62976 / 114272 | training loss: 0.001414880738593638\n",
      "epoch: 6 | 63008 / 114272 | training loss: 0.1015310063958168\n",
      "epoch: 6 | 63040 / 114272 | training loss: 0.001540140830911696\n",
      "epoch: 6 | 63072 / 114272 | training loss: 0.0013962754746899009\n",
      "epoch: 6 | 63104 / 114272 | training loss: 0.0010637572268024087\n",
      "epoch: 6 | 63136 / 114272 | training loss: 0.0014608415076509118\n",
      "epoch: 6 | 63168 / 114272 | training loss: 0.23878344893455505\n",
      "epoch: 6 | 63200 / 114272 | training loss: 0.0009691526065580547\n",
      "epoch: 6 | 63232 / 114272 | training loss: 0.0014809714630246162\n",
      "epoch: 6 | 63264 / 114272 | training loss: 0.0015550146345049143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 63296 / 114272 | training loss: 0.0014491812326014042\n",
      "epoch: 6 | 63328 / 114272 | training loss: 0.0008507903548888862\n",
      "epoch: 6 | 63360 / 114272 | training loss: 0.0005389382131397724\n",
      "epoch: 6 | 63392 / 114272 | training loss: 0.0014251766260713339\n",
      "epoch: 6 | 63424 / 114272 | training loss: 0.0012709025759249926\n",
      "epoch: 6 | 63456 / 114272 | training loss: 0.0018967688083648682\n",
      "epoch: 6 | 63488 / 114272 | training loss: 0.0022727849427610636\n",
      "epoch: 6 | 63520 / 114272 | training loss: 0.023380035534501076\n",
      "epoch: 6 | 63552 / 114272 | training loss: 0.019046008586883545\n",
      "epoch: 6 | 63584 / 114272 | training loss: 0.0011117616668343544\n",
      "epoch: 6 | 63616 / 114272 | training loss: 0.002408078173175454\n",
      "epoch: 6 | 63648 / 114272 | training loss: 0.0009278384386561811\n",
      "epoch: 6 | 63680 / 114272 | training loss: 0.006055892910808325\n",
      "epoch: 6 | 63712 / 114272 | training loss: 0.001502572325989604\n",
      "epoch: 6 | 63744 / 114272 | training loss: 0.0008539754780940711\n",
      "epoch: 6 | 63776 / 114272 | training loss: 0.003890257328748703\n",
      "epoch: 6 | 63808 / 114272 | training loss: 0.17465654015541077\n",
      "epoch: 6 | 63840 / 114272 | training loss: 0.0007837049197405577\n",
      "epoch: 6 | 63872 / 114272 | training loss: 0.0013665560400113463\n",
      "epoch: 6 | 63904 / 114272 | training loss: 0.0022342554293572903\n",
      "epoch: 6 | 63936 / 114272 | training loss: 0.027798401191830635\n",
      "epoch: 6 | 63968 / 114272 | training loss: 0.0012498891446739435\n",
      "epoch: 6 | 64000 / 114272 | training loss: 0.0011840935330837965\n",
      "epoch: 6 | 64032 / 114272 | training loss: 0.0013263601576909423\n",
      "epoch: 6 | 64064 / 114272 | training loss: 0.0006441886071115732\n",
      "epoch: 6 | 64096 / 114272 | training loss: 0.002101259073242545\n",
      "epoch: 6 | 64128 / 114272 | training loss: 0.0011930566979572177\n",
      "epoch: 6 | 64160 / 114272 | training loss: 0.18145792186260223\n",
      "epoch: 6 | 64192 / 114272 | training loss: 0.0008549820049665868\n",
      "epoch: 6 | 64224 / 114272 | training loss: 0.0040993038564920425\n",
      "epoch: 6 | 64256 / 114272 | training loss: 0.0009267524001188576\n",
      "epoch: 6 | 64288 / 114272 | training loss: 0.0008918181410990655\n",
      "epoch: 6 | 64320 / 114272 | training loss: 0.0009188397089019418\n",
      "epoch: 6 | 64352 / 114272 | training loss: 0.1541770100593567\n",
      "epoch: 6 | 64384 / 114272 | training loss: 0.0011868644505739212\n",
      "epoch: 6 | 64416 / 114272 | training loss: 0.0009446091717109084\n",
      "epoch: 6 | 64448 / 114272 | training loss: 0.00152511615306139\n",
      "epoch: 6 | 64480 / 114272 | training loss: 0.0027931693475693464\n",
      "epoch: 6 | 64512 / 114272 | training loss: 0.0016857373993843794\n",
      "epoch: 6 | 64544 / 114272 | training loss: 0.007611311972141266\n",
      "epoch: 6 | 64576 / 114272 | training loss: 0.0010256548412144184\n",
      "epoch: 6 | 64608 / 114272 | training loss: 0.004269083961844444\n",
      "epoch: 6 | 64640 / 114272 | training loss: 0.0012959155719727278\n",
      "epoch: 6 | 64672 / 114272 | training loss: 0.056489087641239166\n",
      "epoch: 6 | 64704 / 114272 | training loss: 0.001139301573857665\n",
      "epoch: 6 | 64736 / 114272 | training loss: 0.0017968473257496953\n",
      "epoch: 6 | 64768 / 114272 | training loss: 0.0011853594332933426\n",
      "epoch: 6 | 64800 / 114272 | training loss: 0.0006933155236765742\n",
      "epoch: 6 | 64832 / 114272 | training loss: 0.001066510216332972\n",
      "epoch: 6 | 64864 / 114272 | training loss: 0.00036233203718438745\n",
      "epoch: 6 | 64896 / 114272 | training loss: 0.0009848689660429955\n",
      "epoch: 6 | 64928 / 114272 | training loss: 0.000999345676973462\n",
      "epoch: 6 | 64960 / 114272 | training loss: 0.0010375091806054115\n",
      "epoch: 6 | 64992 / 114272 | training loss: 0.000674598675686866\n",
      "epoch: 6 | 65024 / 114272 | training loss: 0.0003748471208382398\n",
      "epoch: 6 | 65056 / 114272 | training loss: 0.003436299040913582\n",
      "epoch: 6 | 65088 / 114272 | training loss: 0.0009160821791738272\n",
      "epoch: 6 | 65120 / 114272 | training loss: 0.0008576138061471283\n",
      "epoch: 6 | 65152 / 114272 | training loss: 0.0008302240748889744\n",
      "epoch: 6 | 65184 / 114272 | training loss: 0.0007069105631671846\n",
      "epoch: 6 | 65216 / 114272 | training loss: 0.005028534214943647\n",
      "epoch: 6 | 65248 / 114272 | training loss: 0.002331371884793043\n",
      "epoch: 6 | 65280 / 114272 | training loss: 0.000489699887111783\n",
      "epoch: 6 | 65312 / 114272 | training loss: 0.001137909828685224\n",
      "epoch: 6 | 65344 / 114272 | training loss: 0.001268177991732955\n",
      "epoch: 6 | 65376 / 114272 | training loss: 0.0009974336717277765\n",
      "epoch: 6 | 65408 / 114272 | training loss: 0.0009956663707271218\n",
      "epoch: 6 | 65440 / 114272 | training loss: 0.002930185291916132\n",
      "epoch: 6 | 65472 / 114272 | training loss: 0.0008351809810847044\n",
      "epoch: 6 | 65504 / 114272 | training loss: 0.0008307900279760361\n",
      "epoch: 6 | 65536 / 114272 | training loss: 0.15059074759483337\n",
      "epoch: 6 | 65568 / 114272 | training loss: 0.0007498601917177439\n",
      "epoch: 6 | 65600 / 114272 | training loss: 0.0006913390825502574\n",
      "epoch: 6 | 65632 / 114272 | training loss: 0.0008692641276866198\n",
      "epoch: 6 | 65664 / 114272 | training loss: 0.0004555394116323441\n",
      "epoch: 6 | 65696 / 114272 | training loss: 0.0013813060941174626\n",
      "epoch: 6 | 65728 / 114272 | training loss: 0.08519230782985687\n",
      "epoch: 6 | 65760 / 114272 | training loss: 0.000710807740688324\n",
      "epoch: 6 | 65792 / 114272 | training loss: 0.0012811102205887437\n",
      "epoch: 6 | 65824 / 114272 | training loss: 0.0008751237764954567\n",
      "epoch: 6 | 65856 / 114272 | training loss: 0.0007590991444885731\n",
      "epoch: 6 | 65888 / 114272 | training loss: 0.18054227530956268\n",
      "epoch: 6 | 65920 / 114272 | training loss: 0.0005274618160910904\n",
      "epoch: 6 | 65952 / 114272 | training loss: 0.0020104325376451015\n",
      "epoch: 6 | 65984 / 114272 | training loss: 0.0021886350587010384\n",
      "epoch: 6 | 66016 / 114272 | training loss: 0.0006606435053981841\n",
      "epoch: 6 | 66048 / 114272 | training loss: 0.015208236873149872\n",
      "epoch: 6 | 66080 / 114272 | training loss: 0.16101200878620148\n",
      "epoch: 6 | 66112 / 114272 | training loss: 0.0010980211663991213\n",
      "epoch: 6 | 66144 / 114272 | training loss: 0.0008708007517270744\n",
      "epoch: 6 | 66176 / 114272 | training loss: 0.000794092018622905\n",
      "epoch: 6 | 66208 / 114272 | training loss: 0.1060825064778328\n",
      "epoch: 6 | 66240 / 114272 | training loss: 0.00045758954365737736\n",
      "epoch: 6 | 66272 / 114272 | training loss: 0.11501874029636383\n",
      "epoch: 6 | 66304 / 114272 | training loss: 0.0007869365508668125\n",
      "epoch: 6 | 66336 / 114272 | training loss: 0.0012506850762292743\n",
      "epoch: 6 | 66368 / 114272 | training loss: 0.0014668049989268184\n",
      "epoch: 6 | 66400 / 114272 | training loss: 0.16360226273536682\n",
      "epoch: 6 | 66432 / 114272 | training loss: 0.0010805627098307014\n",
      "epoch: 6 | 66464 / 114272 | training loss: 0.002230435609817505\n",
      "epoch: 6 | 66496 / 114272 | training loss: 0.0014907995937392116\n",
      "epoch: 6 | 66528 / 114272 | training loss: 0.0003612276050262153\n",
      "epoch: 6 | 66560 / 114272 | training loss: 0.0030723491217941046\n",
      "epoch: 6 | 66592 / 114272 | training loss: 0.001138180261477828\n",
      "epoch: 6 | 66624 / 114272 | training loss: 0.000938835961278528\n",
      "epoch: 6 | 66656 / 114272 | training loss: 0.12216467410326004\n",
      "epoch: 6 | 66688 / 114272 | training loss: 0.0013176890788599849\n",
      "epoch: 6 | 66720 / 114272 | training loss: 0.0009307861910201609\n",
      "epoch: 6 | 66752 / 114272 | training loss: 0.0006608128314837813\n",
      "epoch: 6 | 66784 / 114272 | training loss: 0.02935819700360298\n",
      "epoch: 6 | 66816 / 114272 | training loss: 0.0007831331458874047\n",
      "epoch: 6 | 66848 / 114272 | training loss: 0.013965880498290062\n",
      "epoch: 6 | 66880 / 114272 | training loss: 0.001173530938103795\n",
      "epoch: 6 | 66912 / 114272 | training loss: 0.001515057752840221\n",
      "epoch: 6 | 66944 / 114272 | training loss: 0.0008584642200730741\n",
      "epoch: 6 | 66976 / 114272 | training loss: 0.0008344067027792335\n",
      "epoch: 6 | 67008 / 114272 | training loss: 0.0012712700990960002\n",
      "epoch: 6 | 67040 / 114272 | training loss: 0.0007468161056749523\n",
      "epoch: 6 | 67072 / 114272 | training loss: 0.15455052256584167\n",
      "epoch: 6 | 67104 / 114272 | training loss: 0.013978261500597\n",
      "epoch: 6 | 67136 / 114272 | training loss: 0.0014907275326550007\n",
      "epoch: 6 | 67168 / 114272 | training loss: 0.0012915048282593489\n",
      "epoch: 6 | 67200 / 114272 | training loss: 0.13440632820129395\n",
      "epoch: 6 | 67232 / 114272 | training loss: 0.0013339405413717031\n",
      "epoch: 6 | 67264 / 114272 | training loss: 0.0005384377436712384\n",
      "epoch: 6 | 67296 / 114272 | training loss: 0.0007067575352266431\n",
      "epoch: 6 | 67328 / 114272 | training loss: 0.0008911211625672877\n",
      "epoch: 6 | 67360 / 114272 | training loss: 0.001237042248249054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 67392 / 114272 | training loss: 0.0021184806246310472\n",
      "epoch: 6 | 67424 / 114272 | training loss: 0.0008117117686197162\n",
      "epoch: 6 | 67456 / 114272 | training loss: 0.0016887139063328505\n",
      "epoch: 6 | 67488 / 114272 | training loss: 0.0017052555922418833\n",
      "epoch: 6 | 67520 / 114272 | training loss: 0.0025563614908605814\n",
      "epoch: 6 | 67552 / 114272 | training loss: 0.0016836485592648387\n",
      "epoch: 6 | 67584 / 114272 | training loss: 0.001469980925321579\n",
      "epoch: 6 | 67616 / 114272 | training loss: 0.0005305745871737599\n",
      "epoch: 6 | 67648 / 114272 | training loss: 0.0008978734258562326\n",
      "epoch: 6 | 67680 / 114272 | training loss: 0.0013833121629431844\n",
      "epoch: 6 | 67712 / 114272 | training loss: 0.0021672372240573168\n",
      "epoch: 6 | 67744 / 114272 | training loss: 0.0007741334848105907\n",
      "epoch: 6 | 67776 / 114272 | training loss: 0.007705272641032934\n",
      "epoch: 6 | 67808 / 114272 | training loss: 0.0009566459921188653\n",
      "epoch: 6 | 67840 / 114272 | training loss: 0.0016213372582569718\n",
      "epoch: 6 | 67872 / 114272 | training loss: 0.0035248270723968744\n",
      "epoch: 6 | 67904 / 114272 | training loss: 0.001230974099598825\n",
      "epoch: 6 | 67936 / 114272 | training loss: 0.28080055117607117\n",
      "epoch: 6 | 67968 / 114272 | training loss: 0.0012424037558957934\n",
      "epoch: 6 | 68000 / 114272 | training loss: 0.0012046502670273185\n",
      "epoch: 6 | 68032 / 114272 | training loss: 0.0013173323823139071\n",
      "epoch: 6 | 68064 / 114272 | training loss: 0.0012004562886431813\n",
      "epoch: 6 | 68096 / 114272 | training loss: 0.23858051002025604\n",
      "epoch: 6 | 68128 / 114272 | training loss: 0.0003428178606554866\n",
      "epoch: 6 | 68160 / 114272 | training loss: 0.001068555167876184\n",
      "epoch: 6 | 68192 / 114272 | training loss: 0.14683307707309723\n",
      "epoch: 6 | 68224 / 114272 | training loss: 0.0008707796223461628\n",
      "epoch: 6 | 68256 / 114272 | training loss: 0.00040526455268263817\n",
      "epoch: 6 | 68288 / 114272 | training loss: 0.0033819666132330894\n",
      "epoch: 6 | 68320 / 114272 | training loss: 0.0009974606800824404\n",
      "epoch: 6 | 68352 / 114272 | training loss: 0.00017830725118983537\n",
      "epoch: 6 | 68384 / 114272 | training loss: 0.15366554260253906\n",
      "epoch: 6 | 68416 / 114272 | training loss: 0.0030453912913799286\n",
      "epoch: 6 | 68448 / 114272 | training loss: 0.0008783934754319489\n",
      "epoch: 6 | 68480 / 114272 | training loss: 0.0014506368897855282\n",
      "epoch: 6 | 68512 / 114272 | training loss: 0.0011026025749742985\n",
      "epoch: 6 | 68544 / 114272 | training loss: 0.0009483374306000769\n",
      "epoch: 6 | 68576 / 114272 | training loss: 0.0012121449690312147\n",
      "epoch: 6 | 68608 / 114272 | training loss: 0.0015886250184848905\n",
      "epoch: 6 | 68640 / 114272 | training loss: 0.0017757031600922346\n",
      "epoch: 6 | 68672 / 114272 | training loss: 0.0012733755866065621\n",
      "epoch: 6 | 68704 / 114272 | training loss: 0.005086099728941917\n",
      "epoch: 6 | 68736 / 114272 | training loss: 0.0015237731859087944\n",
      "epoch: 6 | 68768 / 114272 | training loss: 0.0011440421221777797\n",
      "epoch: 6 | 68800 / 114272 | training loss: 0.1395168900489807\n",
      "epoch: 6 | 68832 / 114272 | training loss: 0.2709850072860718\n",
      "epoch: 6 | 68864 / 114272 | training loss: 0.0019019594183191657\n",
      "epoch: 6 | 68896 / 114272 | training loss: 0.001650567864999175\n",
      "epoch: 6 | 68928 / 114272 | training loss: 0.04988936334848404\n",
      "epoch: 6 | 68960 / 114272 | training loss: 0.0008989130728878081\n",
      "epoch: 6 | 68992 / 114272 | training loss: 0.0015898790443316102\n",
      "epoch: 6 | 69024 / 114272 | training loss: 0.011312722228467464\n",
      "epoch: 6 | 69056 / 114272 | training loss: 0.0013574810000136495\n",
      "epoch: 6 | 69088 / 114272 | training loss: 0.04481054097414017\n",
      "epoch: 6 | 69120 / 114272 | training loss: 0.0006789413164369762\n",
      "epoch: 6 | 69152 / 114272 | training loss: 0.0011317747412249446\n",
      "epoch: 6 | 69184 / 114272 | training loss: 0.0008656001300550997\n",
      "epoch: 6 | 69216 / 114272 | training loss: 0.001492309384047985\n",
      "epoch: 6 | 69248 / 114272 | training loss: 0.0006866884650662541\n",
      "epoch: 6 | 69280 / 114272 | training loss: 0.0033690426498651505\n",
      "epoch: 6 | 69312 / 114272 | training loss: 0.005318901035934687\n",
      "epoch: 6 | 69344 / 114272 | training loss: 0.0011603590101003647\n",
      "epoch: 6 | 69376 / 114272 | training loss: 0.0007764494512230158\n",
      "epoch: 6 | 69408 / 114272 | training loss: 0.0004907393595203757\n",
      "epoch: 6 | 69440 / 114272 | training loss: 0.0005748781841248274\n",
      "epoch: 6 | 69472 / 114272 | training loss: 0.0007423804490827024\n",
      "epoch: 6 | 69504 / 114272 | training loss: 0.002093640388920903\n",
      "epoch: 6 | 69536 / 114272 | training loss: 0.0006617803010158241\n",
      "epoch: 6 | 69568 / 114272 | training loss: 0.002027971437200904\n",
      "epoch: 6 | 69600 / 114272 | training loss: 0.0006365180015563965\n",
      "epoch: 6 | 69632 / 114272 | training loss: 0.0008930081385187805\n",
      "epoch: 6 | 69664 / 114272 | training loss: 0.009174876846373081\n",
      "epoch: 6 | 69696 / 114272 | training loss: 0.0010663525899872184\n",
      "epoch: 6 | 69728 / 114272 | training loss: 0.0009223989327438176\n",
      "epoch: 6 | 69760 / 114272 | training loss: 0.0007592877955175936\n",
      "epoch: 6 | 69792 / 114272 | training loss: 0.0014634765684604645\n",
      "epoch: 6 | 69824 / 114272 | training loss: 0.0003242420789320022\n",
      "epoch: 6 | 69856 / 114272 | training loss: 0.0008443849510513246\n",
      "epoch: 6 | 69888 / 114272 | training loss: 0.0007882636273279786\n",
      "epoch: 6 | 69920 / 114272 | training loss: 0.0006534582353197038\n",
      "epoch: 6 | 69952 / 114272 | training loss: 0.0008875823696143925\n",
      "epoch: 6 | 69984 / 114272 | training loss: 0.0008793621673248708\n",
      "epoch: 6 | 70016 / 114272 | training loss: 0.0008646853384561837\n",
      "epoch: 6 | 70048 / 114272 | training loss: 0.0006625381065532565\n",
      "epoch: 6 | 70080 / 114272 | training loss: 0.0011052833870053291\n",
      "epoch: 6 | 70112 / 114272 | training loss: 0.0007425987860187888\n",
      "epoch: 6 | 70144 / 114272 | training loss: 0.14009879529476166\n",
      "epoch: 6 | 70176 / 114272 | training loss: 0.0012989869574084878\n",
      "epoch: 6 | 70208 / 114272 | training loss: 0.0005294542643241584\n",
      "epoch: 6 | 70240 / 114272 | training loss: 0.0005014464841224253\n",
      "epoch: 6 | 70272 / 114272 | training loss: 0.002020368818193674\n",
      "epoch: 6 | 70304 / 114272 | training loss: 0.0012870648643001914\n",
      "epoch: 6 | 70336 / 114272 | training loss: 0.0008719622856006026\n",
      "epoch: 6 | 70368 / 114272 | training loss: 0.0007579161901958287\n",
      "epoch: 6 | 70400 / 114272 | training loss: 0.0012968213995918632\n",
      "epoch: 6 | 70432 / 114272 | training loss: 0.0006641332875005901\n",
      "epoch: 6 | 70464 / 114272 | training loss: 0.0005624745390377939\n",
      "epoch: 6 | 70496 / 114272 | training loss: 0.0005196280544623733\n",
      "epoch: 6 | 70528 / 114272 | training loss: 0.32865604758262634\n",
      "epoch: 6 | 70560 / 114272 | training loss: 0.2695687413215637\n",
      "epoch: 6 | 70592 / 114272 | training loss: 0.0019192163599655032\n",
      "epoch: 6 | 70624 / 114272 | training loss: 0.0007677001412957907\n",
      "epoch: 6 | 70656 / 114272 | training loss: 0.002183079021051526\n",
      "epoch: 6 | 70688 / 114272 | training loss: 0.003738261992111802\n",
      "epoch: 6 | 70720 / 114272 | training loss: 0.0014537539100274444\n",
      "epoch: 6 | 70752 / 114272 | training loss: 0.0020106984302401543\n",
      "epoch: 6 | 70784 / 114272 | training loss: 0.0009107013465836644\n",
      "epoch: 6 | 70816 / 114272 | training loss: 0.0006710268789902329\n",
      "epoch: 6 | 70848 / 114272 | training loss: 0.0008633004617877305\n",
      "epoch: 6 | 70880 / 114272 | training loss: 0.0011489780154079199\n",
      "epoch: 6 | 70912 / 114272 | training loss: 0.08260054886341095\n",
      "epoch: 6 | 70944 / 114272 | training loss: 0.0005761191714555025\n",
      "epoch: 6 | 70976 / 114272 | training loss: 0.000513652921654284\n",
      "epoch: 6 | 71008 / 114272 | training loss: 0.0008787643164396286\n",
      "epoch: 6 | 71040 / 114272 | training loss: 0.0006348636816255748\n",
      "epoch: 6 | 71072 / 114272 | training loss: 0.0008447642321698368\n",
      "epoch: 6 | 71104 / 114272 | training loss: 0.012573588639497757\n",
      "epoch: 6 | 71136 / 114272 | training loss: 0.0008265461656264961\n",
      "epoch: 6 | 71168 / 114272 | training loss: 0.0008471627370454371\n",
      "epoch: 6 | 71200 / 114272 | training loss: 0.0023777897004038095\n",
      "epoch: 6 | 71232 / 114272 | training loss: 0.00046297258813865483\n",
      "epoch: 6 | 71264 / 114272 | training loss: 0.0008353673620149493\n",
      "epoch: 6 | 71296 / 114272 | training loss: 0.0011349755804985762\n",
      "epoch: 6 | 71328 / 114272 | training loss: 0.0010883852373808622\n",
      "epoch: 6 | 71360 / 114272 | training loss: 0.0014444033149629831\n",
      "epoch: 6 | 71392 / 114272 | training loss: 0.05053752660751343\n",
      "epoch: 6 | 71424 / 114272 | training loss: 0.0007626867154613137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 71456 / 114272 | training loss: 0.17672593891620636\n",
      "epoch: 6 | 71488 / 114272 | training loss: 0.0008751631248742342\n",
      "epoch: 6 | 71520 / 114272 | training loss: 0.00026874570176005363\n",
      "epoch: 6 | 71552 / 114272 | training loss: 0.0015176591696217656\n",
      "epoch: 6 | 71584 / 114272 | training loss: 0.11734850704669952\n",
      "epoch: 6 | 71616 / 114272 | training loss: 0.0006498455186374485\n",
      "epoch: 6 | 71648 / 114272 | training loss: 0.0005641822353936732\n",
      "epoch: 6 | 71680 / 114272 | training loss: 0.0004800698079634458\n",
      "epoch: 6 | 71712 / 114272 | training loss: 0.0004771654203068465\n",
      "epoch: 6 | 71744 / 114272 | training loss: 0.0008374031749553978\n",
      "epoch: 6 | 71776 / 114272 | training loss: 0.0006621888023801148\n",
      "epoch: 6 | 71808 / 114272 | training loss: 0.0008819959475658834\n",
      "epoch: 6 | 71840 / 114272 | training loss: 0.0004120984231121838\n",
      "epoch: 6 | 71872 / 114272 | training loss: 0.0009839726844802499\n",
      "epoch: 6 | 71904 / 114272 | training loss: 0.0008113922667689621\n",
      "epoch: 6 | 71936 / 114272 | training loss: 0.22070154547691345\n",
      "epoch: 6 | 71968 / 114272 | training loss: 0.0007659349939785898\n",
      "epoch: 6 | 72000 / 114272 | training loss: 0.00216048420406878\n",
      "epoch: 6 | 72032 / 114272 | training loss: 0.0005548204062506557\n",
      "epoch: 6 | 72064 / 114272 | training loss: 0.17416515946388245\n",
      "epoch: 6 | 72096 / 114272 | training loss: 0.03921081870794296\n",
      "epoch: 6 | 72128 / 114272 | training loss: 0.0011529834009706974\n",
      "epoch: 6 | 72160 / 114272 | training loss: 0.04284881427884102\n",
      "epoch: 6 | 72192 / 114272 | training loss: 0.0025004977360367775\n",
      "epoch: 6 | 72224 / 114272 | training loss: 0.00284965755417943\n",
      "epoch: 6 | 72256 / 114272 | training loss: 0.04489641264081001\n",
      "epoch: 6 | 72288 / 114272 | training loss: 0.0015975716523826122\n",
      "epoch: 6 | 72320 / 114272 | training loss: 0.07427135109901428\n",
      "epoch: 6 | 72352 / 114272 | training loss: 0.0006401881691999733\n",
      "epoch: 6 | 72384 / 114272 | training loss: 0.001429342315532267\n",
      "epoch: 6 | 72416 / 114272 | training loss: 0.0009669538121670485\n",
      "epoch: 6 | 72448 / 114272 | training loss: 0.0010284226154908538\n",
      "epoch: 6 | 72480 / 114272 | training loss: 0.0005998968845233321\n",
      "epoch: 6 | 72512 / 114272 | training loss: 0.0011395483743399382\n",
      "epoch: 6 | 72544 / 114272 | training loss: 0.001715026330202818\n",
      "epoch: 6 | 72576 / 114272 | training loss: 0.3311014771461487\n",
      "epoch: 6 | 72608 / 114272 | training loss: 0.002044592285528779\n",
      "epoch: 6 | 72640 / 114272 | training loss: 0.1301935762166977\n",
      "epoch: 6 | 72672 / 114272 | training loss: 0.001338473754003644\n",
      "epoch: 6 | 72704 / 114272 | training loss: 0.0004735999973490834\n",
      "epoch: 6 | 72736 / 114272 | training loss: 0.0005455693462863564\n",
      "epoch: 6 | 72768 / 114272 | training loss: 0.001179558108560741\n",
      "epoch: 6 | 72800 / 114272 | training loss: 0.0005966366734355688\n",
      "epoch: 6 | 72832 / 114272 | training loss: 0.030823705717921257\n",
      "epoch: 6 | 72864 / 114272 | training loss: 0.0014322930946946144\n",
      "epoch: 6 | 72896 / 114272 | training loss: 0.002791862701997161\n",
      "epoch: 6 | 72928 / 114272 | training loss: 0.19066007435321808\n",
      "epoch: 6 | 72960 / 114272 | training loss: 0.0024673219304531813\n",
      "epoch: 6 | 72992 / 114272 | training loss: 0.0010748219210654497\n",
      "epoch: 6 | 73024 / 114272 | training loss: 0.012629966251552105\n",
      "epoch: 6 | 73056 / 114272 | training loss: 0.009891092777252197\n",
      "epoch: 6 | 73088 / 114272 | training loss: 0.0009324119309894741\n",
      "epoch: 6 | 73120 / 114272 | training loss: 0.0024602615740150213\n",
      "epoch: 6 | 73152 / 114272 | training loss: 0.0009439794230274856\n",
      "epoch: 6 | 73184 / 114272 | training loss: 0.0010537587804719806\n",
      "epoch: 6 | 73216 / 114272 | training loss: 0.03867414966225624\n",
      "epoch: 6 | 73248 / 114272 | training loss: 0.0010880054906010628\n",
      "epoch: 6 | 73280 / 114272 | training loss: 0.0010785305639728904\n",
      "epoch: 6 | 73312 / 114272 | training loss: 0.0013901637867093086\n",
      "epoch: 6 | 73344 / 114272 | training loss: 0.0014724484644830227\n",
      "epoch: 6 | 73376 / 114272 | training loss: 0.010216054506599903\n",
      "epoch: 6 | 73408 / 114272 | training loss: 0.007154588121920824\n",
      "epoch: 6 | 73440 / 114272 | training loss: 0.0014728852547705173\n",
      "epoch: 6 | 73472 / 114272 | training loss: 0.002916323719546199\n",
      "epoch: 6 | 73504 / 114272 | training loss: 0.0010464433580636978\n",
      "epoch: 6 | 73536 / 114272 | training loss: 0.0014284462668001652\n",
      "epoch: 6 | 73568 / 114272 | training loss: 0.0024641568306833506\n",
      "epoch: 6 | 73600 / 114272 | training loss: 0.001108707394450903\n",
      "epoch: 6 | 73632 / 114272 | training loss: 0.001900429604575038\n",
      "epoch: 6 | 73664 / 114272 | training loss: 0.16994625329971313\n",
      "epoch: 6 | 73696 / 114272 | training loss: 0.15570871531963348\n",
      "epoch: 6 | 73728 / 114272 | training loss: 0.0008425788255408406\n",
      "epoch: 6 | 73760 / 114272 | training loss: 0.002973373746499419\n",
      "epoch: 6 | 73792 / 114272 | training loss: 0.000980187556706369\n",
      "epoch: 6 | 73824 / 114272 | training loss: 0.0008751843124628067\n",
      "epoch: 6 | 73856 / 114272 | training loss: 0.0010188333690166473\n",
      "epoch: 6 | 73888 / 114272 | training loss: 0.0013718153350055218\n",
      "epoch: 6 | 73920 / 114272 | training loss: 0.0010646118316799402\n",
      "epoch: 6 | 73952 / 114272 | training loss: 0.0005171529483050108\n",
      "epoch: 6 | 73984 / 114272 | training loss: 0.0014604589669033885\n",
      "epoch: 6 | 74016 / 114272 | training loss: 0.0008088680915534496\n",
      "epoch: 6 | 74048 / 114272 | training loss: 0.02777808904647827\n",
      "epoch: 6 | 74080 / 114272 | training loss: 0.0007833153940737247\n",
      "epoch: 6 | 74112 / 114272 | training loss: 0.001678759464994073\n",
      "epoch: 6 | 74144 / 114272 | training loss: 0.0011693320702761412\n",
      "epoch: 6 | 74176 / 114272 | training loss: 0.0008580357534810901\n",
      "epoch: 6 | 74208 / 114272 | training loss: 0.0016654741484671831\n",
      "epoch: 6 | 74240 / 114272 | training loss: 0.0009146433440037072\n",
      "epoch: 6 | 74272 / 114272 | training loss: 0.0014197234995663166\n",
      "epoch: 6 | 74304 / 114272 | training loss: 0.0029709399677813053\n",
      "epoch: 6 | 74336 / 114272 | training loss: 0.13515804708003998\n",
      "epoch: 6 | 74368 / 114272 | training loss: 0.0007838631281629205\n",
      "epoch: 6 | 74400 / 114272 | training loss: 0.007648976519703865\n",
      "epoch: 6 | 74432 / 114272 | training loss: 0.0009058516006916761\n",
      "epoch: 6 | 74464 / 114272 | training loss: 0.0008153354865498841\n",
      "epoch: 6 | 74496 / 114272 | training loss: 0.0021637212485074997\n",
      "epoch: 6 | 74528 / 114272 | training loss: 0.0014318374451249838\n",
      "epoch: 6 | 74560 / 114272 | training loss: 0.0029474112670868635\n",
      "epoch: 6 | 74592 / 114272 | training loss: 0.0005771001451648772\n",
      "epoch: 6 | 74624 / 114272 | training loss: 0.0007038182811811566\n",
      "epoch: 6 | 74656 / 114272 | training loss: 0.001330696395598352\n",
      "epoch: 6 | 74688 / 114272 | training loss: 0.0012276597553864121\n",
      "epoch: 6 | 74720 / 114272 | training loss: 0.19765637814998627\n",
      "epoch: 6 | 74752 / 114272 | training loss: 0.0009081147145479918\n",
      "epoch: 6 | 74784 / 114272 | training loss: 0.0011406512930989265\n",
      "epoch: 6 | 74816 / 114272 | training loss: 0.0007131423917599022\n",
      "epoch: 6 | 74848 / 114272 | training loss: 0.0011384470853954554\n",
      "epoch: 6 | 74880 / 114272 | training loss: 0.0006157948519103229\n",
      "epoch: 6 | 74912 / 114272 | training loss: 0.0007535710465162992\n",
      "epoch: 6 | 74944 / 114272 | training loss: 0.010540198534727097\n",
      "epoch: 6 | 74976 / 114272 | training loss: 0.001240612706169486\n",
      "epoch: 6 | 75008 / 114272 | training loss: 0.09032369405031204\n",
      "epoch: 6 | 75040 / 114272 | training loss: 0.11198841035366058\n",
      "epoch: 6 | 75072 / 114272 | training loss: 0.000800984853412956\n",
      "epoch: 6 | 75104 / 114272 | training loss: 0.0006132536800578237\n",
      "epoch: 6 | 75136 / 114272 | training loss: 0.0008339183405041695\n",
      "epoch: 6 | 75168 / 114272 | training loss: 0.2346802055835724\n",
      "epoch: 6 | 75200 / 114272 | training loss: 0.0008228857186622918\n",
      "epoch: 6 | 75232 / 114272 | training loss: 0.009714489802718163\n",
      "epoch: 6 | 75264 / 114272 | training loss: 0.0007192761986516416\n",
      "epoch: 6 | 75296 / 114272 | training loss: 0.0005854778573848307\n",
      "epoch: 6 | 75328 / 114272 | training loss: 0.0004311651282478124\n",
      "epoch: 6 | 75360 / 114272 | training loss: 0.001421000575646758\n",
      "epoch: 6 | 75392 / 114272 | training loss: 0.0012699769577011466\n",
      "epoch: 6 | 75424 / 114272 | training loss: 0.0010026475647464395\n",
      "epoch: 6 | 75456 / 114272 | training loss: 0.3331720530986786\n",
      "epoch: 6 | 75488 / 114272 | training loss: 0.001660034409724176\n",
      "epoch: 6 | 75520 / 114272 | training loss: 0.0013947675470262766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 75552 / 114272 | training loss: 0.008697654120624065\n",
      "epoch: 6 | 75584 / 114272 | training loss: 0.0005977876135148108\n",
      "epoch: 6 | 75616 / 114272 | training loss: 0.0009760376997292042\n",
      "epoch: 6 | 75648 / 114272 | training loss: 0.0007058245246298611\n",
      "epoch: 6 | 75680 / 114272 | training loss: 0.001149532850831747\n",
      "epoch: 6 | 75712 / 114272 | training loss: 0.0007952617015689611\n",
      "epoch: 6 | 75744 / 114272 | training loss: 0.000904389307834208\n",
      "epoch: 6 | 75776 / 114272 | training loss: 0.0015897315461188555\n",
      "epoch: 6 | 75808 / 114272 | training loss: 0.0006204728269949555\n",
      "epoch: 6 | 75840 / 114272 | training loss: 0.0007306954357773066\n",
      "epoch: 6 | 75872 / 114272 | training loss: 0.0010316111147403717\n",
      "epoch: 6 | 75904 / 114272 | training loss: 0.0009293043403886259\n",
      "epoch: 6 | 75936 / 114272 | training loss: 0.000999520649202168\n",
      "epoch: 6 | 75968 / 114272 | training loss: 0.1719965636730194\n",
      "epoch: 6 | 76000 / 114272 | training loss: 0.0013641256373375654\n",
      "epoch: 6 | 76032 / 114272 | training loss: 0.001158961676992476\n",
      "epoch: 6 | 76064 / 114272 | training loss: 0.0009184064692817628\n",
      "epoch: 6 | 76096 / 114272 | training loss: 0.12677860260009766\n",
      "epoch: 6 | 76128 / 114272 | training loss: 0.0006737041985616088\n",
      "epoch: 6 | 76160 / 114272 | training loss: 0.0017822189256548882\n",
      "epoch: 6 | 76192 / 114272 | training loss: 0.0007691680802963674\n",
      "epoch: 6 | 76224 / 114272 | training loss: 0.01316132117062807\n",
      "epoch: 6 | 76256 / 114272 | training loss: 0.0016771452501416206\n",
      "epoch: 6 | 76288 / 114272 | training loss: 0.00048742088256403804\n",
      "epoch: 6 | 76320 / 114272 | training loss: 0.14707966148853302\n",
      "epoch: 6 | 76352 / 114272 | training loss: 0.0028057119343429804\n",
      "epoch: 6 | 76384 / 114272 | training loss: 0.0038872123695909977\n",
      "epoch: 6 | 76416 / 114272 | training loss: 0.00286104972474277\n",
      "epoch: 6 | 76448 / 114272 | training loss: 0.0012698095524683595\n",
      "epoch: 6 | 76480 / 114272 | training loss: 0.0009038007701747119\n",
      "epoch: 6 | 76512 / 114272 | training loss: 0.26138535141944885\n",
      "epoch: 6 | 76544 / 114272 | training loss: 0.003264570841565728\n",
      "epoch: 6 | 76576 / 114272 | training loss: 0.002160459291189909\n",
      "epoch: 6 | 76608 / 114272 | training loss: 0.011141810566186905\n",
      "epoch: 6 | 76640 / 114272 | training loss: 0.0014622836606577039\n",
      "epoch: 6 | 76672 / 114272 | training loss: 0.001866387203335762\n",
      "epoch: 6 | 76704 / 114272 | training loss: 0.0020069435704499483\n",
      "epoch: 6 | 76736 / 114272 | training loss: 0.0017930177273228765\n",
      "epoch: 6 | 76768 / 114272 | training loss: 0.09992940723896027\n",
      "epoch: 6 | 76800 / 114272 | training loss: 0.0009168130927719176\n",
      "epoch: 6 | 76832 / 114272 | training loss: 0.09145720303058624\n",
      "epoch: 6 | 76864 / 114272 | training loss: 0.10751382261514664\n",
      "epoch: 6 | 76896 / 114272 | training loss: 0.002089523011818528\n",
      "epoch: 6 | 76928 / 114272 | training loss: 0.0027912401128560305\n",
      "epoch: 6 | 76960 / 114272 | training loss: 0.0016490878770127892\n",
      "epoch: 6 | 76992 / 114272 | training loss: 0.0017747509991750121\n",
      "epoch: 6 | 77024 / 114272 | training loss: 0.0015241631772369146\n",
      "epoch: 6 | 77056 / 114272 | training loss: 0.017186103388667107\n",
      "epoch: 6 | 77088 / 114272 | training loss: 0.0018042940646409988\n",
      "epoch: 6 | 77120 / 114272 | training loss: 0.0874197855591774\n",
      "epoch: 6 | 77152 / 114272 | training loss: 0.0008419497171416879\n",
      "epoch: 6 | 77184 / 114272 | training loss: 0.0075448621064424515\n",
      "epoch: 6 | 77216 / 114272 | training loss: 0.002728064078837633\n",
      "epoch: 6 | 77248 / 114272 | training loss: 0.0009444777388125658\n",
      "epoch: 6 | 77280 / 114272 | training loss: 0.003500552149489522\n",
      "epoch: 6 | 77312 / 114272 | training loss: 0.002306406619027257\n",
      "epoch: 6 | 77344 / 114272 | training loss: 0.002578455489128828\n",
      "epoch: 6 | 77376 / 114272 | training loss: 0.0061409142799675465\n",
      "epoch: 6 | 77408 / 114272 | training loss: 0.0018507320201024413\n",
      "epoch: 6 | 77440 / 114272 | training loss: 0.0030391227919608355\n",
      "epoch: 6 | 77472 / 114272 | training loss: 0.000868400966282934\n",
      "epoch: 6 | 77504 / 114272 | training loss: 0.0016456348821520805\n",
      "epoch: 6 | 77536 / 114272 | training loss: 0.0012302577961236238\n",
      "epoch: 6 | 77568 / 114272 | training loss: 0.0007285929750651121\n",
      "epoch: 6 | 77600 / 114272 | training loss: 0.0025981140788644552\n",
      "epoch: 6 | 77632 / 114272 | training loss: 0.0012468203203752637\n",
      "epoch: 6 | 77664 / 114272 | training loss: 0.0022720335982739925\n",
      "epoch: 6 | 77696 / 114272 | training loss: 0.0014148465124890208\n",
      "epoch: 6 | 77728 / 114272 | training loss: 0.0015703306999057531\n",
      "epoch: 6 | 77760 / 114272 | training loss: 0.0018172351410612464\n",
      "epoch: 6 | 77792 / 114272 | training loss: 0.0011265029897913337\n",
      "epoch: 6 | 77824 / 114272 | training loss: 0.0015709033468738198\n",
      "epoch: 6 | 77856 / 114272 | training loss: 0.0014347247779369354\n",
      "epoch: 6 | 77888 / 114272 | training loss: 0.000995904440060258\n",
      "epoch: 6 | 77920 / 114272 | training loss: 0.2011427879333496\n",
      "epoch: 6 | 77952 / 114272 | training loss: 0.37161317467689514\n",
      "epoch: 6 | 77984 / 114272 | training loss: 0.0016987002454698086\n",
      "epoch: 6 | 78016 / 114272 | training loss: 0.0008522327989339828\n",
      "epoch: 6 | 78048 / 114272 | training loss: 0.0005066629964858294\n",
      "epoch: 6 | 78080 / 114272 | training loss: 0.002400606404989958\n",
      "epoch: 6 | 78112 / 114272 | training loss: 0.07398997992277145\n",
      "epoch: 6 | 78144 / 114272 | training loss: 0.20348888635635376\n",
      "epoch: 6 | 78176 / 114272 | training loss: 0.07224328070878983\n",
      "epoch: 6 | 78208 / 114272 | training loss: 0.001046262215822935\n",
      "epoch: 6 | 78240 / 114272 | training loss: 0.0010519297793507576\n",
      "epoch: 6 | 78272 / 114272 | training loss: 0.0009342248667962849\n",
      "epoch: 6 | 78304 / 114272 | training loss: 0.001185212517157197\n",
      "epoch: 6 | 78336 / 114272 | training loss: 0.0015802796697244048\n",
      "epoch: 6 | 78368 / 114272 | training loss: 0.15417295694351196\n",
      "epoch: 6 | 78400 / 114272 | training loss: 0.001154936384409666\n",
      "epoch: 6 | 78432 / 114272 | training loss: 0.003158270614221692\n",
      "epoch: 6 | 78464 / 114272 | training loss: 0.0010629661846905947\n",
      "epoch: 6 | 78496 / 114272 | training loss: 0.001845484017394483\n",
      "epoch: 6 | 78528 / 114272 | training loss: 0.0016659705433994532\n",
      "epoch: 6 | 78560 / 114272 | training loss: 0.0007830772665329278\n",
      "epoch: 6 | 78592 / 114272 | training loss: 0.0006782179698348045\n",
      "epoch: 6 | 78624 / 114272 | training loss: 0.0015188507968559861\n",
      "epoch: 6 | 78656 / 114272 | training loss: 0.20311680436134338\n",
      "epoch: 6 | 78688 / 114272 | training loss: 0.001309288665652275\n",
      "epoch: 6 | 78720 / 114272 | training loss: 0.07481350749731064\n",
      "epoch: 6 | 78752 / 114272 | training loss: 0.0029898295179009438\n",
      "epoch: 6 | 78784 / 114272 | training loss: 0.0019895771984010935\n",
      "epoch: 6 | 78816 / 114272 | training loss: 0.004338935948908329\n",
      "epoch: 6 | 78848 / 114272 | training loss: 0.13286374509334564\n",
      "epoch: 6 | 78880 / 114272 | training loss: 0.08760278671979904\n",
      "epoch: 6 | 78912 / 114272 | training loss: 0.0015484329778701067\n",
      "epoch: 6 | 78944 / 114272 | training loss: 0.002399009419605136\n",
      "epoch: 6 | 78976 / 114272 | training loss: 0.0010266409954056144\n",
      "epoch: 6 | 79008 / 114272 | training loss: 0.0014210556400939822\n",
      "epoch: 6 | 79040 / 114272 | training loss: 0.00692816823720932\n",
      "epoch: 6 | 79072 / 114272 | training loss: 0.0049035027623176575\n",
      "epoch: 6 | 79104 / 114272 | training loss: 0.001906045014038682\n",
      "epoch: 6 | 79136 / 114272 | training loss: 0.0007471331628039479\n",
      "epoch: 6 | 79168 / 114272 | training loss: 0.06601973623037338\n",
      "epoch: 6 | 79200 / 114272 | training loss: 0.005392528139054775\n",
      "epoch: 6 | 79232 / 114272 | training loss: 0.17558683454990387\n",
      "epoch: 6 | 79264 / 114272 | training loss: 0.002703336765989661\n",
      "epoch: 6 | 79296 / 114272 | training loss: 0.03690937161445618\n",
      "epoch: 6 | 79328 / 114272 | training loss: 0.0010525435209274292\n",
      "epoch: 6 | 79360 / 114272 | training loss: 0.0010843529598787427\n",
      "epoch: 6 | 79392 / 114272 | training loss: 0.0011799021158367395\n",
      "epoch: 6 | 79424 / 114272 | training loss: 0.006540173199027777\n",
      "epoch: 6 | 79456 / 114272 | training loss: 0.002722684293985367\n",
      "epoch: 6 | 79488 / 114272 | training loss: 0.001940100803039968\n",
      "epoch: 6 | 79520 / 114272 | training loss: 0.0024886741302907467\n",
      "epoch: 6 | 79552 / 114272 | training loss: 0.0004659375117626041\n",
      "epoch: 6 | 79584 / 114272 | training loss: 0.002089553512632847\n",
      "epoch: 6 | 79616 / 114272 | training loss: 0.0015646220417693257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 79648 / 114272 | training loss: 0.0007059223717078567\n",
      "epoch: 6 | 79680 / 114272 | training loss: 0.0015833943616598845\n",
      "epoch: 6 | 79712 / 114272 | training loss: 0.07290489971637726\n",
      "epoch: 6 | 79744 / 114272 | training loss: 0.0005735399900004268\n",
      "epoch: 6 | 79776 / 114272 | training loss: 0.0012857521651312709\n",
      "epoch: 6 | 79808 / 114272 | training loss: 0.00029992996132932603\n",
      "epoch: 6 | 79840 / 114272 | training loss: 0.0012729365844279528\n",
      "epoch: 6 | 79872 / 114272 | training loss: 0.001554591115564108\n",
      "epoch: 6 | 79904 / 114272 | training loss: 0.07765454798936844\n",
      "epoch: 6 | 79936 / 114272 | training loss: 0.0006394597585313022\n",
      "epoch: 6 | 79968 / 114272 | training loss: 0.0016769036883488297\n",
      "epoch: 6 | 80000 / 114272 | training loss: 0.0005307949613779783\n",
      "epoch: 6 | 80032 / 114272 | training loss: 0.12471874803304672\n",
      "epoch: 6 | 80064 / 114272 | training loss: 0.1217009425163269\n",
      "epoch: 6 | 80096 / 114272 | training loss: 0.0009803551947697997\n",
      "epoch: 6 | 80128 / 114272 | training loss: 0.0046442118473351\n",
      "epoch: 6 | 80160 / 114272 | training loss: 0.0006428994820453227\n",
      "epoch: 6 | 80192 / 114272 | training loss: 0.0023062825202941895\n",
      "epoch: 6 | 80224 / 114272 | training loss: 0.0010369985830038786\n",
      "epoch: 6 | 80256 / 114272 | training loss: 0.002416478004306555\n",
      "epoch: 6 | 80288 / 114272 | training loss: 0.0011392076266929507\n",
      "epoch: 6 | 80320 / 114272 | training loss: 0.0012504450278356671\n",
      "epoch: 6 | 80352 / 114272 | training loss: 0.0007931902073323727\n",
      "epoch: 6 | 80384 / 114272 | training loss: 0.03734908625483513\n",
      "epoch: 6 | 80416 / 114272 | training loss: 0.005475897341966629\n",
      "epoch: 6 | 80448 / 114272 | training loss: 0.0019884032662957907\n",
      "epoch: 6 | 80480 / 114272 | training loss: 0.0005830698064528406\n",
      "epoch: 6 | 80512 / 114272 | training loss: 0.002971339039504528\n",
      "epoch: 6 | 80544 / 114272 | training loss: 0.004900475963950157\n",
      "epoch: 6 | 80576 / 114272 | training loss: 0.16661453247070312\n",
      "epoch: 6 | 80608 / 114272 | training loss: 0.0013966759433969855\n",
      "epoch: 6 | 80640 / 114272 | training loss: 0.0004830168327316642\n",
      "epoch: 6 | 80672 / 114272 | training loss: 0.0006339011015370488\n",
      "epoch: 6 | 80704 / 114272 | training loss: 0.0006102803163230419\n",
      "epoch: 6 | 80736 / 114272 | training loss: 0.08404383063316345\n",
      "epoch: 6 | 80768 / 114272 | training loss: 0.0008514650980941951\n",
      "epoch: 6 | 80800 / 114272 | training loss: 0.0009686955600045621\n",
      "epoch: 6 | 80832 / 114272 | training loss: 0.000622713821940124\n",
      "epoch: 6 | 80864 / 114272 | training loss: 0.0009283227263949811\n",
      "epoch: 6 | 80896 / 114272 | training loss: 0.01835768297314644\n",
      "epoch: 6 | 80928 / 114272 | training loss: 0.00036312005249783397\n",
      "epoch: 6 | 80960 / 114272 | training loss: 0.00030230573611333966\n",
      "epoch: 6 | 80992 / 114272 | training loss: 0.0011832661693915725\n",
      "epoch: 6 | 81024 / 114272 | training loss: 0.023041166365146637\n",
      "epoch: 6 | 81056 / 114272 | training loss: 0.0005679357564076781\n",
      "epoch: 6 | 81088 / 114272 | training loss: 0.0005417997017502785\n",
      "epoch: 6 | 81120 / 114272 | training loss: 0.0008163999300450087\n",
      "epoch: 6 | 81152 / 114272 | training loss: 0.0005128667107783258\n",
      "epoch: 6 | 81184 / 114272 | training loss: 0.0012836087262257934\n",
      "epoch: 6 | 81216 / 114272 | training loss: 0.0006248505087569356\n",
      "epoch: 6 | 81248 / 114272 | training loss: 0.0012125519569963217\n",
      "epoch: 6 | 81280 / 114272 | training loss: 0.009326698258519173\n",
      "epoch: 6 | 81312 / 114272 | training loss: 0.07540928572416306\n",
      "epoch: 6 | 81344 / 114272 | training loss: 0.20132726430892944\n",
      "epoch: 6 | 81376 / 114272 | training loss: 0.0004546875134110451\n",
      "epoch: 6 | 81408 / 114272 | training loss: 0.0004625650472007692\n",
      "epoch: 6 | 81440 / 114272 | training loss: 0.0005829770816490054\n",
      "epoch: 6 | 81472 / 114272 | training loss: 0.00043482406181283295\n",
      "epoch: 6 | 81504 / 114272 | training loss: 0.13566824793815613\n",
      "epoch: 6 | 81536 / 114272 | training loss: 0.004103349521756172\n",
      "epoch: 6 | 81568 / 114272 | training loss: 0.0009184101945720613\n",
      "epoch: 6 | 81600 / 114272 | training loss: 0.0034964808728545904\n",
      "epoch: 6 | 81632 / 114272 | training loss: 0.0009325976716354489\n",
      "epoch: 6 | 81664 / 114272 | training loss: 0.22515977919101715\n",
      "epoch: 6 | 81696 / 114272 | training loss: 0.0008662355248816311\n",
      "epoch: 6 | 81728 / 114272 | training loss: 0.0005439012893475592\n",
      "epoch: 6 | 81760 / 114272 | training loss: 0.2955695688724518\n",
      "epoch: 6 | 81792 / 114272 | training loss: 0.04672393575310707\n",
      "epoch: 6 | 81824 / 114272 | training loss: 0.0020339549519121647\n",
      "epoch: 6 | 81856 / 114272 | training loss: 0.000542149122338742\n",
      "epoch: 6 | 81888 / 114272 | training loss: 0.001033570384606719\n",
      "epoch: 6 | 81920 / 114272 | training loss: 0.0006732584442943335\n",
      "epoch: 6 | 81952 / 114272 | training loss: 0.0004933163872919977\n",
      "epoch: 6 | 81984 / 114272 | training loss: 0.0010438497411087155\n",
      "epoch: 6 | 82016 / 114272 | training loss: 0.0038302645552903414\n",
      "epoch: 6 | 82048 / 114272 | training loss: 0.0005885427817702293\n",
      "epoch: 6 | 82080 / 114272 | training loss: 0.0013450176920741796\n",
      "epoch: 6 | 82112 / 114272 | training loss: 0.0097811259329319\n",
      "epoch: 6 | 82144 / 114272 | training loss: 0.22645898163318634\n",
      "epoch: 6 | 82176 / 114272 | training loss: 0.0011009140871465206\n",
      "epoch: 6 | 82208 / 114272 | training loss: 0.0013889784459024668\n",
      "epoch: 6 | 82240 / 114272 | training loss: 0.00180151651147753\n",
      "epoch: 6 | 82272 / 114272 | training loss: 0.002319748280569911\n",
      "epoch: 6 | 82304 / 114272 | training loss: 0.000703700352460146\n",
      "epoch: 6 | 82336 / 114272 | training loss: 0.00048767373664304614\n",
      "epoch: 6 | 82368 / 114272 | training loss: 0.0007619332754984498\n",
      "epoch: 6 | 82400 / 114272 | training loss: 0.0006412911461666226\n",
      "epoch: 6 | 82432 / 114272 | training loss: 0.00034894104464910924\n",
      "epoch: 6 | 82464 / 114272 | training loss: 0.0006009573116898537\n",
      "epoch: 6 | 82496 / 114272 | training loss: 0.0008532156352885067\n",
      "epoch: 6 | 82528 / 114272 | training loss: 0.001088599907234311\n",
      "epoch: 6 | 82560 / 114272 | training loss: 0.000506677373778075\n",
      "epoch: 6 | 82592 / 114272 | training loss: 0.00034965132363140583\n",
      "epoch: 6 | 82624 / 114272 | training loss: 0.0010837898589670658\n",
      "epoch: 6 | 82656 / 114272 | training loss: 0.0007205242291092873\n",
      "epoch: 6 | 82688 / 114272 | training loss: 0.0005563360173255205\n",
      "epoch: 6 | 82720 / 114272 | training loss: 0.0012075138511136174\n",
      "epoch: 6 | 82752 / 114272 | training loss: 0.0005269169923849404\n",
      "epoch: 6 | 82784 / 114272 | training loss: 0.0007303237216547132\n",
      "epoch: 6 | 82816 / 114272 | training loss: 0.0005396740161813796\n",
      "epoch: 6 | 82848 / 114272 | training loss: 0.00046309587196446955\n",
      "epoch: 6 | 82880 / 114272 | training loss: 0.031502507627010345\n",
      "epoch: 6 | 82912 / 114272 | training loss: 0.02459513396024704\n",
      "epoch: 6 | 82944 / 114272 | training loss: 0.0009053999674506485\n",
      "epoch: 6 | 82976 / 114272 | training loss: 0.0008309862460009754\n",
      "epoch: 6 | 83008 / 114272 | training loss: 0.0013005853397771716\n",
      "epoch: 6 | 83040 / 114272 | training loss: 0.0005918666138313711\n",
      "epoch: 6 | 83072 / 114272 | training loss: 0.000630405091214925\n",
      "epoch: 6 | 83104 / 114272 | training loss: 0.000646576751023531\n",
      "epoch: 6 | 83136 / 114272 | training loss: 0.0004698385309893638\n",
      "epoch: 6 | 83168 / 114272 | training loss: 0.0005918003735132515\n",
      "epoch: 6 | 83200 / 114272 | training loss: 0.0005828510038554668\n",
      "epoch: 6 | 83232 / 114272 | training loss: 0.00048679730389267206\n",
      "epoch: 6 | 83264 / 114272 | training loss: 0.00046913448022678494\n",
      "epoch: 6 | 83296 / 114272 | training loss: 0.0002658044686540961\n",
      "epoch: 6 | 83328 / 114272 | training loss: 0.0003711809404194355\n",
      "epoch: 6 | 83360 / 114272 | training loss: 0.00041806226363405585\n",
      "epoch: 6 | 83392 / 114272 | training loss: 0.0006934814737178385\n",
      "epoch: 6 | 83424 / 114272 | training loss: 0.0018707523122429848\n",
      "epoch: 6 | 83456 / 114272 | training loss: 0.00047321824240498245\n",
      "epoch: 6 | 83488 / 114272 | training loss: 0.0007774805999360979\n",
      "epoch: 6 | 83520 / 114272 | training loss: 0.000683635356836021\n",
      "epoch: 6 | 83552 / 114272 | training loss: 0.0008322706562466919\n",
      "epoch: 6 | 83584 / 114272 | training loss: 0.0006193307926878333\n",
      "epoch: 6 | 83616 / 114272 | training loss: 0.00044121305109001696\n",
      "epoch: 6 | 83648 / 114272 | training loss: 0.009653843007981777\n",
      "epoch: 6 | 83680 / 114272 | training loss: 0.09069272130727768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 83712 / 114272 | training loss: 0.0049050673842430115\n",
      "epoch: 6 | 83744 / 114272 | training loss: 0.00048338231863453984\n",
      "epoch: 6 | 83776 / 114272 | training loss: 0.17211925983428955\n",
      "epoch: 6 | 83808 / 114272 | training loss: 0.0007317331037484109\n",
      "epoch: 6 | 83840 / 114272 | training loss: 0.00044051394797861576\n",
      "epoch: 6 | 83872 / 114272 | training loss: 0.0006254463223740458\n",
      "epoch: 6 | 83904 / 114272 | training loss: 0.10184881836175919\n",
      "epoch: 6 | 83936 / 114272 | training loss: 0.0005369832506403327\n",
      "epoch: 6 | 83968 / 114272 | training loss: 0.0008182400488294661\n",
      "epoch: 6 | 84000 / 114272 | training loss: 0.013574443757534027\n",
      "epoch: 6 | 84032 / 114272 | training loss: 0.0014546436723321676\n",
      "epoch: 6 | 84064 / 114272 | training loss: 0.1525876671075821\n",
      "epoch: 6 | 84096 / 114272 | training loss: 0.00039586014463566244\n",
      "epoch: 6 | 84128 / 114272 | training loss: 0.011838680133223534\n",
      "epoch: 6 | 84160 / 114272 | training loss: 0.00044051118311472237\n",
      "epoch: 6 | 84192 / 114272 | training loss: 0.0010186153231188655\n",
      "epoch: 6 | 84224 / 114272 | training loss: 0.05662782862782478\n",
      "epoch: 6 | 84256 / 114272 | training loss: 0.05218490585684776\n",
      "epoch: 6 | 84288 / 114272 | training loss: 0.0007652094354853034\n",
      "epoch: 6 | 84320 / 114272 | training loss: 0.0007900672499090433\n",
      "epoch: 6 | 84352 / 114272 | training loss: 0.0006294228369370103\n",
      "epoch: 6 | 84384 / 114272 | training loss: 0.11467332392930984\n",
      "epoch: 6 | 84416 / 114272 | training loss: 0.0004493463784456253\n",
      "epoch: 6 | 84448 / 114272 | training loss: 0.0007797181606292725\n",
      "epoch: 6 | 84480 / 114272 | training loss: 0.0004420553450472653\n",
      "epoch: 6 | 84512 / 114272 | training loss: 0.0014196234988048673\n",
      "epoch: 6 | 84544 / 114272 | training loss: 0.20250962674617767\n",
      "epoch: 6 | 84576 / 114272 | training loss: 0.000743582786526531\n",
      "epoch: 6 | 84608 / 114272 | training loss: 0.0015830910997465253\n",
      "epoch: 6 | 84640 / 114272 | training loss: 0.10612194240093231\n",
      "epoch: 6 | 84672 / 114272 | training loss: 0.001272604102268815\n",
      "epoch: 6 | 84704 / 114272 | training loss: 0.001386629999615252\n",
      "epoch: 6 | 84736 / 114272 | training loss: 0.0006717608775943518\n",
      "epoch: 6 | 84768 / 114272 | training loss: 0.04443266615271568\n",
      "epoch: 6 | 84800 / 114272 | training loss: 0.00027419510297477245\n",
      "epoch: 6 | 84832 / 114272 | training loss: 0.0005906830774620175\n",
      "epoch: 6 | 84864 / 114272 | training loss: 0.00042607481009326875\n",
      "epoch: 6 | 84896 / 114272 | training loss: 0.0017776491586118937\n",
      "epoch: 6 | 84928 / 114272 | training loss: 0.001016170484945178\n",
      "epoch: 6 | 84960 / 114272 | training loss: 0.0007013472495600581\n",
      "epoch: 6 | 84992 / 114272 | training loss: 0.09324271976947784\n",
      "epoch: 6 | 85024 / 114272 | training loss: 0.000748854479752481\n",
      "epoch: 6 | 85056 / 114272 | training loss: 0.0015117969596758485\n",
      "epoch: 6 | 85088 / 114272 | training loss: 0.003167224582284689\n",
      "epoch: 6 | 85120 / 114272 | training loss: 0.0007417981396429241\n",
      "epoch: 6 | 85152 / 114272 | training loss: 0.0004759223957080394\n",
      "epoch: 6 | 85184 / 114272 | training loss: 0.0008142954320646822\n",
      "epoch: 6 | 85216 / 114272 | training loss: 0.005033982452005148\n",
      "epoch: 6 | 85248 / 114272 | training loss: 0.0007122827810235322\n",
      "epoch: 6 | 85280 / 114272 | training loss: 0.015150776132941246\n",
      "epoch: 6 | 85312 / 114272 | training loss: 0.0010410579852759838\n",
      "epoch: 6 | 85344 / 114272 | training loss: 0.00037915745633654296\n",
      "epoch: 6 | 85376 / 114272 | training loss: 0.24614514410495758\n",
      "epoch: 6 | 85408 / 114272 | training loss: 0.044878918677568436\n",
      "epoch: 6 | 85440 / 114272 | training loss: 0.0029348807875066996\n",
      "epoch: 6 | 85472 / 114272 | training loss: 0.00028433799161575735\n",
      "epoch: 6 | 85504 / 114272 | training loss: 0.003942179959267378\n",
      "epoch: 6 | 85536 / 114272 | training loss: 0.0008806737023405731\n",
      "epoch: 6 | 85568 / 114272 | training loss: 0.0006589494296349585\n",
      "epoch: 6 | 85600 / 114272 | training loss: 0.0008607292547821999\n",
      "epoch: 6 | 85632 / 114272 | training loss: 0.00051063735736534\n",
      "epoch: 6 | 85664 / 114272 | training loss: 0.17564375698566437\n",
      "epoch: 6 | 85696 / 114272 | training loss: 0.3541260361671448\n",
      "epoch: 6 | 85728 / 114272 | training loss: 0.0007287267362698913\n",
      "epoch: 6 | 85760 / 114272 | training loss: 0.0006030509830452502\n",
      "epoch: 6 | 85792 / 114272 | training loss: 0.0007216802914626896\n",
      "epoch: 6 | 85824 / 114272 | training loss: 0.0006804418517276645\n",
      "epoch: 6 | 85856 / 114272 | training loss: 0.06310699880123138\n",
      "epoch: 6 | 85888 / 114272 | training loss: 0.0007181100081652403\n",
      "epoch: 6 | 85920 / 114272 | training loss: 0.0008369704592041671\n",
      "epoch: 6 | 85952 / 114272 | training loss: 0.0016512249130755663\n",
      "epoch: 6 | 85984 / 114272 | training loss: 0.0011425333796069026\n",
      "epoch: 6 | 86016 / 114272 | training loss: 0.002023897599428892\n",
      "epoch: 6 | 86048 / 114272 | training loss: 0.17946697771549225\n",
      "epoch: 6 | 86080 / 114272 | training loss: 0.0006161621422506869\n",
      "epoch: 6 | 86112 / 114272 | training loss: 0.0014839502982795238\n",
      "epoch: 6 | 86144 / 114272 | training loss: 0.0008311106357723475\n",
      "epoch: 6 | 86176 / 114272 | training loss: 0.002622725209221244\n",
      "epoch: 6 | 86208 / 114272 | training loss: 0.0008153534145094454\n",
      "epoch: 6 | 86240 / 114272 | training loss: 0.04065091162919998\n",
      "epoch: 6 | 86272 / 114272 | training loss: 0.004088593181222677\n",
      "epoch: 6 | 86304 / 114272 | training loss: 0.000431494670920074\n",
      "epoch: 6 | 86336 / 114272 | training loss: 0.0006301820394583046\n",
      "epoch: 6 | 86368 / 114272 | training loss: 0.00047078303759917617\n",
      "epoch: 6 | 86400 / 114272 | training loss: 0.0006374327931553125\n",
      "epoch: 6 | 86432 / 114272 | training loss: 0.0008464276324957609\n",
      "epoch: 6 | 86464 / 114272 | training loss: 0.14063416421413422\n",
      "epoch: 6 | 86496 / 114272 | training loss: 0.0010272260988131166\n",
      "epoch: 6 | 86528 / 114272 | training loss: 0.0011011692695319653\n",
      "epoch: 6 | 86560 / 114272 | training loss: 0.01311594806611538\n",
      "epoch: 6 | 86592 / 114272 | training loss: 0.0005770675488747656\n",
      "epoch: 6 | 86624 / 114272 | training loss: 0.0010337436106055975\n",
      "epoch: 6 | 86656 / 114272 | training loss: 0.0012201097561046481\n",
      "epoch: 6 | 86688 / 114272 | training loss: 0.0005803292151540518\n",
      "epoch: 6 | 86720 / 114272 | training loss: 0.0009506040951237082\n",
      "epoch: 6 | 86752 / 114272 | training loss: 0.0011091541964560747\n",
      "epoch: 6 | 86784 / 114272 | training loss: 0.13315516710281372\n",
      "epoch: 6 | 86816 / 114272 | training loss: 0.0015116891590878367\n",
      "epoch: 6 | 86848 / 114272 | training loss: 0.0005891876644454896\n",
      "epoch: 6 | 86880 / 114272 | training loss: 0.0010912212310358882\n",
      "epoch: 6 | 86912 / 114272 | training loss: 0.0890347883105278\n",
      "epoch: 6 | 86944 / 114272 | training loss: 0.003238832112401724\n",
      "epoch: 6 | 86976 / 114272 | training loss: 0.025312643498182297\n",
      "epoch: 6 | 87008 / 114272 | training loss: 0.0011939116520807147\n",
      "epoch: 6 | 87040 / 114272 | training loss: 0.0013401007745414972\n",
      "epoch: 6 | 87072 / 114272 | training loss: 0.0034869287628680468\n",
      "epoch: 6 | 87104 / 114272 | training loss: 0.0006880503497086465\n",
      "epoch: 6 | 87136 / 114272 | training loss: 0.07205437123775482\n",
      "epoch: 6 | 87168 / 114272 | training loss: 0.03843682259321213\n",
      "epoch: 6 | 87200 / 114272 | training loss: 0.03158380091190338\n",
      "epoch: 6 | 87232 / 114272 | training loss: 0.002140812110155821\n",
      "epoch: 6 | 87264 / 114272 | training loss: 0.18156647682189941\n",
      "epoch: 6 | 87296 / 114272 | training loss: 0.000841381202917546\n",
      "epoch: 6 | 87328 / 114272 | training loss: 0.2260095179080963\n",
      "epoch: 6 | 87360 / 114272 | training loss: 0.0009130550315603614\n",
      "epoch: 6 | 87392 / 114272 | training loss: 0.0011393004097044468\n",
      "epoch: 6 | 87424 / 114272 | training loss: 0.0008423723629675806\n",
      "epoch: 6 | 87456 / 114272 | training loss: 0.001281539211049676\n",
      "epoch: 6 | 87488 / 114272 | training loss: 0.10129155963659286\n",
      "epoch: 6 | 87520 / 114272 | training loss: 0.003474164754152298\n",
      "epoch: 6 | 87552 / 114272 | training loss: 0.002088146982714534\n",
      "epoch: 6 | 87584 / 114272 | training loss: 0.0009033411042764783\n",
      "epoch: 6 | 87616 / 114272 | training loss: 0.003384389914572239\n",
      "epoch: 6 | 87648 / 114272 | training loss: 0.045587893575429916\n",
      "epoch: 6 | 87680 / 114272 | training loss: 0.0007627465529367328\n",
      "epoch: 6 | 87712 / 114272 | training loss: 0.00032171866041608155\n",
      "epoch: 6 | 87744 / 114272 | training loss: 0.0009726974531076849\n",
      "epoch: 6 | 87776 / 114272 | training loss: 0.00046539478353224695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 87808 / 114272 | training loss: 0.062835693359375\n",
      "epoch: 6 | 87840 / 114272 | training loss: 0.08754780143499374\n",
      "epoch: 6 | 87872 / 114272 | training loss: 0.0006727631553076208\n",
      "epoch: 6 | 87904 / 114272 | training loss: 0.00026964209973812103\n",
      "epoch: 6 | 87936 / 114272 | training loss: 0.005823003593832254\n",
      "epoch: 6 | 87968 / 114272 | training loss: 0.0006462500896304846\n",
      "epoch: 6 | 88000 / 114272 | training loss: 0.0013112097512930632\n",
      "epoch: 6 | 88032 / 114272 | training loss: 0.21477511525154114\n",
      "epoch: 6 | 88064 / 114272 | training loss: 0.000597287667915225\n",
      "epoch: 6 | 88096 / 114272 | training loss: 0.006171335466206074\n",
      "epoch: 6 | 88128 / 114272 | training loss: 0.23942866921424866\n",
      "epoch: 6 | 88160 / 114272 | training loss: 0.162403404712677\n",
      "epoch: 6 | 88192 / 114272 | training loss: 0.0006755756912752986\n",
      "epoch: 6 | 88224 / 114272 | training loss: 0.1381218135356903\n",
      "epoch: 6 | 88256 / 114272 | training loss: 0.22529025375843048\n",
      "epoch: 6 | 88288 / 114272 | training loss: 0.0012758126249536872\n",
      "epoch: 6 | 88320 / 114272 | training loss: 0.0005856712232343853\n",
      "epoch: 6 | 88352 / 114272 | training loss: 0.0014597766567021608\n",
      "epoch: 6 | 88384 / 114272 | training loss: 0.001022073091007769\n",
      "epoch: 6 | 88416 / 114272 | training loss: 0.0006234031170606613\n",
      "epoch: 6 | 88448 / 114272 | training loss: 0.0005380366346798837\n",
      "epoch: 6 | 88480 / 114272 | training loss: 0.1315956711769104\n",
      "epoch: 6 | 88512 / 114272 | training loss: 0.0011758387554436922\n",
      "epoch: 6 | 88544 / 114272 | training loss: 0.0021374451462179422\n",
      "epoch: 6 | 88576 / 114272 | training loss: 0.0010030518751591444\n",
      "epoch: 6 | 88608 / 114272 | training loss: 0.0011687901569530368\n",
      "epoch: 6 | 88640 / 114272 | training loss: 0.0022997213527560234\n",
      "epoch: 6 | 88672 / 114272 | training loss: 0.001376409432850778\n",
      "epoch: 6 | 88704 / 114272 | training loss: 0.08769718557596207\n",
      "epoch: 6 | 88736 / 114272 | training loss: 0.013757455162703991\n",
      "epoch: 6 | 88768 / 114272 | training loss: 0.010769498534500599\n",
      "epoch: 6 | 88800 / 114272 | training loss: 0.15316839516162872\n",
      "epoch: 6 | 88832 / 114272 | training loss: 0.0021873132791370153\n",
      "epoch: 6 | 88864 / 114272 | training loss: 0.0008025768911466002\n",
      "epoch: 6 | 88896 / 114272 | training loss: 0.0014220211887732148\n",
      "epoch: 6 | 88928 / 114272 | training loss: 0.0009520946769043803\n",
      "epoch: 6 | 88960 / 114272 | training loss: 0.002983691869303584\n",
      "epoch: 6 | 88992 / 114272 | training loss: 0.0069595458917319775\n",
      "epoch: 6 | 89024 / 114272 | training loss: 0.0018041166476905346\n",
      "epoch: 6 | 89056 / 114272 | training loss: 0.00087048823479563\n",
      "epoch: 6 | 89088 / 114272 | training loss: 0.0031570487190037966\n",
      "epoch: 6 | 89120 / 114272 | training loss: 0.0025089390110224485\n",
      "epoch: 6 | 89152 / 114272 | training loss: 0.003305729478597641\n",
      "epoch: 6 | 89184 / 114272 | training loss: 0.002277658088132739\n",
      "epoch: 6 | 89216 / 114272 | training loss: 0.002108300104737282\n",
      "epoch: 6 | 89248 / 114272 | training loss: 0.0010582009563222528\n",
      "epoch: 6 | 89280 / 114272 | training loss: 0.0016155879711732268\n",
      "epoch: 6 | 89312 / 114272 | training loss: 0.0012673235032707453\n",
      "epoch: 6 | 89344 / 114272 | training loss: 0.001236109179444611\n",
      "epoch: 6 | 89376 / 114272 | training loss: 0.0010362538741901517\n",
      "epoch: 6 | 89408 / 114272 | training loss: 0.0022072545252740383\n",
      "epoch: 6 | 89440 / 114272 | training loss: 0.38354116678237915\n",
      "epoch: 6 | 89472 / 114272 | training loss: 0.0015169274993240833\n",
      "epoch: 6 | 89504 / 114272 | training loss: 0.0030208700336515903\n",
      "epoch: 6 | 89536 / 114272 | training loss: 0.007148344535380602\n",
      "epoch: 6 | 89568 / 114272 | training loss: 0.0005789897404611111\n",
      "epoch: 6 | 89600 / 114272 | training loss: 0.004561380948871374\n",
      "epoch: 6 | 89632 / 114272 | training loss: 0.18196241557598114\n",
      "epoch: 6 | 89664 / 114272 | training loss: 0.001103662420064211\n",
      "epoch: 6 | 89696 / 114272 | training loss: 0.0011122624855488539\n",
      "epoch: 6 | 89728 / 114272 | training loss: 0.001376998145133257\n",
      "epoch: 6 | 89760 / 114272 | training loss: 0.0010797077557072043\n",
      "epoch: 6 | 89792 / 114272 | training loss: 0.0008246412617154419\n",
      "epoch: 6 | 89824 / 114272 | training loss: 0.15095318853855133\n",
      "epoch: 6 | 89856 / 114272 | training loss: 0.0013722997391596437\n",
      "epoch: 6 | 89888 / 114272 | training loss: 0.001135815167799592\n",
      "epoch: 6 | 89920 / 114272 | training loss: 0.000279088388197124\n",
      "epoch: 6 | 89952 / 114272 | training loss: 0.341738224029541\n",
      "epoch: 6 | 89984 / 114272 | training loss: 0.0007810701499693096\n",
      "epoch: 6 | 90016 / 114272 | training loss: 0.0004140798992011696\n",
      "epoch: 6 | 90048 / 114272 | training loss: 0.10867340862751007\n",
      "epoch: 6 | 90080 / 114272 | training loss: 0.0009355542133562267\n",
      "epoch: 6 | 90112 / 114272 | training loss: 0.000887008965946734\n",
      "epoch: 6 | 90144 / 114272 | training loss: 0.0007292262162081897\n",
      "epoch: 6 | 90176 / 114272 | training loss: 0.15686608850955963\n",
      "epoch: 6 | 90208 / 114272 | training loss: 0.0006256249616853893\n",
      "epoch: 6 | 90240 / 114272 | training loss: 0.0007014412549324334\n",
      "epoch: 6 | 90272 / 114272 | training loss: 0.0030895406380295753\n",
      "epoch: 6 | 90304 / 114272 | training loss: 0.0010305714095011353\n",
      "epoch: 6 | 90336 / 114272 | training loss: 0.000940744997933507\n",
      "epoch: 6 | 90368 / 114272 | training loss: 0.0009482668247073889\n",
      "epoch: 6 | 90400 / 114272 | training loss: 0.16153576970100403\n",
      "epoch: 6 | 90432 / 114272 | training loss: 0.001253116293810308\n",
      "epoch: 6 | 90464 / 114272 | training loss: 0.002575873862951994\n",
      "epoch: 6 | 90496 / 114272 | training loss: 0.17299635708332062\n",
      "epoch: 6 | 90528 / 114272 | training loss: 0.0011626392370089889\n",
      "epoch: 6 | 90560 / 114272 | training loss: 0.07068808376789093\n",
      "epoch: 6 | 90592 / 114272 | training loss: 0.0005820160731673241\n",
      "epoch: 6 | 90624 / 114272 | training loss: 0.0016070525161921978\n",
      "epoch: 6 | 90656 / 114272 | training loss: 0.0015340852551162243\n",
      "epoch: 6 | 90688 / 114272 | training loss: 0.09537200629711151\n",
      "epoch: 6 | 90720 / 114272 | training loss: 0.0011900162789970636\n",
      "epoch: 6 | 90752 / 114272 | training loss: 0.0015099641168490052\n",
      "epoch: 6 | 90784 / 114272 | training loss: 0.0006704060360789299\n",
      "epoch: 6 | 90816 / 114272 | training loss: 0.0014633992686867714\n",
      "epoch: 6 | 90848 / 114272 | training loss: 0.0017852721503004432\n",
      "epoch: 6 | 90880 / 114272 | training loss: 0.0012840671697631478\n",
      "epoch: 6 | 90912 / 114272 | training loss: 0.0012873506639152765\n",
      "epoch: 6 | 90944 / 114272 | training loss: 0.0019129774300381541\n",
      "epoch: 6 | 90976 / 114272 | training loss: 0.1746712625026703\n",
      "epoch: 6 | 91008 / 114272 | training loss: 0.001560867065563798\n",
      "epoch: 6 | 91040 / 114272 | training loss: 0.0019964268431067467\n",
      "epoch: 6 | 91072 / 114272 | training loss: 0.001216441742144525\n",
      "epoch: 6 | 91104 / 114272 | training loss: 0.0006673493189737201\n",
      "epoch: 6 | 91136 / 114272 | training loss: 0.0017177790869027376\n",
      "epoch: 6 | 91168 / 114272 | training loss: 0.00077933439752087\n",
      "epoch: 6 | 91200 / 114272 | training loss: 0.1450081765651703\n",
      "epoch: 6 | 91232 / 114272 | training loss: 0.002918778220191598\n",
      "epoch: 6 | 91264 / 114272 | training loss: 0.0017399984644725919\n",
      "epoch: 6 | 91296 / 114272 | training loss: 0.0018153556156903505\n",
      "epoch: 6 | 91328 / 114272 | training loss: 0.04691667854785919\n",
      "epoch: 6 | 91360 / 114272 | training loss: 0.17070859670639038\n",
      "epoch: 6 | 91392 / 114272 | training loss: 0.0009425801108591259\n",
      "epoch: 6 | 91424 / 114272 | training loss: 0.002005071844905615\n",
      "epoch: 6 | 91456 / 114272 | training loss: 0.0008620937587693334\n",
      "epoch: 6 | 91488 / 114272 | training loss: 0.0014055784558877349\n",
      "epoch: 6 | 91520 / 114272 | training loss: 0.0658741369843483\n",
      "epoch: 6 | 91552 / 114272 | training loss: 0.0014080904657021165\n",
      "epoch: 6 | 91584 / 114272 | training loss: 0.0029224285390228033\n",
      "epoch: 6 | 91616 / 114272 | training loss: 0.22695660591125488\n",
      "epoch: 6 | 91648 / 114272 | training loss: 0.0019866167567670345\n",
      "epoch: 6 | 91680 / 114272 | training loss: 0.0012508239597082138\n",
      "epoch: 6 | 91712 / 114272 | training loss: 0.0008082337444648147\n",
      "epoch: 6 | 91744 / 114272 | training loss: 0.0012044187169522047\n",
      "epoch: 6 | 91776 / 114272 | training loss: 0.00871183443814516\n",
      "epoch: 6 | 91808 / 114272 | training loss: 0.002258927095681429\n",
      "epoch: 6 | 91840 / 114272 | training loss: 0.017084382474422455\n",
      "epoch: 6 | 91872 / 114272 | training loss: 0.021357625722885132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 91904 / 114272 | training loss: 0.0013292417861521244\n",
      "epoch: 6 | 91936 / 114272 | training loss: 0.002870677737519145\n",
      "epoch: 6 | 91968 / 114272 | training loss: 0.0012940051965415478\n",
      "epoch: 6 | 92000 / 114272 | training loss: 0.2481842339038849\n",
      "epoch: 6 | 92032 / 114272 | training loss: 0.0009861239232122898\n",
      "epoch: 6 | 92064 / 114272 | training loss: 0.0008021120447665453\n",
      "epoch: 6 | 92096 / 114272 | training loss: 0.01779172755777836\n",
      "epoch: 6 | 92128 / 114272 | training loss: 0.0012478167191147804\n",
      "epoch: 6 | 92160 / 114272 | training loss: 0.09325022250413895\n",
      "epoch: 6 | 92192 / 114272 | training loss: 0.0009347653831355274\n",
      "epoch: 6 | 92224 / 114272 | training loss: 0.0010905712842941284\n",
      "epoch: 6 | 92256 / 114272 | training loss: 0.00045681738993152976\n",
      "epoch: 6 | 92288 / 114272 | training loss: 0.0015818807296454906\n",
      "epoch: 6 | 92320 / 114272 | training loss: 0.0013279082486405969\n",
      "epoch: 6 | 92352 / 114272 | training loss: 0.0007246118038892746\n",
      "epoch: 6 | 92384 / 114272 | training loss: 0.16806524991989136\n",
      "epoch: 6 | 92416 / 114272 | training loss: 0.0014546280726790428\n",
      "epoch: 6 | 92448 / 114272 | training loss: 0.0023283285554498434\n",
      "epoch: 6 | 92480 / 114272 | training loss: 0.0010876404121518135\n",
      "epoch: 6 | 92512 / 114272 | training loss: 0.01885969191789627\n",
      "epoch: 6 | 92544 / 114272 | training loss: 0.0014457054203376174\n",
      "epoch: 6 | 92576 / 114272 | training loss: 0.09792206436395645\n",
      "epoch: 6 | 92608 / 114272 | training loss: 0.0015876166289672256\n",
      "epoch: 6 | 92640 / 114272 | training loss: 0.001710078096948564\n",
      "epoch: 6 | 92672 / 114272 | training loss: 0.002740242052823305\n",
      "epoch: 6 | 92704 / 114272 | training loss: 0.00118730787653476\n",
      "epoch: 6 | 92736 / 114272 | training loss: 0.04476316645741463\n",
      "epoch: 6 | 92768 / 114272 | training loss: 0.0017717040609568357\n",
      "epoch: 6 | 92800 / 114272 | training loss: 0.0010356107959523797\n",
      "epoch: 6 | 92832 / 114272 | training loss: 0.054819364100694656\n",
      "epoch: 6 | 92864 / 114272 | training loss: 0.007296203635632992\n",
      "epoch: 6 | 92896 / 114272 | training loss: 0.0019871965050697327\n",
      "epoch: 6 | 92928 / 114272 | training loss: 0.0016834025736898184\n",
      "epoch: 6 | 92960 / 114272 | training loss: 0.2881782650947571\n",
      "epoch: 6 | 92992 / 114272 | training loss: 0.002670904388651252\n",
      "epoch: 6 | 93024 / 114272 | training loss: 0.002743916818872094\n",
      "epoch: 6 | 93056 / 114272 | training loss: 0.0006234017782844603\n",
      "epoch: 6 | 93088 / 114272 | training loss: 0.003288408275693655\n",
      "epoch: 6 | 93120 / 114272 | training loss: 0.009289111942052841\n",
      "epoch: 6 | 93152 / 114272 | training loss: 0.00140061613637954\n",
      "epoch: 6 | 93184 / 114272 | training loss: 0.0015101776225492358\n",
      "epoch: 6 | 93216 / 114272 | training loss: 0.0016849783714860678\n",
      "epoch: 6 | 93248 / 114272 | training loss: 0.002246050164103508\n",
      "epoch: 6 | 93280 / 114272 | training loss: 0.0007109580328688025\n",
      "epoch: 6 | 93312 / 114272 | training loss: 0.0007401197799481452\n",
      "epoch: 6 | 93344 / 114272 | training loss: 0.0016510075656697154\n",
      "epoch: 6 | 93376 / 114272 | training loss: 0.14320185780525208\n",
      "epoch: 6 | 93408 / 114272 | training loss: 0.07163513451814651\n",
      "epoch: 6 | 93440 / 114272 | training loss: 0.0013707882026210427\n",
      "epoch: 6 | 93472 / 114272 | training loss: 0.013408473692834377\n",
      "epoch: 6 | 93504 / 114272 | training loss: 0.001052730600349605\n",
      "epoch: 6 | 93536 / 114272 | training loss: 0.1760905534029007\n",
      "epoch: 6 | 93568 / 114272 | training loss: 0.0020135976374149323\n",
      "epoch: 6 | 93600 / 114272 | training loss: 0.1526215821504593\n",
      "epoch: 6 | 93632 / 114272 | training loss: 0.0008937102393247187\n",
      "epoch: 6 | 93664 / 114272 | training loss: 0.000524103466887027\n",
      "epoch: 6 | 93696 / 114272 | training loss: 0.001780630787834525\n",
      "epoch: 6 | 93728 / 114272 | training loss: 0.0035926848649978638\n",
      "epoch: 6 | 93760 / 114272 | training loss: 0.004107321612536907\n",
      "epoch: 6 | 93792 / 114272 | training loss: 0.0006317688967101276\n",
      "epoch: 6 | 93824 / 114272 | training loss: 0.0965796411037445\n",
      "epoch: 6 | 93856 / 114272 | training loss: 0.002638198435306549\n",
      "epoch: 6 | 93888 / 114272 | training loss: 0.0017268138471990824\n",
      "epoch: 6 | 93920 / 114272 | training loss: 0.0006476104608736932\n",
      "epoch: 6 | 93952 / 114272 | training loss: 0.002778079826384783\n",
      "epoch: 6 | 93984 / 114272 | training loss: 0.001037648762576282\n",
      "epoch: 6 | 94016 / 114272 | training loss: 0.001223987783305347\n",
      "epoch: 6 | 94048 / 114272 | training loss: 0.0009002928272821009\n",
      "epoch: 6 | 94080 / 114272 | training loss: 0.0018131192773580551\n",
      "epoch: 6 | 94112 / 114272 | training loss: 0.002246192190796137\n",
      "epoch: 6 | 94144 / 114272 | training loss: 0.002473163651302457\n",
      "epoch: 6 | 94176 / 114272 | training loss: 0.0008488813182339072\n",
      "epoch: 6 | 94208 / 114272 | training loss: 0.001249121269211173\n",
      "epoch: 6 | 94240 / 114272 | training loss: 0.0015147655503824353\n",
      "epoch: 6 | 94272 / 114272 | training loss: 0.0009799826657399535\n",
      "epoch: 6 | 94304 / 114272 | training loss: 0.0015300170052796602\n",
      "epoch: 6 | 94336 / 114272 | training loss: 0.0020001069642603397\n",
      "epoch: 6 | 94368 / 114272 | training loss: 0.0024362760595977306\n",
      "epoch: 6 | 94400 / 114272 | training loss: 0.2486601322889328\n",
      "epoch: 6 | 94432 / 114272 | training loss: 0.001916143111884594\n",
      "epoch: 6 | 94464 / 114272 | training loss: 0.1448027342557907\n",
      "epoch: 6 | 94496 / 114272 | training loss: 0.0008063568384386599\n",
      "epoch: 6 | 94528 / 114272 | training loss: 0.119272381067276\n",
      "epoch: 6 | 94560 / 114272 | training loss: 0.0026293518021702766\n",
      "epoch: 6 | 94592 / 114272 | training loss: 0.006912698037922382\n",
      "epoch: 6 | 94624 / 114272 | training loss: 0.08732158690690994\n",
      "epoch: 6 | 94656 / 114272 | training loss: 0.0016814206028357148\n",
      "epoch: 6 | 94688 / 114272 | training loss: 0.0011537635000422597\n",
      "epoch: 6 | 94720 / 114272 | training loss: 0.0033672337885946035\n",
      "epoch: 6 | 94752 / 114272 | training loss: 0.0033762939274311066\n",
      "epoch: 6 | 94784 / 114272 | training loss: 0.001997509505599737\n",
      "epoch: 6 | 94816 / 114272 | training loss: 0.001571823377162218\n",
      "epoch: 6 | 94848 / 114272 | training loss: 0.0014353658771142364\n",
      "epoch: 6 | 94880 / 114272 | training loss: 0.0957777202129364\n",
      "epoch: 6 | 94912 / 114272 | training loss: 0.0012760363752022386\n",
      "epoch: 6 | 94944 / 114272 | training loss: 0.1331639438867569\n",
      "epoch: 6 | 94976 / 114272 | training loss: 0.001846659928560257\n",
      "epoch: 6 | 95008 / 114272 | training loss: 0.021874763071537018\n",
      "epoch: 6 | 95040 / 114272 | training loss: 0.00900515541434288\n",
      "epoch: 6 | 95072 / 114272 | training loss: 0.0012159497709944844\n",
      "epoch: 6 | 95104 / 114272 | training loss: 0.07708164304494858\n",
      "epoch: 6 | 95136 / 114272 | training loss: 0.32241615653038025\n",
      "epoch: 6 | 95168 / 114272 | training loss: 0.002250195946544409\n",
      "epoch: 6 | 95200 / 114272 | training loss: 0.0014205984771251678\n",
      "epoch: 6 | 95232 / 114272 | training loss: 0.001856246730312705\n",
      "epoch: 6 | 95264 / 114272 | training loss: 0.2664336562156677\n",
      "epoch: 6 | 95296 / 114272 | training loss: 0.0015474797692149878\n",
      "epoch: 6 | 95328 / 114272 | training loss: 0.003905969439074397\n",
      "epoch: 6 | 95360 / 114272 | training loss: 0.0025447856169193983\n",
      "epoch: 6 | 95392 / 114272 | training loss: 0.001663822797127068\n",
      "epoch: 6 | 95424 / 114272 | training loss: 0.0022470790427178144\n",
      "epoch: 6 | 95456 / 114272 | training loss: 0.17209650576114655\n",
      "epoch: 6 | 95488 / 114272 | training loss: 0.0022958111949265003\n",
      "epoch: 6 | 95520 / 114272 | training loss: 0.003056925954297185\n",
      "epoch: 6 | 95552 / 114272 | training loss: 0.0016450873808935285\n",
      "epoch: 6 | 95584 / 114272 | training loss: 0.002635534154251218\n",
      "epoch: 6 | 95616 / 114272 | training loss: 0.003393983468413353\n",
      "epoch: 6 | 95648 / 114272 | training loss: 0.08593570441007614\n",
      "epoch: 6 | 95680 / 114272 | training loss: 0.01689353585243225\n",
      "epoch: 6 | 95712 / 114272 | training loss: 0.004543468821793795\n",
      "epoch: 6 | 95744 / 114272 | training loss: 0.0014292129781097174\n",
      "epoch: 6 | 95776 / 114272 | training loss: 0.002468900755047798\n",
      "epoch: 6 | 95808 / 114272 | training loss: 0.0008694252464920282\n",
      "epoch: 6 | 95840 / 114272 | training loss: 0.001560624223202467\n",
      "epoch: 6 | 95872 / 114272 | training loss: 0.0014105569571256638\n",
      "epoch: 6 | 95904 / 114272 | training loss: 0.000747639627661556\n",
      "epoch: 6 | 95936 / 114272 | training loss: 0.007844856008887291\n",
      "epoch: 6 | 95968 / 114272 | training loss: 0.0009878737619146705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 96000 / 114272 | training loss: 0.0026007534470409155\n",
      "epoch: 6 | 96032 / 114272 | training loss: 0.000885523040778935\n",
      "epoch: 6 | 96064 / 114272 | training loss: 0.0013735884567722678\n",
      "epoch: 6 | 96096 / 114272 | training loss: 0.17319676280021667\n",
      "epoch: 6 | 96128 / 114272 | training loss: 0.1960253119468689\n",
      "epoch: 6 | 96160 / 114272 | training loss: 0.0019507781835272908\n",
      "epoch: 6 | 96192 / 114272 | training loss: 0.04532225430011749\n",
      "epoch: 6 | 96224 / 114272 | training loss: 0.0012885257601737976\n",
      "epoch: 6 | 96256 / 114272 | training loss: 0.0018716580234467983\n",
      "epoch: 6 | 96288 / 114272 | training loss: 0.0018704661633819342\n",
      "epoch: 6 | 96320 / 114272 | training loss: 0.17791809141635895\n",
      "epoch: 6 | 96352 / 114272 | training loss: 0.0015445019816979766\n",
      "epoch: 6 | 96384 / 114272 | training loss: 0.002411816967651248\n",
      "epoch: 6 | 96416 / 114272 | training loss: 0.014541121199727058\n",
      "epoch: 6 | 96448 / 114272 | training loss: 0.0017730422550812364\n",
      "epoch: 6 | 96480 / 114272 | training loss: 0.002222367562353611\n",
      "epoch: 6 | 96512 / 114272 | training loss: 0.001467731548473239\n",
      "epoch: 6 | 96544 / 114272 | training loss: 0.0018247312400490046\n",
      "epoch: 6 | 96576 / 114272 | training loss: 0.0016798764700070024\n",
      "epoch: 6 | 96608 / 114272 | training loss: 0.004452832043170929\n",
      "epoch: 6 | 96640 / 114272 | training loss: 0.001865906990133226\n",
      "epoch: 6 | 96672 / 114272 | training loss: 0.001660032314248383\n",
      "epoch: 6 | 96704 / 114272 | training loss: 0.0011265079956501722\n",
      "epoch: 6 | 96736 / 114272 | training loss: 0.002424422651529312\n",
      "epoch: 6 | 96768 / 114272 | training loss: 0.001515652285888791\n",
      "epoch: 6 | 96800 / 114272 | training loss: 0.008174304850399494\n",
      "epoch: 6 | 96832 / 114272 | training loss: 0.0008858787477947772\n",
      "epoch: 6 | 96864 / 114272 | training loss: 0.21628893911838531\n",
      "epoch: 6 | 96896 / 114272 | training loss: 0.0019367444328963757\n",
      "epoch: 6 | 96928 / 114272 | training loss: 0.025713004171848297\n",
      "epoch: 6 | 96960 / 114272 | training loss: 0.0022511049173772335\n",
      "epoch: 6 | 96992 / 114272 | training loss: 0.001784142921678722\n",
      "epoch: 6 | 97024 / 114272 | training loss: 0.0007735976832918823\n",
      "epoch: 6 | 97056 / 114272 | training loss: 0.002048935741186142\n",
      "epoch: 6 | 97088 / 114272 | training loss: 0.0014715571887791157\n",
      "epoch: 6 | 97120 / 114272 | training loss: 0.08468956500291824\n",
      "epoch: 6 | 97152 / 114272 | training loss: 0.22408808767795563\n",
      "epoch: 6 | 97184 / 114272 | training loss: 0.0019278133986517787\n",
      "epoch: 6 | 97216 / 114272 | training loss: 0.001190921408124268\n",
      "epoch: 6 | 97248 / 114272 | training loss: 0.0019722299184650183\n",
      "epoch: 6 | 97280 / 114272 | training loss: 0.0012107780203223228\n",
      "epoch: 6 | 97312 / 114272 | training loss: 0.002118069212883711\n",
      "epoch: 6 | 97344 / 114272 | training loss: 0.001592810032889247\n",
      "epoch: 6 | 97376 / 114272 | training loss: 0.0006458109128288925\n",
      "epoch: 6 | 97408 / 114272 | training loss: 0.0026441095396876335\n",
      "epoch: 6 | 97440 / 114272 | training loss: 0.0011788891861215234\n",
      "epoch: 6 | 97472 / 114272 | training loss: 0.0005105389282107353\n",
      "epoch: 6 | 97504 / 114272 | training loss: 0.0020051675383001566\n",
      "epoch: 6 | 97536 / 114272 | training loss: 0.0014644660986959934\n",
      "epoch: 6 | 97568 / 114272 | training loss: 0.0022093532606959343\n",
      "epoch: 6 | 97600 / 114272 | training loss: 0.06300335377454758\n",
      "epoch: 6 | 97632 / 114272 | training loss: 0.0021710784640163183\n",
      "epoch: 6 | 97664 / 114272 | training loss: 0.0013478017644956708\n",
      "epoch: 6 | 97696 / 114272 | training loss: 0.0013854112476110458\n",
      "epoch: 6 | 97728 / 114272 | training loss: 0.002300859661772847\n",
      "epoch: 6 | 97760 / 114272 | training loss: 0.0014515507500618696\n",
      "epoch: 6 | 97792 / 114272 | training loss: 0.0008362239459529519\n",
      "epoch: 6 | 97824 / 114272 | training loss: 0.0018930804217234254\n",
      "epoch: 6 | 97856 / 114272 | training loss: 0.0008928863098844886\n",
      "epoch: 6 | 97888 / 114272 | training loss: 0.0019005328649654984\n",
      "epoch: 6 | 97920 / 114272 | training loss: 0.11274560540914536\n",
      "epoch: 6 | 97952 / 114272 | training loss: 0.001306513324379921\n",
      "epoch: 6 | 97984 / 114272 | training loss: 0.09417232125997543\n",
      "epoch: 6 | 98016 / 114272 | training loss: 0.036839213222265244\n",
      "epoch: 6 | 98048 / 114272 | training loss: 0.16625934839248657\n",
      "epoch: 6 | 98080 / 114272 | training loss: 0.0012540051247924566\n",
      "epoch: 6 | 98112 / 114272 | training loss: 0.0018662066431716084\n",
      "epoch: 6 | 98144 / 114272 | training loss: 0.0010247745085507631\n",
      "epoch: 6 | 98176 / 114272 | training loss: 0.0006959680467844009\n",
      "epoch: 6 | 98208 / 114272 | training loss: 0.002359609119594097\n",
      "epoch: 6 | 98240 / 114272 | training loss: 0.1057644635438919\n",
      "epoch: 6 | 98272 / 114272 | training loss: 0.0022397914435714483\n",
      "epoch: 6 | 98304 / 114272 | training loss: 0.0016171940369531512\n",
      "epoch: 6 | 98336 / 114272 | training loss: 0.03143897280097008\n",
      "epoch: 6 | 98368 / 114272 | training loss: 0.0019197396468371153\n",
      "epoch: 6 | 98400 / 114272 | training loss: 0.0006457495619542897\n",
      "epoch: 6 | 98432 / 114272 | training loss: 0.0014421840896829963\n",
      "epoch: 6 | 98464 / 114272 | training loss: 0.0010337121784687042\n",
      "epoch: 6 | 98496 / 114272 | training loss: 0.12061690539121628\n",
      "epoch: 6 | 98528 / 114272 | training loss: 0.0015899467980489135\n",
      "epoch: 6 | 98560 / 114272 | training loss: 0.0012863017618656158\n",
      "epoch: 6 | 98592 / 114272 | training loss: 0.0011268022935837507\n",
      "epoch: 6 | 98624 / 114272 | training loss: 0.0008237616275437176\n",
      "epoch: 6 | 98656 / 114272 | training loss: 0.15230858325958252\n",
      "epoch: 6 | 98688 / 114272 | training loss: 0.0009611035347916186\n",
      "epoch: 6 | 98720 / 114272 | training loss: 0.009105130098760128\n",
      "epoch: 6 | 98752 / 114272 | training loss: 0.0016172800678759813\n",
      "epoch: 6 | 98784 / 114272 | training loss: 0.0007291194633580744\n",
      "epoch: 6 | 98816 / 114272 | training loss: 0.0007579716038890183\n",
      "epoch: 6 | 98848 / 114272 | training loss: 0.0008229818777181208\n",
      "epoch: 6 | 98880 / 114272 | training loss: 0.0013061536010354757\n",
      "epoch: 6 | 98912 / 114272 | training loss: 0.001600924413651228\n",
      "epoch: 6 | 98944 / 114272 | training loss: 0.0011583758750930429\n",
      "epoch: 6 | 98976 / 114272 | training loss: 0.0006043071625754237\n",
      "epoch: 6 | 99008 / 114272 | training loss: 0.002162496792152524\n",
      "epoch: 6 | 99040 / 114272 | training loss: 0.0011899159289896488\n",
      "epoch: 6 | 99072 / 114272 | training loss: 0.0025626819115132093\n",
      "epoch: 6 | 99104 / 114272 | training loss: 0.03644124045968056\n",
      "epoch: 6 | 99136 / 114272 | training loss: 0.008287756703794003\n",
      "epoch: 6 | 99168 / 114272 | training loss: 0.001531450659967959\n",
      "epoch: 6 | 99200 / 114272 | training loss: 0.0012971577234566212\n",
      "epoch: 6 | 99232 / 114272 | training loss: 0.09715086221694946\n",
      "epoch: 6 | 99264 / 114272 | training loss: 0.14599278569221497\n",
      "epoch: 6 | 99296 / 114272 | training loss: 0.001995534636080265\n",
      "epoch: 6 | 99328 / 114272 | training loss: 0.0014985207235440612\n",
      "epoch: 6 | 99360 / 114272 | training loss: 0.0006230400176718831\n",
      "epoch: 6 | 99392 / 114272 | training loss: 0.0009655613685026765\n",
      "epoch: 6 | 99424 / 114272 | training loss: 0.01137921679764986\n",
      "epoch: 6 | 99456 / 114272 | training loss: 0.0004822277696803212\n",
      "epoch: 6 | 99488 / 114272 | training loss: 0.0007572314934805036\n",
      "epoch: 6 | 99520 / 114272 | training loss: 0.0013068902771919966\n",
      "epoch: 6 | 99552 / 114272 | training loss: 0.0007517535705119371\n",
      "epoch: 6 | 99584 / 114272 | training loss: 0.10140536725521088\n",
      "epoch: 6 | 99616 / 114272 | training loss: 0.0008457554504275322\n",
      "epoch: 6 | 99648 / 114272 | training loss: 0.0015029116766527295\n",
      "epoch: 6 | 99680 / 114272 | training loss: 0.0012553833657875657\n",
      "epoch: 6 | 99712 / 114272 | training loss: 0.0012173146242275834\n",
      "epoch: 6 | 99744 / 114272 | training loss: 0.0011686331126838923\n",
      "epoch: 6 | 99776 / 114272 | training loss: 0.000876127218361944\n",
      "epoch: 6 | 99808 / 114272 | training loss: 0.0006738979136571288\n",
      "epoch: 6 | 99840 / 114272 | training loss: 0.0010541551746428013\n",
      "epoch: 6 | 99872 / 114272 | training loss: 0.000539397238753736\n",
      "epoch: 6 | 99904 / 114272 | training loss: 0.0007205745205283165\n",
      "epoch: 6 | 99936 / 114272 | training loss: 0.0008296272135339677\n",
      "epoch: 6 | 99968 / 114272 | training loss: 0.014391263015568256\n",
      "epoch: 6 | 100000 / 114272 | training loss: 0.10974298417568207\n",
      "epoch: 6 | 100032 / 114272 | training loss: 0.0011701015755534172\n",
      "epoch: 6 | 100064 / 114272 | training loss: 0.0017575877718627453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 100096 / 114272 | training loss: 0.00105170882306993\n",
      "epoch: 6 | 100128 / 114272 | training loss: 0.0009205645183101296\n",
      "epoch: 6 | 100160 / 114272 | training loss: 0.0007167707080952823\n",
      "epoch: 6 | 100192 / 114272 | training loss: 0.0016624958952888846\n",
      "epoch: 6 | 100224 / 114272 | training loss: 0.0012307408032938838\n",
      "epoch: 6 | 100256 / 114272 | training loss: 0.0014384225942194462\n",
      "epoch: 6 | 100288 / 114272 | training loss: 0.0021318376529961824\n",
      "epoch: 6 | 100320 / 114272 | training loss: 0.23609818518161774\n",
      "epoch: 6 | 100352 / 114272 | training loss: 0.0008549271733500063\n",
      "epoch: 6 | 100384 / 114272 | training loss: 0.000586142938118428\n",
      "epoch: 6 | 100416 / 114272 | training loss: 0.0005936722736805677\n",
      "epoch: 6 | 100448 / 114272 | training loss: 0.0007303195889107883\n",
      "epoch: 6 | 100480 / 114272 | training loss: 0.11460641026496887\n",
      "epoch: 6 | 100512 / 114272 | training loss: 0.0006242997478693724\n",
      "epoch: 6 | 100544 / 114272 | training loss: 0.0015217704931274056\n",
      "epoch: 6 | 100576 / 114272 | training loss: 0.0008921208791434765\n",
      "epoch: 6 | 100608 / 114272 | training loss: 0.0004723977472167462\n",
      "epoch: 6 | 100640 / 114272 | training loss: 0.0008801586809568107\n",
      "epoch: 6 | 100672 / 114272 | training loss: 0.004359422251582146\n",
      "epoch: 6 | 100704 / 114272 | training loss: 0.23412227630615234\n",
      "epoch: 6 | 100736 / 114272 | training loss: 0.000768544094171375\n",
      "epoch: 6 | 100768 / 114272 | training loss: 0.0009134144056588411\n",
      "epoch: 6 | 100800 / 114272 | training loss: 0.0008417171193286777\n",
      "epoch: 6 | 100832 / 114272 | training loss: 0.0017131669446825981\n",
      "epoch: 6 | 100864 / 114272 | training loss: 0.005276636686176062\n",
      "epoch: 6 | 100896 / 114272 | training loss: 0.0008039296953938901\n",
      "epoch: 6 | 100928 / 114272 | training loss: 0.06095118820667267\n",
      "epoch: 6 | 100960 / 114272 | training loss: 0.0016712083015590906\n",
      "epoch: 6 | 100992 / 114272 | training loss: 0.000754241831600666\n",
      "epoch: 6 | 101024 / 114272 | training loss: 0.1501322090625763\n",
      "epoch: 6 | 101056 / 114272 | training loss: 0.2355685830116272\n",
      "epoch: 6 | 101088 / 114272 | training loss: 0.00832983199506998\n",
      "epoch: 6 | 101120 / 114272 | training loss: 0.16231641173362732\n",
      "epoch: 6 | 101152 / 114272 | training loss: 0.0007758609717711806\n",
      "epoch: 6 | 101184 / 114272 | training loss: 0.0036851628683507442\n",
      "epoch: 6 | 101216 / 114272 | training loss: 0.0009035420371219516\n",
      "epoch: 6 | 101248 / 114272 | training loss: 0.0009025545441545546\n",
      "epoch: 6 | 101280 / 114272 | training loss: 0.1344119757413864\n",
      "epoch: 6 | 101312 / 114272 | training loss: 0.23887939751148224\n",
      "epoch: 6 | 101344 / 114272 | training loss: 0.0013676913222298026\n",
      "epoch: 6 | 101376 / 114272 | training loss: 0.1558605283498764\n",
      "epoch: 6 | 101408 / 114272 | training loss: 0.0015335489297285676\n",
      "epoch: 6 | 101440 / 114272 | training loss: 0.0015502755995839834\n",
      "epoch: 6 | 101472 / 114272 | training loss: 0.006412640679627657\n",
      "epoch: 6 | 101504 / 114272 | training loss: 0.0033314027823507786\n",
      "epoch: 6 | 101536 / 114272 | training loss: 0.0024568589869886637\n",
      "epoch: 6 | 101568 / 114272 | training loss: 0.0006159510812722147\n",
      "epoch: 6 | 101600 / 114272 | training loss: 0.06257789582014084\n",
      "epoch: 6 | 101632 / 114272 | training loss: 0.14493019878864288\n",
      "epoch: 6 | 101664 / 114272 | training loss: 0.0008025357965379953\n",
      "epoch: 6 | 101696 / 114272 | training loss: 0.0008175534894689918\n",
      "epoch: 6 | 101728 / 114272 | training loss: 0.0013018000172451138\n",
      "epoch: 6 | 101760 / 114272 | training loss: 0.0010649347677826881\n",
      "epoch: 6 | 101792 / 114272 | training loss: 0.0013504500966519117\n",
      "epoch: 6 | 101824 / 114272 | training loss: 0.0013235005317255855\n",
      "epoch: 6 | 101856 / 114272 | training loss: 0.0013570706360042095\n",
      "epoch: 6 | 101888 / 114272 | training loss: 0.0036034819204360247\n",
      "epoch: 6 | 101920 / 114272 | training loss: 0.0019359091529622674\n",
      "epoch: 6 | 101952 / 114272 | training loss: 0.0018414795631542802\n",
      "epoch: 6 | 101984 / 114272 | training loss: 0.0022214693017303944\n",
      "epoch: 6 | 102016 / 114272 | training loss: 0.0011880106758326292\n",
      "epoch: 6 | 102048 / 114272 | training loss: 0.0012410067720338702\n",
      "epoch: 6 | 102080 / 114272 | training loss: 0.003971402533352375\n",
      "epoch: 6 | 102112 / 114272 | training loss: 0.002420542063191533\n",
      "epoch: 6 | 102144 / 114272 | training loss: 0.0035715748090296984\n",
      "epoch: 6 | 102176 / 114272 | training loss: 0.0034082618076354265\n",
      "epoch: 6 | 102208 / 114272 | training loss: 0.0025081862695515156\n",
      "epoch: 6 | 102240 / 114272 | training loss: 0.0009350160253234208\n",
      "epoch: 6 | 102272 / 114272 | training loss: 0.000539058935828507\n",
      "epoch: 6 | 102304 / 114272 | training loss: 0.0008938746759667993\n",
      "epoch: 6 | 102336 / 114272 | training loss: 0.001023642485961318\n",
      "epoch: 6 | 102368 / 114272 | training loss: 0.002972465008497238\n",
      "epoch: 6 | 102400 / 114272 | training loss: 0.0018091133097186685\n",
      "epoch: 6 | 102432 / 114272 | training loss: 0.0006154185975901783\n",
      "epoch: 6 | 102464 / 114272 | training loss: 0.001045523094944656\n",
      "epoch: 6 | 102496 / 114272 | training loss: 0.0012495851842686534\n",
      "epoch: 6 | 102528 / 114272 | training loss: 0.002032113028690219\n",
      "epoch: 6 | 102560 / 114272 | training loss: 0.0011900952085852623\n",
      "epoch: 6 | 102592 / 114272 | training loss: 0.0037165400572121143\n",
      "epoch: 6 | 102624 / 114272 | training loss: 0.0027607285883277655\n",
      "epoch: 6 | 102656 / 114272 | training loss: 0.0006666190456598997\n",
      "epoch: 6 | 102688 / 114272 | training loss: 0.05242610722780228\n",
      "epoch: 6 | 102720 / 114272 | training loss: 0.0008815232431516051\n",
      "epoch: 6 | 102752 / 114272 | training loss: 0.27203357219696045\n",
      "epoch: 6 | 102784 / 114272 | training loss: 0.13491825759410858\n",
      "epoch: 6 | 102816 / 114272 | training loss: 0.00041327925282530487\n",
      "epoch: 6 | 102848 / 114272 | training loss: 0.001021606381982565\n",
      "epoch: 6 | 102880 / 114272 | training loss: 0.0006046388298273087\n",
      "epoch: 6 | 102912 / 114272 | training loss: 0.000866516085807234\n",
      "epoch: 6 | 102944 / 114272 | training loss: 0.0009301771060563624\n",
      "epoch: 6 | 102976 / 114272 | training loss: 0.19152280688285828\n",
      "epoch: 6 | 103008 / 114272 | training loss: 0.0007877610041759908\n",
      "epoch: 6 | 103040 / 114272 | training loss: 0.000852791010402143\n",
      "epoch: 6 | 103072 / 114272 | training loss: 0.002742242533713579\n",
      "epoch: 6 | 103104 / 114272 | training loss: 0.000998260104097426\n",
      "epoch: 6 | 103136 / 114272 | training loss: 0.0883902981877327\n",
      "epoch: 6 | 103168 / 114272 | training loss: 0.0014609332429245114\n",
      "epoch: 6 | 103200 / 114272 | training loss: 0.0017982450081035495\n",
      "epoch: 6 | 103232 / 114272 | training loss: 0.0005633099935948849\n",
      "epoch: 6 | 103264 / 114272 | training loss: 0.2921812832355499\n",
      "epoch: 6 | 103296 / 114272 | training loss: 0.08783694356679916\n",
      "epoch: 6 | 103328 / 114272 | training loss: 0.0008493277709931135\n",
      "epoch: 6 | 103360 / 114272 | training loss: 0.0008536104578524828\n",
      "epoch: 6 | 103392 / 114272 | training loss: 0.0013499695342034101\n",
      "epoch: 6 | 103424 / 114272 | training loss: 0.10123340040445328\n",
      "epoch: 6 | 103456 / 114272 | training loss: 0.0007043445366434753\n",
      "epoch: 6 | 103488 / 114272 | training loss: 0.0008488012827001512\n",
      "epoch: 6 | 103520 / 114272 | training loss: 0.0010101771913468838\n",
      "epoch: 6 | 103552 / 114272 | training loss: 0.0013354701222851872\n",
      "epoch: 6 | 103584 / 114272 | training loss: 0.0009951137471944094\n",
      "epoch: 6 | 103616 / 114272 | training loss: 0.000815832638181746\n",
      "epoch: 6 | 103648 / 114272 | training loss: 0.0012276003835722804\n",
      "epoch: 6 | 103680 / 114272 | training loss: 0.0005670644459314644\n",
      "epoch: 6 | 103712 / 114272 | training loss: 0.0005167832132428885\n",
      "epoch: 6 | 103744 / 114272 | training loss: 0.0015040591824799776\n",
      "epoch: 6 | 103776 / 114272 | training loss: 0.08145515620708466\n",
      "epoch: 6 | 103808 / 114272 | training loss: 0.11975289136171341\n",
      "epoch: 6 | 103840 / 114272 | training loss: 0.0007373975240625441\n",
      "epoch: 6 | 103872 / 114272 | training loss: 0.001269676722586155\n",
      "epoch: 6 | 103904 / 114272 | training loss: 0.006589087191969156\n",
      "epoch: 6 | 103936 / 114272 | training loss: 0.0008456092909909785\n",
      "epoch: 6 | 103968 / 114272 | training loss: 0.0006293253973126411\n",
      "epoch: 6 | 104000 / 114272 | training loss: 0.0007148109143599868\n",
      "epoch: 6 | 104032 / 114272 | training loss: 0.0004470510466489941\n",
      "epoch: 6 | 104064 / 114272 | training loss: 0.0006247945711947978\n",
      "epoch: 6 | 104096 / 114272 | training loss: 0.000541415938641876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 104128 / 114272 | training loss: 0.0010811175452545285\n",
      "epoch: 6 | 104160 / 114272 | training loss: 0.0008948440663516521\n",
      "epoch: 6 | 104192 / 114272 | training loss: 0.0008666059002280235\n",
      "epoch: 6 | 104224 / 114272 | training loss: 0.0011066022561863065\n",
      "epoch: 6 | 104256 / 114272 | training loss: 0.0011543533764779568\n",
      "epoch: 6 | 104288 / 114272 | training loss: 0.002770724706351757\n",
      "epoch: 6 | 104320 / 114272 | training loss: 0.0007957234629429877\n",
      "epoch: 6 | 104352 / 114272 | training loss: 0.00042229724931530654\n",
      "epoch: 6 | 104384 / 114272 | training loss: 0.04100998863577843\n",
      "epoch: 6 | 104416 / 114272 | training loss: 0.015100106596946716\n",
      "epoch: 6 | 104448 / 114272 | training loss: 0.005101164802908897\n",
      "epoch: 6 | 104480 / 114272 | training loss: 0.00113109708763659\n",
      "epoch: 6 | 104512 / 114272 | training loss: 0.023903314024209976\n",
      "epoch: 6 | 104544 / 114272 | training loss: 0.22370651364326477\n",
      "epoch: 6 | 104576 / 114272 | training loss: 0.000735407811589539\n",
      "epoch: 6 | 104608 / 114272 | training loss: 0.0016272547654807568\n",
      "epoch: 6 | 104640 / 114272 | training loss: 0.0013107344275340438\n",
      "epoch: 6 | 104672 / 114272 | training loss: 0.0006170007400214672\n",
      "epoch: 6 | 104704 / 114272 | training loss: 0.0005018843803554773\n",
      "epoch: 6 | 104736 / 114272 | training loss: 0.0009567002998664975\n",
      "epoch: 6 | 104768 / 114272 | training loss: 0.0012222069781273603\n",
      "epoch: 6 | 104800 / 114272 | training loss: 0.00025257159722968936\n",
      "epoch: 6 | 104832 / 114272 | training loss: 0.0022399972658604383\n",
      "epoch: 6 | 104864 / 114272 | training loss: 0.0005045653670094907\n",
      "epoch: 6 | 104896 / 114272 | training loss: 0.0007627657032571733\n",
      "epoch: 6 | 104928 / 114272 | training loss: 0.003106890246272087\n",
      "epoch: 6 | 104960 / 114272 | training loss: 0.0032370861154049635\n",
      "epoch: 6 | 104992 / 114272 | training loss: 0.001125548267737031\n",
      "epoch: 6 | 105024 / 114272 | training loss: 0.000635819393210113\n",
      "epoch: 6 | 105056 / 114272 | training loss: 0.0013587992871180177\n",
      "epoch: 6 | 105088 / 114272 | training loss: 0.03268091753125191\n",
      "epoch: 6 | 105120 / 114272 | training loss: 0.07564681023359299\n",
      "epoch: 6 | 105152 / 114272 | training loss: 0.0005502476124092937\n",
      "epoch: 6 | 105184 / 114272 | training loss: 0.0008912094635888934\n",
      "epoch: 6 | 105216 / 114272 | training loss: 0.06683166325092316\n",
      "epoch: 6 | 105248 / 114272 | training loss: 0.0008678052690811455\n",
      "epoch: 6 | 105280 / 114272 | training loss: 0.0012464337050914764\n",
      "epoch: 6 | 105312 / 114272 | training loss: 0.1519509106874466\n",
      "epoch: 6 | 105344 / 114272 | training loss: 0.0007078532944433391\n",
      "epoch: 6 | 105376 / 114272 | training loss: 0.0008530135964974761\n",
      "epoch: 6 | 105408 / 114272 | training loss: 0.000789846817497164\n",
      "epoch: 6 | 105440 / 114272 | training loss: 0.0015945183113217354\n",
      "epoch: 6 | 105472 / 114272 | training loss: 0.04758729413151741\n",
      "epoch: 6 | 105504 / 114272 | training loss: 0.0015201813075691462\n",
      "epoch: 6 | 105536 / 114272 | training loss: 0.0011069679167121649\n",
      "epoch: 6 | 105568 / 114272 | training loss: 0.02332373335957527\n",
      "epoch: 6 | 105600 / 114272 | training loss: 0.0005652281688526273\n",
      "epoch: 6 | 105632 / 114272 | training loss: 0.000519262976013124\n",
      "epoch: 6 | 105664 / 114272 | training loss: 0.0005588049534708261\n",
      "epoch: 6 | 105696 / 114272 | training loss: 0.001261409604921937\n",
      "epoch: 6 | 105728 / 114272 | training loss: 0.0017945277504622936\n",
      "epoch: 6 | 105760 / 114272 | training loss: 0.0006553127313964069\n",
      "epoch: 6 | 105792 / 114272 | training loss: 0.21362300217151642\n",
      "epoch: 6 | 105824 / 114272 | training loss: 0.0007120940717868507\n",
      "epoch: 6 | 105856 / 114272 | training loss: 0.0006659102509729564\n",
      "epoch: 6 | 105888 / 114272 | training loss: 0.007789005525410175\n",
      "epoch: 6 | 105920 / 114272 | training loss: 0.00041621230775490403\n",
      "epoch: 6 | 105952 / 114272 | training loss: 0.00035497971111908555\n",
      "epoch: 6 | 105984 / 114272 | training loss: 0.02998865768313408\n",
      "epoch: 6 | 106016 / 114272 | training loss: 0.0007593217887915671\n",
      "epoch: 6 | 106048 / 114272 | training loss: 0.000911588198505342\n",
      "epoch: 6 | 106080 / 114272 | training loss: 0.004180440213531256\n",
      "epoch: 6 | 106112 / 114272 | training loss: 0.001485138083808124\n",
      "epoch: 6 | 106144 / 114272 | training loss: 0.009844671003520489\n",
      "epoch: 6 | 106176 / 114272 | training loss: 0.0007994357729330659\n",
      "epoch: 6 | 106208 / 114272 | training loss: 0.0008087295573204756\n",
      "epoch: 6 | 106240 / 114272 | training loss: 0.0008450827444903553\n",
      "epoch: 6 | 106272 / 114272 | training loss: 0.0005158699932508171\n",
      "epoch: 6 | 106304 / 114272 | training loss: 0.0003462480381131172\n",
      "epoch: 6 | 106336 / 114272 | training loss: 0.0016497100004926324\n",
      "epoch: 6 | 106368 / 114272 | training loss: 0.0005295521696098149\n",
      "epoch: 6 | 106400 / 114272 | training loss: 0.0003893877728842199\n",
      "epoch: 6 | 106432 / 114272 | training loss: 0.0006456017727032304\n",
      "epoch: 6 | 106464 / 114272 | training loss: 0.00033986871130764484\n",
      "epoch: 6 | 106496 / 114272 | training loss: 0.24587856233119965\n",
      "epoch: 6 | 106528 / 114272 | training loss: 0.0005798365455120802\n",
      "epoch: 6 | 106560 / 114272 | training loss: 0.022218074649572372\n",
      "epoch: 6 | 106592 / 114272 | training loss: 0.0007671108469367027\n",
      "epoch: 6 | 106624 / 114272 | training loss: 0.0007063025259412825\n",
      "epoch: 6 | 106656 / 114272 | training loss: 0.01860988326370716\n",
      "epoch: 6 | 106688 / 114272 | training loss: 0.0005094927619211376\n",
      "epoch: 6 | 106720 / 114272 | training loss: 0.05060836300253868\n",
      "epoch: 6 | 106752 / 114272 | training loss: 0.0005408485885709524\n",
      "epoch: 6 | 106784 / 114272 | training loss: 0.0014955546939745545\n",
      "epoch: 6 | 106816 / 114272 | training loss: 0.0004648157919291407\n",
      "epoch: 6 | 106848 / 114272 | training loss: 0.0006030148942954838\n",
      "epoch: 6 | 106880 / 114272 | training loss: 0.0006081716273911297\n",
      "epoch: 6 | 106912 / 114272 | training loss: 0.002413456793874502\n",
      "epoch: 6 | 106944 / 114272 | training loss: 0.0008497581002302468\n",
      "epoch: 6 | 106976 / 114272 | training loss: 0.007414080668240786\n",
      "epoch: 6 | 107008 / 114272 | training loss: 0.0010219450341537595\n",
      "epoch: 6 | 107040 / 114272 | training loss: 0.000694276241119951\n",
      "epoch: 6 | 107072 / 114272 | training loss: 0.0008275429136119783\n",
      "epoch: 6 | 107104 / 114272 | training loss: 0.001041274517774582\n",
      "epoch: 6 | 107136 / 114272 | training loss: 0.0006922046886757016\n",
      "epoch: 6 | 107168 / 114272 | training loss: 0.0009311262401752174\n",
      "epoch: 6 | 107200 / 114272 | training loss: 0.0005762902437709272\n",
      "epoch: 6 | 107232 / 114272 | training loss: 0.0009466020856052637\n",
      "epoch: 6 | 107264 / 114272 | training loss: 0.060934435576200485\n",
      "epoch: 6 | 107296 / 114272 | training loss: 0.035405661910772324\n",
      "epoch: 6 | 107328 / 114272 | training loss: 0.0007673734799027443\n",
      "epoch: 6 | 107360 / 114272 | training loss: 0.0010321428999304771\n",
      "epoch: 6 | 107392 / 114272 | training loss: 0.0008857822394929826\n",
      "epoch: 6 | 107424 / 114272 | training loss: 0.023222023621201515\n",
      "epoch: 6 | 107456 / 114272 | training loss: 0.002347474917769432\n",
      "epoch: 6 | 107488 / 114272 | training loss: 0.0007194299250841141\n",
      "epoch: 6 | 107520 / 114272 | training loss: 0.0006458116113208234\n",
      "epoch: 6 | 107552 / 114272 | training loss: 0.0008013274054974318\n",
      "epoch: 6 | 107584 / 114272 | training loss: 0.0007150935707613826\n",
      "epoch: 6 | 107616 / 114272 | training loss: 0.18355119228363037\n",
      "epoch: 6 | 107648 / 114272 | training loss: 0.00048178568249568343\n",
      "epoch: 6 | 107680 / 114272 | training loss: 0.000548552896361798\n",
      "epoch: 6 | 107712 / 114272 | training loss: 0.0838698074221611\n",
      "epoch: 6 | 107744 / 114272 | training loss: 0.19122759997844696\n",
      "epoch: 6 | 107776 / 114272 | training loss: 0.09514302015304565\n",
      "epoch: 6 | 107808 / 114272 | training loss: 0.002241784241050482\n",
      "epoch: 6 | 107840 / 114272 | training loss: 0.17924529314041138\n",
      "epoch: 6 | 107872 / 114272 | training loss: 0.0016640083631500602\n",
      "epoch: 6 | 107904 / 114272 | training loss: 0.09920772165060043\n",
      "epoch: 6 | 107936 / 114272 | training loss: 0.002245899522677064\n",
      "epoch: 6 | 107968 / 114272 | training loss: 0.0007019854383543134\n",
      "epoch: 6 | 108000 / 114272 | training loss: 0.0008765523671172559\n",
      "epoch: 6 | 108032 / 114272 | training loss: 0.000846862094476819\n",
      "epoch: 6 | 108064 / 114272 | training loss: 0.000871454190928489\n",
      "epoch: 6 | 108096 / 114272 | training loss: 0.0007970478036440909\n",
      "epoch: 6 | 108128 / 114272 | training loss: 0.001015429268591106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 108160 / 114272 | training loss: 0.0009329842287115753\n",
      "epoch: 6 | 108192 / 114272 | training loss: 0.007050362881273031\n",
      "epoch: 6 | 108224 / 114272 | training loss: 0.001128551783040166\n",
      "epoch: 6 | 108256 / 114272 | training loss: 0.0008818938513286412\n",
      "epoch: 6 | 108288 / 114272 | training loss: 0.0006551374681293964\n",
      "epoch: 6 | 108320 / 114272 | training loss: 0.04879995808005333\n",
      "epoch: 6 | 108352 / 114272 | training loss: 0.16457752883434296\n",
      "epoch: 6 | 108384 / 114272 | training loss: 0.0018180086044594646\n",
      "epoch: 6 | 108416 / 114272 | training loss: 0.0006376286037266254\n",
      "epoch: 6 | 108448 / 114272 | training loss: 0.0016294998349621892\n",
      "epoch: 6 | 108480 / 114272 | training loss: 0.12516026198863983\n",
      "epoch: 6 | 108512 / 114272 | training loss: 0.10522225499153137\n",
      "epoch: 6 | 108544 / 114272 | training loss: 0.0007601922261528671\n",
      "epoch: 6 | 108576 / 114272 | training loss: 0.005556029733270407\n",
      "epoch: 6 | 108608 / 114272 | training loss: 0.048995524644851685\n",
      "epoch: 6 | 108640 / 114272 | training loss: 0.0012640334898605943\n",
      "epoch: 6 | 108672 / 114272 | training loss: 0.0007278815610334277\n",
      "epoch: 6 | 108704 / 114272 | training loss: 0.0012029269710183144\n",
      "epoch: 6 | 108736 / 114272 | training loss: 0.001048587029799819\n",
      "epoch: 6 | 108768 / 114272 | training loss: 0.001955736428499222\n",
      "epoch: 6 | 108800 / 114272 | training loss: 0.0006973864510655403\n",
      "epoch: 6 | 108832 / 114272 | training loss: 0.11233843117952347\n",
      "epoch: 6 | 108864 / 114272 | training loss: 0.0008895047940313816\n",
      "epoch: 6 | 108896 / 114272 | training loss: 0.0008385140099562705\n",
      "epoch: 6 | 108928 / 114272 | training loss: 0.0005729953991249204\n",
      "epoch: 6 | 108960 / 114272 | training loss: 0.0011797841871157289\n",
      "epoch: 6 | 108992 / 114272 | training loss: 0.007730595767498016\n",
      "epoch: 6 | 109024 / 114272 | training loss: 0.002226540818810463\n",
      "epoch: 6 | 109056 / 114272 | training loss: 0.0012729589361697435\n",
      "epoch: 6 | 109088 / 114272 | training loss: 0.0014599032001569867\n",
      "epoch: 6 | 109120 / 114272 | training loss: 0.0014751701382920146\n",
      "epoch: 6 | 109152 / 114272 | training loss: 0.0005953119834885001\n",
      "epoch: 6 | 109184 / 114272 | training loss: 0.0010589295998215675\n",
      "epoch: 6 | 109216 / 114272 | training loss: 0.0013774834806099534\n",
      "epoch: 6 | 109248 / 114272 | training loss: 0.0009681113297119737\n",
      "epoch: 6 | 109280 / 114272 | training loss: 0.0006924804183654487\n",
      "epoch: 6 | 109312 / 114272 | training loss: 0.0017546179005876184\n",
      "epoch: 6 | 109344 / 114272 | training loss: 0.06822457909584045\n",
      "epoch: 6 | 109376 / 114272 | training loss: 0.16081570088863373\n",
      "epoch: 6 | 109408 / 114272 | training loss: 0.0005678372690454125\n",
      "epoch: 6 | 109440 / 114272 | training loss: 0.0006467493367381394\n",
      "epoch: 6 | 109472 / 114272 | training loss: 0.0013811090029776096\n",
      "epoch: 6 | 109504 / 114272 | training loss: 0.0033152971882373095\n",
      "epoch: 6 | 109536 / 114272 | training loss: 0.14758607745170593\n",
      "epoch: 6 | 109568 / 114272 | training loss: 0.0011741103371605277\n",
      "epoch: 6 | 109600 / 114272 | training loss: 0.0036314174067229033\n",
      "epoch: 6 | 109632 / 114272 | training loss: 0.0008082755375653505\n",
      "epoch: 6 | 109664 / 114272 | training loss: 0.0015076999552547932\n",
      "epoch: 6 | 109696 / 114272 | training loss: 0.003360726870596409\n",
      "epoch: 6 | 109728 / 114272 | training loss: 0.023006003350019455\n",
      "epoch: 6 | 109760 / 114272 | training loss: 0.021489474922418594\n",
      "epoch: 6 | 109792 / 114272 | training loss: 0.0013427369995042682\n",
      "epoch: 6 | 109824 / 114272 | training loss: 0.0017524156719446182\n",
      "epoch: 6 | 109856 / 114272 | training loss: 0.001857177005149424\n",
      "epoch: 6 | 109888 / 114272 | training loss: 0.0020885136909782887\n",
      "epoch: 6 | 109920 / 114272 | training loss: 0.000662132166326046\n",
      "epoch: 6 | 109952 / 114272 | training loss: 0.0011223715264350176\n",
      "epoch: 6 | 109984 / 114272 | training loss: 0.0014970515621826053\n",
      "epoch: 6 | 110016 / 114272 | training loss: 0.0009366147569380701\n",
      "epoch: 6 | 110048 / 114272 | training loss: 0.0010551034938544035\n",
      "epoch: 6 | 110080 / 114272 | training loss: 0.0026511293835937977\n",
      "epoch: 6 | 110112 / 114272 | training loss: 0.0005530694616027176\n",
      "epoch: 6 | 110144 / 114272 | training loss: 0.0010879678884521127\n",
      "epoch: 6 | 110176 / 114272 | training loss: 0.0037345748860388994\n",
      "epoch: 6 | 110208 / 114272 | training loss: 0.0016511472640559077\n",
      "epoch: 6 | 110240 / 114272 | training loss: 0.014988550916314125\n",
      "epoch: 6 | 110272 / 114272 | training loss: 0.001518026227131486\n",
      "epoch: 6 | 110304 / 114272 | training loss: 0.0006104756030254066\n",
      "epoch: 6 | 110336 / 114272 | training loss: 0.0011098922695964575\n",
      "epoch: 6 | 110368 / 114272 | training loss: 0.0007080654613673687\n",
      "epoch: 6 | 110400 / 114272 | training loss: 0.0017211040249094367\n",
      "epoch: 6 | 110432 / 114272 | training loss: 0.0010883278446272016\n",
      "epoch: 6 | 110464 / 114272 | training loss: 0.0012593924766406417\n",
      "epoch: 6 | 110496 / 114272 | training loss: 0.0016630432801321149\n",
      "epoch: 6 | 110528 / 114272 | training loss: 0.024241825565695763\n",
      "epoch: 6 | 110560 / 114272 | training loss: 0.009350916370749474\n",
      "epoch: 6 | 110592 / 114272 | training loss: 0.000909378519281745\n",
      "epoch: 6 | 110624 / 114272 | training loss: 0.001751844072714448\n",
      "epoch: 6 | 110656 / 114272 | training loss: 0.003414842998608947\n",
      "epoch: 6 | 110688 / 114272 | training loss: 0.0004942676750943065\n",
      "epoch: 6 | 110720 / 114272 | training loss: 0.0005498028476722538\n",
      "epoch: 6 | 110752 / 114272 | training loss: 0.03928885981440544\n",
      "epoch: 6 | 110784 / 114272 | training loss: 0.003933549392968416\n",
      "epoch: 6 | 110816 / 114272 | training loss: 0.0006916552083566785\n",
      "epoch: 6 | 110848 / 114272 | training loss: 0.18826323747634888\n",
      "epoch: 6 | 110880 / 114272 | training loss: 0.0006710101733915508\n",
      "epoch: 6 | 110912 / 114272 | training loss: 0.19319355487823486\n",
      "epoch: 6 | 110944 / 114272 | training loss: 0.048471417278051376\n",
      "epoch: 6 | 110976 / 114272 | training loss: 0.16419446468353271\n",
      "epoch: 6 | 111008 / 114272 | training loss: 0.0006738791707903147\n",
      "epoch: 6 | 111040 / 114272 | training loss: 0.0007155677885748446\n",
      "epoch: 6 | 111072 / 114272 | training loss: 0.21132268011569977\n",
      "epoch: 6 | 111104 / 114272 | training loss: 0.000798880064394325\n",
      "epoch: 6 | 111136 / 114272 | training loss: 0.0006170753622427583\n",
      "epoch: 6 | 111168 / 114272 | training loss: 0.0008984392625279725\n",
      "epoch: 6 | 111200 / 114272 | training loss: 0.0017202429007738829\n",
      "epoch: 6 | 111232 / 114272 | training loss: 0.0006961342296563089\n",
      "epoch: 6 | 111264 / 114272 | training loss: 0.0015339545207098126\n",
      "epoch: 6 | 111296 / 114272 | training loss: 0.0010406234068796039\n",
      "epoch: 6 | 111328 / 114272 | training loss: 0.001250989967957139\n",
      "epoch: 6 | 111360 / 114272 | training loss: 0.0011581575963646173\n",
      "epoch: 6 | 111392 / 114272 | training loss: 0.0009963762713596225\n",
      "epoch: 6 | 111424 / 114272 | training loss: 0.0008427331340499222\n",
      "epoch: 6 | 111456 / 114272 | training loss: 0.0007134037441574037\n",
      "epoch: 6 | 111488 / 114272 | training loss: 0.0006388930487446487\n",
      "epoch: 6 | 111520 / 114272 | training loss: 0.04313826188445091\n",
      "epoch: 6 | 111552 / 114272 | training loss: 0.0015503133181482553\n",
      "epoch: 6 | 111584 / 114272 | training loss: 0.0008014047052711248\n",
      "epoch: 6 | 111616 / 114272 | training loss: 0.001263385871425271\n",
      "epoch: 6 | 111648 / 114272 | training loss: 0.05484878271818161\n",
      "epoch: 6 | 111680 / 114272 | training loss: 0.024462783709168434\n",
      "epoch: 6 | 111712 / 114272 | training loss: 0.0013410765677690506\n",
      "epoch: 6 | 111744 / 114272 | training loss: 0.002961573423817754\n",
      "epoch: 6 | 111776 / 114272 | training loss: 0.16978158056735992\n",
      "epoch: 6 | 111808 / 114272 | training loss: 0.0006157818133942783\n",
      "epoch: 6 | 111840 / 114272 | training loss: 0.0028547688852995634\n",
      "epoch: 6 | 111872 / 114272 | training loss: 0.03864937275648117\n",
      "epoch: 6 | 111904 / 114272 | training loss: 0.016578150913119316\n",
      "epoch: 6 | 111936 / 114272 | training loss: 0.0016271233325824142\n",
      "epoch: 6 | 111968 / 114272 | training loss: 0.0013648462481796741\n",
      "epoch: 6 | 112000 / 114272 | training loss: 0.0010173305636271834\n",
      "epoch: 6 | 112032 / 114272 | training loss: 0.0006439477438107133\n",
      "epoch: 6 | 112064 / 114272 | training loss: 0.0009232645970769227\n",
      "epoch: 6 | 112096 / 114272 | training loss: 0.12500116229057312\n",
      "epoch: 6 | 112128 / 114272 | training loss: 0.0007485237438231707\n",
      "epoch: 6 | 112160 / 114272 | training loss: 0.0028668164741247892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | 112192 / 114272 | training loss: 0.0008329669944941998\n",
      "epoch: 6 | 112224 / 114272 | training loss: 0.002334339777007699\n",
      "epoch: 6 | 112256 / 114272 | training loss: 0.000626821187324822\n",
      "epoch: 6 | 112288 / 114272 | training loss: 0.0004901588545180857\n",
      "epoch: 6 | 112320 / 114272 | training loss: 0.0015129292150959373\n",
      "epoch: 6 | 112352 / 114272 | training loss: 0.0003372167993802577\n",
      "epoch: 6 | 112384 / 114272 | training loss: 0.0009316478972323239\n",
      "epoch: 6 | 112416 / 114272 | training loss: 0.18781417608261108\n",
      "epoch: 6 | 112448 / 114272 | training loss: 0.0009744340204633772\n",
      "epoch: 6 | 112480 / 114272 | training loss: 0.0009677346097305417\n",
      "epoch: 6 | 112512 / 114272 | training loss: 0.0012203649384900928\n",
      "epoch: 6 | 112544 / 114272 | training loss: 0.0006261369562707841\n",
      "epoch: 6 | 112576 / 114272 | training loss: 0.0015696465270593762\n",
      "epoch: 6 | 112608 / 114272 | training loss: 0.0005595911061391234\n",
      "epoch: 6 | 112640 / 114272 | training loss: 0.0015718986978754401\n",
      "epoch: 6 | 112672 / 114272 | training loss: 0.05806555226445198\n",
      "epoch: 6 | 112704 / 114272 | training loss: 0.0008592085796408355\n",
      "epoch: 6 | 112736 / 114272 | training loss: 0.0010995639022439718\n",
      "epoch: 6 | 112768 / 114272 | training loss: 0.0006607780233025551\n",
      "epoch: 6 | 112800 / 114272 | training loss: 0.00110424286685884\n",
      "epoch: 6 | 112832 / 114272 | training loss: 0.044451385736465454\n",
      "epoch: 6 | 112864 / 114272 | training loss: 0.0008615399710834026\n",
      "epoch: 6 | 112896 / 114272 | training loss: 0.24140574038028717\n",
      "epoch: 6 | 112928 / 114272 | training loss: 0.0012301915558055043\n",
      "epoch: 6 | 112960 / 114272 | training loss: 0.001474788528867066\n",
      "epoch: 6 | 112992 / 114272 | training loss: 0.0005761738284491003\n",
      "epoch: 6 | 113024 / 114272 | training loss: 0.21104326844215393\n",
      "epoch: 6 | 113056 / 114272 | training loss: 0.0022033618297427893\n",
      "epoch: 6 | 113088 / 114272 | training loss: 0.0011405986733734608\n",
      "epoch: 6 | 113120 / 114272 | training loss: 0.0008317717583850026\n",
      "epoch: 6 | 113152 / 114272 | training loss: 0.005141787696629763\n",
      "epoch: 6 | 113184 / 114272 | training loss: 0.0012241852236911654\n",
      "epoch: 6 | 113216 / 114272 | training loss: 0.0008858836954459548\n",
      "epoch: 6 | 113248 / 114272 | training loss: 0.001813849899917841\n",
      "epoch: 6 | 113280 / 114272 | training loss: 0.007883885875344276\n",
      "epoch: 6 | 113312 / 114272 | training loss: 0.0008444823906756938\n",
      "epoch: 6 | 113344 / 114272 | training loss: 0.0010512478183954954\n",
      "epoch: 6 | 113376 / 114272 | training loss: 0.001242122147232294\n",
      "epoch: 6 | 113408 / 114272 | training loss: 0.0019184325356036425\n",
      "epoch: 6 | 113440 / 114272 | training loss: 0.1616356074810028\n",
      "epoch: 6 | 113472 / 114272 | training loss: 0.0008150042849592865\n",
      "epoch: 6 | 113504 / 114272 | training loss: 0.0008899588719941676\n",
      "epoch: 6 | 113536 / 114272 | training loss: 0.001176274730823934\n",
      "epoch: 6 | 113568 / 114272 | training loss: 0.001895276247523725\n",
      "epoch: 6 | 113600 / 114272 | training loss: 0.005498935468494892\n",
      "epoch: 6 | 113632 / 114272 | training loss: 0.03448504954576492\n",
      "epoch: 6 | 113664 / 114272 | training loss: 0.004864677786827087\n",
      "epoch: 6 | 113696 / 114272 | training loss: 0.001487095607444644\n",
      "epoch: 6 | 113728 / 114272 | training loss: 0.0006944743799977005\n",
      "epoch: 6 | 113760 / 114272 | training loss: 0.0009076951537281275\n",
      "epoch: 6 | 113792 / 114272 | training loss: 0.0013607693836092949\n",
      "epoch: 6 | 113824 / 114272 | training loss: 0.13699489831924438\n",
      "epoch: 6 | 113856 / 114272 | training loss: 0.0031281388364732265\n",
      "epoch: 6 | 113888 / 114272 | training loss: 0.002903866581618786\n",
      "epoch: 6 | 113920 / 114272 | training loss: 0.0007838111487217247\n",
      "epoch: 6 | 113952 / 114272 | training loss: 0.006352109368890524\n",
      "epoch: 6 | 113984 / 114272 | training loss: 0.0008971259230747819\n",
      "epoch: 6 | 114016 / 114272 | training loss: 0.0007501195068471134\n",
      "epoch: 6 | 114048 / 114272 | training loss: 0.0007859096513129771\n",
      "epoch: 6 | 114080 / 114272 | training loss: 0.20036731660366058\n",
      "epoch: 6 | 114112 / 114272 | training loss: 0.12013174593448639\n",
      "epoch: 6 | 114144 / 114272 | training loss: 0.0010748885106295347\n",
      "epoch: 6 | 114176 / 114272 | training loss: 0.00561111094430089\n",
      "epoch: 6 | 114208 / 114272 | training loss: 0.0006124812643975019\n",
      "epoch: 6 | 114240 / 114272 | training loss: 0.0030408112797886133\n",
      "Training epoch 6 done! Average loss: 0.022236410758811106. Accuracy: 0.9947756230747691\n",
      "Validation epoch 6 done! Average loss: 0.23062395957057413. Accurage: 0.9543484340044742\n",
      "Epoch 8 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 7 | 0 / 114272 | training loss: 0.0024876007810235023\n",
      "epoch: 7 | 32 / 114272 | training loss: 0.001865803962573409\n",
      "epoch: 7 | 64 / 114272 | training loss: 0.0010913050500676036\n",
      "epoch: 7 | 96 / 114272 | training loss: 0.06194991245865822\n",
      "epoch: 7 | 128 / 114272 | training loss: 0.0012665092945098877\n",
      "epoch: 7 | 160 / 114272 | training loss: 0.0010413702111691236\n",
      "epoch: 7 | 192 / 114272 | training loss: 0.0010643192799761891\n",
      "epoch: 7 | 224 / 114272 | training loss: 0.001185727072879672\n",
      "epoch: 7 | 256 / 114272 | training loss: 0.0010347920469939709\n",
      "epoch: 7 | 288 / 114272 | training loss: 0.0008188561769202352\n",
      "epoch: 7 | 320 / 114272 | training loss: 0.009981533512473106\n",
      "epoch: 7 | 352 / 114272 | training loss: 0.17818191647529602\n",
      "epoch: 7 | 384 / 114272 | training loss: 0.0007838474120944738\n",
      "epoch: 7 | 416 / 114272 | training loss: 0.0020527427550405264\n",
      "epoch: 7 | 448 / 114272 | training loss: 0.002327606314793229\n",
      "epoch: 7 | 480 / 114272 | training loss: 0.12359373271465302\n",
      "epoch: 7 | 512 / 114272 | training loss: 0.0014414585893973708\n",
      "epoch: 7 | 544 / 114272 | training loss: 0.001681849011220038\n",
      "epoch: 7 | 576 / 114272 | training loss: 0.0029915065970271826\n",
      "epoch: 7 | 608 / 114272 | training loss: 0.0009095756686292589\n",
      "epoch: 7 | 640 / 114272 | training loss: 0.008958873338997364\n",
      "epoch: 7 | 672 / 114272 | training loss: 0.0014526302693411708\n",
      "epoch: 7 | 704 / 114272 | training loss: 0.001552011352032423\n",
      "epoch: 7 | 736 / 114272 | training loss: 0.0012947754003107548\n",
      "epoch: 7 | 768 / 114272 | training loss: 0.0010105090914294124\n",
      "epoch: 7 | 800 / 114272 | training loss: 0.0012281798990443349\n",
      "epoch: 7 | 832 / 114272 | training loss: 0.002264256589114666\n",
      "epoch: 7 | 864 / 114272 | training loss: 0.01921568065881729\n",
      "epoch: 7 | 896 / 114272 | training loss: 0.0007475763559341431\n",
      "epoch: 7 | 928 / 114272 | training loss: 0.0013665249571204185\n",
      "epoch: 7 | 960 / 114272 | training loss: 0.010155808180570602\n",
      "epoch: 7 | 992 / 114272 | training loss: 0.000986777595244348\n",
      "epoch: 7 | 1024 / 114272 | training loss: 0.0011414638720452785\n",
      "epoch: 7 | 1056 / 114272 | training loss: 0.06534848362207413\n",
      "epoch: 7 | 1088 / 114272 | training loss: 0.05351834371685982\n",
      "epoch: 7 | 1120 / 114272 | training loss: 0.0008358136401511729\n",
      "epoch: 7 | 1152 / 114272 | training loss: 0.001098644221201539\n",
      "epoch: 7 | 1184 / 114272 | training loss: 0.022487929090857506\n",
      "epoch: 7 | 1216 / 114272 | training loss: 0.00120025803335011\n",
      "epoch: 7 | 1248 / 114272 | training loss: 0.0005823516403324902\n",
      "epoch: 7 | 1280 / 114272 | training loss: 0.0007540499209426343\n",
      "epoch: 7 | 1312 / 114272 | training loss: 0.000893681775778532\n",
      "epoch: 7 | 1344 / 114272 | training loss: 0.018470250070095062\n",
      "epoch: 7 | 1376 / 114272 | training loss: 0.0006685372209176421\n",
      "epoch: 7 | 1408 / 114272 | training loss: 0.0011635562404990196\n",
      "epoch: 7 | 1440 / 114272 | training loss: 0.0025852471590042114\n",
      "epoch: 7 | 1472 / 114272 | training loss: 0.0026182394940406084\n",
      "epoch: 7 | 1504 / 114272 | training loss: 0.0006773305358365178\n",
      "epoch: 7 | 1536 / 114272 | training loss: 0.0007937794434837997\n",
      "epoch: 7 | 1568 / 114272 | training loss: 0.001838364521972835\n",
      "epoch: 7 | 1600 / 114272 | training loss: 0.0010769149521365762\n",
      "epoch: 7 | 1632 / 114272 | training loss: 0.0008527752943336964\n",
      "epoch: 7 | 1664 / 114272 | training loss: 0.0005174449179321527\n",
      "epoch: 7 | 1696 / 114272 | training loss: 0.0006763662095181644\n",
      "epoch: 7 | 1728 / 114272 | training loss: 0.0006671618903055787\n",
      "epoch: 7 | 1760 / 114272 | training loss: 0.0014322514180094004\n",
      "epoch: 7 | 1792 / 114272 | training loss: 0.00022013293346390128\n",
      "epoch: 7 | 1824 / 114272 | training loss: 0.0028987161349505186\n",
      "epoch: 7 | 1856 / 114272 | training loss: 0.000602831831201911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 1888 / 114272 | training loss: 0.10458416491746902\n",
      "epoch: 7 | 1920 / 114272 | training loss: 0.00064483453752473\n",
      "epoch: 7 | 1952 / 114272 | training loss: 0.000553503108676523\n",
      "epoch: 7 | 1984 / 114272 | training loss: 0.16871121525764465\n",
      "epoch: 7 | 2016 / 114272 | training loss: 0.0016125920228660107\n",
      "epoch: 7 | 2048 / 114272 | training loss: 0.0009057971183210611\n",
      "epoch: 7 | 2080 / 114272 | training loss: 0.000590896001085639\n",
      "epoch: 7 | 2112 / 114272 | training loss: 0.0005709488759748638\n",
      "epoch: 7 | 2144 / 114272 | training loss: 0.0008729425608180463\n",
      "epoch: 7 | 2176 / 114272 | training loss: 0.0006094194832257926\n",
      "epoch: 7 | 2208 / 114272 | training loss: 0.0008769495761953294\n",
      "epoch: 7 | 2240 / 114272 | training loss: 0.022215204313397408\n",
      "epoch: 7 | 2272 / 114272 | training loss: 0.0007607646402902901\n",
      "epoch: 7 | 2304 / 114272 | training loss: 0.001719977823086083\n",
      "epoch: 7 | 2336 / 114272 | training loss: 0.0034493389539420605\n",
      "epoch: 7 | 2368 / 114272 | training loss: 0.006513163913041353\n",
      "epoch: 7 | 2400 / 114272 | training loss: 0.00047070597065612674\n",
      "epoch: 7 | 2432 / 114272 | training loss: 0.00046927997027523816\n",
      "epoch: 7 | 2464 / 114272 | training loss: 0.0006818637484684587\n",
      "epoch: 7 | 2496 / 114272 | training loss: 0.0008972836658358574\n",
      "epoch: 7 | 2528 / 114272 | training loss: 0.002750277752056718\n",
      "epoch: 7 | 2560 / 114272 | training loss: 0.0007211963529698551\n",
      "epoch: 7 | 2592 / 114272 | training loss: 0.0006950460956431925\n",
      "epoch: 7 | 2624 / 114272 | training loss: 0.00037527241511270404\n",
      "epoch: 7 | 2656 / 114272 | training loss: 0.0007308765198104084\n",
      "epoch: 7 | 2688 / 114272 | training loss: 0.0005538310506381094\n",
      "epoch: 7 | 2720 / 114272 | training loss: 0.17316992580890656\n",
      "epoch: 7 | 2752 / 114272 | training loss: 0.1484813690185547\n",
      "epoch: 7 | 2784 / 114272 | training loss: 0.00027302128728479147\n",
      "epoch: 7 | 2816 / 114272 | training loss: 0.0012421268038451672\n",
      "epoch: 7 | 2848 / 114272 | training loss: 0.0004629777104128152\n",
      "epoch: 7 | 2880 / 114272 | training loss: 0.0005321074277162552\n",
      "epoch: 7 | 2912 / 114272 | training loss: 0.0010701629798859358\n",
      "epoch: 7 | 2944 / 114272 | training loss: 0.0008448646985925734\n",
      "epoch: 7 | 2976 / 114272 | training loss: 0.0007932506268844008\n",
      "epoch: 7 | 3008 / 114272 | training loss: 0.000899498991202563\n",
      "epoch: 7 | 3040 / 114272 | training loss: 0.0005465194117277861\n",
      "epoch: 7 | 3072 / 114272 | training loss: 0.10539358109235764\n",
      "epoch: 7 | 3104 / 114272 | training loss: 0.00029585184529423714\n",
      "epoch: 7 | 3136 / 114272 | training loss: 0.0004339211736805737\n",
      "epoch: 7 | 3168 / 114272 | training loss: 0.0006360645638778806\n",
      "epoch: 7 | 3200 / 114272 | training loss: 0.000583843735512346\n",
      "epoch: 7 | 3232 / 114272 | training loss: 0.0007116430206224322\n",
      "epoch: 7 | 3264 / 114272 | training loss: 0.0018124451162293553\n",
      "epoch: 7 | 3296 / 114272 | training loss: 0.0013986647827550769\n",
      "epoch: 7 | 3328 / 114272 | training loss: 0.0006638073828071356\n",
      "epoch: 7 | 3360 / 114272 | training loss: 0.12720687687397003\n",
      "epoch: 7 | 3392 / 114272 | training loss: 0.0005957268294878304\n",
      "epoch: 7 | 3424 / 114272 | training loss: 0.00044683527084998786\n",
      "epoch: 7 | 3456 / 114272 | training loss: 0.0013318085111677647\n",
      "epoch: 7 | 3488 / 114272 | training loss: 0.00046562342322431505\n",
      "epoch: 7 | 3520 / 114272 | training loss: 0.0008060804684646428\n",
      "epoch: 7 | 3552 / 114272 | training loss: 0.0002251054538646713\n",
      "epoch: 7 | 3584 / 114272 | training loss: 0.000415005546528846\n",
      "epoch: 7 | 3616 / 114272 | training loss: 0.03942195326089859\n",
      "epoch: 7 | 3648 / 114272 | training loss: 0.00020630624203477055\n",
      "epoch: 7 | 3680 / 114272 | training loss: 0.407650887966156\n",
      "epoch: 7 | 3712 / 114272 | training loss: 0.07210265100002289\n",
      "epoch: 7 | 3744 / 114272 | training loss: 0.003388504032045603\n",
      "epoch: 7 | 3776 / 114272 | training loss: 0.0009189605480059981\n",
      "epoch: 7 | 3808 / 114272 | training loss: 0.00035666051553562284\n",
      "epoch: 7 | 3840 / 114272 | training loss: 0.0017765123629942536\n",
      "epoch: 7 | 3872 / 114272 | training loss: 0.0844767764210701\n",
      "epoch: 7 | 3904 / 114272 | training loss: 0.0007545254193246365\n",
      "epoch: 7 | 3936 / 114272 | training loss: 0.0010274241212755442\n",
      "epoch: 7 | 3968 / 114272 | training loss: 0.0010485370876267552\n",
      "epoch: 7 | 4000 / 114272 | training loss: 0.0009608362452127039\n",
      "epoch: 7 | 4032 / 114272 | training loss: 0.0006053901743143797\n",
      "epoch: 7 | 4064 / 114272 | training loss: 0.0011515713995322585\n",
      "epoch: 7 | 4096 / 114272 | training loss: 0.00039424520218744874\n",
      "epoch: 7 | 4128 / 114272 | training loss: 0.0005697418819181621\n",
      "epoch: 7 | 4160 / 114272 | training loss: 0.00046540144830942154\n",
      "epoch: 7 | 4192 / 114272 | training loss: 0.0007356201531365514\n",
      "epoch: 7 | 4224 / 114272 | training loss: 0.0009136033477261662\n",
      "epoch: 7 | 4256 / 114272 | training loss: 0.0002488510508555919\n",
      "epoch: 7 | 4288 / 114272 | training loss: 0.0006966697983443737\n",
      "epoch: 7 | 4320 / 114272 | training loss: 0.0004868272808380425\n",
      "epoch: 7 | 4352 / 114272 | training loss: 0.0003658602072391659\n",
      "epoch: 7 | 4384 / 114272 | training loss: 0.0004863254725933075\n",
      "epoch: 7 | 4416 / 114272 | training loss: 0.00012869788042735308\n",
      "epoch: 7 | 4448 / 114272 | training loss: 0.0003953185223508626\n",
      "epoch: 7 | 4480 / 114272 | training loss: 0.0009571837726980448\n",
      "epoch: 7 | 4512 / 114272 | training loss: 0.34635528922080994\n",
      "epoch: 7 | 4544 / 114272 | training loss: 0.0003045547055080533\n",
      "epoch: 7 | 4576 / 114272 | training loss: 0.043006282299757004\n",
      "epoch: 7 | 4608 / 114272 | training loss: 0.0005878072115592659\n",
      "epoch: 7 | 4640 / 114272 | training loss: 0.2204873114824295\n",
      "epoch: 7 | 4672 / 114272 | training loss: 0.0010459998156875372\n",
      "epoch: 7 | 4704 / 114272 | training loss: 0.0021059969440102577\n",
      "epoch: 7 | 4736 / 114272 | training loss: 0.0005594344111159444\n",
      "epoch: 7 | 4768 / 114272 | training loss: 0.00034907809458673\n",
      "epoch: 7 | 4800 / 114272 | training loss: 0.0005661207251250744\n",
      "epoch: 7 | 4832 / 114272 | training loss: 0.0001736807607812807\n",
      "epoch: 7 | 4864 / 114272 | training loss: 0.0005569191416725516\n",
      "epoch: 7 | 4896 / 114272 | training loss: 0.0010562280658632517\n",
      "epoch: 7 | 4928 / 114272 | training loss: 0.0007397754234261811\n",
      "epoch: 7 | 4960 / 114272 | training loss: 0.17502686381340027\n",
      "epoch: 7 | 4992 / 114272 | training loss: 0.000783893745392561\n",
      "epoch: 7 | 5024 / 114272 | training loss: 0.1393994837999344\n",
      "epoch: 7 | 5056 / 114272 | training loss: 0.0008965298184193671\n",
      "epoch: 7 | 5088 / 114272 | training loss: 0.0009105203789658844\n",
      "epoch: 7 | 5120 / 114272 | training loss: 0.00045374251203611493\n",
      "epoch: 7 | 5152 / 114272 | training loss: 0.0014286059886217117\n",
      "epoch: 7 | 5184 / 114272 | training loss: 0.13649477064609528\n",
      "epoch: 7 | 5216 / 114272 | training loss: 0.0010279149282723665\n",
      "epoch: 7 | 5248 / 114272 | training loss: 0.000921054685022682\n",
      "epoch: 7 | 5280 / 114272 | training loss: 0.0013419046299532056\n",
      "epoch: 7 | 5312 / 114272 | training loss: 0.00083763716975227\n",
      "epoch: 7 | 5344 / 114272 | training loss: 0.0004609610477928072\n",
      "epoch: 7 | 5376 / 114272 | training loss: 0.0010240440024062991\n",
      "epoch: 7 | 5408 / 114272 | training loss: 0.001045198179781437\n",
      "epoch: 7 | 5440 / 114272 | training loss: 0.0013509889831766486\n",
      "epoch: 7 | 5472 / 114272 | training loss: 0.0005948523757979274\n",
      "epoch: 7 | 5504 / 114272 | training loss: 0.0015822842251509428\n",
      "epoch: 7 | 5536 / 114272 | training loss: 0.0020763820502907038\n",
      "epoch: 7 | 5568 / 114272 | training loss: 0.001019782037474215\n",
      "epoch: 7 | 5600 / 114272 | training loss: 0.0009701885282993317\n",
      "epoch: 7 | 5632 / 114272 | training loss: 0.0012535704299807549\n",
      "epoch: 7 | 5664 / 114272 | training loss: 0.007218293379992247\n",
      "epoch: 7 | 5696 / 114272 | training loss: 0.08795365691184998\n",
      "epoch: 7 | 5728 / 114272 | training loss: 0.0005614866968244314\n",
      "epoch: 7 | 5760 / 114272 | training loss: 0.0004515571054071188\n",
      "epoch: 7 | 5792 / 114272 | training loss: 0.0008791338768787682\n",
      "epoch: 7 | 5824 / 114272 | training loss: 0.00089176872279495\n",
      "epoch: 7 | 5856 / 114272 | training loss: 0.001115600112825632\n",
      "epoch: 7 | 5888 / 114272 | training loss: 0.0010766618652269244\n",
      "epoch: 7 | 5920 / 114272 | training loss: 0.0007408842793665826\n",
      "epoch: 7 | 5952 / 114272 | training loss: 0.0010419541504234076\n",
      "epoch: 7 | 5984 / 114272 | training loss: 0.0008018530788831413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 6016 / 114272 | training loss: 0.00029188598273321986\n",
      "epoch: 7 | 6048 / 114272 | training loss: 0.0008487110026180744\n",
      "epoch: 7 | 6080 / 114272 | training loss: 0.000680943310726434\n",
      "epoch: 7 | 6112 / 114272 | training loss: 0.0009458165732212365\n",
      "epoch: 7 | 6144 / 114272 | training loss: 0.0007329582003876567\n",
      "epoch: 7 | 6176 / 114272 | training loss: 0.0004457116883713752\n",
      "epoch: 7 | 6208 / 114272 | training loss: 0.00021720732911489904\n",
      "epoch: 7 | 6240 / 114272 | training loss: 0.0005911079933866858\n",
      "epoch: 7 | 6272 / 114272 | training loss: 0.0009510210948064923\n",
      "epoch: 7 | 6304 / 114272 | training loss: 0.0007940044160932302\n",
      "epoch: 7 | 6336 / 114272 | training loss: 0.000584902532864362\n",
      "epoch: 7 | 6368 / 114272 | training loss: 0.0010462884092703462\n",
      "epoch: 7 | 6400 / 114272 | training loss: 0.000958493328653276\n",
      "epoch: 7 | 6432 / 114272 | training loss: 0.0009396396926604211\n",
      "epoch: 7 | 6464 / 114272 | training loss: 0.0005973676452413201\n",
      "epoch: 7 | 6496 / 114272 | training loss: 0.003814124967902899\n",
      "epoch: 7 | 6528 / 114272 | training loss: 0.003350629471242428\n",
      "epoch: 7 | 6560 / 114272 | training loss: 0.0027100066654384136\n",
      "epoch: 7 | 6592 / 114272 | training loss: 0.0004929043934680521\n",
      "epoch: 7 | 6624 / 114272 | training loss: 0.00155280577018857\n",
      "epoch: 7 | 6656 / 114272 | training loss: 0.0006036245031282306\n",
      "epoch: 7 | 6688 / 114272 | training loss: 0.03432584926486015\n",
      "epoch: 7 | 6720 / 114272 | training loss: 0.001562603865750134\n",
      "epoch: 7 | 6752 / 114272 | training loss: 0.0006541968323290348\n",
      "epoch: 7 | 6784 / 114272 | training loss: 0.00059708789922297\n",
      "epoch: 7 | 6816 / 114272 | training loss: 0.0004887657705694437\n",
      "epoch: 7 | 6848 / 114272 | training loss: 0.0005787056870758533\n",
      "epoch: 7 | 6880 / 114272 | training loss: 0.0006130615947768092\n",
      "epoch: 7 | 6912 / 114272 | training loss: 0.0010360911255702376\n",
      "epoch: 7 | 6944 / 114272 | training loss: 0.00047586465370841324\n",
      "epoch: 7 | 6976 / 114272 | training loss: 0.001718130661174655\n",
      "epoch: 7 | 7008 / 114272 | training loss: 0.000509166915435344\n",
      "epoch: 7 | 7040 / 114272 | training loss: 0.0006959923775866628\n",
      "epoch: 7 | 7072 / 114272 | training loss: 0.0006572267739102244\n",
      "epoch: 7 | 7104 / 114272 | training loss: 0.0004675846139434725\n",
      "epoch: 7 | 7136 / 114272 | training loss: 0.0009804100263863802\n",
      "epoch: 7 | 7168 / 114272 | training loss: 0.0007938825874589384\n",
      "epoch: 7 | 7200 / 114272 | training loss: 0.0008220192976295948\n",
      "epoch: 7 | 7232 / 114272 | training loss: 0.0006299047963693738\n",
      "epoch: 7 | 7264 / 114272 | training loss: 0.0006601089844480157\n",
      "epoch: 7 | 7296 / 114272 | training loss: 0.002203169045969844\n",
      "epoch: 7 | 7328 / 114272 | training loss: 0.0013668882893398404\n",
      "epoch: 7 | 7360 / 114272 | training loss: 0.0006683344254270196\n",
      "epoch: 7 | 7392 / 114272 | training loss: 0.0005237244185991585\n",
      "epoch: 7 | 7424 / 114272 | training loss: 0.005141418427228928\n",
      "epoch: 7 | 7456 / 114272 | training loss: 0.000668890483211726\n",
      "epoch: 7 | 7488 / 114272 | training loss: 0.0002432260080240667\n",
      "epoch: 7 | 7520 / 114272 | training loss: 0.00030892129871062934\n",
      "epoch: 7 | 7552 / 114272 | training loss: 0.0005979501875117421\n",
      "epoch: 7 | 7584 / 114272 | training loss: 0.010813144966959953\n",
      "epoch: 7 | 7616 / 114272 | training loss: 0.0009965215576812625\n",
      "epoch: 7 | 7648 / 114272 | training loss: 0.000315391254844144\n",
      "epoch: 7 | 7680 / 114272 | training loss: 0.0008799268398433924\n",
      "epoch: 7 | 7712 / 114272 | training loss: 0.00037036469439044595\n",
      "epoch: 7 | 7744 / 114272 | training loss: 0.0006297716172412038\n",
      "epoch: 7 | 7776 / 114272 | training loss: 0.0006271427846513689\n",
      "epoch: 7 | 7808 / 114272 | training loss: 0.002630016766488552\n",
      "epoch: 7 | 7840 / 114272 | training loss: 0.00042711736750788987\n",
      "epoch: 7 | 7872 / 114272 | training loss: 0.0013682316057384014\n",
      "epoch: 7 | 7904 / 114272 | training loss: 0.1678796410560608\n",
      "epoch: 7 | 7936 / 114272 | training loss: 0.0003507506917230785\n",
      "epoch: 7 | 7968 / 114272 | training loss: 0.0006619755295105278\n",
      "epoch: 7 | 8000 / 114272 | training loss: 0.14128504693508148\n",
      "epoch: 7 | 8032 / 114272 | training loss: 0.0009651503060013056\n",
      "epoch: 7 | 8064 / 114272 | training loss: 0.09534229338169098\n",
      "epoch: 7 | 8096 / 114272 | training loss: 0.00036481136339716613\n",
      "epoch: 7 | 8128 / 114272 | training loss: 0.0005104584852233529\n",
      "epoch: 7 | 8160 / 114272 | training loss: 0.000596294819843024\n",
      "epoch: 7 | 8192 / 114272 | training loss: 0.0008709861431270838\n",
      "epoch: 7 | 8224 / 114272 | training loss: 0.0006878894637338817\n",
      "epoch: 7 | 8256 / 114272 | training loss: 0.00028020425816066563\n",
      "epoch: 7 | 8288 / 114272 | training loss: 0.0004215254448354244\n",
      "epoch: 7 | 8320 / 114272 | training loss: 0.0011139364214614034\n",
      "epoch: 7 | 8352 / 114272 | training loss: 0.0011147623881697655\n",
      "epoch: 7 | 8384 / 114272 | training loss: 0.0016173931071534753\n",
      "epoch: 7 | 8416 / 114272 | training loss: 0.0009137502638623118\n",
      "epoch: 7 | 8448 / 114272 | training loss: 0.0016659487737342715\n",
      "epoch: 7 | 8480 / 114272 | training loss: 0.0006344880093820393\n",
      "epoch: 7 | 8512 / 114272 | training loss: 0.0011612002272158861\n",
      "epoch: 7 | 8544 / 114272 | training loss: 0.0016712337965145707\n",
      "epoch: 7 | 8576 / 114272 | training loss: 0.0004335049306973815\n",
      "epoch: 7 | 8608 / 114272 | training loss: 0.005269439425319433\n",
      "epoch: 7 | 8640 / 114272 | training loss: 0.0011217552237212658\n",
      "epoch: 7 | 8672 / 114272 | training loss: 0.0010733744129538536\n",
      "epoch: 7 | 8704 / 114272 | training loss: 0.0004379783058539033\n",
      "epoch: 7 | 8736 / 114272 | training loss: 0.0009408431360498071\n",
      "epoch: 7 | 8768 / 114272 | training loss: 0.0012965734349563718\n",
      "epoch: 7 | 8800 / 114272 | training loss: 0.0007362810429185629\n",
      "epoch: 7 | 8832 / 114272 | training loss: 0.0006998098688200116\n",
      "epoch: 7 | 8864 / 114272 | training loss: 0.0010332984384149313\n",
      "epoch: 7 | 8896 / 114272 | training loss: 0.0009699985384941101\n",
      "epoch: 7 | 8928 / 114272 | training loss: 0.0005416113999672234\n",
      "epoch: 7 | 8960 / 114272 | training loss: 0.0009398114634677768\n",
      "epoch: 7 | 8992 / 114272 | training loss: 0.000802168098744005\n",
      "epoch: 7 | 9024 / 114272 | training loss: 0.0004081063671037555\n",
      "epoch: 7 | 9056 / 114272 | training loss: 0.0004273715312592685\n",
      "epoch: 7 | 9088 / 114272 | training loss: 0.00046586370444856584\n",
      "epoch: 7 | 9120 / 114272 | training loss: 0.00036815463681705296\n",
      "epoch: 7 | 9152 / 114272 | training loss: 0.0007345901685766876\n",
      "epoch: 7 | 9184 / 114272 | training loss: 0.0006230117287486792\n",
      "epoch: 7 | 9216 / 114272 | training loss: 0.0003791683993767947\n",
      "epoch: 7 | 9248 / 114272 | training loss: 0.001246022991836071\n",
      "epoch: 7 | 9280 / 114272 | training loss: 0.0011650677770376205\n",
      "epoch: 7 | 9312 / 114272 | training loss: 0.0003610457933973521\n",
      "epoch: 7 | 9344 / 114272 | training loss: 0.18555223941802979\n",
      "epoch: 7 | 9376 / 114272 | training loss: 0.11112509667873383\n",
      "epoch: 7 | 9408 / 114272 | training loss: 0.0004964309046044946\n",
      "epoch: 7 | 9440 / 114272 | training loss: 0.0005122742732055485\n",
      "epoch: 7 | 9472 / 114272 | training loss: 0.0004105309781152755\n",
      "epoch: 7 | 9504 / 114272 | training loss: 0.000986529397778213\n",
      "epoch: 7 | 9536 / 114272 | training loss: 0.000683862017467618\n",
      "epoch: 7 | 9568 / 114272 | training loss: 0.0007590672466903925\n",
      "epoch: 7 | 9600 / 114272 | training loss: 0.0004244424926582724\n",
      "epoch: 7 | 9632 / 114272 | training loss: 0.015200446359813213\n",
      "epoch: 7 | 9664 / 114272 | training loss: 0.0006179717020131648\n",
      "epoch: 7 | 9696 / 114272 | training loss: 0.0006168741965666413\n",
      "epoch: 7 | 9728 / 114272 | training loss: 0.12109369039535522\n",
      "epoch: 7 | 9760 / 114272 | training loss: 0.0006027693161740899\n",
      "epoch: 7 | 9792 / 114272 | training loss: 0.0004811549442820251\n",
      "epoch: 7 | 9824 / 114272 | training loss: 0.0008446965366601944\n",
      "epoch: 7 | 9856 / 114272 | training loss: 0.0009217286133207381\n",
      "epoch: 7 | 9888 / 114272 | training loss: 0.06750569492578506\n",
      "epoch: 7 | 9920 / 114272 | training loss: 0.029195206239819527\n",
      "epoch: 7 | 9952 / 114272 | training loss: 0.0007840495673008263\n",
      "epoch: 7 | 9984 / 114272 | training loss: 0.0013662700075656176\n",
      "epoch: 7 | 10016 / 114272 | training loss: 0.0007657362148165703\n",
      "epoch: 7 | 10048 / 114272 | training loss: 0.00023771129781380296\n",
      "epoch: 7 | 10080 / 114272 | training loss: 0.0008770293206907809\n",
      "epoch: 7 | 10112 / 114272 | training loss: 0.00038603166467510164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 10144 / 114272 | training loss: 0.0009078975417651236\n",
      "epoch: 7 | 10176 / 114272 | training loss: 0.0014174599200487137\n",
      "epoch: 7 | 10208 / 114272 | training loss: 0.001528802327811718\n",
      "epoch: 7 | 10240 / 114272 | training loss: 0.0011951938504353166\n",
      "epoch: 7 | 10272 / 114272 | training loss: 0.00017784553347155452\n",
      "epoch: 7 | 10304 / 114272 | training loss: 0.00041931364103220403\n",
      "epoch: 7 | 10336 / 114272 | training loss: 0.0009897510753944516\n",
      "epoch: 7 | 10368 / 114272 | training loss: 0.002899784129112959\n",
      "epoch: 7 | 10400 / 114272 | training loss: 0.0005876400391571224\n",
      "epoch: 7 | 10432 / 114272 | training loss: 0.0010981642408296466\n",
      "epoch: 7 | 10464 / 114272 | training loss: 0.0011218644212931395\n",
      "epoch: 7 | 10496 / 114272 | training loss: 0.0009864334715530276\n",
      "epoch: 7 | 10528 / 114272 | training loss: 0.0003988161915913224\n",
      "epoch: 7 | 10560 / 114272 | training loss: 0.0007720607100054622\n",
      "epoch: 7 | 10592 / 114272 | training loss: 0.0007411479018628597\n",
      "epoch: 7 | 10624 / 114272 | training loss: 0.00233089248649776\n",
      "epoch: 7 | 10656 / 114272 | training loss: 0.0012456916738301516\n",
      "epoch: 7 | 10688 / 114272 | training loss: 0.0012444918975234032\n",
      "epoch: 7 | 10720 / 114272 | training loss: 0.0012468285858631134\n",
      "epoch: 7 | 10752 / 114272 | training loss: 0.0005014552734792233\n",
      "epoch: 7 | 10784 / 114272 | training loss: 0.0007370906532742083\n",
      "epoch: 7 | 10816 / 114272 | training loss: 0.00030793267069384456\n",
      "epoch: 7 | 10848 / 114272 | training loss: 0.0009844969026744366\n",
      "epoch: 7 | 10880 / 114272 | training loss: 0.0003453180834185332\n",
      "epoch: 7 | 10912 / 114272 | training loss: 0.0007065223180688918\n",
      "epoch: 7 | 10944 / 114272 | training loss: 0.000493197119794786\n",
      "epoch: 7 | 10976 / 114272 | training loss: 0.0005941250128671527\n",
      "epoch: 7 | 11008 / 114272 | training loss: 0.0005461977561935782\n",
      "epoch: 7 | 11040 / 114272 | training loss: 0.000871466938406229\n",
      "epoch: 7 | 11072 / 114272 | training loss: 0.0005966750904917717\n",
      "epoch: 7 | 11104 / 114272 | training loss: 0.00036879570689052343\n",
      "epoch: 7 | 11136 / 114272 | training loss: 0.17835552990436554\n",
      "epoch: 7 | 11168 / 114272 | training loss: 0.000638703175354749\n",
      "epoch: 7 | 11200 / 114272 | training loss: 0.00032125553116202354\n",
      "epoch: 7 | 11232 / 114272 | training loss: 0.0004364612977951765\n",
      "epoch: 7 | 11264 / 114272 | training loss: 0.0010740655707195401\n",
      "epoch: 7 | 11296 / 114272 | training loss: 0.0005677520530298352\n",
      "epoch: 7 | 11328 / 114272 | training loss: 0.00048203542246483266\n",
      "epoch: 7 | 11360 / 114272 | training loss: 0.00022211925534065813\n",
      "epoch: 7 | 11392 / 114272 | training loss: 0.0005611890810541809\n",
      "epoch: 7 | 11424 / 114272 | training loss: 0.0010018185712397099\n",
      "epoch: 7 | 11456 / 114272 | training loss: 0.0004095091426279396\n",
      "epoch: 7 | 11488 / 114272 | training loss: 0.000995499431155622\n",
      "epoch: 7 | 11520 / 114272 | training loss: 0.0004373242554720491\n",
      "epoch: 7 | 11552 / 114272 | training loss: 0.0007198589155450463\n",
      "epoch: 7 | 11584 / 114272 | training loss: 0.0002979211276397109\n",
      "epoch: 7 | 11616 / 114272 | training loss: 0.0009383977157995105\n",
      "epoch: 7 | 11648 / 114272 | training loss: 0.0006557204760611057\n",
      "epoch: 7 | 11680 / 114272 | training loss: 0.0010194865753874183\n",
      "epoch: 7 | 11712 / 114272 | training loss: 0.0018469563219696283\n",
      "epoch: 7 | 11744 / 114272 | training loss: 0.0004563715774565935\n",
      "epoch: 7 | 11776 / 114272 | training loss: 0.0008802857482805848\n",
      "epoch: 7 | 11808 / 114272 | training loss: 0.0008971839561127126\n",
      "epoch: 7 | 11840 / 114272 | training loss: 0.00037168466951698065\n",
      "epoch: 7 | 11872 / 114272 | training loss: 0.001126522896811366\n",
      "epoch: 7 | 11904 / 114272 | training loss: 0.0006004450260661542\n",
      "epoch: 7 | 11936 / 114272 | training loss: 0.00011373819143045694\n",
      "epoch: 7 | 11968 / 114272 | training loss: 0.0005009184242226183\n",
      "epoch: 7 | 12000 / 114272 | training loss: 0.0007281654980033636\n",
      "epoch: 7 | 12032 / 114272 | training loss: 0.00016714569937903434\n",
      "epoch: 7 | 12064 / 114272 | training loss: 0.0010166801512241364\n",
      "epoch: 7 | 12096 / 114272 | training loss: 0.0005368014681152999\n",
      "epoch: 7 | 12128 / 114272 | training loss: 0.00038938195211812854\n",
      "epoch: 7 | 12160 / 114272 | training loss: 0.0010410364484414458\n",
      "epoch: 7 | 12192 / 114272 | training loss: 0.0003704410628415644\n",
      "epoch: 7 | 12224 / 114272 | training loss: 0.25033846497535706\n",
      "epoch: 7 | 12256 / 114272 | training loss: 0.0003492335672490299\n",
      "epoch: 7 | 12288 / 114272 | training loss: 0.0004412341513670981\n",
      "epoch: 7 | 12320 / 114272 | training loss: 0.0034218167420476675\n",
      "epoch: 7 | 12352 / 114272 | training loss: 0.00032624215236864984\n",
      "epoch: 7 | 12384 / 114272 | training loss: 0.003151537152007222\n",
      "epoch: 7 | 12416 / 114272 | training loss: 0.0003175046877004206\n",
      "epoch: 7 | 12448 / 114272 | training loss: 0.0005073673091828823\n",
      "epoch: 7 | 12480 / 114272 | training loss: 0.12546797096729279\n",
      "epoch: 7 | 12512 / 114272 | training loss: 0.0004572327306959778\n",
      "epoch: 7 | 12544 / 114272 | training loss: 0.0006348873139359057\n",
      "epoch: 7 | 12576 / 114272 | training loss: 0.0006364542059600353\n",
      "epoch: 7 | 12608 / 114272 | training loss: 0.0009826795430853963\n",
      "epoch: 7 | 12640 / 114272 | training loss: 0.0003183552762493491\n",
      "epoch: 7 | 12672 / 114272 | training loss: 0.0004876343300566077\n",
      "epoch: 7 | 12704 / 114272 | training loss: 0.0005630882224068046\n",
      "epoch: 7 | 12736 / 114272 | training loss: 0.27401867508888245\n",
      "epoch: 7 | 12768 / 114272 | training loss: 0.0005975756794214249\n",
      "epoch: 7 | 12800 / 114272 | training loss: 0.00043105220538564026\n",
      "epoch: 7 | 12832 / 114272 | training loss: 0.0006910231313668191\n",
      "epoch: 7 | 12864 / 114272 | training loss: 0.0005898180534131825\n",
      "epoch: 7 | 12896 / 114272 | training loss: 0.0004196001682430506\n",
      "epoch: 7 | 12928 / 114272 | training loss: 0.0003590895503293723\n",
      "epoch: 7 | 12960 / 114272 | training loss: 0.3378679156303406\n",
      "epoch: 7 | 12992 / 114272 | training loss: 0.1443576067686081\n",
      "epoch: 7 | 13024 / 114272 | training loss: 0.0008018338121473789\n",
      "epoch: 7 | 13056 / 114272 | training loss: 0.0003654564206954092\n",
      "epoch: 7 | 13088 / 114272 | training loss: 0.0008999411948025227\n",
      "epoch: 7 | 13120 / 114272 | training loss: 0.000654743576887995\n",
      "epoch: 7 | 13152 / 114272 | training loss: 0.0005170672084204853\n",
      "epoch: 7 | 13184 / 114272 | training loss: 0.00025237491354346275\n",
      "epoch: 7 | 13216 / 114272 | training loss: 0.002847302006557584\n",
      "epoch: 7 | 13248 / 114272 | training loss: 0.0005781372310593724\n",
      "epoch: 7 | 13280 / 114272 | training loss: 0.11376212537288666\n",
      "epoch: 7 | 13312 / 114272 | training loss: 0.001738766091875732\n",
      "epoch: 7 | 13344 / 114272 | training loss: 0.0003308191953692585\n",
      "epoch: 7 | 13376 / 114272 | training loss: 0.0003988581884186715\n",
      "epoch: 7 | 13408 / 114272 | training loss: 0.000977968331426382\n",
      "epoch: 7 | 13440 / 114272 | training loss: 0.0006725731655023992\n",
      "epoch: 7 | 13472 / 114272 | training loss: 0.0034381665755063295\n",
      "epoch: 7 | 13504 / 114272 | training loss: 0.0004448308900464326\n",
      "epoch: 7 | 13536 / 114272 | training loss: 0.04463823139667511\n",
      "epoch: 7 | 13568 / 114272 | training loss: 0.0010603616246953607\n",
      "epoch: 7 | 13600 / 114272 | training loss: 0.0008177271229214966\n",
      "epoch: 7 | 13632 / 114272 | training loss: 0.00041951920138671994\n",
      "epoch: 7 | 13664 / 114272 | training loss: 0.0006301810499280691\n",
      "epoch: 7 | 13696 / 114272 | training loss: 0.0008109569898806512\n",
      "epoch: 7 | 13728 / 114272 | training loss: 0.17144936323165894\n",
      "epoch: 7 | 13760 / 114272 | training loss: 0.0016784141771495342\n",
      "epoch: 7 | 13792 / 114272 | training loss: 0.19630767405033112\n",
      "epoch: 7 | 13824 / 114272 | training loss: 0.000877498066984117\n",
      "epoch: 7 | 13856 / 114272 | training loss: 0.0005284639773890376\n",
      "epoch: 7 | 13888 / 114272 | training loss: 0.004119962453842163\n",
      "epoch: 7 | 13920 / 114272 | training loss: 0.000472530402475968\n",
      "epoch: 7 | 13952 / 114272 | training loss: 0.0005759322666563094\n",
      "epoch: 7 | 13984 / 114272 | training loss: 0.000345835171174258\n",
      "epoch: 7 | 14016 / 114272 | training loss: 0.016201866790652275\n",
      "epoch: 7 | 14048 / 114272 | training loss: 0.00040701526449993253\n",
      "epoch: 7 | 14080 / 114272 | training loss: 0.0003816442913375795\n",
      "epoch: 7 | 14112 / 114272 | training loss: 0.002119858516380191\n",
      "epoch: 7 | 14144 / 114272 | training loss: 0.0005391730228438973\n",
      "epoch: 7 | 14176 / 114272 | training loss: 0.0004062744847033173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 14208 / 114272 | training loss: 0.0003921526367776096\n",
      "epoch: 7 | 14240 / 114272 | training loss: 0.000312850927002728\n",
      "epoch: 7 | 14272 / 114272 | training loss: 0.0002742551441770047\n",
      "epoch: 7 | 14304 / 114272 | training loss: 0.0005627595819532871\n",
      "epoch: 7 | 14336 / 114272 | training loss: 0.0009060717420652509\n",
      "epoch: 7 | 14368 / 114272 | training loss: 0.10860908776521683\n",
      "epoch: 7 | 14400 / 114272 | training loss: 0.0005905540310777724\n",
      "epoch: 7 | 14432 / 114272 | training loss: 0.0006322344415821135\n",
      "epoch: 7 | 14464 / 114272 | training loss: 0.0003374242514837533\n",
      "epoch: 7 | 14496 / 114272 | training loss: 0.0004664512234739959\n",
      "epoch: 7 | 14528 / 114272 | training loss: 0.0005274037830531597\n",
      "epoch: 7 | 14560 / 114272 | training loss: 0.0005416996427811682\n",
      "epoch: 7 | 14592 / 114272 | training loss: 0.0003499863960314542\n",
      "epoch: 7 | 14624 / 114272 | training loss: 0.0007989930454641581\n",
      "epoch: 7 | 14656 / 114272 | training loss: 0.0013456837041303515\n",
      "epoch: 7 | 14688 / 114272 | training loss: 0.1640770584344864\n",
      "epoch: 7 | 14720 / 114272 | training loss: 0.0003830528585240245\n",
      "epoch: 7 | 14752 / 114272 | training loss: 0.023104364052414894\n",
      "epoch: 7 | 14784 / 114272 | training loss: 0.0006197865586727858\n",
      "epoch: 7 | 14816 / 114272 | training loss: 0.0007176320068538189\n",
      "epoch: 7 | 14848 / 114272 | training loss: 0.0005848436267115176\n",
      "epoch: 7 | 14880 / 114272 | training loss: 0.00044945100671611726\n",
      "epoch: 7 | 14912 / 114272 | training loss: 0.0005039242678321898\n",
      "epoch: 7 | 14944 / 114272 | training loss: 0.000444892852101475\n",
      "epoch: 7 | 14976 / 114272 | training loss: 0.00024908152408897877\n",
      "epoch: 7 | 15008 / 114272 | training loss: 0.00036233916762284935\n",
      "epoch: 7 | 15040 / 114272 | training loss: 0.14867781102657318\n",
      "epoch: 7 | 15072 / 114272 | training loss: 0.0009565738728269935\n",
      "epoch: 7 | 15104 / 114272 | training loss: 0.000593081524129957\n",
      "epoch: 7 | 15136 / 114272 | training loss: 0.00040889999945648015\n",
      "epoch: 7 | 15168 / 114272 | training loss: 0.0005968250334262848\n",
      "epoch: 7 | 15200 / 114272 | training loss: 0.00022567085397895426\n",
      "epoch: 7 | 15232 / 114272 | training loss: 0.0012174390722066164\n",
      "epoch: 7 | 15264 / 114272 | training loss: 0.0010319175198674202\n",
      "epoch: 7 | 15296 / 114272 | training loss: 0.0007801191532053053\n",
      "epoch: 7 | 15328 / 114272 | training loss: 0.00045026341103948653\n",
      "epoch: 7 | 15360 / 114272 | training loss: 0.0039038355462253094\n",
      "epoch: 7 | 15392 / 114272 | training loss: 0.0006852094666101038\n",
      "epoch: 7 | 15424 / 114272 | training loss: 0.0008248052326962352\n",
      "epoch: 7 | 15456 / 114272 | training loss: 0.0005380597431212664\n",
      "epoch: 7 | 15488 / 114272 | training loss: 0.0013363172765821218\n",
      "epoch: 7 | 15520 / 114272 | training loss: 0.00030715178581885993\n",
      "epoch: 7 | 15552 / 114272 | training loss: 0.002338659716770053\n",
      "epoch: 7 | 15584 / 114272 | training loss: 0.0005828720750287175\n",
      "epoch: 7 | 15616 / 114272 | training loss: 0.010950300842523575\n",
      "epoch: 7 | 15648 / 114272 | training loss: 0.0009023614111356437\n",
      "epoch: 7 | 15680 / 114272 | training loss: 0.0005710713448934257\n",
      "epoch: 7 | 15712 / 114272 | training loss: 0.000465421995613724\n",
      "epoch: 7 | 15744 / 114272 | training loss: 0.001071515609510243\n",
      "epoch: 7 | 15776 / 114272 | training loss: 0.10475534945726395\n",
      "epoch: 7 | 15808 / 114272 | training loss: 0.0015147808007895947\n",
      "epoch: 7 | 15840 / 114272 | training loss: 0.0009726986172609031\n",
      "epoch: 7 | 15872 / 114272 | training loss: 0.0004711904330179095\n",
      "epoch: 7 | 15904 / 114272 | training loss: 0.0003904371988028288\n",
      "epoch: 7 | 15936 / 114272 | training loss: 0.000701446202583611\n",
      "epoch: 7 | 15968 / 114272 | training loss: 0.0002986576873809099\n",
      "epoch: 7 | 16000 / 114272 | training loss: 0.0011939241085201502\n",
      "epoch: 7 | 16032 / 114272 | training loss: 0.0016950588906183839\n",
      "epoch: 7 | 16064 / 114272 | training loss: 0.0005567562184296548\n",
      "epoch: 7 | 16096 / 114272 | training loss: 0.00032760054455138743\n",
      "epoch: 7 | 16128 / 114272 | training loss: 0.00039647810626775026\n",
      "epoch: 7 | 16160 / 114272 | training loss: 0.0004470587009564042\n",
      "epoch: 7 | 16192 / 114272 | training loss: 0.0004754039691761136\n",
      "epoch: 7 | 16224 / 114272 | training loss: 0.00035428255796432495\n",
      "epoch: 7 | 16256 / 114272 | training loss: 0.00032210408244282007\n",
      "epoch: 7 | 16288 / 114272 | training loss: 0.0007238583639264107\n",
      "epoch: 7 | 16320 / 114272 | training loss: 0.0005040843971073627\n",
      "epoch: 7 | 16352 / 114272 | training loss: 0.18464887142181396\n",
      "epoch: 7 | 16384 / 114272 | training loss: 0.0004234049702063203\n",
      "epoch: 7 | 16416 / 114272 | training loss: 0.0007561012753285468\n",
      "epoch: 7 | 16448 / 114272 | training loss: 0.0004871788260061294\n",
      "epoch: 7 | 16480 / 114272 | training loss: 0.0033025762531906366\n",
      "epoch: 7 | 16512 / 114272 | training loss: 0.0013530144933611155\n",
      "epoch: 7 | 16544 / 114272 | training loss: 0.0005502676940523088\n",
      "epoch: 7 | 16576 / 114272 | training loss: 0.13055944442749023\n",
      "epoch: 7 | 16608 / 114272 | training loss: 0.00018523828475736082\n",
      "epoch: 7 | 16640 / 114272 | training loss: 0.0003316556685604155\n",
      "epoch: 7 | 16672 / 114272 | training loss: 0.005509117618203163\n",
      "epoch: 7 | 16704 / 114272 | training loss: 0.0003623127122409642\n",
      "epoch: 7 | 16736 / 114272 | training loss: 0.0007432368001900613\n",
      "epoch: 7 | 16768 / 114272 | training loss: 0.0004090182774234563\n",
      "epoch: 7 | 16800 / 114272 | training loss: 0.0005833738832734525\n",
      "epoch: 7 | 16832 / 114272 | training loss: 0.0010701073333621025\n",
      "epoch: 7 | 16864 / 114272 | training loss: 0.0007785679772496223\n",
      "epoch: 7 | 16896 / 114272 | training loss: 0.0015022695297375321\n",
      "epoch: 7 | 16928 / 114272 | training loss: 0.0018575326539576054\n",
      "epoch: 7 | 16960 / 114272 | training loss: 0.10906051099300385\n",
      "epoch: 7 | 16992 / 114272 | training loss: 0.00044873630395159125\n",
      "epoch: 7 | 17024 / 114272 | training loss: 0.0005316204042173922\n",
      "epoch: 7 | 17056 / 114272 | training loss: 0.0016006624791771173\n",
      "epoch: 7 | 17088 / 114272 | training loss: 0.08278115838766098\n",
      "epoch: 7 | 17120 / 114272 | training loss: 0.0002984385355375707\n",
      "epoch: 7 | 17152 / 114272 | training loss: 0.0005909405299462378\n",
      "epoch: 7 | 17184 / 114272 | training loss: 0.0003730720200110227\n",
      "epoch: 7 | 17216 / 114272 | training loss: 0.0016661020927131176\n",
      "epoch: 7 | 17248 / 114272 | training loss: 0.1463114470243454\n",
      "epoch: 7 | 17280 / 114272 | training loss: 0.0009044448379427195\n",
      "epoch: 7 | 17312 / 114272 | training loss: 0.0008859722875058651\n",
      "epoch: 7 | 17344 / 114272 | training loss: 0.0007996096392162144\n",
      "epoch: 7 | 17376 / 114272 | training loss: 0.12914922833442688\n",
      "epoch: 7 | 17408 / 114272 | training loss: 0.00043006372288800776\n",
      "epoch: 7 | 17440 / 114272 | training loss: 0.0003868067869916558\n",
      "epoch: 7 | 17472 / 114272 | training loss: 0.03856562077999115\n",
      "epoch: 7 | 17504 / 114272 | training loss: 0.0004478036134969443\n",
      "epoch: 7 | 17536 / 114272 | training loss: 0.0004269426572136581\n",
      "epoch: 7 | 17568 / 114272 | training loss: 0.0019491907441988587\n",
      "epoch: 7 | 17600 / 114272 | training loss: 0.15410585701465607\n",
      "epoch: 7 | 17632 / 114272 | training loss: 0.0006912741810083389\n",
      "epoch: 7 | 17664 / 114272 | training loss: 0.0010768931824713945\n",
      "epoch: 7 | 17696 / 114272 | training loss: 0.0002262116177007556\n",
      "epoch: 7 | 17728 / 114272 | training loss: 0.00044993110350333154\n",
      "epoch: 7 | 17760 / 114272 | training loss: 0.0009231353178620338\n",
      "epoch: 7 | 17792 / 114272 | training loss: 0.00025636714417487383\n",
      "epoch: 7 | 17824 / 114272 | training loss: 0.0008949573966674507\n",
      "epoch: 7 | 17856 / 114272 | training loss: 0.0002692487614694983\n",
      "epoch: 7 | 17888 / 114272 | training loss: 0.00043267488945275545\n",
      "epoch: 7 | 17920 / 114272 | training loss: 0.00019486398377921432\n",
      "epoch: 7 | 17952 / 114272 | training loss: 0.00029736763099208474\n",
      "epoch: 7 | 17984 / 114272 | training loss: 0.00042104031308554113\n",
      "epoch: 7 | 18016 / 114272 | training loss: 0.1371549367904663\n",
      "epoch: 7 | 18048 / 114272 | training loss: 0.000488892022985965\n",
      "epoch: 7 | 18080 / 114272 | training loss: 0.0007633534842170775\n",
      "epoch: 7 | 18112 / 114272 | training loss: 0.007616627030074596\n",
      "epoch: 7 | 18144 / 114272 | training loss: 0.00019874637655448169\n",
      "epoch: 7 | 18176 / 114272 | training loss: 0.16780634224414825\n",
      "epoch: 7 | 18208 / 114272 | training loss: 0.0013743712333962321\n",
      "epoch: 7 | 18240 / 114272 | training loss: 0.0007525262772105634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 18272 / 114272 | training loss: 0.0006884171743877232\n",
      "epoch: 7 | 18304 / 114272 | training loss: 0.0002947855682577938\n",
      "epoch: 7 | 18336 / 114272 | training loss: 0.0018160321051254869\n",
      "epoch: 7 | 18368 / 114272 | training loss: 0.0017054449999704957\n",
      "epoch: 7 | 18400 / 114272 | training loss: 0.0005764236557297409\n",
      "epoch: 7 | 18432 / 114272 | training loss: 0.00032283185282722116\n",
      "epoch: 7 | 18464 / 114272 | training loss: 0.000683845893945545\n",
      "epoch: 7 | 18496 / 114272 | training loss: 0.0012063976610079408\n",
      "epoch: 7 | 18528 / 114272 | training loss: 0.0004979255609214306\n",
      "epoch: 7 | 18560 / 114272 | training loss: 0.0004077896592207253\n",
      "epoch: 7 | 18592 / 114272 | training loss: 0.00024387790472246706\n",
      "epoch: 7 | 18624 / 114272 | training loss: 0.0017646882915869355\n",
      "epoch: 7 | 18656 / 114272 | training loss: 0.0009683581301942468\n",
      "epoch: 7 | 18688 / 114272 | training loss: 0.025818713009357452\n",
      "epoch: 7 | 18720 / 114272 | training loss: 0.0022969890851527452\n",
      "epoch: 7 | 18752 / 114272 | training loss: 0.000906255270820111\n",
      "epoch: 7 | 18784 / 114272 | training loss: 0.00021758562070317566\n",
      "epoch: 7 | 18816 / 114272 | training loss: 0.0019788455683737993\n",
      "epoch: 7 | 18848 / 114272 | training loss: 0.0008761903154663742\n",
      "epoch: 7 | 18880 / 114272 | training loss: 0.0003065051860176027\n",
      "epoch: 7 | 18912 / 114272 | training loss: 0.00036650491529144347\n",
      "epoch: 7 | 18944 / 114272 | training loss: 0.0008767503313720226\n",
      "epoch: 7 | 18976 / 114272 | training loss: 0.0033757996279746294\n",
      "epoch: 7 | 19008 / 114272 | training loss: 0.0002475878281984478\n",
      "epoch: 7 | 19040 / 114272 | training loss: 0.0003483547188807279\n",
      "epoch: 7 | 19072 / 114272 | training loss: 0.0005315567832440138\n",
      "epoch: 7 | 19104 / 114272 | training loss: 0.0005587196792475879\n",
      "epoch: 7 | 19136 / 114272 | training loss: 0.0013085026293992996\n",
      "epoch: 7 | 19168 / 114272 | training loss: 0.00032261013984680176\n",
      "epoch: 7 | 19200 / 114272 | training loss: 0.0012563880300149322\n",
      "epoch: 7 | 19232 / 114272 | training loss: 0.0003664208925329149\n",
      "epoch: 7 | 19264 / 114272 | training loss: 0.0008125157910399139\n",
      "epoch: 7 | 19296 / 114272 | training loss: 0.0003222524537704885\n",
      "epoch: 7 | 19328 / 114272 | training loss: 0.0006463428144343197\n",
      "epoch: 7 | 19360 / 114272 | training loss: 0.16363845765590668\n",
      "epoch: 7 | 19392 / 114272 | training loss: 0.08383937925100327\n",
      "epoch: 7 | 19424 / 114272 | training loss: 0.0011185858165845275\n",
      "epoch: 7 | 19456 / 114272 | training loss: 0.0007845030049793422\n",
      "epoch: 7 | 19488 / 114272 | training loss: 0.0003724222769960761\n",
      "epoch: 7 | 19520 / 114272 | training loss: 0.00048778357449918985\n",
      "epoch: 7 | 19552 / 114272 | training loss: 0.0008847991703078151\n",
      "epoch: 7 | 19584 / 114272 | training loss: 0.0002821567468345165\n",
      "epoch: 7 | 19616 / 114272 | training loss: 0.0003094622807111591\n",
      "epoch: 7 | 19648 / 114272 | training loss: 0.0003294378111604601\n",
      "epoch: 7 | 19680 / 114272 | training loss: 0.0004640868864953518\n",
      "epoch: 7 | 19712 / 114272 | training loss: 0.0006169332773424685\n",
      "epoch: 7 | 19744 / 114272 | training loss: 0.00028553532320074737\n",
      "epoch: 7 | 19776 / 114272 | training loss: 0.0004128418513573706\n",
      "epoch: 7 | 19808 / 114272 | training loss: 0.001101558213122189\n",
      "epoch: 7 | 19840 / 114272 | training loss: 0.0007219882099889219\n",
      "epoch: 7 | 19872 / 114272 | training loss: 0.0010463758371770382\n",
      "epoch: 7 | 19904 / 114272 | training loss: 0.0003161395725328475\n",
      "epoch: 7 | 19936 / 114272 | training loss: 0.00043281688704155385\n",
      "epoch: 7 | 19968 / 114272 | training loss: 0.0007655160152353346\n",
      "epoch: 7 | 20000 / 114272 | training loss: 0.00032007909612730145\n",
      "epoch: 7 | 20032 / 114272 | training loss: 0.0003359482216183096\n",
      "epoch: 7 | 20064 / 114272 | training loss: 0.000244251627009362\n",
      "epoch: 7 | 20096 / 114272 | training loss: 0.0004565215203911066\n",
      "epoch: 7 | 20128 / 114272 | training loss: 0.00041131622856482863\n",
      "epoch: 7 | 20160 / 114272 | training loss: 0.00038124137790873647\n",
      "epoch: 7 | 20192 / 114272 | training loss: 0.00037754414370283484\n",
      "epoch: 7 | 20224 / 114272 | training loss: 0.00015130524116102606\n",
      "epoch: 7 | 20256 / 114272 | training loss: 0.0017388304695487022\n",
      "epoch: 7 | 20288 / 114272 | training loss: 0.000325566332321614\n",
      "epoch: 7 | 20320 / 114272 | training loss: 0.00037000374868512154\n",
      "epoch: 7 | 20352 / 114272 | training loss: 0.00018150682444684207\n",
      "epoch: 7 | 20384 / 114272 | training loss: 0.00018076978449244052\n",
      "epoch: 7 | 20416 / 114272 | training loss: 0.0003015651891473681\n",
      "epoch: 7 | 20448 / 114272 | training loss: 0.0010111830197274685\n",
      "epoch: 7 | 20480 / 114272 | training loss: 0.15771180391311646\n",
      "epoch: 7 | 20512 / 114272 | training loss: 0.0002755311143118888\n",
      "epoch: 7 | 20544 / 114272 | training loss: 0.000321842118864879\n",
      "epoch: 7 | 20576 / 114272 | training loss: 0.0002937091630883515\n",
      "epoch: 7 | 20608 / 114272 | training loss: 0.0006007088231854141\n",
      "epoch: 7 | 20640 / 114272 | training loss: 0.0001348809018963948\n",
      "epoch: 7 | 20672 / 114272 | training loss: 0.007175218779593706\n",
      "epoch: 7 | 20704 / 114272 | training loss: 0.07512157410383224\n",
      "epoch: 7 | 20736 / 114272 | training loss: 0.0686848983168602\n",
      "epoch: 7 | 20768 / 114272 | training loss: 0.0007753235404379666\n",
      "epoch: 7 | 20800 / 114272 | training loss: 0.000842635752633214\n",
      "epoch: 7 | 20832 / 114272 | training loss: 0.00023110823531169444\n",
      "epoch: 7 | 20864 / 114272 | training loss: 0.0006157935713417828\n",
      "epoch: 7 | 20896 / 114272 | training loss: 0.00530769070610404\n",
      "epoch: 7 | 20928 / 114272 | training loss: 0.1764923334121704\n",
      "epoch: 7 | 20960 / 114272 | training loss: 0.0004272450169082731\n",
      "epoch: 7 | 20992 / 114272 | training loss: 0.0006314723286777735\n",
      "epoch: 7 | 21024 / 114272 | training loss: 0.0008460943936370313\n",
      "epoch: 7 | 21056 / 114272 | training loss: 0.0007202640408650041\n",
      "epoch: 7 | 21088 / 114272 | training loss: 0.00037924450589343905\n",
      "epoch: 7 | 21120 / 114272 | training loss: 0.0012699203798547387\n",
      "epoch: 7 | 21152 / 114272 | training loss: 0.000428685947554186\n",
      "epoch: 7 | 21184 / 114272 | training loss: 0.0003054479311686009\n",
      "epoch: 7 | 21216 / 114272 | training loss: 0.0009747022413648665\n",
      "epoch: 7 | 21248 / 114272 | training loss: 0.0007369712693616748\n",
      "epoch: 7 | 21280 / 114272 | training loss: 0.0011541798012331128\n",
      "epoch: 7 | 21312 / 114272 | training loss: 0.0005197416758164763\n",
      "epoch: 7 | 21344 / 114272 | training loss: 0.0002688104286789894\n",
      "epoch: 7 | 21376 / 114272 | training loss: 0.11431580036878586\n",
      "epoch: 7 | 21408 / 114272 | training loss: 0.0002517214452382177\n",
      "epoch: 7 | 21440 / 114272 | training loss: 0.00033416174119338393\n",
      "epoch: 7 | 21472 / 114272 | training loss: 0.00025114230811595917\n",
      "epoch: 7 | 21504 / 114272 | training loss: 0.0004056401667185128\n",
      "epoch: 7 | 21536 / 114272 | training loss: 0.00027087723719887435\n",
      "epoch: 7 | 21568 / 114272 | training loss: 0.0005223889020271599\n",
      "epoch: 7 | 21600 / 114272 | training loss: 0.00202727597206831\n",
      "epoch: 7 | 21632 / 114272 | training loss: 0.0009513135300949216\n",
      "epoch: 7 | 21664 / 114272 | training loss: 0.00024138667504303157\n",
      "epoch: 7 | 21696 / 114272 | training loss: 0.13192158937454224\n",
      "epoch: 7 | 21728 / 114272 | training loss: 0.0003406277683097869\n",
      "epoch: 7 | 21760 / 114272 | training loss: 0.0007054868037812412\n",
      "epoch: 7 | 21792 / 114272 | training loss: 0.0012973933480679989\n",
      "epoch: 7 | 21824 / 114272 | training loss: 0.0014047477161511779\n",
      "epoch: 7 | 21856 / 114272 | training loss: 0.0003253627219237387\n",
      "epoch: 7 | 21888 / 114272 | training loss: 0.0007012462010607123\n",
      "epoch: 7 | 21920 / 114272 | training loss: 0.0002649944508448243\n",
      "epoch: 7 | 21952 / 114272 | training loss: 0.0002600269508548081\n",
      "epoch: 7 | 21984 / 114272 | training loss: 0.06490763276815414\n",
      "epoch: 7 | 22016 / 114272 | training loss: 0.0004955335753038526\n",
      "epoch: 7 | 22048 / 114272 | training loss: 0.0004764201585203409\n",
      "epoch: 7 | 22080 / 114272 | training loss: 0.000575084937736392\n",
      "epoch: 7 | 22112 / 114272 | training loss: 0.0003100750036537647\n",
      "epoch: 7 | 22144 / 114272 | training loss: 0.0002524901065044105\n",
      "epoch: 7 | 22176 / 114272 | training loss: 0.07554568350315094\n",
      "epoch: 7 | 22208 / 114272 | training loss: 0.0009088669321499765\n",
      "epoch: 7 | 22240 / 114272 | training loss: 0.0002517472021281719\n",
      "epoch: 7 | 22272 / 114272 | training loss: 0.0004310799122322351\n",
      "epoch: 7 | 22304 / 114272 | training loss: 0.00047654996160417795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 22336 / 114272 | training loss: 0.0006448549684137106\n",
      "epoch: 7 | 22368 / 114272 | training loss: 0.0008326524985022843\n",
      "epoch: 7 | 22400 / 114272 | training loss: 0.0003205661487299949\n",
      "epoch: 7 | 22432 / 114272 | training loss: 0.0003125338989775628\n",
      "epoch: 7 | 22464 / 114272 | training loss: 0.00028386173653416336\n",
      "epoch: 7 | 22496 / 114272 | training loss: 0.0003418874985072762\n",
      "epoch: 7 | 22528 / 114272 | training loss: 0.00023569610493723303\n",
      "epoch: 7 | 22560 / 114272 | training loss: 0.0010046787792816758\n",
      "epoch: 7 | 22592 / 114272 | training loss: 0.0003186524554621428\n",
      "epoch: 7 | 22624 / 114272 | training loss: 0.00027382871485315263\n",
      "epoch: 7 | 22656 / 114272 | training loss: 0.0005585545441135764\n",
      "epoch: 7 | 22688 / 114272 | training loss: 0.009113066829741001\n",
      "epoch: 7 | 22720 / 114272 | training loss: 0.00019973100279457867\n",
      "epoch: 7 | 22752 / 114272 | training loss: 0.033276528120040894\n",
      "epoch: 7 | 22784 / 114272 | training loss: 0.00039287144318223\n",
      "epoch: 7 | 22816 / 114272 | training loss: 0.00026561005506664515\n",
      "epoch: 7 | 22848 / 114272 | training loss: 0.00023363513173535466\n",
      "epoch: 7 | 22880 / 114272 | training loss: 0.0003448448551353067\n",
      "epoch: 7 | 22912 / 114272 | training loss: 0.00035977250081487\n",
      "epoch: 7 | 22944 / 114272 | training loss: 0.0004086577973794192\n",
      "epoch: 7 | 22976 / 114272 | training loss: 0.00029419720522128046\n",
      "epoch: 7 | 23008 / 114272 | training loss: 0.0004012904246337712\n",
      "epoch: 7 | 23040 / 114272 | training loss: 0.00039631486288271844\n",
      "epoch: 7 | 23072 / 114272 | training loss: 0.00034182198578491807\n",
      "epoch: 7 | 23104 / 114272 | training loss: 0.00013954288442619145\n",
      "epoch: 7 | 23136 / 114272 | training loss: 0.001377000822685659\n",
      "epoch: 7 | 23168 / 114272 | training loss: 0.0005847017164342105\n",
      "epoch: 7 | 23200 / 114272 | training loss: 0.000368407549103722\n",
      "epoch: 7 | 23232 / 114272 | training loss: 0.0005182444583624601\n",
      "epoch: 7 | 23264 / 114272 | training loss: 0.00039286204264499247\n",
      "epoch: 7 | 23296 / 114272 | training loss: 0.00020959156972821802\n",
      "epoch: 7 | 23328 / 114272 | training loss: 0.00023919901286717504\n",
      "epoch: 7 | 23360 / 114272 | training loss: 0.0003071859246119857\n",
      "epoch: 7 | 23392 / 114272 | training loss: 0.00016579551447648555\n",
      "epoch: 7 | 23424 / 114272 | training loss: 0.0001341848255833611\n",
      "epoch: 7 | 23456 / 114272 | training loss: 0.00020444352412596345\n",
      "epoch: 7 | 23488 / 114272 | training loss: 0.007213344797492027\n",
      "epoch: 7 | 23520 / 114272 | training loss: 0.0003883370663970709\n",
      "epoch: 7 | 23552 / 114272 | training loss: 0.00017144645971711725\n",
      "epoch: 7 | 23584 / 114272 | training loss: 0.17615726590156555\n",
      "epoch: 7 | 23616 / 114272 | training loss: 0.00029569488833658397\n",
      "epoch: 7 | 23648 / 114272 | training loss: 0.004121709614992142\n",
      "epoch: 7 | 23680 / 114272 | training loss: 0.00026233517564833164\n",
      "epoch: 7 | 23712 / 114272 | training loss: 0.0004618968232534826\n",
      "epoch: 7 | 23744 / 114272 | training loss: 0.0002589271462056786\n",
      "epoch: 7 | 23776 / 114272 | training loss: 0.00037739475374110043\n",
      "epoch: 7 | 23808 / 114272 | training loss: 0.00031182041857391596\n",
      "epoch: 7 | 23840 / 114272 | training loss: 0.00020570505876094103\n",
      "epoch: 7 | 23872 / 114272 | training loss: 0.000362234452040866\n",
      "epoch: 7 | 23904 / 114272 | training loss: 0.0010067205876111984\n",
      "epoch: 7 | 23936 / 114272 | training loss: 0.0005048023886047304\n",
      "epoch: 7 | 23968 / 114272 | training loss: 0.20863300561904907\n",
      "epoch: 7 | 24000 / 114272 | training loss: 0.0009486065828241408\n",
      "epoch: 7 | 24032 / 114272 | training loss: 0.0004988427390344441\n",
      "epoch: 7 | 24064 / 114272 | training loss: 0.0002765539684332907\n",
      "epoch: 7 | 24096 / 114272 | training loss: 0.0012639096239581704\n",
      "epoch: 7 | 24128 / 114272 | training loss: 0.0004227223398629576\n",
      "epoch: 7 | 24160 / 114272 | training loss: 0.00016844506899360567\n",
      "epoch: 7 | 24192 / 114272 | training loss: 0.0008138819830492139\n",
      "epoch: 7 | 24224 / 114272 | training loss: 0.0007859859033487737\n",
      "epoch: 7 | 24256 / 114272 | training loss: 0.001299906289204955\n",
      "epoch: 7 | 24288 / 114272 | training loss: 0.0002518268011044711\n",
      "epoch: 7 | 24320 / 114272 | training loss: 0.0009753684280440211\n",
      "epoch: 7 | 24352 / 114272 | training loss: 0.0006038158317096531\n",
      "epoch: 7 | 24384 / 114272 | training loss: 0.0009991317056119442\n",
      "epoch: 7 | 24416 / 114272 | training loss: 0.0004297983832657337\n",
      "epoch: 7 | 24448 / 114272 | training loss: 0.00034578359918668866\n",
      "epoch: 7 | 24480 / 114272 | training loss: 0.00027458302793093026\n",
      "epoch: 7 | 24512 / 114272 | training loss: 0.00046015315456315875\n",
      "epoch: 7 | 24544 / 114272 | training loss: 0.00035727862268686295\n",
      "epoch: 7 | 24576 / 114272 | training loss: 0.0002714704896789044\n",
      "epoch: 7 | 24608 / 114272 | training loss: 0.0008115108357742429\n",
      "epoch: 7 | 24640 / 114272 | training loss: 0.0003410136851016432\n",
      "epoch: 7 | 24672 / 114272 | training loss: 0.0003355553781148046\n",
      "epoch: 7 | 24704 / 114272 | training loss: 0.0002495305670890957\n",
      "epoch: 7 | 24736 / 114272 | training loss: 0.000528797332663089\n",
      "epoch: 7 | 24768 / 114272 | training loss: 0.20817534625530243\n",
      "epoch: 7 | 24800 / 114272 | training loss: 0.0002978217671625316\n",
      "epoch: 7 | 24832 / 114272 | training loss: 0.00026662665186449885\n",
      "epoch: 7 | 24864 / 114272 | training loss: 0.00030208349926397204\n",
      "epoch: 7 | 24896 / 114272 | training loss: 0.0009602003847248852\n",
      "epoch: 7 | 24928 / 114272 | training loss: 0.0017255558632314205\n",
      "epoch: 7 | 24960 / 114272 | training loss: 0.0006713608745485544\n",
      "epoch: 7 | 24992 / 114272 | training loss: 0.00017043274419847876\n",
      "epoch: 7 | 25024 / 114272 | training loss: 0.00036622275365516543\n",
      "epoch: 7 | 25056 / 114272 | training loss: 0.0006560338661074638\n",
      "epoch: 7 | 25088 / 114272 | training loss: 0.0008236708235926926\n",
      "epoch: 7 | 25120 / 114272 | training loss: 0.0002195285342168063\n",
      "epoch: 7 | 25152 / 114272 | training loss: 0.05700388178229332\n",
      "epoch: 7 | 25184 / 114272 | training loss: 0.0005836284253746271\n",
      "epoch: 7 | 25216 / 114272 | training loss: 0.0009853040101006627\n",
      "epoch: 7 | 25248 / 114272 | training loss: 0.00034947189851664007\n",
      "epoch: 7 | 25280 / 114272 | training loss: 0.0004506820405367762\n",
      "epoch: 7 | 25312 / 114272 | training loss: 0.00021453625231515616\n",
      "epoch: 7 | 25344 / 114272 | training loss: 0.00013380944437813014\n",
      "epoch: 7 | 25376 / 114272 | training loss: 0.000508668425027281\n",
      "epoch: 7 | 25408 / 114272 | training loss: 0.0003893750545103103\n",
      "epoch: 7 | 25440 / 114272 | training loss: 0.0005903224227949977\n",
      "epoch: 7 | 25472 / 114272 | training loss: 0.0003067069628741592\n",
      "epoch: 7 | 25504 / 114272 | training loss: 0.00041500339284539223\n",
      "epoch: 7 | 25536 / 114272 | training loss: 0.00043844885658472776\n",
      "epoch: 7 | 25568 / 114272 | training loss: 0.00016580958617851138\n",
      "epoch: 7 | 25600 / 114272 | training loss: 0.00036426333826966584\n",
      "epoch: 7 | 25632 / 114272 | training loss: 0.0003155082231387496\n",
      "epoch: 7 | 25664 / 114272 | training loss: 0.0002632631512824446\n",
      "epoch: 7 | 25696 / 114272 | training loss: 0.00045522436266765\n",
      "epoch: 7 | 25728 / 114272 | training loss: 0.00025143640232272446\n",
      "epoch: 7 | 25760 / 114272 | training loss: 0.00034986698301509023\n",
      "epoch: 7 | 25792 / 114272 | training loss: 0.0002496443048585206\n",
      "epoch: 7 | 25824 / 114272 | training loss: 0.000370117777492851\n",
      "epoch: 7 | 25856 / 114272 | training loss: 0.0007330001099035144\n",
      "epoch: 7 | 25888 / 114272 | training loss: 0.012349594384431839\n",
      "epoch: 7 | 25920 / 114272 | training loss: 0.0003717513754963875\n",
      "epoch: 7 | 25952 / 114272 | training loss: 0.00047201712732203305\n",
      "epoch: 7 | 25984 / 114272 | training loss: 0.0011929728789255023\n",
      "epoch: 7 | 26016 / 114272 | training loss: 0.000432392320362851\n",
      "epoch: 7 | 26048 / 114272 | training loss: 0.16348734498023987\n",
      "epoch: 7 | 26080 / 114272 | training loss: 0.0005498878308571875\n",
      "epoch: 7 | 26112 / 114272 | training loss: 0.00030632049310952425\n",
      "epoch: 7 | 26144 / 114272 | training loss: 0.00030389364110305905\n",
      "epoch: 7 | 26176 / 114272 | training loss: 0.20653171837329865\n",
      "epoch: 7 | 26208 / 114272 | training loss: 0.001438899664208293\n",
      "epoch: 7 | 26240 / 114272 | training loss: 0.0002241628972114995\n",
      "epoch: 7 | 26272 / 114272 | training loss: 0.09682492166757584\n",
      "epoch: 7 | 26304 / 114272 | training loss: 0.0004344868939369917\n",
      "epoch: 7 | 26336 / 114272 | training loss: 0.00028705812292173505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 26368 / 114272 | training loss: 0.00032327204826287925\n",
      "epoch: 7 | 26400 / 114272 | training loss: 0.00044202248682267964\n",
      "epoch: 7 | 26432 / 114272 | training loss: 0.11222298443317413\n",
      "epoch: 7 | 26464 / 114272 | training loss: 0.0005736980237998068\n",
      "epoch: 7 | 26496 / 114272 | training loss: 0.19233623147010803\n",
      "epoch: 7 | 26528 / 114272 | training loss: 0.0003781793639063835\n",
      "epoch: 7 | 26560 / 114272 | training loss: 0.0007086404366418719\n",
      "epoch: 7 | 26592 / 114272 | training loss: 0.05343092978000641\n",
      "epoch: 7 | 26624 / 114272 | training loss: 0.00046929981908760965\n",
      "epoch: 7 | 26656 / 114272 | training loss: 0.0007206912850961089\n",
      "epoch: 7 | 26688 / 114272 | training loss: 0.006301507819443941\n",
      "epoch: 7 | 26720 / 114272 | training loss: 0.0003230760630685836\n",
      "epoch: 7 | 26752 / 114272 | training loss: 0.0016048842808231711\n",
      "epoch: 7 | 26784 / 114272 | training loss: 0.0005339135532267392\n",
      "epoch: 7 | 26816 / 114272 | training loss: 0.0002766139223240316\n",
      "epoch: 7 | 26848 / 114272 | training loss: 0.0015254199970513582\n",
      "epoch: 7 | 26880 / 114272 | training loss: 0.0006093240226618946\n",
      "epoch: 7 | 26912 / 114272 | training loss: 0.00034402156597934663\n",
      "epoch: 7 | 26944 / 114272 | training loss: 0.0008774411980994046\n",
      "epoch: 7 | 26976 / 114272 | training loss: 0.0002624750486575067\n",
      "epoch: 7 | 27008 / 114272 | training loss: 0.0008752115536481142\n",
      "epoch: 7 | 27040 / 114272 | training loss: 0.0008164477767422795\n",
      "epoch: 7 | 27072 / 114272 | training loss: 0.0003842213482130319\n",
      "epoch: 7 | 27104 / 114272 | training loss: 0.0008273014100268483\n",
      "epoch: 7 | 27136 / 114272 | training loss: 0.0007247698376886547\n",
      "epoch: 7 | 27168 / 114272 | training loss: 0.0011840024963021278\n",
      "epoch: 7 | 27200 / 114272 | training loss: 0.0007410207763314247\n",
      "epoch: 7 | 27232 / 114272 | training loss: 0.0010546878911554813\n",
      "epoch: 7 | 27264 / 114272 | training loss: 0.0007314385147765279\n",
      "epoch: 7 | 27296 / 114272 | training loss: 0.0037173815071582794\n",
      "epoch: 7 | 27328 / 114272 | training loss: 0.0012583112111315131\n",
      "epoch: 7 | 27360 / 114272 | training loss: 0.0007163832779042423\n",
      "epoch: 7 | 27392 / 114272 | training loss: 0.0008367038099095225\n",
      "epoch: 7 | 27424 / 114272 | training loss: 0.07482977956533432\n",
      "epoch: 7 | 27456 / 114272 | training loss: 0.0006253428291529417\n",
      "epoch: 7 | 27488 / 114272 | training loss: 0.0005676726577803493\n",
      "epoch: 7 | 27520 / 114272 | training loss: 0.0011780833592638373\n",
      "epoch: 7 | 27552 / 114272 | training loss: 0.0003030349616892636\n",
      "epoch: 7 | 27584 / 114272 | training loss: 0.0007247598259709775\n",
      "epoch: 7 | 27616 / 114272 | training loss: 0.000447659200290218\n",
      "epoch: 7 | 27648 / 114272 | training loss: 0.0008148952038027346\n",
      "epoch: 7 | 27680 / 114272 | training loss: 0.0008838088833726943\n",
      "epoch: 7 | 27712 / 114272 | training loss: 0.000988704850897193\n",
      "epoch: 7 | 27744 / 114272 | training loss: 0.0010595213389024138\n",
      "epoch: 7 | 27776 / 114272 | training loss: 0.07025362551212311\n",
      "epoch: 7 | 27808 / 114272 | training loss: 0.0005269415560178459\n",
      "epoch: 7 | 27840 / 114272 | training loss: 0.00039423193084076047\n",
      "epoch: 7 | 27872 / 114272 | training loss: 0.0006718167569488287\n",
      "epoch: 7 | 27904 / 114272 | training loss: 0.0009560534963384271\n",
      "epoch: 7 | 27936 / 114272 | training loss: 0.0007014261791482568\n",
      "epoch: 7 | 27968 / 114272 | training loss: 0.0007193320780061185\n",
      "epoch: 7 | 28000 / 114272 | training loss: 0.0004613379423972219\n",
      "epoch: 7 | 28032 / 114272 | training loss: 0.0004584390844684094\n",
      "epoch: 7 | 28064 / 114272 | training loss: 0.005473633296787739\n",
      "epoch: 7 | 28096 / 114272 | training loss: 0.0003240826481487602\n",
      "epoch: 7 | 28128 / 114272 | training loss: 0.002000224543735385\n",
      "epoch: 7 | 28160 / 114272 | training loss: 0.0002688467502593994\n",
      "epoch: 7 | 28192 / 114272 | training loss: 0.0005394216277636588\n",
      "epoch: 7 | 28224 / 114272 | training loss: 0.0021236492320895195\n",
      "epoch: 7 | 28256 / 114272 | training loss: 0.0007012586575001478\n",
      "epoch: 7 | 28288 / 114272 | training loss: 0.0003988327516708523\n",
      "epoch: 7 | 28320 / 114272 | training loss: 0.0006595261511392891\n",
      "epoch: 7 | 28352 / 114272 | training loss: 0.0007484359666705132\n",
      "epoch: 7 | 28384 / 114272 | training loss: 0.0018157117301598191\n",
      "epoch: 7 | 28416 / 114272 | training loss: 0.0003958934685215354\n",
      "epoch: 7 | 28448 / 114272 | training loss: 0.0003927277575712651\n",
      "epoch: 7 | 28480 / 114272 | training loss: 0.0003947290824726224\n",
      "epoch: 7 | 28512 / 114272 | training loss: 0.00016293814405798912\n",
      "epoch: 7 | 28544 / 114272 | training loss: 0.0006626049289479852\n",
      "epoch: 7 | 28576 / 114272 | training loss: 0.0003386397147551179\n",
      "epoch: 7 | 28608 / 114272 | training loss: 0.0010054728481918573\n",
      "epoch: 7 | 28640 / 114272 | training loss: 0.00048549476196058095\n",
      "epoch: 7 | 28672 / 114272 | training loss: 0.00041905854595825076\n",
      "epoch: 7 | 28704 / 114272 | training loss: 0.0001876487658591941\n",
      "epoch: 7 | 28736 / 114272 | training loss: 0.0004077900666743517\n",
      "epoch: 7 | 28768 / 114272 | training loss: 0.0017229472286999226\n",
      "epoch: 7 | 28800 / 114272 | training loss: 0.00048638967564329505\n",
      "epoch: 7 | 28832 / 114272 | training loss: 0.17154575884342194\n",
      "epoch: 7 | 28864 / 114272 | training loss: 0.0003311052860226482\n",
      "epoch: 7 | 28896 / 114272 | training loss: 0.02583559788763523\n",
      "epoch: 7 | 28928 / 114272 | training loss: 0.0004464763624127954\n",
      "epoch: 7 | 28960 / 114272 | training loss: 0.0013637070078402758\n",
      "epoch: 7 | 28992 / 114272 | training loss: 0.00021223787916824222\n",
      "epoch: 7 | 29024 / 114272 | training loss: 0.00043901638127863407\n",
      "epoch: 7 | 29056 / 114272 | training loss: 0.0002748215920291841\n",
      "epoch: 7 | 29088 / 114272 | training loss: 0.00032726424979045987\n",
      "epoch: 7 | 29120 / 114272 | training loss: 0.0023558216635137796\n",
      "epoch: 7 | 29152 / 114272 | training loss: 0.0002806582779157907\n",
      "epoch: 7 | 29184 / 114272 | training loss: 0.0004455069429241121\n",
      "epoch: 7 | 29216 / 114272 | training loss: 0.0005346962134353817\n",
      "epoch: 7 | 29248 / 114272 | training loss: 0.0004897791659459472\n",
      "epoch: 7 | 29280 / 114272 | training loss: 0.00029572643688879907\n",
      "epoch: 7 | 29312 / 114272 | training loss: 0.0003517679579090327\n",
      "epoch: 7 | 29344 / 114272 | training loss: 0.0003326850710436702\n",
      "epoch: 7 | 29376 / 114272 | training loss: 0.0004493969026952982\n",
      "epoch: 7 | 29408 / 114272 | training loss: 0.00015342776896432042\n",
      "epoch: 7 | 29440 / 114272 | training loss: 0.00022064376389607787\n",
      "epoch: 7 | 29472 / 114272 | training loss: 0.0002404538681730628\n",
      "epoch: 7 | 29504 / 114272 | training loss: 0.000391463894629851\n",
      "epoch: 7 | 29536 / 114272 | training loss: 0.010522529482841492\n",
      "epoch: 7 | 29568 / 114272 | training loss: 0.0002872111217584461\n",
      "epoch: 7 | 29600 / 114272 | training loss: 0.06712570041418076\n",
      "epoch: 7 | 29632 / 114272 | training loss: 0.02988981083035469\n",
      "epoch: 7 | 29664 / 114272 | training loss: 0.00021612073760479689\n",
      "epoch: 7 | 29696 / 114272 | training loss: 0.00013894074072595686\n",
      "epoch: 7 | 29728 / 114272 | training loss: 0.021344894543290138\n",
      "epoch: 7 | 29760 / 114272 | training loss: 0.00044380969484336674\n",
      "epoch: 7 | 29792 / 114272 | training loss: 0.0003423060697969049\n",
      "epoch: 7 | 29824 / 114272 | training loss: 0.00023509428137913346\n",
      "epoch: 7 | 29856 / 114272 | training loss: 0.0007238088874146342\n",
      "epoch: 7 | 29888 / 114272 | training loss: 0.000301398744340986\n",
      "epoch: 7 | 29920 / 114272 | training loss: 0.0006203750963322818\n",
      "epoch: 7 | 29952 / 114272 | training loss: 0.000557627179659903\n",
      "epoch: 7 | 29984 / 114272 | training loss: 0.00042229192331433296\n",
      "epoch: 7 | 30016 / 114272 | training loss: 0.00032767237280495465\n",
      "epoch: 7 | 30048 / 114272 | training loss: 0.00046637901687063277\n",
      "epoch: 7 | 30080 / 114272 | training loss: 0.0002422728284727782\n",
      "epoch: 7 | 30112 / 114272 | training loss: 0.00038009718991816044\n",
      "epoch: 7 | 30144 / 114272 | training loss: 0.0006432965165004134\n",
      "epoch: 7 | 30176 / 114272 | training loss: 0.00038731074891984463\n",
      "epoch: 7 | 30208 / 114272 | training loss: 0.000418432813603431\n",
      "epoch: 7 | 30240 / 114272 | training loss: 0.0003523687773849815\n",
      "epoch: 7 | 30272 / 114272 | training loss: 0.08165743201971054\n",
      "epoch: 7 | 30304 / 114272 | training loss: 0.00042925990419462323\n",
      "epoch: 7 | 30336 / 114272 | training loss: 0.00014757375174667686\n",
      "epoch: 7 | 30368 / 114272 | training loss: 0.00016564999532420188\n",
      "epoch: 7 | 30400 / 114272 | training loss: 0.00023880755179561675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 30432 / 114272 | training loss: 0.00029224989702925086\n",
      "epoch: 7 | 30464 / 114272 | training loss: 0.0004392830596771091\n",
      "epoch: 7 | 30496 / 114272 | training loss: 0.0004103598475921899\n",
      "epoch: 7 | 30528 / 114272 | training loss: 0.0002658897137735039\n",
      "epoch: 7 | 30560 / 114272 | training loss: 0.0001881378557300195\n",
      "epoch: 7 | 30592 / 114272 | training loss: 0.24473415315151215\n",
      "epoch: 7 | 30624 / 114272 | training loss: 0.0005133458762429655\n",
      "epoch: 7 | 30656 / 114272 | training loss: 0.0004118768556509167\n",
      "epoch: 7 | 30688 / 114272 | training loss: 0.00017193119856528938\n",
      "epoch: 7 | 30720 / 114272 | training loss: 0.0004723167512565851\n",
      "epoch: 7 | 30752 / 114272 | training loss: 0.00018951766833197325\n",
      "epoch: 7 | 30784 / 114272 | training loss: 0.0006360633415170014\n",
      "epoch: 7 | 30816 / 114272 | training loss: 0.00041423217044211924\n",
      "epoch: 7 | 30848 / 114272 | training loss: 0.00023424357641488314\n",
      "epoch: 7 | 30880 / 114272 | training loss: 0.00021242565708234906\n",
      "epoch: 7 | 30912 / 114272 | training loss: 0.00028087638202123344\n",
      "epoch: 7 | 30944 / 114272 | training loss: 0.0004040650965180248\n",
      "epoch: 7 | 30976 / 114272 | training loss: 0.0005528502515517175\n",
      "epoch: 7 | 31008 / 114272 | training loss: 0.00033427393645979464\n",
      "epoch: 7 | 31040 / 114272 | training loss: 0.00029604212613776326\n",
      "epoch: 7 | 31072 / 114272 | training loss: 0.000310324365273118\n",
      "epoch: 7 | 31104 / 114272 | training loss: 0.00031836042762733996\n",
      "epoch: 7 | 31136 / 114272 | training loss: 0.00032313138945028186\n",
      "epoch: 7 | 31168 / 114272 | training loss: 0.0005565419560298324\n",
      "epoch: 7 | 31200 / 114272 | training loss: 0.0002591016818769276\n",
      "epoch: 7 | 31232 / 114272 | training loss: 0.0007256220560520887\n",
      "epoch: 7 | 31264 / 114272 | training loss: 0.0003306648868601769\n",
      "epoch: 7 | 31296 / 114272 | training loss: 0.0002674163843039423\n",
      "epoch: 7 | 31328 / 114272 | training loss: 0.0003273840411566198\n",
      "epoch: 7 | 31360 / 114272 | training loss: 0.00029134866781532764\n",
      "epoch: 7 | 31392 / 114272 | training loss: 0.0003451173542998731\n",
      "epoch: 7 | 31424 / 114272 | training loss: 0.11151088774204254\n",
      "epoch: 7 | 31456 / 114272 | training loss: 0.0003738966188393533\n",
      "epoch: 7 | 31488 / 114272 | training loss: 0.0003391747595742345\n",
      "epoch: 7 | 31520 / 114272 | training loss: 0.0002752903092186898\n",
      "epoch: 7 | 31552 / 114272 | training loss: 0.00025727046886458993\n",
      "epoch: 7 | 31584 / 114272 | training loss: 0.2012125551700592\n",
      "epoch: 7 | 31616 / 114272 | training loss: 0.00030504228197969496\n",
      "epoch: 7 | 31648 / 114272 | training loss: 0.00016179567319341004\n",
      "epoch: 7 | 31680 / 114272 | training loss: 0.0003215587348677218\n",
      "epoch: 7 | 31712 / 114272 | training loss: 0.0005655553541146219\n",
      "epoch: 7 | 31744 / 114272 | training loss: 0.00018950759840663522\n",
      "epoch: 7 | 31776 / 114272 | training loss: 0.07092227786779404\n",
      "epoch: 7 | 31808 / 114272 | training loss: 0.001068238285370171\n",
      "epoch: 7 | 31840 / 114272 | training loss: 0.0003429728385526687\n",
      "epoch: 7 | 31872 / 114272 | training loss: 0.00020654991385526955\n",
      "epoch: 7 | 31904 / 114272 | training loss: 0.0005131633952260017\n",
      "epoch: 7 | 31936 / 114272 | training loss: 0.011786545626819134\n",
      "epoch: 7 | 31968 / 114272 | training loss: 0.0004638831887859851\n",
      "epoch: 7 | 32000 / 114272 | training loss: 0.0004186540609225631\n",
      "epoch: 7 | 32032 / 114272 | training loss: 0.0006377535755746067\n",
      "epoch: 7 | 32064 / 114272 | training loss: 0.0020298275630921125\n",
      "epoch: 7 | 32096 / 114272 | training loss: 0.0003662349481601268\n",
      "epoch: 7 | 32128 / 114272 | training loss: 0.0006296738865785301\n",
      "epoch: 7 | 32160 / 114272 | training loss: 0.0008255638531409204\n",
      "epoch: 7 | 32192 / 114272 | training loss: 0.004344915505498648\n",
      "epoch: 7 | 32224 / 114272 | training loss: 0.0004373716365080327\n",
      "epoch: 7 | 32256 / 114272 | training loss: 0.0003438964195083827\n",
      "epoch: 7 | 32288 / 114272 | training loss: 0.0007509575225412846\n",
      "epoch: 7 | 32320 / 114272 | training loss: 0.0006019350257702172\n",
      "epoch: 7 | 32352 / 114272 | training loss: 0.00042179939919151366\n",
      "epoch: 7 | 32384 / 114272 | training loss: 0.1273002177476883\n",
      "epoch: 7 | 32416 / 114272 | training loss: 0.00036453420761972666\n",
      "epoch: 7 | 32448 / 114272 | training loss: 0.0008556268876418471\n",
      "epoch: 7 | 32480 / 114272 | training loss: 0.00031774723902344704\n",
      "epoch: 7 | 32512 / 114272 | training loss: 0.0007457188330590725\n",
      "epoch: 7 | 32544 / 114272 | training loss: 0.0003140209591947496\n",
      "epoch: 7 | 32576 / 114272 | training loss: 0.0006796184461563826\n",
      "epoch: 7 | 32608 / 114272 | training loss: 0.2675432860851288\n",
      "epoch: 7 | 32640 / 114272 | training loss: 0.000617202022112906\n",
      "epoch: 7 | 32672 / 114272 | training loss: 0.01070338487625122\n",
      "epoch: 7 | 32704 / 114272 | training loss: 0.0004095502954442054\n",
      "epoch: 7 | 32736 / 114272 | training loss: 0.014689476229250431\n",
      "epoch: 7 | 32768 / 114272 | training loss: 0.00018686390831135213\n",
      "epoch: 7 | 32800 / 114272 | training loss: 0.00029926386196166277\n",
      "epoch: 7 | 32832 / 114272 | training loss: 0.11135254055261612\n",
      "epoch: 7 | 32864 / 114272 | training loss: 0.0002967129403259605\n",
      "epoch: 7 | 32896 / 114272 | training loss: 0.06607396900653839\n",
      "epoch: 7 | 32928 / 114272 | training loss: 9.56106377998367e-05\n",
      "epoch: 7 | 32960 / 114272 | training loss: 0.0004157734219916165\n",
      "epoch: 7 | 32992 / 114272 | training loss: 0.0004306046466808766\n",
      "epoch: 7 | 33024 / 114272 | training loss: 0.0006892699748277664\n",
      "epoch: 7 | 33056 / 114272 | training loss: 0.00013872470299247652\n",
      "epoch: 7 | 33088 / 114272 | training loss: 0.0003331329789943993\n",
      "epoch: 7 | 33120 / 114272 | training loss: 0.0005627425271086395\n",
      "epoch: 7 | 33152 / 114272 | training loss: 0.21440240740776062\n",
      "epoch: 7 | 33184 / 114272 | training loss: 0.00031793848029337823\n",
      "epoch: 7 | 33216 / 114272 | training loss: 0.00030135904671624303\n",
      "epoch: 7 | 33248 / 114272 | training loss: 0.2737736403942108\n",
      "epoch: 7 | 33280 / 114272 | training loss: 0.0005275084404274821\n",
      "epoch: 7 | 33312 / 114272 | training loss: 0.000771730556152761\n",
      "epoch: 7 | 33344 / 114272 | training loss: 0.00022850444656796753\n",
      "epoch: 7 | 33376 / 114272 | training loss: 0.00077957654139027\n",
      "epoch: 7 | 33408 / 114272 | training loss: 0.00046420146827585995\n",
      "epoch: 7 | 33440 / 114272 | training loss: 0.00032963676494546235\n",
      "epoch: 7 | 33472 / 114272 | training loss: 0.00039327616104856133\n",
      "epoch: 7 | 33504 / 114272 | training loss: 0.0004041216161567718\n",
      "epoch: 7 | 33536 / 114272 | training loss: 0.001295369234867394\n",
      "epoch: 7 | 33568 / 114272 | training loss: 0.0006235882174223661\n",
      "epoch: 7 | 33600 / 114272 | training loss: 0.0003348944883327931\n",
      "epoch: 7 | 33632 / 114272 | training loss: 0.0013952809385955334\n",
      "epoch: 7 | 33664 / 114272 | training loss: 0.035682834684848785\n",
      "epoch: 7 | 33696 / 114272 | training loss: 0.0004148231237195432\n",
      "epoch: 7 | 33728 / 114272 | training loss: 0.0008238045847974718\n",
      "epoch: 7 | 33760 / 114272 | training loss: 0.0032190412748605013\n",
      "epoch: 7 | 33792 / 114272 | training loss: 0.0003991766134276986\n",
      "epoch: 7 | 33824 / 114272 | training loss: 0.00043057158472947776\n",
      "epoch: 7 | 33856 / 114272 | training loss: 0.0003505451895762235\n",
      "epoch: 7 | 33888 / 114272 | training loss: 0.00044094762415625155\n",
      "epoch: 7 | 33920 / 114272 | training loss: 0.0006067497306503356\n",
      "epoch: 7 | 33952 / 114272 | training loss: 0.0006185175734572113\n",
      "epoch: 7 | 33984 / 114272 | training loss: 0.005977912340313196\n",
      "epoch: 7 | 34016 / 114272 | training loss: 0.00035244785249233246\n",
      "epoch: 7 | 34048 / 114272 | training loss: 0.00038368895184248686\n",
      "epoch: 7 | 34080 / 114272 | training loss: 0.0004809863166883588\n",
      "epoch: 7 | 34112 / 114272 | training loss: 0.0003445398178882897\n",
      "epoch: 7 | 34144 / 114272 | training loss: 0.0003958102606702596\n",
      "epoch: 7 | 34176 / 114272 | training loss: 0.0012963537592440844\n",
      "epoch: 7 | 34208 / 114272 | training loss: 0.0004362194740679115\n",
      "epoch: 7 | 34240 / 114272 | training loss: 0.0002925485314335674\n",
      "epoch: 7 | 34272 / 114272 | training loss: 0.0003232752496842295\n",
      "epoch: 7 | 34304 / 114272 | training loss: 0.18291683495044708\n",
      "epoch: 7 | 34336 / 114272 | training loss: 0.004702560603618622\n",
      "epoch: 7 | 34368 / 114272 | training loss: 0.00038457673508673906\n",
      "epoch: 7 | 34400 / 114272 | training loss: 0.0004007386742159724\n",
      "epoch: 7 | 34432 / 114272 | training loss: 0.0004487176483962685\n",
      "epoch: 7 | 34464 / 114272 | training loss: 0.0004889375995844603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 34496 / 114272 | training loss: 0.000704050122294575\n",
      "epoch: 7 | 34528 / 114272 | training loss: 0.0004326616181060672\n",
      "epoch: 7 | 34560 / 114272 | training loss: 0.0003727212024386972\n",
      "epoch: 7 | 34592 / 114272 | training loss: 0.001195654273033142\n",
      "epoch: 7 | 34624 / 114272 | training loss: 0.0008555339882150292\n",
      "epoch: 7 | 34656 / 114272 | training loss: 0.0004966349224559963\n",
      "epoch: 7 | 34688 / 114272 | training loss: 0.0011912668123841286\n",
      "epoch: 7 | 34720 / 114272 | training loss: 0.0008905227878130972\n",
      "epoch: 7 | 34752 / 114272 | training loss: 0.0005092255305498838\n",
      "epoch: 7 | 34784 / 114272 | training loss: 0.0003845126193482429\n",
      "epoch: 7 | 34816 / 114272 | training loss: 0.10972382128238678\n",
      "epoch: 7 | 34848 / 114272 | training loss: 0.002162809483706951\n",
      "epoch: 7 | 34880 / 114272 | training loss: 0.0006072117248550057\n",
      "epoch: 7 | 34912 / 114272 | training loss: 0.0005959299160167575\n",
      "epoch: 7 | 34944 / 114272 | training loss: 0.00033226871164515615\n",
      "epoch: 7 | 34976 / 114272 | training loss: 0.0023945753928273916\n",
      "epoch: 7 | 35008 / 114272 | training loss: 0.0004786659264937043\n",
      "epoch: 7 | 35040 / 114272 | training loss: 0.160877525806427\n",
      "epoch: 7 | 35072 / 114272 | training loss: 0.0003946151118725538\n",
      "epoch: 7 | 35104 / 114272 | training loss: 0.0007814682903699577\n",
      "epoch: 7 | 35136 / 114272 | training loss: 0.0004905302776023746\n",
      "epoch: 7 | 35168 / 114272 | training loss: 0.0007786061614751816\n",
      "epoch: 7 | 35200 / 114272 | training loss: 0.0005161137669347227\n",
      "epoch: 7 | 35232 / 114272 | training loss: 0.00032543030101805925\n",
      "epoch: 7 | 35264 / 114272 | training loss: 0.0003449716023169458\n",
      "epoch: 7 | 35296 / 114272 | training loss: 0.0006588489632122219\n",
      "epoch: 7 | 35328 / 114272 | training loss: 0.0004599435778800398\n",
      "epoch: 7 | 35360 / 114272 | training loss: 0.0009290969464927912\n",
      "epoch: 7 | 35392 / 114272 | training loss: 0.0006364015862345695\n",
      "epoch: 7 | 35424 / 114272 | training loss: 0.00047463102964684367\n",
      "epoch: 7 | 35456 / 114272 | training loss: 0.0020175070967525244\n",
      "epoch: 7 | 35488 / 114272 | training loss: 0.0003140052722301334\n",
      "epoch: 7 | 35520 / 114272 | training loss: 0.0004747570783365518\n",
      "epoch: 7 | 35552 / 114272 | training loss: 0.0537833571434021\n",
      "epoch: 7 | 35584 / 114272 | training loss: 0.0006352948839776218\n",
      "epoch: 7 | 35616 / 114272 | training loss: 0.001164364512078464\n",
      "epoch: 7 | 35648 / 114272 | training loss: 0.0005195000558160245\n",
      "epoch: 7 | 35680 / 114272 | training loss: 0.00022495734447147697\n",
      "epoch: 7 | 35712 / 114272 | training loss: 0.0002510804042685777\n",
      "epoch: 7 | 35744 / 114272 | training loss: 0.00021529989317059517\n",
      "epoch: 7 | 35776 / 114272 | training loss: 0.00016449522809125483\n",
      "epoch: 7 | 35808 / 114272 | training loss: 0.000563379842787981\n",
      "epoch: 7 | 35840 / 114272 | training loss: 0.0004952264716848731\n",
      "epoch: 7 | 35872 / 114272 | training loss: 0.0003550722321961075\n",
      "epoch: 7 | 35904 / 114272 | training loss: 0.00022245946456678212\n",
      "epoch: 7 | 35936 / 114272 | training loss: 0.009216737002134323\n",
      "epoch: 7 | 35968 / 114272 | training loss: 0.00032282533356919885\n",
      "epoch: 7 | 36000 / 114272 | training loss: 0.10311158746480942\n",
      "epoch: 7 | 36032 / 114272 | training loss: 0.00025415641721338034\n",
      "epoch: 7 | 36064 / 114272 | training loss: 0.1031908243894577\n",
      "epoch: 7 | 36096 / 114272 | training loss: 0.0005174227408133447\n",
      "epoch: 7 | 36128 / 114272 | training loss: 0.0004454152367543429\n",
      "epoch: 7 | 36160 / 114272 | training loss: 0.00037072435952723026\n",
      "epoch: 7 | 36192 / 114272 | training loss: 0.0003368766629137099\n",
      "epoch: 7 | 36224 / 114272 | training loss: 0.00047950432053767145\n",
      "epoch: 7 | 36256 / 114272 | training loss: 0.0002921389532275498\n",
      "epoch: 7 | 36288 / 114272 | training loss: 0.0004237322718836367\n",
      "epoch: 7 | 36320 / 114272 | training loss: 0.051967788487672806\n",
      "epoch: 7 | 36352 / 114272 | training loss: 0.00269985175691545\n",
      "epoch: 7 | 36384 / 114272 | training loss: 0.00025711130001582205\n",
      "epoch: 7 | 36416 / 114272 | training loss: 0.00028219318483024836\n",
      "epoch: 7 | 36448 / 114272 | training loss: 0.02243194915354252\n",
      "epoch: 7 | 36480 / 114272 | training loss: 0.00036372788599692285\n",
      "epoch: 7 | 36512 / 114272 | training loss: 0.00041586169390939176\n",
      "epoch: 7 | 36544 / 114272 | training loss: 0.0005217319121584296\n",
      "epoch: 7 | 36576 / 114272 | training loss: 0.00038306432543322444\n",
      "epoch: 7 | 36608 / 114272 | training loss: 0.0004277782572899014\n",
      "epoch: 7 | 36640 / 114272 | training loss: 0.0002017577935475856\n",
      "epoch: 7 | 36672 / 114272 | training loss: 0.00032362830825150013\n",
      "epoch: 7 | 36704 / 114272 | training loss: 0.0012319437228143215\n",
      "epoch: 7 | 36736 / 114272 | training loss: 0.0002575874677859247\n",
      "epoch: 7 | 36768 / 114272 | training loss: 0.00029071554308757186\n",
      "epoch: 7 | 36800 / 114272 | training loss: 0.000970110937487334\n",
      "epoch: 7 | 36832 / 114272 | training loss: 0.00023240185691975057\n",
      "epoch: 7 | 36864 / 114272 | training loss: 0.0002304247609572485\n",
      "epoch: 7 | 36896 / 114272 | training loss: 0.0006969455280341208\n",
      "epoch: 7 | 36928 / 114272 | training loss: 0.0002318198385182768\n",
      "epoch: 7 | 36960 / 114272 | training loss: 0.00047622958663851023\n",
      "epoch: 7 | 36992 / 114272 | training loss: 0.00028726362506859004\n",
      "epoch: 7 | 37024 / 114272 | training loss: 0.0003534573770593852\n",
      "epoch: 7 | 37056 / 114272 | training loss: 0.0003715966595336795\n",
      "epoch: 7 | 37088 / 114272 | training loss: 0.002836948726326227\n",
      "epoch: 7 | 37120 / 114272 | training loss: 0.0007145243580453098\n",
      "epoch: 7 | 37152 / 114272 | training loss: 0.00025192220346070826\n",
      "epoch: 7 | 37184 / 114272 | training loss: 0.0004112133174203336\n",
      "epoch: 7 | 37216 / 114272 | training loss: 0.0003675925254356116\n",
      "epoch: 7 | 37248 / 114272 | training loss: 0.0002871917386073619\n",
      "epoch: 7 | 37280 / 114272 | training loss: 0.15728232264518738\n",
      "epoch: 7 | 37312 / 114272 | training loss: 0.0003452429373282939\n",
      "epoch: 7 | 37344 / 114272 | training loss: 0.00037673706538043916\n",
      "epoch: 7 | 37376 / 114272 | training loss: 0.012653530575335026\n",
      "epoch: 7 | 37408 / 114272 | training loss: 0.0004052995936945081\n",
      "epoch: 7 | 37440 / 114272 | training loss: 0.0002567062620073557\n",
      "epoch: 7 | 37472 / 114272 | training loss: 0.0002437862567603588\n",
      "epoch: 7 | 37504 / 114272 | training loss: 0.20880353450775146\n",
      "epoch: 7 | 37536 / 114272 | training loss: 0.1534934639930725\n",
      "epoch: 7 | 37568 / 114272 | training loss: 0.0015318446094170213\n",
      "epoch: 7 | 37600 / 114272 | training loss: 0.0001239294942934066\n",
      "epoch: 7 | 37632 / 114272 | training loss: 0.00021834041399415582\n",
      "epoch: 7 | 37664 / 114272 | training loss: 0.00023220523144118488\n",
      "epoch: 7 | 37696 / 114272 | training loss: 0.0003022767195943743\n",
      "epoch: 7 | 37728 / 114272 | training loss: 0.11835430562496185\n",
      "epoch: 7 | 37760 / 114272 | training loss: 0.0004006962408311665\n",
      "epoch: 7 | 37792 / 114272 | training loss: 0.0006527943187393248\n",
      "epoch: 7 | 37824 / 114272 | training loss: 0.0007199484389275312\n",
      "epoch: 7 | 37856 / 114272 | training loss: 0.00042320977081544697\n",
      "epoch: 7 | 37888 / 114272 | training loss: 0.0003246622218284756\n",
      "epoch: 7 | 37920 / 114272 | training loss: 0.0003588433610275388\n",
      "epoch: 7 | 37952 / 114272 | training loss: 9.894270624499768e-05\n",
      "epoch: 7 | 37984 / 114272 | training loss: 0.00043509277747944\n",
      "epoch: 7 | 38016 / 114272 | training loss: 0.0008220464223995805\n",
      "epoch: 7 | 38048 / 114272 | training loss: 0.00013104418758302927\n",
      "epoch: 7 | 38080 / 114272 | training loss: 0.0002422110119368881\n",
      "epoch: 7 | 38112 / 114272 | training loss: 0.0002785715914797038\n",
      "epoch: 7 | 38144 / 114272 | training loss: 0.036745063960552216\n",
      "epoch: 7 | 38176 / 114272 | training loss: 0.00031576905166730285\n",
      "epoch: 7 | 38208 / 114272 | training loss: 0.0002693236747290939\n",
      "epoch: 7 | 38240 / 114272 | training loss: 0.04120836406946182\n",
      "epoch: 7 | 38272 / 114272 | training loss: 0.00028788825147785246\n",
      "epoch: 7 | 38304 / 114272 | training loss: 0.00061465660110116\n",
      "epoch: 7 | 38336 / 114272 | training loss: 0.0013327505439519882\n",
      "epoch: 7 | 38368 / 114272 | training loss: 0.000458340480690822\n",
      "epoch: 7 | 38400 / 114272 | training loss: 0.07272658497095108\n",
      "epoch: 7 | 38432 / 114272 | training loss: 0.12952373921871185\n",
      "epoch: 7 | 38464 / 114272 | training loss: 0.0003094542189501226\n",
      "epoch: 7 | 38496 / 114272 | training loss: 0.10573671013116837\n",
      "epoch: 7 | 38528 / 114272 | training loss: 0.0002448942104820162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 38560 / 114272 | training loss: 0.027532944455742836\n",
      "epoch: 7 | 38592 / 114272 | training loss: 0.0005649756058119237\n",
      "epoch: 7 | 38624 / 114272 | training loss: 0.00031899852911010385\n",
      "epoch: 7 | 38656 / 114272 | training loss: 0.05515299737453461\n",
      "epoch: 7 | 38688 / 114272 | training loss: 0.017883317545056343\n",
      "epoch: 7 | 38720 / 114272 | training loss: 0.17105913162231445\n",
      "epoch: 7 | 38752 / 114272 | training loss: 0.014918252825737\n",
      "epoch: 7 | 38784 / 114272 | training loss: 0.0005505024455487728\n",
      "epoch: 7 | 38816 / 114272 | training loss: 0.015685800462961197\n",
      "epoch: 7 | 38848 / 114272 | training loss: 0.0010444863000884652\n",
      "epoch: 7 | 38880 / 114272 | training loss: 0.0004652641946449876\n",
      "epoch: 7 | 38912 / 114272 | training loss: 0.00025275564985349774\n",
      "epoch: 7 | 38944 / 114272 | training loss: 0.0009804223664104939\n",
      "epoch: 7 | 38976 / 114272 | training loss: 0.0003194143937435001\n",
      "epoch: 7 | 39008 / 114272 | training loss: 0.0005167693016119301\n",
      "epoch: 7 | 39040 / 114272 | training loss: 0.0003873511159326881\n",
      "epoch: 7 | 39072 / 114272 | training loss: 0.0002555064857006073\n",
      "epoch: 7 | 39104 / 114272 | training loss: 0.0002998272830154747\n",
      "epoch: 7 | 39136 / 114272 | training loss: 0.0004274775565136224\n",
      "epoch: 7 | 39168 / 114272 | training loss: 0.0006275610066950321\n",
      "epoch: 7 | 39200 / 114272 | training loss: 0.0002957088581752032\n",
      "epoch: 7 | 39232 / 114272 | training loss: 0.0003975829458795488\n",
      "epoch: 7 | 39264 / 114272 | training loss: 0.00025609863223508\n",
      "epoch: 7 | 39296 / 114272 | training loss: 0.00035437470069155097\n",
      "epoch: 7 | 39328 / 114272 | training loss: 0.00015927196363918483\n",
      "epoch: 7 | 39360 / 114272 | training loss: 0.00014897869550623\n",
      "epoch: 7 | 39392 / 114272 | training loss: 0.0007792078540660441\n",
      "epoch: 7 | 39424 / 114272 | training loss: 0.001171997282654047\n",
      "epoch: 7 | 39456 / 114272 | training loss: 0.0007921569049358368\n",
      "epoch: 7 | 39488 / 114272 | training loss: 0.0005468815215863287\n",
      "epoch: 7 | 39520 / 114272 | training loss: 0.12265554815530777\n",
      "epoch: 7 | 39552 / 114272 | training loss: 0.0004482472431845963\n",
      "epoch: 7 | 39584 / 114272 | training loss: 0.0002136518305633217\n",
      "epoch: 7 | 39616 / 114272 | training loss: 0.0004752500681206584\n",
      "epoch: 7 | 39648 / 114272 | training loss: 0.0006094372947700322\n",
      "epoch: 7 | 39680 / 114272 | training loss: 0.001458999002352357\n",
      "epoch: 7 | 39712 / 114272 | training loss: 0.0015268813585862517\n",
      "epoch: 7 | 39744 / 114272 | training loss: 0.0007137014763429761\n",
      "epoch: 7 | 39776 / 114272 | training loss: 0.0004371996910776943\n",
      "epoch: 7 | 39808 / 114272 | training loss: 0.00047579166130162776\n",
      "epoch: 7 | 39840 / 114272 | training loss: 0.046619560569524765\n",
      "epoch: 7 | 39872 / 114272 | training loss: 0.035113465040922165\n",
      "epoch: 7 | 39904 / 114272 | training loss: 0.13378864526748657\n",
      "epoch: 7 | 39936 / 114272 | training loss: 0.0006526579963974655\n",
      "epoch: 7 | 39968 / 114272 | training loss: 0.1684214174747467\n",
      "epoch: 7 | 40000 / 114272 | training loss: 0.0005676415166817605\n",
      "epoch: 7 | 40032 / 114272 | training loss: 0.00020555600349325687\n",
      "epoch: 7 | 40064 / 114272 | training loss: 0.09340637922286987\n",
      "epoch: 7 | 40096 / 114272 | training loss: 0.00046903055044822395\n",
      "epoch: 7 | 40128 / 114272 | training loss: 0.00699550099670887\n",
      "epoch: 7 | 40160 / 114272 | training loss: 0.003272231901064515\n",
      "epoch: 7 | 40192 / 114272 | training loss: 0.000294251658488065\n",
      "epoch: 7 | 40224 / 114272 | training loss: 0.16356419026851654\n",
      "epoch: 7 | 40256 / 114272 | training loss: 0.00020761584164574742\n",
      "epoch: 7 | 40288 / 114272 | training loss: 0.005917140748351812\n",
      "epoch: 7 | 40320 / 114272 | training loss: 0.007319247350096703\n",
      "epoch: 7 | 40352 / 114272 | training loss: 0.0004304730100557208\n",
      "epoch: 7 | 40384 / 114272 | training loss: 0.00033053357037715614\n",
      "epoch: 7 | 40416 / 114272 | training loss: 0.23180225491523743\n",
      "epoch: 7 | 40448 / 114272 | training loss: 0.0003661161463242024\n",
      "epoch: 7 | 40480 / 114272 | training loss: 0.00032086073770187795\n",
      "epoch: 7 | 40512 / 114272 | training loss: 0.00045557948760688305\n",
      "epoch: 7 | 40544 / 114272 | training loss: 0.19091211259365082\n",
      "epoch: 7 | 40576 / 114272 | training loss: 0.13581904768943787\n",
      "epoch: 7 | 40608 / 114272 | training loss: 0.0042928606271743774\n",
      "epoch: 7 | 40640 / 114272 | training loss: 0.13470861315727234\n",
      "epoch: 7 | 40672 / 114272 | training loss: 0.0005115381209179759\n",
      "epoch: 7 | 40704 / 114272 | training loss: 0.0021065177861601114\n",
      "epoch: 7 | 40736 / 114272 | training loss: 0.21091735363006592\n",
      "epoch: 7 | 40768 / 114272 | training loss: 0.0002869816089514643\n",
      "epoch: 7 | 40800 / 114272 | training loss: 0.14012080430984497\n",
      "epoch: 7 | 40832 / 114272 | training loss: 0.000570288801100105\n",
      "epoch: 7 | 40864 / 114272 | training loss: 0.001425501424819231\n",
      "epoch: 7 | 40896 / 114272 | training loss: 0.0005355685134418309\n",
      "epoch: 7 | 40928 / 114272 | training loss: 0.000988149200566113\n",
      "epoch: 7 | 40960 / 114272 | training loss: 0.0008992397342808545\n",
      "epoch: 7 | 40992 / 114272 | training loss: 0.000729742634575814\n",
      "epoch: 7 | 41024 / 114272 | training loss: 0.0008143816376104951\n",
      "epoch: 7 | 41056 / 114272 | training loss: 0.0006919430452398956\n",
      "epoch: 7 | 41088 / 114272 | training loss: 0.1270715445280075\n",
      "epoch: 7 | 41120 / 114272 | training loss: 0.00026197644183412194\n",
      "epoch: 7 | 41152 / 114272 | training loss: 0.0008044323767535388\n",
      "epoch: 7 | 41184 / 114272 | training loss: 0.0005520094418898225\n",
      "epoch: 7 | 41216 / 114272 | training loss: 0.052003126591444016\n",
      "epoch: 7 | 41248 / 114272 | training loss: 0.000986198429018259\n",
      "epoch: 7 | 41280 / 114272 | training loss: 0.1972273290157318\n",
      "epoch: 7 | 41312 / 114272 | training loss: 0.0006219596834853292\n",
      "epoch: 7 | 41344 / 114272 | training loss: 0.0013264347799122334\n",
      "epoch: 7 | 41376 / 114272 | training loss: 0.03190796822309494\n",
      "epoch: 7 | 41408 / 114272 | training loss: 0.0004901320789940655\n",
      "epoch: 7 | 41440 / 114272 | training loss: 0.0011287612142041326\n",
      "epoch: 7 | 41472 / 114272 | training loss: 0.0011179707944393158\n",
      "epoch: 7 | 41504 / 114272 | training loss: 0.0009281968232244253\n",
      "epoch: 7 | 41536 / 114272 | training loss: 0.0006610372802242637\n",
      "epoch: 7 | 41568 / 114272 | training loss: 0.0006637605256401002\n",
      "epoch: 7 | 41600 / 114272 | training loss: 0.0058219535276293755\n",
      "epoch: 7 | 41632 / 114272 | training loss: 0.0008513361681252718\n",
      "epoch: 7 | 41664 / 114272 | training loss: 0.0016130957519635558\n",
      "epoch: 7 | 41696 / 114272 | training loss: 0.0012030849466100335\n",
      "epoch: 7 | 41728 / 114272 | training loss: 0.001767221256159246\n",
      "epoch: 7 | 41760 / 114272 | training loss: 0.0017372665461152792\n",
      "epoch: 7 | 41792 / 114272 | training loss: 0.005207243841141462\n",
      "epoch: 7 | 41824 / 114272 | training loss: 0.00012841833813581616\n",
      "epoch: 7 | 41856 / 114272 | training loss: 0.0005520047852769494\n",
      "epoch: 7 | 41888 / 114272 | training loss: 0.0012998981401324272\n",
      "epoch: 7 | 41920 / 114272 | training loss: 0.0010477886535227299\n",
      "epoch: 7 | 41952 / 114272 | training loss: 0.0004344414337538183\n",
      "epoch: 7 | 41984 / 114272 | training loss: 0.0003459186409600079\n",
      "epoch: 7 | 42016 / 114272 | training loss: 0.0004406127263791859\n",
      "epoch: 7 | 42048 / 114272 | training loss: 0.0005374862812459469\n",
      "epoch: 7 | 42080 / 114272 | training loss: 0.01377852912992239\n",
      "epoch: 7 | 42112 / 114272 | training loss: 0.11883487552404404\n",
      "epoch: 7 | 42144 / 114272 | training loss: 0.0009774230420589447\n",
      "epoch: 7 | 42176 / 114272 | training loss: 0.0008630053489468992\n",
      "epoch: 7 | 42208 / 114272 | training loss: 0.0009532127878628671\n",
      "epoch: 7 | 42240 / 114272 | training loss: 0.00044287569471634924\n",
      "epoch: 7 | 42272 / 114272 | training loss: 0.0005265994113869965\n",
      "epoch: 7 | 42304 / 114272 | training loss: 0.0010056662140414119\n",
      "epoch: 7 | 42336 / 114272 | training loss: 0.0008416781201958656\n",
      "epoch: 7 | 42368 / 114272 | training loss: 0.0007959158974699676\n",
      "epoch: 7 | 42400 / 114272 | training loss: 0.0005736929015256464\n",
      "epoch: 7 | 42432 / 114272 | training loss: 0.00046494233538396657\n",
      "epoch: 7 | 42464 / 114272 | training loss: 0.0018559959717094898\n",
      "epoch: 7 | 42496 / 114272 | training loss: 0.0007392825791612267\n",
      "epoch: 7 | 42528 / 114272 | training loss: 0.0003352265921421349\n",
      "epoch: 7 | 42560 / 114272 | training loss: 0.0008337223553098738\n",
      "epoch: 7 | 42592 / 114272 | training loss: 0.003614430781453848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 42624 / 114272 | training loss: 0.0005012491019442677\n",
      "epoch: 7 | 42656 / 114272 | training loss: 0.0010363230248913169\n",
      "epoch: 7 | 42688 / 114272 | training loss: 0.0010301891015842557\n",
      "epoch: 7 | 42720 / 114272 | training loss: 0.0008808830752968788\n",
      "epoch: 7 | 42752 / 114272 | training loss: 0.0006471164524555206\n",
      "epoch: 7 | 42784 / 114272 | training loss: 0.0005416467320173979\n",
      "epoch: 7 | 42816 / 114272 | training loss: 0.0007378250011242926\n",
      "epoch: 7 | 42848 / 114272 | training loss: 0.0005200104205869138\n",
      "epoch: 7 | 42880 / 114272 | training loss: 0.00784359872341156\n",
      "epoch: 7 | 42912 / 114272 | training loss: 0.0007037547766231\n",
      "epoch: 7 | 42944 / 114272 | training loss: 0.0010984354885295033\n",
      "epoch: 7 | 42976 / 114272 | training loss: 0.17615370452404022\n",
      "epoch: 7 | 43008 / 114272 | training loss: 0.0011974892113357782\n",
      "epoch: 7 | 43040 / 114272 | training loss: 0.002261342015117407\n",
      "epoch: 7 | 43072 / 114272 | training loss: 0.11478012055158615\n",
      "epoch: 7 | 43104 / 114272 | training loss: 0.0009947482030838728\n",
      "epoch: 7 | 43136 / 114272 | training loss: 0.001267581363208592\n",
      "epoch: 7 | 43168 / 114272 | training loss: 0.0015366470906883478\n",
      "epoch: 7 | 43200 / 114272 | training loss: 0.0002732850844040513\n",
      "epoch: 7 | 43232 / 114272 | training loss: 0.0005976624088361859\n",
      "epoch: 7 | 43264 / 114272 | training loss: 0.000725486665032804\n",
      "epoch: 7 | 43296 / 114272 | training loss: 0.0010173964546993375\n",
      "epoch: 7 | 43328 / 114272 | training loss: 0.0005567580810748041\n",
      "epoch: 7 | 43360 / 114272 | training loss: 0.0017976531526073813\n",
      "epoch: 7 | 43392 / 114272 | training loss: 0.0008303766371682286\n",
      "epoch: 7 | 43424 / 114272 | training loss: 0.0008664153865538538\n",
      "epoch: 7 | 43456 / 114272 | training loss: 0.000621474115177989\n",
      "epoch: 7 | 43488 / 114272 | training loss: 0.0005139975692145526\n",
      "epoch: 7 | 43520 / 114272 | training loss: 0.0948442816734314\n",
      "epoch: 7 | 43552 / 114272 | training loss: 0.0003997551975771785\n",
      "epoch: 7 | 43584 / 114272 | training loss: 0.0006645314861088991\n",
      "epoch: 7 | 43616 / 114272 | training loss: 0.006652531214058399\n",
      "epoch: 7 | 43648 / 114272 | training loss: 0.00037457013968378305\n",
      "epoch: 7 | 43680 / 114272 | training loss: 0.0008442533435299993\n",
      "epoch: 7 | 43712 / 114272 | training loss: 0.001807285356335342\n",
      "epoch: 7 | 43744 / 114272 | training loss: 0.0017401976510882378\n",
      "epoch: 7 | 43776 / 114272 | training loss: 0.0006050192750990391\n",
      "epoch: 7 | 43808 / 114272 | training loss: 0.48077017068862915\n",
      "epoch: 7 | 43840 / 114272 | training loss: 0.0005879981908947229\n",
      "epoch: 7 | 43872 / 114272 | training loss: 0.0013436974259093404\n",
      "epoch: 7 | 43904 / 114272 | training loss: 0.0003256454656366259\n",
      "epoch: 7 | 43936 / 114272 | training loss: 0.00018800792167894542\n",
      "epoch: 7 | 43968 / 114272 | training loss: 0.0009133527637459338\n",
      "epoch: 7 | 44000 / 114272 | training loss: 0.00036763297975994647\n",
      "epoch: 7 | 44032 / 114272 | training loss: 0.03447404131293297\n",
      "epoch: 7 | 44064 / 114272 | training loss: 0.0014113894430920482\n",
      "epoch: 7 | 44096 / 114272 | training loss: 0.09689177572727203\n",
      "epoch: 7 | 44128 / 114272 | training loss: 0.001311234082095325\n",
      "epoch: 7 | 44160 / 114272 | training loss: 0.0008320850320160389\n",
      "epoch: 7 | 44192 / 114272 | training loss: 0.0013759755529463291\n",
      "epoch: 7 | 44224 / 114272 | training loss: 0.0014920019311830401\n",
      "epoch: 7 | 44256 / 114272 | training loss: 0.00030346272978931665\n",
      "epoch: 7 | 44288 / 114272 | training loss: 0.0015261066146194935\n",
      "epoch: 7 | 44320 / 114272 | training loss: 0.0006815597880631685\n",
      "epoch: 7 | 44352 / 114272 | training loss: 0.0003882674500346184\n",
      "epoch: 7 | 44384 / 114272 | training loss: 0.00026323425117880106\n",
      "epoch: 7 | 44416 / 114272 | training loss: 0.0006362817948684096\n",
      "epoch: 7 | 44448 / 114272 | training loss: 0.0006925504421815276\n",
      "epoch: 7 | 44480 / 114272 | training loss: 0.000991067267023027\n",
      "epoch: 7 | 44512 / 114272 | training loss: 0.24565105140209198\n",
      "epoch: 7 | 44544 / 114272 | training loss: 0.000566664501093328\n",
      "epoch: 7 | 44576 / 114272 | training loss: 0.0023855087347328663\n",
      "epoch: 7 | 44608 / 114272 | training loss: 0.12597787380218506\n",
      "epoch: 7 | 44640 / 114272 | training loss: 0.0007825391367077827\n",
      "epoch: 7 | 44672 / 114272 | training loss: 0.0005801958031952381\n",
      "epoch: 7 | 44704 / 114272 | training loss: 0.0007942147785797715\n",
      "epoch: 7 | 44736 / 114272 | training loss: 0.24461424350738525\n",
      "epoch: 7 | 44768 / 114272 | training loss: 0.0017111031338572502\n",
      "epoch: 7 | 44800 / 114272 | training loss: 0.0006817405810579658\n",
      "epoch: 7 | 44832 / 114272 | training loss: 0.0006010705255903304\n",
      "epoch: 7 | 44864 / 114272 | training loss: 0.0038986734580248594\n",
      "epoch: 7 | 44896 / 114272 | training loss: 0.14906476438045502\n",
      "epoch: 7 | 44928 / 114272 | training loss: 0.004712787456810474\n",
      "epoch: 7 | 44960 / 114272 | training loss: 0.18047918379306793\n",
      "epoch: 7 | 44992 / 114272 | training loss: 0.0017878146609291434\n",
      "epoch: 7 | 45024 / 114272 | training loss: 0.0005679396563209593\n",
      "epoch: 7 | 45056 / 114272 | training loss: 0.10293234884738922\n",
      "epoch: 7 | 45088 / 114272 | training loss: 0.08890634030103683\n",
      "epoch: 7 | 45120 / 114272 | training loss: 0.00047467969125136733\n",
      "epoch: 7 | 45152 / 114272 | training loss: 0.0008807740523479879\n",
      "epoch: 7 | 45184 / 114272 | training loss: 0.000804391922429204\n",
      "epoch: 7 | 45216 / 114272 | training loss: 0.0007953190943226218\n",
      "epoch: 7 | 45248 / 114272 | training loss: 0.0017739986069500446\n",
      "epoch: 7 | 45280 / 114272 | training loss: 0.18155455589294434\n",
      "epoch: 7 | 45312 / 114272 | training loss: 0.0008913165656849742\n",
      "epoch: 7 | 45344 / 114272 | training loss: 0.0010298273991793394\n",
      "epoch: 7 | 45376 / 114272 | training loss: 0.000377071148250252\n",
      "epoch: 7 | 45408 / 114272 | training loss: 0.0008462672121822834\n",
      "epoch: 7 | 45440 / 114272 | training loss: 0.001080899965018034\n",
      "epoch: 7 | 45472 / 114272 | training loss: 0.0015885597094893456\n",
      "epoch: 7 | 45504 / 114272 | training loss: 0.0006622910150326788\n",
      "epoch: 7 | 45536 / 114272 | training loss: 0.0005996726104058325\n",
      "epoch: 7 | 45568 / 114272 | training loss: 0.0003371701168362051\n",
      "epoch: 7 | 45600 / 114272 | training loss: 0.0015178574249148369\n",
      "epoch: 7 | 45632 / 114272 | training loss: 0.1787916123867035\n",
      "epoch: 7 | 45664 / 114272 | training loss: 0.0015483115566894412\n",
      "epoch: 7 | 45696 / 114272 | training loss: 0.0013660870026797056\n",
      "epoch: 7 | 45728 / 114272 | training loss: 0.0017663906328380108\n",
      "epoch: 7 | 45760 / 114272 | training loss: 0.0010651572374626994\n",
      "epoch: 7 | 45792 / 114272 | training loss: 0.0006108161760494113\n",
      "epoch: 7 | 45824 / 114272 | training loss: 0.18951177597045898\n",
      "epoch: 7 | 45856 / 114272 | training loss: 0.0010074781021103263\n",
      "epoch: 7 | 45888 / 114272 | training loss: 0.0013348400825634599\n",
      "epoch: 7 | 45920 / 114272 | training loss: 0.0011532178614288568\n",
      "epoch: 7 | 45952 / 114272 | training loss: 0.0035072288010269403\n",
      "epoch: 7 | 45984 / 114272 | training loss: 0.0006525255739688873\n",
      "epoch: 7 | 46016 / 114272 | training loss: 0.176956444978714\n",
      "epoch: 7 | 46048 / 114272 | training loss: 0.0005496408557519317\n",
      "epoch: 7 | 46080 / 114272 | training loss: 0.001095571555197239\n",
      "epoch: 7 | 46112 / 114272 | training loss: 0.0015469153877347708\n",
      "epoch: 7 | 46144 / 114272 | training loss: 0.0009482085588388145\n",
      "epoch: 7 | 46176 / 114272 | training loss: 0.0015159243484959006\n",
      "epoch: 7 | 46208 / 114272 | training loss: 0.0007469975389540195\n",
      "epoch: 7 | 46240 / 114272 | training loss: 0.007478571962565184\n",
      "epoch: 7 | 46272 / 114272 | training loss: 0.0013451882405206561\n",
      "epoch: 7 | 46304 / 114272 | training loss: 0.0009347636951133609\n",
      "epoch: 7 | 46336 / 114272 | training loss: 0.0007236963720060885\n",
      "epoch: 7 | 46368 / 114272 | training loss: 0.005702735856175423\n",
      "epoch: 7 | 46400 / 114272 | training loss: 0.0009765260037966073\n",
      "epoch: 7 | 46432 / 114272 | training loss: 0.00043935771100223064\n",
      "epoch: 7 | 46464 / 114272 | training loss: 0.002519675763323903\n",
      "epoch: 7 | 46496 / 114272 | training loss: 0.0006452997913584113\n",
      "epoch: 7 | 46528 / 114272 | training loss: 0.001757090212777257\n",
      "epoch: 7 | 46560 / 114272 | training loss: 0.11457059532403946\n",
      "epoch: 7 | 46592 / 114272 | training loss: 0.0006405458552762866\n",
      "epoch: 7 | 46624 / 114272 | training loss: 0.08405441045761108\n",
      "epoch: 7 | 46656 / 114272 | training loss: 0.0009585039224475622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 46688 / 114272 | training loss: 0.001056939596310258\n",
      "epoch: 7 | 46720 / 114272 | training loss: 0.0004618719103746116\n",
      "epoch: 7 | 46752 / 114272 | training loss: 0.0011548952898010612\n",
      "epoch: 7 | 46784 / 114272 | training loss: 0.0006806054152548313\n",
      "epoch: 7 | 46816 / 114272 | training loss: 0.0010908616241067648\n",
      "epoch: 7 | 46848 / 114272 | training loss: 0.0007518769707530737\n",
      "epoch: 7 | 46880 / 114272 | training loss: 0.0011195044498890638\n",
      "epoch: 7 | 46912 / 114272 | training loss: 0.0029513132758438587\n",
      "epoch: 7 | 46944 / 114272 | training loss: 0.002341749146580696\n",
      "epoch: 7 | 46976 / 114272 | training loss: 0.004145193379372358\n",
      "epoch: 7 | 47008 / 114272 | training loss: 0.0016662569250911474\n",
      "epoch: 7 | 47040 / 114272 | training loss: 0.0008733311551623046\n",
      "epoch: 7 | 47072 / 114272 | training loss: 0.00883442722260952\n",
      "epoch: 7 | 47104 / 114272 | training loss: 0.002093844348564744\n",
      "epoch: 7 | 47136 / 114272 | training loss: 0.001304435427300632\n",
      "epoch: 7 | 47168 / 114272 | training loss: 0.0009050819789990783\n",
      "epoch: 7 | 47200 / 114272 | training loss: 0.000908835674636066\n",
      "epoch: 7 | 47232 / 114272 | training loss: 0.0005438028019852936\n",
      "epoch: 7 | 47264 / 114272 | training loss: 0.0018755107885226607\n",
      "epoch: 7 | 47296 / 114272 | training loss: 0.0007990555022843182\n",
      "epoch: 7 | 47328 / 114272 | training loss: 0.0004014780861325562\n",
      "epoch: 7 | 47360 / 114272 | training loss: 0.001511072856374085\n",
      "epoch: 7 | 47392 / 114272 | training loss: 0.09766610711812973\n",
      "epoch: 7 | 47424 / 114272 | training loss: 0.0010583442635834217\n",
      "epoch: 7 | 47456 / 114272 | training loss: 0.0007230173796415329\n",
      "epoch: 7 | 47488 / 114272 | training loss: 0.28717344999313354\n",
      "epoch: 7 | 47520 / 114272 | training loss: 0.00039806426502764225\n",
      "epoch: 7 | 47552 / 114272 | training loss: 0.0018246716354042292\n",
      "epoch: 7 | 47584 / 114272 | training loss: 0.0008288331446237862\n",
      "epoch: 7 | 47616 / 114272 | training loss: 0.001326665049418807\n",
      "epoch: 7 | 47648 / 114272 | training loss: 0.0008606933988630772\n",
      "epoch: 7 | 47680 / 114272 | training loss: 0.0012421846622601151\n",
      "epoch: 7 | 47712 / 114272 | training loss: 0.00325800315476954\n",
      "epoch: 7 | 47744 / 114272 | training loss: 0.001309425919316709\n",
      "epoch: 7 | 47776 / 114272 | training loss: 0.002509069163352251\n",
      "epoch: 7 | 47808 / 114272 | training loss: 0.0005433542537502944\n",
      "epoch: 7 | 47840 / 114272 | training loss: 0.0011743530631065369\n",
      "epoch: 7 | 47872 / 114272 | training loss: 0.05086851865053177\n",
      "epoch: 7 | 47904 / 114272 | training loss: 0.0005689634708687663\n",
      "epoch: 7 | 47936 / 114272 | training loss: 0.10102042555809021\n",
      "epoch: 7 | 47968 / 114272 | training loss: 0.0008007452706806362\n",
      "epoch: 7 | 48000 / 114272 | training loss: 0.0006025348557159305\n",
      "epoch: 7 | 48032 / 114272 | training loss: 0.0010129665024578571\n",
      "epoch: 7 | 48064 / 114272 | training loss: 0.0015313089825212955\n",
      "epoch: 7 | 48096 / 114272 | training loss: 0.0006142210913822055\n",
      "epoch: 7 | 48128 / 114272 | training loss: 0.0009257845813408494\n",
      "epoch: 7 | 48160 / 114272 | training loss: 0.0009517821599729359\n",
      "epoch: 7 | 48192 / 114272 | training loss: 0.0012306396383792162\n",
      "epoch: 7 | 48224 / 114272 | training loss: 0.008262421004474163\n",
      "epoch: 7 | 48256 / 114272 | training loss: 0.001119277672842145\n",
      "epoch: 7 | 48288 / 114272 | training loss: 0.0003486293717287481\n",
      "epoch: 7 | 48320 / 114272 | training loss: 0.0016464965883642435\n",
      "epoch: 7 | 48352 / 114272 | training loss: 0.0008599379798397422\n",
      "epoch: 7 | 48384 / 114272 | training loss: 0.0008540298440493643\n",
      "epoch: 7 | 48416 / 114272 | training loss: 0.0005918795359320939\n",
      "epoch: 7 | 48448 / 114272 | training loss: 0.000671057787258178\n",
      "epoch: 7 | 48480 / 114272 | training loss: 0.0021267591509968042\n",
      "epoch: 7 | 48512 / 114272 | training loss: 0.0008540425333194435\n",
      "epoch: 7 | 48544 / 114272 | training loss: 0.0007652194472029805\n",
      "epoch: 7 | 48576 / 114272 | training loss: 0.0006784234428778291\n",
      "epoch: 7 | 48608 / 114272 | training loss: 0.001588905113749206\n",
      "epoch: 7 | 48640 / 114272 | training loss: 0.0004907801630906761\n",
      "epoch: 7 | 48672 / 114272 | training loss: 0.0491553358733654\n",
      "epoch: 7 | 48704 / 114272 | training loss: 0.000767737568821758\n",
      "epoch: 7 | 48736 / 114272 | training loss: 0.002216105815023184\n",
      "epoch: 7 | 48768 / 114272 | training loss: 0.0009864694438874722\n",
      "epoch: 7 | 48800 / 114272 | training loss: 0.0004240243579261005\n",
      "epoch: 7 | 48832 / 114272 | training loss: 0.0007948576239868999\n",
      "epoch: 7 | 48864 / 114272 | training loss: 0.00037319210241548717\n",
      "epoch: 7 | 48896 / 114272 | training loss: 0.0003803898871410638\n",
      "epoch: 7 | 48928 / 114272 | training loss: 0.09268061071634293\n",
      "epoch: 7 | 48960 / 114272 | training loss: 0.0007858866010792553\n",
      "epoch: 7 | 48992 / 114272 | training loss: 0.000496521417517215\n",
      "epoch: 7 | 49024 / 114272 | training loss: 0.0005474530044011772\n",
      "epoch: 7 | 49056 / 114272 | training loss: 0.001060275244526565\n",
      "epoch: 7 | 49088 / 114272 | training loss: 0.0011499285465106368\n",
      "epoch: 7 | 49120 / 114272 | training loss: 0.0004611009208019823\n",
      "epoch: 7 | 49152 / 114272 | training loss: 0.0006286854622885585\n",
      "epoch: 7 | 49184 / 114272 | training loss: 0.0010542873060330749\n",
      "epoch: 7 | 49216 / 114272 | training loss: 0.00036978634307160974\n",
      "epoch: 7 | 49248 / 114272 | training loss: 0.000525261159054935\n",
      "epoch: 7 | 49280 / 114272 | training loss: 0.00034171860897913575\n",
      "epoch: 7 | 49312 / 114272 | training loss: 0.00043764355359598994\n",
      "epoch: 7 | 49344 / 114272 | training loss: 0.0007627673912793398\n",
      "epoch: 7 | 49376 / 114272 | training loss: 0.060396794229745865\n",
      "epoch: 7 | 49408 / 114272 | training loss: 0.0008449059096165001\n",
      "epoch: 7 | 49440 / 114272 | training loss: 0.0003368231118656695\n",
      "epoch: 7 | 49472 / 114272 | training loss: 0.0003763493150472641\n",
      "epoch: 7 | 49504 / 114272 | training loss: 0.0004893210134468973\n",
      "epoch: 7 | 49536 / 114272 | training loss: 0.0003887095663230866\n",
      "epoch: 7 | 49568 / 114272 | training loss: 0.00037529764813371\n",
      "epoch: 7 | 49600 / 114272 | training loss: 0.1359420269727707\n",
      "epoch: 7 | 49632 / 114272 | training loss: 0.001031522173434496\n",
      "epoch: 7 | 49664 / 114272 | training loss: 0.0003864769241772592\n",
      "epoch: 7 | 49696 / 114272 | training loss: 0.0008748616091907024\n",
      "epoch: 7 | 49728 / 114272 | training loss: 0.0008530085324309766\n",
      "epoch: 7 | 49760 / 114272 | training loss: 0.0009712556493468583\n",
      "epoch: 7 | 49792 / 114272 | training loss: 0.000982580822892487\n",
      "epoch: 7 | 49824 / 114272 | training loss: 0.01997995376586914\n",
      "epoch: 7 | 49856 / 114272 | training loss: 0.005319128278642893\n",
      "epoch: 7 | 49888 / 114272 | training loss: 0.0008951154304668307\n",
      "epoch: 7 | 49920 / 114272 | training loss: 0.0005407286807894707\n",
      "epoch: 7 | 49952 / 114272 | training loss: 0.00048122095176950097\n",
      "epoch: 7 | 49984 / 114272 | training loss: 0.0007805472705513239\n",
      "epoch: 7 | 50016 / 114272 | training loss: 0.010393006727099419\n",
      "epoch: 7 | 50048 / 114272 | training loss: 0.0003796656965278089\n",
      "epoch: 7 | 50080 / 114272 | training loss: 0.00016260948905255646\n",
      "epoch: 7 | 50112 / 114272 | training loss: 0.0004030178824905306\n",
      "epoch: 7 | 50144 / 114272 | training loss: 0.0035787581000477076\n",
      "epoch: 7 | 50176 / 114272 | training loss: 0.0016788267530500889\n",
      "epoch: 7 | 50208 / 114272 | training loss: 0.0006561768823303282\n",
      "epoch: 7 | 50240 / 114272 | training loss: 0.0021678870543837547\n",
      "epoch: 7 | 50272 / 114272 | training loss: 0.0006032783421687782\n",
      "epoch: 7 | 50304 / 114272 | training loss: 0.002103762701153755\n",
      "epoch: 7 | 50336 / 114272 | training loss: 0.00034046731889247894\n",
      "epoch: 7 | 50368 / 114272 | training loss: 0.0009891410591080785\n",
      "epoch: 7 | 50400 / 114272 | training loss: 0.0007823106716386974\n",
      "epoch: 7 | 50432 / 114272 | training loss: 0.0028443175833672285\n",
      "epoch: 7 | 50464 / 114272 | training loss: 0.0002685189829207957\n",
      "epoch: 7 | 50496 / 114272 | training loss: 0.0009627817198634148\n",
      "epoch: 7 | 50528 / 114272 | training loss: 0.1068805456161499\n",
      "epoch: 7 | 50560 / 114272 | training loss: 0.00037768794572912157\n",
      "epoch: 7 | 50592 / 114272 | training loss: 0.025801431387662888\n",
      "epoch: 7 | 50624 / 114272 | training loss: 0.0009069796069525182\n",
      "epoch: 7 | 50656 / 114272 | training loss: 0.00026024787803180516\n",
      "epoch: 7 | 50688 / 114272 | training loss: 0.003908745013177395\n",
      "epoch: 7 | 50720 / 114272 | training loss: 0.0005688295350410044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 50752 / 114272 | training loss: 0.00039523313171230257\n",
      "epoch: 7 | 50784 / 114272 | training loss: 0.0009828518377617002\n",
      "epoch: 7 | 50816 / 114272 | training loss: 0.0004030694253742695\n",
      "epoch: 7 | 50848 / 114272 | training loss: 0.00042842215043492615\n",
      "epoch: 7 | 50880 / 114272 | training loss: 0.0003039773437194526\n",
      "epoch: 7 | 50912 / 114272 | training loss: 0.00030567837529815733\n",
      "epoch: 7 | 50944 / 114272 | training loss: 0.000264844304183498\n",
      "epoch: 7 | 50976 / 114272 | training loss: 0.000510796788148582\n",
      "epoch: 7 | 51008 / 114272 | training loss: 0.0004339511797297746\n",
      "epoch: 7 | 51040 / 114272 | training loss: 0.00041553290793672204\n",
      "epoch: 7 | 51072 / 114272 | training loss: 0.00046308652963489294\n",
      "epoch: 7 | 51104 / 114272 | training loss: 0.0006170550477690995\n",
      "epoch: 7 | 51136 / 114272 | training loss: 0.0006028471980243921\n",
      "epoch: 7 | 51168 / 114272 | training loss: 0.0006760710966773331\n",
      "epoch: 7 | 51200 / 114272 | training loss: 0.00042956401011906564\n",
      "epoch: 7 | 51232 / 114272 | training loss: 0.0006180782220326364\n",
      "epoch: 7 | 51264 / 114272 | training loss: 0.0004666999739129096\n",
      "epoch: 7 | 51296 / 114272 | training loss: 0.00040165905375033617\n",
      "epoch: 7 | 51328 / 114272 | training loss: 0.0006308259908109903\n",
      "epoch: 7 | 51360 / 114272 | training loss: 0.0007531417068094015\n",
      "epoch: 7 | 51392 / 114272 | training loss: 0.0007204978028312325\n",
      "epoch: 7 | 51424 / 114272 | training loss: 0.00040564878145232797\n",
      "epoch: 7 | 51456 / 114272 | training loss: 0.00024049954663496464\n",
      "epoch: 7 | 51488 / 114272 | training loss: 0.000717263959813863\n",
      "epoch: 7 | 51520 / 114272 | training loss: 0.0003909811784978956\n",
      "epoch: 7 | 51552 / 114272 | training loss: 0.14572006464004517\n",
      "epoch: 7 | 51584 / 114272 | training loss: 0.0002159479190595448\n",
      "epoch: 7 | 51616 / 114272 | training loss: 0.0005622151074931026\n",
      "epoch: 7 | 51648 / 114272 | training loss: 0.0003396435349714011\n",
      "epoch: 7 | 51680 / 114272 | training loss: 0.00019771848747041076\n",
      "epoch: 7 | 51712 / 114272 | training loss: 0.000515528314281255\n",
      "epoch: 7 | 51744 / 114272 | training loss: 0.00021088507492095232\n",
      "epoch: 7 | 51776 / 114272 | training loss: 0.00042053929064422846\n",
      "epoch: 7 | 51808 / 114272 | training loss: 0.018591424450278282\n",
      "epoch: 7 | 51840 / 114272 | training loss: 0.032833345234394073\n",
      "epoch: 7 | 51872 / 114272 | training loss: 0.0006330460892058909\n",
      "epoch: 7 | 51904 / 114272 | training loss: 0.03635852783918381\n",
      "epoch: 7 | 51936 / 114272 | training loss: 0.00033112819073721766\n",
      "epoch: 7 | 51968 / 114272 | training loss: 0.0004622561391443014\n",
      "epoch: 7 | 52000 / 114272 | training loss: 0.006908135488629341\n",
      "epoch: 7 | 52032 / 114272 | training loss: 0.0003297398507129401\n",
      "epoch: 7 | 52064 / 114272 | training loss: 0.00034544008667580783\n",
      "epoch: 7 | 52096 / 114272 | training loss: 0.0016366398194804788\n",
      "epoch: 7 | 52128 / 114272 | training loss: 0.000258642336120829\n",
      "epoch: 7 | 52160 / 114272 | training loss: 0.0004479259660001844\n",
      "epoch: 7 | 52192 / 114272 | training loss: 0.0006756908842362463\n",
      "epoch: 7 | 52224 / 114272 | training loss: 0.0002815371844917536\n",
      "epoch: 7 | 52256 / 114272 | training loss: 0.00031669833697378635\n",
      "epoch: 7 | 52288 / 114272 | training loss: 0.0008140634163282812\n",
      "epoch: 7 | 52320 / 114272 | training loss: 0.0006246728007681668\n",
      "epoch: 7 | 52352 / 114272 | training loss: 0.06452974677085876\n",
      "epoch: 7 | 52384 / 114272 | training loss: 0.0015951296081766486\n",
      "epoch: 7 | 52416 / 114272 | training loss: 0.0003388477489352226\n",
      "epoch: 7 | 52448 / 114272 | training loss: 0.0004302413435652852\n",
      "epoch: 7 | 52480 / 114272 | training loss: 0.0013122932286933064\n",
      "epoch: 7 | 52512 / 114272 | training loss: 0.0003477497084531933\n",
      "epoch: 7 | 52544 / 114272 | training loss: 0.0022310023196041584\n",
      "epoch: 7 | 52576 / 114272 | training loss: 0.00030021314159967005\n",
      "epoch: 7 | 52608 / 114272 | training loss: 0.0017840958898887038\n",
      "epoch: 7 | 52640 / 114272 | training loss: 0.020070338621735573\n",
      "epoch: 7 | 52672 / 114272 | training loss: 0.1891116499900818\n",
      "epoch: 7 | 52704 / 114272 | training loss: 0.0025449523236602545\n",
      "epoch: 7 | 52736 / 114272 | training loss: 0.0016174851916730404\n",
      "epoch: 7 | 52768 / 114272 | training loss: 0.00036889599869027734\n",
      "epoch: 7 | 52800 / 114272 | training loss: 0.0006212929147295654\n",
      "epoch: 7 | 52832 / 114272 | training loss: 0.0003876977425534278\n",
      "epoch: 7 | 52864 / 114272 | training loss: 0.0006089030066505075\n",
      "epoch: 7 | 52896 / 114272 | training loss: 0.001978218322619796\n",
      "epoch: 7 | 52928 / 114272 | training loss: 0.000500632100738585\n",
      "epoch: 7 | 52960 / 114272 | training loss: 0.0003748551243916154\n",
      "epoch: 7 | 52992 / 114272 | training loss: 0.0002642348117660731\n",
      "epoch: 7 | 53024 / 114272 | training loss: 0.000376797717763111\n",
      "epoch: 7 | 53056 / 114272 | training loss: 0.00015539412561338395\n",
      "epoch: 7 | 53088 / 114272 | training loss: 0.0020448844879865646\n",
      "epoch: 7 | 53120 / 114272 | training loss: 0.0003292435430921614\n",
      "epoch: 7 | 53152 / 114272 | training loss: 0.0005646585486829281\n",
      "epoch: 7 | 53184 / 114272 | training loss: 0.00046282607945613563\n",
      "epoch: 7 | 53216 / 114272 | training loss: 0.0002129336935468018\n",
      "epoch: 7 | 53248 / 114272 | training loss: 0.0002834386832546443\n",
      "epoch: 7 | 53280 / 114272 | training loss: 0.0004672507056966424\n",
      "epoch: 7 | 53312 / 114272 | training loss: 0.0003750768373720348\n",
      "epoch: 7 | 53344 / 114272 | training loss: 0.0008500864496454597\n",
      "epoch: 7 | 53376 / 114272 | training loss: 0.00031395870610140264\n",
      "epoch: 7 | 53408 / 114272 | training loss: 0.0004650037444662303\n",
      "epoch: 7 | 53440 / 114272 | training loss: 0.000370390887837857\n",
      "epoch: 7 | 53472 / 114272 | training loss: 0.0009240927756763995\n",
      "epoch: 7 | 53504 / 114272 | training loss: 0.16923244297504425\n",
      "epoch: 7 | 53536 / 114272 | training loss: 0.000511070538777858\n",
      "epoch: 7 | 53568 / 114272 | training loss: 0.0004384387284517288\n",
      "epoch: 7 | 53600 / 114272 | training loss: 0.001489711576141417\n",
      "epoch: 7 | 53632 / 114272 | training loss: 0.0003498232108540833\n",
      "epoch: 7 | 53664 / 114272 | training loss: 0.00031732706702314317\n",
      "epoch: 7 | 53696 / 114272 | training loss: 0.00022586507839150727\n",
      "epoch: 7 | 53728 / 114272 | training loss: 0.00014519522665068507\n",
      "epoch: 7 | 53760 / 114272 | training loss: 0.0016276109963655472\n",
      "epoch: 7 | 53792 / 114272 | training loss: 0.07563570886850357\n",
      "epoch: 7 | 53824 / 114272 | training loss: 0.15996873378753662\n",
      "epoch: 7 | 53856 / 114272 | training loss: 0.000743479875382036\n",
      "epoch: 7 | 53888 / 114272 | training loss: 0.0004151840985286981\n",
      "epoch: 7 | 53920 / 114272 | training loss: 0.00046516655129380524\n",
      "epoch: 7 | 53952 / 114272 | training loss: 0.0006333704805001616\n",
      "epoch: 7 | 53984 / 114272 | training loss: 0.00010608813317958266\n",
      "epoch: 7 | 54016 / 114272 | training loss: 0.0025779157876968384\n",
      "epoch: 7 | 54048 / 114272 | training loss: 0.0005512912757694721\n",
      "epoch: 7 | 54080 / 114272 | training loss: 0.0006133015267550945\n",
      "epoch: 7 | 54112 / 114272 | training loss: 0.0005755882011726499\n",
      "epoch: 7 | 54144 / 114272 | training loss: 0.0006813856307417154\n",
      "epoch: 7 | 54176 / 114272 | training loss: 0.0005586495390161872\n",
      "epoch: 7 | 54208 / 114272 | training loss: 0.00043302023550495505\n",
      "epoch: 7 | 54240 / 114272 | training loss: 0.000554154219571501\n",
      "epoch: 7 | 54272 / 114272 | training loss: 0.027040177956223488\n",
      "epoch: 7 | 54304 / 114272 | training loss: 0.000568962306715548\n",
      "epoch: 7 | 54336 / 114272 | training loss: 0.00013988347200211138\n",
      "epoch: 7 | 54368 / 114272 | training loss: 0.0009155314182862639\n",
      "epoch: 7 | 54400 / 114272 | training loss: 0.0002894312492571771\n",
      "epoch: 7 | 54432 / 114272 | training loss: 0.00019315122335683554\n",
      "epoch: 7 | 54464 / 114272 | training loss: 0.2885327935218811\n",
      "epoch: 7 | 54496 / 114272 | training loss: 0.00028431491227820516\n",
      "epoch: 7 | 54528 / 114272 | training loss: 0.0002715067530516535\n",
      "epoch: 7 | 54560 / 114272 | training loss: 0.0002870897005777806\n",
      "epoch: 7 | 54592 / 114272 | training loss: 0.00016043057257775217\n",
      "epoch: 7 | 54624 / 114272 | training loss: 0.00042178266448900104\n",
      "epoch: 7 | 54656 / 114272 | training loss: 0.00030334520852193236\n",
      "epoch: 7 | 54688 / 114272 | training loss: 0.056256089359521866\n",
      "epoch: 7 | 54720 / 114272 | training loss: 0.000995009671896696\n",
      "epoch: 7 | 54752 / 114272 | training loss: 0.00022182178508955985\n",
      "epoch: 7 | 54784 / 114272 | training loss: 0.0005825624102726579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 54816 / 114272 | training loss: 0.0004150246095377952\n",
      "epoch: 7 | 54848 / 114272 | training loss: 0.00045197666622698307\n",
      "epoch: 7 | 54880 / 114272 | training loss: 0.0006887535564601421\n",
      "epoch: 7 | 54912 / 114272 | training loss: 0.00042722371290437877\n",
      "epoch: 7 | 54944 / 114272 | training loss: 0.0003902409807778895\n",
      "epoch: 7 | 54976 / 114272 | training loss: 0.0004804991476703435\n",
      "epoch: 7 | 55008 / 114272 | training loss: 0.0003159851476084441\n",
      "epoch: 7 | 55040 / 114272 | training loss: 0.0003184610977768898\n",
      "epoch: 7 | 55072 / 114272 | training loss: 0.30068308115005493\n",
      "epoch: 7 | 55104 / 114272 | training loss: 0.0006942256586626172\n",
      "epoch: 7 | 55136 / 114272 | training loss: 0.0005087464232929051\n",
      "epoch: 7 | 55168 / 114272 | training loss: 0.003940162248909473\n",
      "epoch: 7 | 55200 / 114272 | training loss: 0.0002734491426963359\n",
      "epoch: 7 | 55232 / 114272 | training loss: 0.23625528812408447\n",
      "epoch: 7 | 55264 / 114272 | training loss: 0.0006761886179447174\n",
      "epoch: 7 | 55296 / 114272 | training loss: 0.19516725838184357\n",
      "epoch: 7 | 55328 / 114272 | training loss: 0.000330622133333236\n",
      "epoch: 7 | 55360 / 114272 | training loss: 0.0004559132212307304\n",
      "epoch: 7 | 55392 / 114272 | training loss: 0.0003670866717584431\n",
      "epoch: 7 | 55424 / 114272 | training loss: 0.1787130981683731\n",
      "epoch: 7 | 55456 / 114272 | training loss: 0.0003506859648041427\n",
      "epoch: 7 | 55488 / 114272 | training loss: 0.03201858326792717\n",
      "epoch: 7 | 55520 / 114272 | training loss: 0.000474369153380394\n",
      "epoch: 7 | 55552 / 114272 | training loss: 0.00044367162627168\n",
      "epoch: 7 | 55584 / 114272 | training loss: 0.0006079733138903975\n",
      "epoch: 7 | 55616 / 114272 | training loss: 0.0005350321880541742\n",
      "epoch: 7 | 55648 / 114272 | training loss: 0.0004352947580628097\n",
      "epoch: 7 | 55680 / 114272 | training loss: 0.0005983517039567232\n",
      "epoch: 7 | 55712 / 114272 | training loss: 0.002515388187021017\n",
      "epoch: 7 | 55744 / 114272 | training loss: 0.0004772367828991264\n",
      "epoch: 7 | 55776 / 114272 | training loss: 0.000915169483050704\n",
      "epoch: 7 | 55808 / 114272 | training loss: 0.0007545342086814344\n",
      "epoch: 7 | 55840 / 114272 | training loss: 0.0005927314632572234\n",
      "epoch: 7 | 55872 / 114272 | training loss: 0.0017165819881483912\n",
      "epoch: 7 | 55904 / 114272 | training loss: 0.00090308632934466\n",
      "epoch: 7 | 55936 / 114272 | training loss: 0.0005440376698970795\n",
      "epoch: 7 | 55968 / 114272 | training loss: 0.0007836309378035367\n",
      "epoch: 7 | 56000 / 114272 | training loss: 0.0008771849097684026\n",
      "epoch: 7 | 56032 / 114272 | training loss: 0.0008231097017414868\n",
      "epoch: 7 | 56064 / 114272 | training loss: 0.0005787576083093882\n",
      "epoch: 7 | 56096 / 114272 | training loss: 0.10597428679466248\n",
      "epoch: 7 | 56128 / 114272 | training loss: 0.0006453256937675178\n",
      "epoch: 7 | 56160 / 114272 | training loss: 0.0007001370540820062\n",
      "epoch: 7 | 56192 / 114272 | training loss: 0.0004770374216604978\n",
      "epoch: 7 | 56224 / 114272 | training loss: 0.00044784226338379085\n",
      "epoch: 7 | 56256 / 114272 | training loss: 0.12259677797555923\n",
      "epoch: 7 | 56288 / 114272 | training loss: 0.0993548259139061\n",
      "epoch: 7 | 56320 / 114272 | training loss: 0.19155454635620117\n",
      "epoch: 7 | 56352 / 114272 | training loss: 0.0014216736890375614\n",
      "epoch: 7 | 56384 / 114272 | training loss: 0.0007752986275590956\n",
      "epoch: 7 | 56416 / 114272 | training loss: 0.0019977574702352285\n",
      "epoch: 7 | 56448 / 114272 | training loss: 0.09658388048410416\n",
      "epoch: 7 | 56480 / 114272 | training loss: 0.0008585924515500665\n",
      "epoch: 7 | 56512 / 114272 | training loss: 0.0003960121830459684\n",
      "epoch: 7 | 56544 / 114272 | training loss: 0.0010136117925867438\n",
      "epoch: 7 | 56576 / 114272 | training loss: 0.0011499950196594\n",
      "epoch: 7 | 56608 / 114272 | training loss: 0.3631909191608429\n",
      "epoch: 7 | 56640 / 114272 | training loss: 0.0007118777721188962\n",
      "epoch: 7 | 56672 / 114272 | training loss: 0.0007521772058680654\n",
      "epoch: 7 | 56704 / 114272 | training loss: 0.0009112575207836926\n",
      "epoch: 7 | 56736 / 114272 | training loss: 0.0013102249940857291\n",
      "epoch: 7 | 56768 / 114272 | training loss: 0.002081925282254815\n",
      "epoch: 7 | 56800 / 114272 | training loss: 0.0008436657371930778\n",
      "epoch: 7 | 56832 / 114272 | training loss: 0.001639113761484623\n",
      "epoch: 7 | 56864 / 114272 | training loss: 0.0007598867523483932\n",
      "epoch: 7 | 56896 / 114272 | training loss: 0.0012983661144971848\n",
      "epoch: 7 | 56928 / 114272 | training loss: 0.005265576299279928\n",
      "epoch: 7 | 56960 / 114272 | training loss: 0.0010417512385174632\n",
      "epoch: 7 | 56992 / 114272 | training loss: 0.0009507802315056324\n",
      "epoch: 7 | 57024 / 114272 | training loss: 0.0007249476620927453\n",
      "epoch: 7 | 57056 / 114272 | training loss: 0.0014218644937500358\n",
      "epoch: 7 | 57088 / 114272 | training loss: 0.001898835296742618\n",
      "epoch: 7 | 57120 / 114272 | training loss: 0.05328848958015442\n",
      "epoch: 7 | 57152 / 114272 | training loss: 0.001614784705452621\n",
      "epoch: 7 | 57184 / 114272 | training loss: 0.0008114227675832808\n",
      "epoch: 7 | 57216 / 114272 | training loss: 0.001086987555027008\n",
      "epoch: 7 | 57248 / 114272 | training loss: 0.0013361130841076374\n",
      "epoch: 7 | 57280 / 114272 | training loss: 0.0006188444676809013\n",
      "epoch: 7 | 57312 / 114272 | training loss: 0.0007415710133500397\n",
      "epoch: 7 | 57344 / 114272 | training loss: 0.0013704458251595497\n",
      "epoch: 7 | 57376 / 114272 | training loss: 0.0008447222644463181\n",
      "epoch: 7 | 57408 / 114272 | training loss: 0.001232924172654748\n",
      "epoch: 7 | 57440 / 114272 | training loss: 0.0011021560057997704\n",
      "epoch: 7 | 57472 / 114272 | training loss: 0.0009957714937627316\n",
      "epoch: 7 | 57504 / 114272 | training loss: 0.0010345210321247578\n",
      "epoch: 7 | 57536 / 114272 | training loss: 0.0007410862599499524\n",
      "epoch: 7 | 57568 / 114272 | training loss: 0.0010087593691423535\n",
      "epoch: 7 | 57600 / 114272 | training loss: 0.0019335036631673574\n",
      "epoch: 7 | 57632 / 114272 | training loss: 0.0008369091083295643\n",
      "epoch: 7 | 57664 / 114272 | training loss: 0.0007997353095561266\n",
      "epoch: 7 | 57696 / 114272 | training loss: 0.0011150410864502192\n",
      "epoch: 7 | 57728 / 114272 | training loss: 0.001746100140735507\n",
      "epoch: 7 | 57760 / 114272 | training loss: 0.12235474586486816\n",
      "epoch: 7 | 57792 / 114272 | training loss: 0.001116646220907569\n",
      "epoch: 7 | 57824 / 114272 | training loss: 0.0010153766488656402\n",
      "epoch: 7 | 57856 / 114272 | training loss: 0.0009565297514200211\n",
      "epoch: 7 | 57888 / 114272 | training loss: 0.0005247134831734002\n",
      "epoch: 7 | 57920 / 114272 | training loss: 0.3105105757713318\n",
      "epoch: 7 | 57952 / 114272 | training loss: 0.0007795614656060934\n",
      "epoch: 7 | 57984 / 114272 | training loss: 0.0007868539541959763\n",
      "epoch: 7 | 58016 / 114272 | training loss: 0.0009349547326564789\n",
      "epoch: 7 | 58048 / 114272 | training loss: 0.0009434442035853863\n",
      "epoch: 7 | 58080 / 114272 | training loss: 0.0005587671184912324\n",
      "epoch: 7 | 58112 / 114272 | training loss: 0.0009021463338285685\n",
      "epoch: 7 | 58144 / 114272 | training loss: 0.000563379842787981\n",
      "epoch: 7 | 58176 / 114272 | training loss: 0.0011349491542205215\n",
      "epoch: 7 | 58208 / 114272 | training loss: 0.0004199152463115752\n",
      "epoch: 7 | 58240 / 114272 | training loss: 0.0007100396906025708\n",
      "epoch: 7 | 58272 / 114272 | training loss: 0.0007482566288672388\n",
      "epoch: 7 | 58304 / 114272 | training loss: 0.0011573852971196175\n",
      "epoch: 7 | 58336 / 114272 | training loss: 0.000801262678578496\n",
      "epoch: 7 | 58368 / 114272 | training loss: 0.0007583660772070289\n",
      "epoch: 7 | 58400 / 114272 | training loss: 0.0024500328581780195\n",
      "epoch: 7 | 58432 / 114272 | training loss: 0.0007887876126915216\n",
      "epoch: 7 | 58464 / 114272 | training loss: 0.0005725861992686987\n",
      "epoch: 7 | 58496 / 114272 | training loss: 0.0007635655929334462\n",
      "epoch: 7 | 58528 / 114272 | training loss: 0.0006722689722664654\n",
      "epoch: 7 | 58560 / 114272 | training loss: 0.001813625218346715\n",
      "epoch: 7 | 58592 / 114272 | training loss: 0.0006291134050115943\n",
      "epoch: 7 | 58624 / 114272 | training loss: 0.000629214511718601\n",
      "epoch: 7 | 58656 / 114272 | training loss: 0.0005116247339174151\n",
      "epoch: 7 | 58688 / 114272 | training loss: 0.15321585536003113\n",
      "epoch: 7 | 58720 / 114272 | training loss: 0.000451562344096601\n",
      "epoch: 7 | 58752 / 114272 | training loss: 0.20815597474575043\n",
      "epoch: 7 | 58784 / 114272 | training loss: 0.21501941978931427\n",
      "epoch: 7 | 58816 / 114272 | training loss: 0.000910703674890101\n",
      "epoch: 7 | 58848 / 114272 | training loss: 0.0007359539158642292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 58880 / 114272 | training loss: 0.0015905832406133413\n",
      "epoch: 7 | 58912 / 114272 | training loss: 0.18525202572345734\n",
      "epoch: 7 | 58944 / 114272 | training loss: 0.011329732835292816\n",
      "epoch: 7 | 58976 / 114272 | training loss: 0.0007279616547748446\n",
      "epoch: 7 | 59008 / 114272 | training loss: 0.0008378262282349169\n",
      "epoch: 7 | 59040 / 114272 | training loss: 0.0008201613673008978\n",
      "epoch: 7 | 59072 / 114272 | training loss: 0.0008361937943845987\n",
      "epoch: 7 | 59104 / 114272 | training loss: 0.0063259778544306755\n",
      "epoch: 7 | 59136 / 114272 | training loss: 0.0020035186316818\n",
      "epoch: 7 | 59168 / 114272 | training loss: 0.00044413743307814\n",
      "epoch: 7 | 59200 / 114272 | training loss: 0.3440774381160736\n",
      "epoch: 7 | 59232 / 114272 | training loss: 0.0010492425644770265\n",
      "epoch: 7 | 59264 / 114272 | training loss: 0.04364058002829552\n",
      "epoch: 7 | 59296 / 114272 | training loss: 0.0004950116272084415\n",
      "epoch: 7 | 59328 / 114272 | training loss: 0.0009038003627210855\n",
      "epoch: 7 | 59360 / 114272 | training loss: 0.0005107310134917498\n",
      "epoch: 7 | 59392 / 114272 | training loss: 0.0007646780577488244\n",
      "epoch: 7 | 59424 / 114272 | training loss: 0.006705977953970432\n",
      "epoch: 7 | 59456 / 114272 | training loss: 0.0010922456858679652\n",
      "epoch: 7 | 59488 / 114272 | training loss: 0.001048863516189158\n",
      "epoch: 7 | 59520 / 114272 | training loss: 0.0012695285258814692\n",
      "epoch: 7 | 59552 / 114272 | training loss: 0.0005710957339033484\n",
      "epoch: 7 | 59584 / 114272 | training loss: 0.0006563256029039621\n",
      "epoch: 7 | 59616 / 114272 | training loss: 0.0006157553871162236\n",
      "epoch: 7 | 59648 / 114272 | training loss: 0.000809818971902132\n",
      "epoch: 7 | 59680 / 114272 | training loss: 0.001872560242190957\n",
      "epoch: 7 | 59712 / 114272 | training loss: 0.0005791879957541823\n",
      "epoch: 7 | 59744 / 114272 | training loss: 0.0007668862817808986\n",
      "epoch: 7 | 59776 / 114272 | training loss: 0.00048205850180238485\n",
      "epoch: 7 | 59808 / 114272 | training loss: 0.0008743832004256546\n",
      "epoch: 7 | 59840 / 114272 | training loss: 0.0006832391954958439\n",
      "epoch: 7 | 59872 / 114272 | training loss: 0.09464255720376968\n",
      "epoch: 7 | 59904 / 114272 | training loss: 0.00134839687962085\n",
      "epoch: 7 | 59936 / 114272 | training loss: 0.0008566697360947728\n",
      "epoch: 7 | 59968 / 114272 | training loss: 0.0010435476433485746\n",
      "epoch: 7 | 60000 / 114272 | training loss: 0.0006992267444729805\n",
      "epoch: 7 | 60032 / 114272 | training loss: 0.0005463160341605544\n",
      "epoch: 7 | 60064 / 114272 | training loss: 0.06428056955337524\n",
      "epoch: 7 | 60096 / 114272 | training loss: 0.0005403936957009137\n",
      "epoch: 7 | 60128 / 114272 | training loss: 0.0007884235237725079\n",
      "epoch: 7 | 60160 / 114272 | training loss: 0.0007005189545452595\n",
      "epoch: 7 | 60192 / 114272 | training loss: 0.0005920872208662331\n",
      "epoch: 7 | 60224 / 114272 | training loss: 0.0012631204444915056\n",
      "epoch: 7 | 60256 / 114272 | training loss: 0.000842120498418808\n",
      "epoch: 7 | 60288 / 114272 | training loss: 0.0007942738593555987\n",
      "epoch: 7 | 60320 / 114272 | training loss: 0.0007443446666002274\n",
      "epoch: 7 | 60352 / 114272 | training loss: 0.000615654862485826\n",
      "epoch: 7 | 60384 / 114272 | training loss: 0.0005801053484901786\n",
      "epoch: 7 | 60416 / 114272 | training loss: 0.0006061229505576193\n",
      "epoch: 7 | 60448 / 114272 | training loss: 0.006740442011505365\n",
      "epoch: 7 | 60480 / 114272 | training loss: 0.001125241513364017\n",
      "epoch: 7 | 60512 / 114272 | training loss: 0.0005533175426535308\n",
      "epoch: 7 | 60544 / 114272 | training loss: 0.05814184993505478\n",
      "epoch: 7 | 60576 / 114272 | training loss: 0.0007457062602043152\n",
      "epoch: 7 | 60608 / 114272 | training loss: 0.008986142463982105\n",
      "epoch: 7 | 60640 / 114272 | training loss: 0.0007271768408827484\n",
      "epoch: 7 | 60672 / 114272 | training loss: 0.0039030578918755054\n",
      "epoch: 7 | 60704 / 114272 | training loss: 0.0007716321852058172\n",
      "epoch: 7 | 60736 / 114272 | training loss: 0.0008926885202527046\n",
      "epoch: 7 | 60768 / 114272 | training loss: 0.0005328258848749101\n",
      "epoch: 7 | 60800 / 114272 | training loss: 0.0006122677586972713\n",
      "epoch: 7 | 60832 / 114272 | training loss: 0.0006697204662486911\n",
      "epoch: 7 | 60864 / 114272 | training loss: 0.15484865009784698\n",
      "epoch: 7 | 60896 / 114272 | training loss: 0.0003108177043031901\n",
      "epoch: 7 | 60928 / 114272 | training loss: 0.0005595041438937187\n",
      "epoch: 7 | 60960 / 114272 | training loss: 0.0007767181377857924\n",
      "epoch: 7 | 60992 / 114272 | training loss: 0.0022094182204455137\n",
      "epoch: 7 | 61024 / 114272 | training loss: 0.0005893688648939133\n",
      "epoch: 7 | 61056 / 114272 | training loss: 0.0005581848090514541\n",
      "epoch: 7 | 61088 / 114272 | training loss: 0.0005528585170395672\n",
      "epoch: 7 | 61120 / 114272 | training loss: 0.12208902090787888\n",
      "epoch: 7 | 61152 / 114272 | training loss: 0.0015301713719964027\n",
      "epoch: 7 | 61184 / 114272 | training loss: 0.0005675583961419761\n",
      "epoch: 7 | 61216 / 114272 | training loss: 0.00031687598675489426\n",
      "epoch: 7 | 61248 / 114272 | training loss: 0.0008564188028685749\n",
      "epoch: 7 | 61280 / 114272 | training loss: 0.029638759791851044\n",
      "epoch: 7 | 61312 / 114272 | training loss: 0.001106110867112875\n",
      "epoch: 7 | 61344 / 114272 | training loss: 0.000993118155747652\n",
      "epoch: 7 | 61376 / 114272 | training loss: 0.0005302514764480293\n",
      "epoch: 7 | 61408 / 114272 | training loss: 0.0009063392644748092\n",
      "epoch: 7 | 61440 / 114272 | training loss: 0.011947116814553738\n",
      "epoch: 7 | 61472 / 114272 | training loss: 0.00039872105116955936\n",
      "epoch: 7 | 61504 / 114272 | training loss: 0.0008763547521084547\n",
      "epoch: 7 | 61536 / 114272 | training loss: 0.0005785776302218437\n",
      "epoch: 7 | 61568 / 114272 | training loss: 0.0005254019051790237\n",
      "epoch: 7 | 61600 / 114272 | training loss: 0.0007749561918899417\n",
      "epoch: 7 | 61632 / 114272 | training loss: 0.0011428624857217073\n",
      "epoch: 7 | 61664 / 114272 | training loss: 0.0006180949276313186\n",
      "epoch: 7 | 61696 / 114272 | training loss: 0.0005184262990951538\n",
      "epoch: 7 | 61728 / 114272 | training loss: 0.09433172643184662\n",
      "epoch: 7 | 61760 / 114272 | training loss: 0.13854235410690308\n",
      "epoch: 7 | 61792 / 114272 | training loss: 0.00040229244041256607\n",
      "epoch: 7 | 61824 / 114272 | training loss: 0.0006244006217457354\n",
      "epoch: 7 | 61856 / 114272 | training loss: 0.0016905444208532572\n",
      "epoch: 7 | 61888 / 114272 | training loss: 0.000787820783443749\n",
      "epoch: 7 | 61920 / 114272 | training loss: 0.00031115824822336435\n",
      "epoch: 7 | 61952 / 114272 | training loss: 0.29557767510414124\n",
      "epoch: 7 | 61984 / 114272 | training loss: 0.00035723779001273215\n",
      "epoch: 7 | 62016 / 114272 | training loss: 0.0033819067757576704\n",
      "epoch: 7 | 62048 / 114272 | training loss: 0.07690601050853729\n",
      "epoch: 7 | 62080 / 114272 | training loss: 0.0005372219602577388\n",
      "epoch: 7 | 62112 / 114272 | training loss: 0.0004103699466213584\n",
      "epoch: 7 | 62144 / 114272 | training loss: 0.0004293056554161012\n",
      "epoch: 7 | 62176 / 114272 | training loss: 0.0008240269962698221\n",
      "epoch: 7 | 62208 / 114272 | training loss: 0.0003839928249362856\n",
      "epoch: 7 | 62240 / 114272 | training loss: 0.0004498300841078162\n",
      "epoch: 7 | 62272 / 114272 | training loss: 0.010948361828923225\n",
      "epoch: 7 | 62304 / 114272 | training loss: 0.0004051447904203087\n",
      "epoch: 7 | 62336 / 114272 | training loss: 0.0019063777290284634\n",
      "epoch: 7 | 62368 / 114272 | training loss: 0.000821964698843658\n",
      "epoch: 7 | 62400 / 114272 | training loss: 0.000434153713285923\n",
      "epoch: 7 | 62432 / 114272 | training loss: 0.00034298369428142905\n",
      "epoch: 7 | 62464 / 114272 | training loss: 0.0005914552020840347\n",
      "epoch: 7 | 62496 / 114272 | training loss: 0.0007941816002130508\n",
      "epoch: 7 | 62528 / 114272 | training loss: 0.0004162476980127394\n",
      "epoch: 7 | 62560 / 114272 | training loss: 0.0004131724708713591\n",
      "epoch: 7 | 62592 / 114272 | training loss: 0.0004918106133118272\n",
      "epoch: 7 | 62624 / 114272 | training loss: 0.0006212114240042865\n",
      "epoch: 7 | 62656 / 114272 | training loss: 0.045112766325473785\n",
      "epoch: 7 | 62688 / 114272 | training loss: 0.0005365610122680664\n",
      "epoch: 7 | 62720 / 114272 | training loss: 0.056461699306964874\n",
      "epoch: 7 | 62752 / 114272 | training loss: 0.0007402319461107254\n",
      "epoch: 7 | 62784 / 114272 | training loss: 0.1744914948940277\n",
      "epoch: 7 | 62816 / 114272 | training loss: 0.0022083779331296682\n",
      "epoch: 7 | 62848 / 114272 | training loss: 0.0005402256501838565\n",
      "epoch: 7 | 62880 / 114272 | training loss: 0.0005905354046262801\n",
      "epoch: 7 | 62912 / 114272 | training loss: 0.0008628062787465751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 62944 / 114272 | training loss: 0.0007804109482094646\n",
      "epoch: 7 | 62976 / 114272 | training loss: 0.0005361611838452518\n",
      "epoch: 7 | 63008 / 114272 | training loss: 0.00030499036074616015\n",
      "epoch: 7 | 63040 / 114272 | training loss: 0.1974841058254242\n",
      "epoch: 7 | 63072 / 114272 | training loss: 0.000526487419847399\n",
      "epoch: 7 | 63104 / 114272 | training loss: 0.0004530689329840243\n",
      "epoch: 7 | 63136 / 114272 | training loss: 0.0004047811671625823\n",
      "epoch: 7 | 63168 / 114272 | training loss: 0.000457179470686242\n",
      "epoch: 7 | 63200 / 114272 | training loss: 0.0006772575434297323\n",
      "epoch: 7 | 63232 / 114272 | training loss: 0.0006447990308515728\n",
      "epoch: 7 | 63264 / 114272 | training loss: 0.0017354136798530817\n",
      "epoch: 7 | 63296 / 114272 | training loss: 0.0008181401644833386\n",
      "epoch: 7 | 63328 / 114272 | training loss: 0.000696006987709552\n",
      "epoch: 7 | 63360 / 114272 | training loss: 0.0007431826088577509\n",
      "epoch: 7 | 63392 / 114272 | training loss: 0.0005503285792656243\n",
      "epoch: 7 | 63424 / 114272 | training loss: 0.0016991840675473213\n",
      "epoch: 7 | 63456 / 114272 | training loss: 0.0008101704879663885\n",
      "epoch: 7 | 63488 / 114272 | training loss: 0.0007116009946912527\n",
      "epoch: 7 | 63520 / 114272 | training loss: 0.0010107450652867556\n",
      "epoch: 7 | 63552 / 114272 | training loss: 0.0006050296942703426\n",
      "epoch: 7 | 63584 / 114272 | training loss: 0.125019371509552\n",
      "epoch: 7 | 63616 / 114272 | training loss: 0.0009577111341059208\n",
      "epoch: 7 | 63648 / 114272 | training loss: 0.023003660142421722\n",
      "epoch: 7 | 63680 / 114272 | training loss: 0.0008692428236827254\n",
      "epoch: 7 | 63712 / 114272 | training loss: 0.0005624447367154062\n",
      "epoch: 7 | 63744 / 114272 | training loss: 0.0017942608101293445\n",
      "epoch: 7 | 63776 / 114272 | training loss: 0.08022084087133408\n",
      "epoch: 7 | 63808 / 114272 | training loss: 0.0006878927815705538\n",
      "epoch: 7 | 63840 / 114272 | training loss: 0.05275821313261986\n",
      "epoch: 7 | 63872 / 114272 | training loss: 0.00033321077353321016\n",
      "epoch: 7 | 63904 / 114272 | training loss: 0.00048272666754201055\n",
      "epoch: 7 | 63936 / 114272 | training loss: 0.0006495206616818905\n",
      "epoch: 7 | 63968 / 114272 | training loss: 0.0006185875972732902\n",
      "epoch: 7 | 64000 / 114272 | training loss: 0.0005553088267333806\n",
      "epoch: 7 | 64032 / 114272 | training loss: 0.0005445072310976684\n",
      "epoch: 7 | 64064 / 114272 | training loss: 0.0004251696518622339\n",
      "epoch: 7 | 64096 / 114272 | training loss: 0.0007223432185128331\n",
      "epoch: 7 | 64128 / 114272 | training loss: 0.0037445235066115856\n",
      "epoch: 7 | 64160 / 114272 | training loss: 0.000953277456574142\n",
      "epoch: 7 | 64192 / 114272 | training loss: 0.0015193622093647718\n",
      "epoch: 7 | 64224 / 114272 | training loss: 0.0006988499662838876\n",
      "epoch: 7 | 64256 / 114272 | training loss: 0.0014377039624378085\n",
      "epoch: 7 | 64288 / 114272 | training loss: 0.0007269940106198192\n",
      "epoch: 7 | 64320 / 114272 | training loss: 0.0009104783530347049\n",
      "epoch: 7 | 64352 / 114272 | training loss: 0.0009748482261784375\n",
      "epoch: 7 | 64384 / 114272 | training loss: 0.13643935322761536\n",
      "epoch: 7 | 64416 / 114272 | training loss: 0.0036635405849665403\n",
      "epoch: 7 | 64448 / 114272 | training loss: 0.0005202841712161899\n",
      "epoch: 7 | 64480 / 114272 | training loss: 0.000459584261989221\n",
      "epoch: 7 | 64512 / 114272 | training loss: 0.00042389624286442995\n",
      "epoch: 7 | 64544 / 114272 | training loss: 0.00027285763644613326\n",
      "epoch: 7 | 64576 / 114272 | training loss: 0.000840300926938653\n",
      "epoch: 7 | 64608 / 114272 | training loss: 0.0007180345128290355\n",
      "epoch: 7 | 64640 / 114272 | training loss: 0.0005848791333846748\n",
      "epoch: 7 | 64672 / 114272 | training loss: 0.0011072707129642367\n",
      "epoch: 7 | 64704 / 114272 | training loss: 0.0008688891539350152\n",
      "epoch: 7 | 64736 / 114272 | training loss: 0.0010031515266746283\n",
      "epoch: 7 | 64768 / 114272 | training loss: 0.0028671598993241787\n",
      "epoch: 7 | 64800 / 114272 | training loss: 0.0007025495287962258\n",
      "epoch: 7 | 64832 / 114272 | training loss: 0.002483400981873274\n",
      "epoch: 7 | 64864 / 114272 | training loss: 0.08264830708503723\n",
      "epoch: 7 | 64896 / 114272 | training loss: 0.017255468294024467\n",
      "epoch: 7 | 64928 / 114272 | training loss: 0.00041931861778721213\n",
      "epoch: 7 | 64960 / 114272 | training loss: 0.001221088576130569\n",
      "epoch: 7 | 64992 / 114272 | training loss: 0.002856925595551729\n",
      "epoch: 7 | 65024 / 114272 | training loss: 0.0005322293727658689\n",
      "epoch: 7 | 65056 / 114272 | training loss: 0.07066132128238678\n",
      "epoch: 7 | 65088 / 114272 | training loss: 0.0005538695259019732\n",
      "epoch: 7 | 65120 / 114272 | training loss: 0.0005809114663861692\n",
      "epoch: 7 | 65152 / 114272 | training loss: 0.0010487537365406752\n",
      "epoch: 7 | 65184 / 114272 | training loss: 0.0002710580884013325\n",
      "epoch: 7 | 65216 / 114272 | training loss: 0.0006321774562820792\n",
      "epoch: 7 | 65248 / 114272 | training loss: 0.00033914606319740415\n",
      "epoch: 7 | 65280 / 114272 | training loss: 0.00038698167190887034\n",
      "epoch: 7 | 65312 / 114272 | training loss: 0.0004755195986945182\n",
      "epoch: 7 | 65344 / 114272 | training loss: 0.0007077454938553274\n",
      "epoch: 7 | 65376 / 114272 | training loss: 0.0007398893358185887\n",
      "epoch: 7 | 65408 / 114272 | training loss: 0.000664571940433234\n",
      "epoch: 7 | 65440 / 114272 | training loss: 0.000563846668228507\n",
      "epoch: 7 | 65472 / 114272 | training loss: 0.01391490176320076\n",
      "epoch: 7 | 65504 / 114272 | training loss: 0.0005209359806030989\n",
      "epoch: 7 | 65536 / 114272 | training loss: 0.11486497521400452\n",
      "epoch: 7 | 65568 / 114272 | training loss: 0.00042508551268838346\n",
      "epoch: 7 | 65600 / 114272 | training loss: 0.0006159257027320564\n",
      "epoch: 7 | 65632 / 114272 | training loss: 0.00033775484189391136\n",
      "epoch: 7 | 65664 / 114272 | training loss: 0.0047601661644876\n",
      "epoch: 7 | 65696 / 114272 | training loss: 0.000932731491047889\n",
      "epoch: 7 | 65728 / 114272 | training loss: 0.03665654733777046\n",
      "epoch: 7 | 65760 / 114272 | training loss: 0.00020344468066468835\n",
      "epoch: 7 | 65792 / 114272 | training loss: 0.0004685849417001009\n",
      "epoch: 7 | 65824 / 114272 | training loss: 0.00038300477899610996\n",
      "epoch: 7 | 65856 / 114272 | training loss: 0.00039355127955786884\n",
      "epoch: 7 | 65888 / 114272 | training loss: 0.0004075716424267739\n",
      "epoch: 7 | 65920 / 114272 | training loss: 0.00028520726482383907\n",
      "epoch: 7 | 65952 / 114272 | training loss: 0.18489092588424683\n",
      "epoch: 7 | 65984 / 114272 | training loss: 0.000512877304572612\n",
      "epoch: 7 | 66016 / 114272 | training loss: 0.0008595120161771774\n",
      "epoch: 7 | 66048 / 114272 | training loss: 0.04190123826265335\n",
      "epoch: 7 | 66080 / 114272 | training loss: 0.0010009725810959935\n",
      "epoch: 7 | 66112 / 114272 | training loss: 0.0003696015337482095\n",
      "epoch: 7 | 66144 / 114272 | training loss: 0.0006888405187055469\n",
      "epoch: 7 | 66176 / 114272 | training loss: 0.0007512390147894621\n",
      "epoch: 7 | 66208 / 114272 | training loss: 0.0003482744505163282\n",
      "epoch: 7 | 66240 / 114272 | training loss: 0.000532005331479013\n",
      "epoch: 7 | 66272 / 114272 | training loss: 0.00042062587453983724\n",
      "epoch: 7 | 66304 / 114272 | training loss: 0.16706399619579315\n",
      "epoch: 7 | 66336 / 114272 | training loss: 0.0003280826495029032\n",
      "epoch: 7 | 66368 / 114272 | training loss: 0.0004303090972825885\n",
      "epoch: 7 | 66400 / 114272 | training loss: 0.00040665382402949035\n",
      "epoch: 7 | 66432 / 114272 | training loss: 0.00038307998329401016\n",
      "epoch: 7 | 66464 / 114272 | training loss: 0.0005963868461549282\n",
      "epoch: 7 | 66496 / 114272 | training loss: 0.000770287704654038\n",
      "epoch: 7 | 66528 / 114272 | training loss: 0.000417642091633752\n",
      "epoch: 7 | 66560 / 114272 | training loss: 0.00044759988668374717\n",
      "epoch: 7 | 66592 / 114272 | training loss: 0.20464183390140533\n",
      "epoch: 7 | 66624 / 114272 | training loss: 0.008785072714090347\n",
      "epoch: 7 | 66656 / 114272 | training loss: 0.0006104035419411957\n",
      "epoch: 7 | 66688 / 114272 | training loss: 0.0002693792339414358\n",
      "epoch: 7 | 66720 / 114272 | training loss: 0.010151569731533527\n",
      "epoch: 7 | 66752 / 114272 | training loss: 0.005336185917258263\n",
      "epoch: 7 | 66784 / 114272 | training loss: 0.00046824652235955\n",
      "epoch: 7 | 66816 / 114272 | training loss: 0.25910499691963196\n",
      "epoch: 7 | 66848 / 114272 | training loss: 0.2283705621957779\n",
      "epoch: 7 | 66880 / 114272 | training loss: 0.0004009092226624489\n",
      "epoch: 7 | 66912 / 114272 | training loss: 0.0007265459280461073\n",
      "epoch: 7 | 66944 / 114272 | training loss: 0.00048455342766828835\n",
      "epoch: 7 | 66976 / 114272 | training loss: 0.0011441053356975317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 67008 / 114272 | training loss: 0.0004939058562740684\n",
      "epoch: 7 | 67040 / 114272 | training loss: 0.0009871961083263159\n",
      "epoch: 7 | 67072 / 114272 | training loss: 0.2150440663099289\n",
      "epoch: 7 | 67104 / 114272 | training loss: 0.0002569261414464563\n",
      "epoch: 7 | 67136 / 114272 | training loss: 0.0004192667838651687\n",
      "epoch: 7 | 67168 / 114272 | training loss: 0.0007230354240164161\n",
      "epoch: 7 | 67200 / 114272 | training loss: 0.0004885310190729797\n",
      "epoch: 7 | 67232 / 114272 | training loss: 0.00026898810756392777\n",
      "epoch: 7 | 67264 / 114272 | training loss: 0.2551555633544922\n",
      "epoch: 7 | 67296 / 114272 | training loss: 0.0005771900177933276\n",
      "epoch: 7 | 67328 / 114272 | training loss: 0.032930053770542145\n",
      "epoch: 7 | 67360 / 114272 | training loss: 0.0004552867030724883\n",
      "epoch: 7 | 67392 / 114272 | training loss: 0.0004179020179435611\n",
      "epoch: 7 | 67424 / 114272 | training loss: 0.16229939460754395\n",
      "epoch: 7 | 67456 / 114272 | training loss: 0.15160436928272247\n",
      "epoch: 7 | 67488 / 114272 | training loss: 0.001066348166204989\n",
      "epoch: 7 | 67520 / 114272 | training loss: 0.000709601619746536\n",
      "epoch: 7 | 67552 / 114272 | training loss: 0.0006204687524586916\n",
      "epoch: 7 | 67584 / 114272 | training loss: 0.0007790154195390642\n",
      "epoch: 7 | 67616 / 114272 | training loss: 0.000792865757830441\n",
      "epoch: 7 | 67648 / 114272 | training loss: 0.0008283057832159102\n",
      "epoch: 7 | 67680 / 114272 | training loss: 0.0015035022515803576\n",
      "epoch: 7 | 67712 / 114272 | training loss: 0.0018913140520453453\n",
      "epoch: 7 | 67744 / 114272 | training loss: 0.19428683817386627\n",
      "epoch: 7 | 67776 / 114272 | training loss: 0.0009456992847844958\n",
      "epoch: 7 | 67808 / 114272 | training loss: 0.0007066234829835594\n",
      "epoch: 7 | 67840 / 114272 | training loss: 0.0010455024894326925\n",
      "epoch: 7 | 67872 / 114272 | training loss: 0.0016458629397675395\n",
      "epoch: 7 | 67904 / 114272 | training loss: 0.015771448612213135\n",
      "epoch: 7 | 67936 / 114272 | training loss: 0.0014481756370514631\n",
      "epoch: 7 | 67968 / 114272 | training loss: 0.004123639315366745\n",
      "epoch: 7 | 68000 / 114272 | training loss: 0.00851164385676384\n",
      "epoch: 7 | 68032 / 114272 | training loss: 0.0024819145910441875\n",
      "epoch: 7 | 68064 / 114272 | training loss: 0.001521183759905398\n",
      "epoch: 7 | 68096 / 114272 | training loss: 0.002642563544213772\n",
      "epoch: 7 | 68128 / 114272 | training loss: 0.0007387844379991293\n",
      "epoch: 7 | 68160 / 114272 | training loss: 0.0011688657104969025\n",
      "epoch: 7 | 68192 / 114272 | training loss: 0.015036518685519695\n",
      "epoch: 7 | 68224 / 114272 | training loss: 0.0012101337779313326\n",
      "epoch: 7 | 68256 / 114272 | training loss: 0.0005963106523267925\n",
      "epoch: 7 | 68288 / 114272 | training loss: 0.0006231177831068635\n",
      "epoch: 7 | 68320 / 114272 | training loss: 0.0009435277315787971\n",
      "epoch: 7 | 68352 / 114272 | training loss: 0.0007296112016774714\n",
      "epoch: 7 | 68384 / 114272 | training loss: 0.0007411229889839888\n",
      "epoch: 7 | 68416 / 114272 | training loss: 0.0006421151338145137\n",
      "epoch: 7 | 68448 / 114272 | training loss: 0.08927559852600098\n",
      "epoch: 7 | 68480 / 114272 | training loss: 0.1926766037940979\n",
      "epoch: 7 | 68512 / 114272 | training loss: 0.0005298487958498299\n",
      "epoch: 7 | 68544 / 114272 | training loss: 0.0021408190950751305\n",
      "epoch: 7 | 68576 / 114272 | training loss: 0.0004971966845914721\n",
      "epoch: 7 | 68608 / 114272 | training loss: 0.012629718519747257\n",
      "epoch: 7 | 68640 / 114272 | training loss: 0.000610658316873014\n",
      "epoch: 7 | 68672 / 114272 | training loss: 0.0007347858045250177\n",
      "epoch: 7 | 68704 / 114272 | training loss: 0.34720954298973083\n",
      "epoch: 7 | 68736 / 114272 | training loss: 0.0006805257871747017\n",
      "epoch: 7 | 68768 / 114272 | training loss: 0.029812272638082504\n",
      "epoch: 7 | 68800 / 114272 | training loss: 0.0010895063169300556\n",
      "epoch: 7 | 68832 / 114272 | training loss: 0.0005314589361660182\n",
      "epoch: 7 | 68864 / 114272 | training loss: 0.0020693750120699406\n",
      "epoch: 7 | 68896 / 114272 | training loss: 0.0011926518054679036\n",
      "epoch: 7 | 68928 / 114272 | training loss: 0.0006637118640355766\n",
      "epoch: 7 | 68960 / 114272 | training loss: 0.0016177541110664606\n",
      "epoch: 7 | 68992 / 114272 | training loss: 0.00038429806591011584\n",
      "epoch: 7 | 69024 / 114272 | training loss: 0.0006137514719739556\n",
      "epoch: 7 | 69056 / 114272 | training loss: 0.0004886693204753101\n",
      "epoch: 7 | 69088 / 114272 | training loss: 0.0014981512213125825\n",
      "epoch: 7 | 69120 / 114272 | training loss: 0.0005572133231908083\n",
      "epoch: 7 | 69152 / 114272 | training loss: 0.00030207436066120863\n",
      "epoch: 7 | 69184 / 114272 | training loss: 0.11560201644897461\n",
      "epoch: 7 | 69216 / 114272 | training loss: 0.0005468573071993887\n",
      "epoch: 7 | 69248 / 114272 | training loss: 0.00046249269507825375\n",
      "epoch: 7 | 69280 / 114272 | training loss: 0.0007401487673632801\n",
      "epoch: 7 | 69312 / 114272 | training loss: 0.0005028515006415546\n",
      "epoch: 7 | 69344 / 114272 | training loss: 0.00039361772360280156\n",
      "epoch: 7 | 69376 / 114272 | training loss: 0.001980750123038888\n",
      "epoch: 7 | 69408 / 114272 | training loss: 0.0008504859870299697\n",
      "epoch: 7 | 69440 / 114272 | training loss: 0.0005649907980114222\n",
      "epoch: 7 | 69472 / 114272 | training loss: 0.000417265051510185\n",
      "epoch: 7 | 69504 / 114272 | training loss: 0.0004369345842860639\n",
      "epoch: 7 | 69536 / 114272 | training loss: 0.0006248598801903427\n",
      "epoch: 7 | 69568 / 114272 | training loss: 0.175766259431839\n",
      "epoch: 7 | 69600 / 114272 | training loss: 0.000858996354509145\n",
      "epoch: 7 | 69632 / 114272 | training loss: 0.13327746093273163\n",
      "epoch: 7 | 69664 / 114272 | training loss: 0.0004586955183185637\n",
      "epoch: 7 | 69696 / 114272 | training loss: 0.0005368780693970621\n",
      "epoch: 7 | 69728 / 114272 | training loss: 0.0003683010581880808\n",
      "epoch: 7 | 69760 / 114272 | training loss: 0.00034827928175218403\n",
      "epoch: 7 | 69792 / 114272 | training loss: 0.0007868586108088493\n",
      "epoch: 7 | 69824 / 114272 | training loss: 0.0006854638922959566\n",
      "epoch: 7 | 69856 / 114272 | training loss: 0.0005435450002551079\n",
      "epoch: 7 | 69888 / 114272 | training loss: 0.0006526768556796014\n",
      "epoch: 7 | 69920 / 114272 | training loss: 0.0007652757922187448\n",
      "epoch: 7 | 69952 / 114272 | training loss: 0.0005705011426471174\n",
      "epoch: 7 | 69984 / 114272 | training loss: 0.0006090411334298551\n",
      "epoch: 7 | 70016 / 114272 | training loss: 0.0004272274672985077\n",
      "epoch: 7 | 70048 / 114272 | training loss: 0.0005122439470142126\n",
      "epoch: 7 | 70080 / 114272 | training loss: 0.00101188023108989\n",
      "epoch: 7 | 70112 / 114272 | training loss: 0.00044222534052096307\n",
      "epoch: 7 | 70144 / 114272 | training loss: 0.0009102656622417271\n",
      "epoch: 7 | 70176 / 114272 | training loss: 0.07741647958755493\n",
      "epoch: 7 | 70208 / 114272 | training loss: 0.0004853509890381247\n",
      "epoch: 7 | 70240 / 114272 | training loss: 0.0003802187566179782\n",
      "epoch: 7 | 70272 / 114272 | training loss: 0.0005490680923685431\n",
      "epoch: 7 | 70304 / 114272 | training loss: 0.0004100930236745626\n",
      "epoch: 7 | 70336 / 114272 | training loss: 0.0005568956257775426\n",
      "epoch: 7 | 70368 / 114272 | training loss: 0.0004534449544735253\n",
      "epoch: 7 | 70400 / 114272 | training loss: 0.06504806876182556\n",
      "epoch: 7 | 70432 / 114272 | training loss: 0.0004359950253274292\n",
      "epoch: 7 | 70464 / 114272 | training loss: 0.00026210900978185236\n",
      "epoch: 7 | 70496 / 114272 | training loss: 0.0004141644749324769\n",
      "epoch: 7 | 70528 / 114272 | training loss: 0.0005822072271257639\n",
      "epoch: 7 | 70560 / 114272 | training loss: 0.00046831529471091926\n",
      "epoch: 7 | 70592 / 114272 | training loss: 0.0005616885609924793\n",
      "epoch: 7 | 70624 / 114272 | training loss: 0.0004033852892462164\n",
      "epoch: 7 | 70656 / 114272 | training loss: 0.0005193913821130991\n",
      "epoch: 7 | 70688 / 114272 | training loss: 0.000600264233071357\n",
      "epoch: 7 | 70720 / 114272 | training loss: 0.0011136106913909316\n",
      "epoch: 7 | 70752 / 114272 | training loss: 0.00043607401312328875\n",
      "epoch: 7 | 70784 / 114272 | training loss: 0.00039739132625982165\n",
      "epoch: 7 | 70816 / 114272 | training loss: 0.0006416073883883655\n",
      "epoch: 7 | 70848 / 114272 | training loss: 0.00034356085234321654\n",
      "epoch: 7 | 70880 / 114272 | training loss: 0.0004843911447096616\n",
      "epoch: 7 | 70912 / 114272 | training loss: 0.00037385153700597584\n",
      "epoch: 7 | 70944 / 114272 | training loss: 0.000463582924567163\n",
      "epoch: 7 | 70976 / 114272 | training loss: 0.0007686609169468284\n",
      "epoch: 7 | 71008 / 114272 | training loss: 0.0005614563124254346\n",
      "epoch: 7 | 71040 / 114272 | training loss: 0.0005044351564720273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 71072 / 114272 | training loss: 0.19287391006946564\n",
      "epoch: 7 | 71104 / 114272 | training loss: 0.00036542644375003874\n",
      "epoch: 7 | 71136 / 114272 | training loss: 0.0003877672425005585\n",
      "epoch: 7 | 71168 / 114272 | training loss: 0.0005899447132833302\n",
      "epoch: 7 | 71200 / 114272 | training loss: 0.0007852319977246225\n",
      "epoch: 7 | 71232 / 114272 | training loss: 0.0006289620068855584\n",
      "epoch: 7 | 71264 / 114272 | training loss: 0.0005565127357840538\n",
      "epoch: 7 | 71296 / 114272 | training loss: 0.0017929039895534515\n",
      "epoch: 7 | 71328 / 114272 | training loss: 0.0006809675833210349\n",
      "epoch: 7 | 71360 / 114272 | training loss: 0.0005471944459713995\n",
      "epoch: 7 | 71392 / 114272 | training loss: 0.00047200953122228384\n",
      "epoch: 7 | 71424 / 114272 | training loss: 0.000327487476170063\n",
      "epoch: 7 | 71456 / 114272 | training loss: 0.00031456773285754025\n",
      "epoch: 7 | 71488 / 114272 | training loss: 0.0004742975870613009\n",
      "epoch: 7 | 71520 / 114272 | training loss: 0.0011059234384447336\n",
      "epoch: 7 | 71552 / 114272 | training loss: 0.00047848152462393045\n",
      "epoch: 7 | 71584 / 114272 | training loss: 0.0004968225839547813\n",
      "epoch: 7 | 71616 / 114272 | training loss: 0.0007058060145936906\n",
      "epoch: 7 | 71648 / 114272 | training loss: 0.19700881838798523\n",
      "epoch: 7 | 71680 / 114272 | training loss: 0.0029058782383799553\n",
      "epoch: 7 | 71712 / 114272 | training loss: 0.00039469668990932405\n",
      "epoch: 7 | 71744 / 114272 | training loss: 0.0005661235190927982\n",
      "epoch: 7 | 71776 / 114272 | training loss: 0.0009291709284298122\n",
      "epoch: 7 | 71808 / 114272 | training loss: 0.0005375791224651039\n",
      "epoch: 7 | 71840 / 114272 | training loss: 0.0004139856027904898\n",
      "epoch: 7 | 71872 / 114272 | training loss: 0.0005644065677188337\n",
      "epoch: 7 | 71904 / 114272 | training loss: 0.0003355351509526372\n",
      "epoch: 7 | 71936 / 114272 | training loss: 0.0008478324743919075\n",
      "epoch: 7 | 71968 / 114272 | training loss: 0.000812860147561878\n",
      "epoch: 7 | 72000 / 114272 | training loss: 0.00021229867707006633\n",
      "epoch: 7 | 72032 / 114272 | training loss: 0.0011812937445938587\n",
      "epoch: 7 | 72064 / 114272 | training loss: 0.0006791754858568311\n",
      "epoch: 7 | 72096 / 114272 | training loss: 0.0012742826947942376\n",
      "epoch: 7 | 72128 / 114272 | training loss: 0.0023101300466805696\n",
      "epoch: 7 | 72160 / 114272 | training loss: 0.0005505437729880214\n",
      "epoch: 7 | 72192 / 114272 | training loss: 0.0017062824917957187\n",
      "epoch: 7 | 72224 / 114272 | training loss: 0.0009947519283741713\n",
      "epoch: 7 | 72256 / 114272 | training loss: 0.0013851284747943282\n",
      "epoch: 7 | 72288 / 114272 | training loss: 0.0002929168113041669\n",
      "epoch: 7 | 72320 / 114272 | training loss: 0.0017846738919615746\n",
      "epoch: 7 | 72352 / 114272 | training loss: 0.0008499258547089994\n",
      "epoch: 7 | 72384 / 114272 | training loss: 0.011802221648395061\n",
      "epoch: 7 | 72416 / 114272 | training loss: 0.0009427519398741424\n",
      "epoch: 7 | 72448 / 114272 | training loss: 0.0009464147151447833\n",
      "epoch: 7 | 72480 / 114272 | training loss: 0.0006274203769862652\n",
      "epoch: 7 | 72512 / 114272 | training loss: 0.0004387456283438951\n",
      "epoch: 7 | 72544 / 114272 | training loss: 0.001110343961045146\n",
      "epoch: 7 | 72576 / 114272 | training loss: 0.00035831061541102827\n",
      "epoch: 7 | 72608 / 114272 | training loss: 0.0005038400995545089\n",
      "epoch: 7 | 72640 / 114272 | training loss: 0.0006911074742674828\n",
      "epoch: 7 | 72672 / 114272 | training loss: 0.14805352687835693\n",
      "epoch: 7 | 72704 / 114272 | training loss: 0.001738165970891714\n",
      "epoch: 7 | 72736 / 114272 | training loss: 0.001034087035804987\n",
      "epoch: 7 | 72768 / 114272 | training loss: 0.0008455294882878661\n",
      "epoch: 7 | 72800 / 114272 | training loss: 0.0007371804676949978\n",
      "epoch: 7 | 72832 / 114272 | training loss: 0.0009054939728230238\n",
      "epoch: 7 | 72864 / 114272 | training loss: 0.0003669045981951058\n",
      "epoch: 7 | 72896 / 114272 | training loss: 0.0004969543078914285\n",
      "epoch: 7 | 72928 / 114272 | training loss: 0.037902068346738815\n",
      "epoch: 7 | 72960 / 114272 | training loss: 0.00043302858830429614\n",
      "epoch: 7 | 72992 / 114272 | training loss: 0.00043354835361242294\n",
      "epoch: 7 | 73024 / 114272 | training loss: 0.0006879408610984683\n",
      "epoch: 7 | 73056 / 114272 | training loss: 0.2544805407524109\n",
      "epoch: 7 | 73088 / 114272 | training loss: 0.0004403691564220935\n",
      "epoch: 7 | 73120 / 114272 | training loss: 0.0010220055701211095\n",
      "epoch: 7 | 73152 / 114272 | training loss: 0.0006031273514963686\n",
      "epoch: 7 | 73184 / 114272 | training loss: 0.13187187910079956\n",
      "epoch: 7 | 73216 / 114272 | training loss: 0.0003877340641338378\n",
      "epoch: 7 | 73248 / 114272 | training loss: 0.000536836392711848\n",
      "epoch: 7 | 73280 / 114272 | training loss: 0.0003412567311897874\n",
      "epoch: 7 | 73312 / 114272 | training loss: 0.000641919206827879\n",
      "epoch: 7 | 73344 / 114272 | training loss: 0.2737673819065094\n",
      "epoch: 7 | 73376 / 114272 | training loss: 0.001074214931577444\n",
      "epoch: 7 | 73408 / 114272 | training loss: 0.0006668636342510581\n",
      "epoch: 7 | 73440 / 114272 | training loss: 0.0012692635646089911\n",
      "epoch: 7 | 73472 / 114272 | training loss: 0.0006266723503358662\n",
      "epoch: 7 | 73504 / 114272 | training loss: 0.0005999140557833016\n",
      "epoch: 7 | 73536 / 114272 | training loss: 0.0004405994259286672\n",
      "epoch: 7 | 73568 / 114272 | training loss: 0.000951496884226799\n",
      "epoch: 7 | 73600 / 114272 | training loss: 0.0005157883861102164\n",
      "epoch: 7 | 73632 / 114272 | training loss: 0.001662064460106194\n",
      "epoch: 7 | 73664 / 114272 | training loss: 0.03015441633760929\n",
      "epoch: 7 | 73696 / 114272 | training loss: 0.0004008958349004388\n",
      "epoch: 7 | 73728 / 114272 | training loss: 0.0006937443977221847\n",
      "epoch: 7 | 73760 / 114272 | training loss: 0.1138005182147026\n",
      "epoch: 7 | 73792 / 114272 | training loss: 0.000926564505789429\n",
      "epoch: 7 | 73824 / 114272 | training loss: 0.0008603326277807355\n",
      "epoch: 7 | 73856 / 114272 | training loss: 0.05595466494560242\n",
      "epoch: 7 | 73888 / 114272 | training loss: 0.03231016919016838\n",
      "epoch: 7 | 73920 / 114272 | training loss: 0.0005281539633870125\n",
      "epoch: 7 | 73952 / 114272 | training loss: 0.20499084889888763\n",
      "epoch: 7 | 73984 / 114272 | training loss: 0.002535681240260601\n",
      "epoch: 7 | 74016 / 114272 | training loss: 0.00054331956198439\n",
      "epoch: 7 | 74048 / 114272 | training loss: 0.0013926711399108171\n",
      "epoch: 7 | 74080 / 114272 | training loss: 0.0006165336235426366\n",
      "epoch: 7 | 74112 / 114272 | training loss: 0.03796912357211113\n",
      "epoch: 7 | 74144 / 114272 | training loss: 0.00216667796485126\n",
      "epoch: 7 | 74176 / 114272 | training loss: 0.000894375960342586\n",
      "epoch: 7 | 74208 / 114272 | training loss: 0.0025556725449860096\n",
      "epoch: 7 | 74240 / 114272 | training loss: 0.002496618777513504\n",
      "epoch: 7 | 74272 / 114272 | training loss: 0.0016275641974061728\n",
      "epoch: 7 | 74304 / 114272 | training loss: 0.0009710686281323433\n",
      "epoch: 7 | 74336 / 114272 | training loss: 0.000977533170953393\n",
      "epoch: 7 | 74368 / 114272 | training loss: 0.0069144428707659245\n",
      "epoch: 7 | 74400 / 114272 | training loss: 0.0011397789930924773\n",
      "epoch: 7 | 74432 / 114272 | training loss: 0.0010291538201272488\n",
      "epoch: 7 | 74464 / 114272 | training loss: 0.001408128533512354\n",
      "epoch: 7 | 74496 / 114272 | training loss: 0.0021527002099901438\n",
      "epoch: 7 | 74528 / 114272 | training loss: 0.003496301593258977\n",
      "epoch: 7 | 74560 / 114272 | training loss: 0.02592187561094761\n",
      "epoch: 7 | 74592 / 114272 | training loss: 0.0011182120069861412\n",
      "epoch: 7 | 74624 / 114272 | training loss: 0.0009885655017569661\n",
      "epoch: 7 | 74656 / 114272 | training loss: 0.17630158364772797\n",
      "epoch: 7 | 74688 / 114272 | training loss: 0.0014699286548420787\n",
      "epoch: 7 | 74720 / 114272 | training loss: 0.0011900912504643202\n",
      "epoch: 7 | 74752 / 114272 | training loss: 0.14027775824069977\n",
      "epoch: 7 | 74784 / 114272 | training loss: 0.0008043604902923107\n",
      "epoch: 7 | 74816 / 114272 | training loss: 0.0009768654126673937\n",
      "epoch: 7 | 74848 / 114272 | training loss: 0.0005403822869993746\n",
      "epoch: 7 | 74880 / 114272 | training loss: 0.0021511090453714132\n",
      "epoch: 7 | 74912 / 114272 | training loss: 0.09262680262327194\n",
      "epoch: 7 | 74944 / 114272 | training loss: 0.0011528328759595752\n",
      "epoch: 7 | 74976 / 114272 | training loss: 0.0008095773519016802\n",
      "epoch: 7 | 75008 / 114272 | training loss: 0.001987341558560729\n",
      "epoch: 7 | 75040 / 114272 | training loss: 0.0011092335917055607\n",
      "epoch: 7 | 75072 / 114272 | training loss: 0.07026994228363037\n",
      "epoch: 7 | 75104 / 114272 | training loss: 0.0007796270656399429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 75136 / 114272 | training loss: 0.0011317918542772532\n",
      "epoch: 7 | 75168 / 114272 | training loss: 0.000920860911719501\n",
      "epoch: 7 | 75200 / 114272 | training loss: 0.0009212084114551544\n",
      "epoch: 7 | 75232 / 114272 | training loss: 0.0023585527669638395\n",
      "epoch: 7 | 75264 / 114272 | training loss: 0.000888260081410408\n",
      "epoch: 7 | 75296 / 114272 | training loss: 0.0009267852874472737\n",
      "epoch: 7 | 75328 / 114272 | training loss: 0.0009054281981661916\n",
      "epoch: 7 | 75360 / 114272 | training loss: 0.0017775770975276828\n",
      "epoch: 7 | 75392 / 114272 | training loss: 0.0008818359347060323\n",
      "epoch: 7 | 75424 / 114272 | training loss: 0.0015099740121513605\n",
      "epoch: 7 | 75456 / 114272 | training loss: 0.0016885490622371435\n",
      "epoch: 7 | 75488 / 114272 | training loss: 0.0012311976170167327\n",
      "epoch: 7 | 75520 / 114272 | training loss: 0.0010010257828980684\n",
      "epoch: 7 | 75552 / 114272 | training loss: 0.00043428968638181686\n",
      "epoch: 7 | 75584 / 114272 | training loss: 0.12753541767597198\n",
      "epoch: 7 | 75616 / 114272 | training loss: 0.0006494107656180859\n",
      "epoch: 7 | 75648 / 114272 | training loss: 0.00038757926085963845\n",
      "epoch: 7 | 75680 / 114272 | training loss: 0.1285218596458435\n",
      "epoch: 7 | 75712 / 114272 | training loss: 0.0009672825690358877\n",
      "epoch: 7 | 75744 / 114272 | training loss: 0.0009664288372732699\n",
      "epoch: 7 | 75776 / 114272 | training loss: 0.0010908800177276134\n",
      "epoch: 7 | 75808 / 114272 | training loss: 0.0006301802350208163\n",
      "epoch: 7 | 75840 / 114272 | training loss: 0.06058178469538689\n",
      "epoch: 7 | 75872 / 114272 | training loss: 0.0007498954655602574\n",
      "epoch: 7 | 75904 / 114272 | training loss: 0.0007195527432486415\n",
      "epoch: 7 | 75936 / 114272 | training loss: 0.003950568847358227\n",
      "epoch: 7 | 75968 / 114272 | training loss: 0.0015073895920068026\n",
      "epoch: 7 | 76000 / 114272 | training loss: 0.0004219184920657426\n",
      "epoch: 7 | 76032 / 114272 | training loss: 0.0009091593092307448\n",
      "epoch: 7 | 76064 / 114272 | training loss: 0.00105186621658504\n",
      "epoch: 7 | 76096 / 114272 | training loss: 0.0006223539239726961\n",
      "epoch: 7 | 76128 / 114272 | training loss: 0.0008540542330592871\n",
      "epoch: 7 | 76160 / 114272 | training loss: 0.0005399652873165905\n",
      "epoch: 7 | 76192 / 114272 | training loss: 0.0011141003342345357\n",
      "epoch: 7 | 76224 / 114272 | training loss: 0.0006527747609652579\n",
      "epoch: 7 | 76256 / 114272 | training loss: 0.0011901955585926771\n",
      "epoch: 7 | 76288 / 114272 | training loss: 0.08833980560302734\n",
      "epoch: 7 | 76320 / 114272 | training loss: 0.0004325224435888231\n",
      "epoch: 7 | 76352 / 114272 | training loss: 0.0005768256378360093\n",
      "epoch: 7 | 76384 / 114272 | training loss: 0.0006052859243936837\n",
      "epoch: 7 | 76416 / 114272 | training loss: 0.0014248330844566226\n",
      "epoch: 7 | 76448 / 114272 | training loss: 0.0006994723808020353\n",
      "epoch: 7 | 76480 / 114272 | training loss: 0.0007755269180051982\n",
      "epoch: 7 | 76512 / 114272 | training loss: 0.0015494981780648232\n",
      "epoch: 7 | 76544 / 114272 | training loss: 0.16264022886753082\n",
      "epoch: 7 | 76576 / 114272 | training loss: 0.0006205022218637168\n",
      "epoch: 7 | 76608 / 114272 | training loss: 0.002712762448936701\n",
      "epoch: 7 | 76640 / 114272 | training loss: 0.001584553625434637\n",
      "epoch: 7 | 76672 / 114272 | training loss: 0.1058541089296341\n",
      "epoch: 7 | 76704 / 114272 | training loss: 0.2441725879907608\n",
      "epoch: 7 | 76736 / 114272 | training loss: 0.0008982893778011203\n",
      "epoch: 7 | 76768 / 114272 | training loss: 0.0013176200445741415\n",
      "epoch: 7 | 76800 / 114272 | training loss: 0.0006641605868935585\n",
      "epoch: 7 | 76832 / 114272 | training loss: 0.0010451788548380136\n",
      "epoch: 7 | 76864 / 114272 | training loss: 0.01063554547727108\n",
      "epoch: 7 | 76896 / 114272 | training loss: 0.0007141422247514129\n",
      "epoch: 7 | 76928 / 114272 | training loss: 0.16243146359920502\n",
      "epoch: 7 | 76960 / 114272 | training loss: 0.0003981750342063606\n",
      "epoch: 7 | 76992 / 114272 | training loss: 0.12668445706367493\n",
      "epoch: 7 | 77024 / 114272 | training loss: 0.0005736902821809053\n",
      "epoch: 7 | 77056 / 114272 | training loss: 0.0025941519998013973\n",
      "epoch: 7 | 77088 / 114272 | training loss: 0.0007319548749364913\n",
      "epoch: 7 | 77120 / 114272 | training loss: 0.0007843476487323642\n",
      "epoch: 7 | 77152 / 114272 | training loss: 0.0005679341848008335\n",
      "epoch: 7 | 77184 / 114272 | training loss: 0.0031925789080560207\n",
      "epoch: 7 | 77216 / 114272 | training loss: 0.0005445093847811222\n",
      "epoch: 7 | 77248 / 114272 | training loss: 0.0018133849371224642\n",
      "epoch: 7 | 77280 / 114272 | training loss: 0.0005341696087270975\n",
      "epoch: 7 | 77312 / 114272 | training loss: 0.008929317817091942\n",
      "epoch: 7 | 77344 / 114272 | training loss: 0.0005262754857540131\n",
      "epoch: 7 | 77376 / 114272 | training loss: 0.0007865201332606375\n",
      "epoch: 7 | 77408 / 114272 | training loss: 0.0003509029629640281\n",
      "epoch: 7 | 77440 / 114272 | training loss: 0.00029767004889436066\n",
      "epoch: 7 | 77472 / 114272 | training loss: 0.17216257750988007\n",
      "epoch: 7 | 77504 / 114272 | training loss: 0.0005092016654089093\n",
      "epoch: 7 | 77536 / 114272 | training loss: 0.1192774847149849\n",
      "epoch: 7 | 77568 / 114272 | training loss: 0.0012870060745626688\n",
      "epoch: 7 | 77600 / 114272 | training loss: 0.005249800626188517\n",
      "epoch: 7 | 77632 / 114272 | training loss: 0.0009834199445322156\n",
      "epoch: 7 | 77664 / 114272 | training loss: 0.001642653252929449\n",
      "epoch: 7 | 77696 / 114272 | training loss: 0.0016078019980341196\n",
      "epoch: 7 | 77728 / 114272 | training loss: 0.0004013042780570686\n",
      "epoch: 7 | 77760 / 114272 | training loss: 0.0009024360333569348\n",
      "epoch: 7 | 77792 / 114272 | training loss: 0.002797005232423544\n",
      "epoch: 7 | 77824 / 114272 | training loss: 0.0015267642447724938\n",
      "epoch: 7 | 77856 / 114272 | training loss: 0.0019467107485979795\n",
      "epoch: 7 | 77888 / 114272 | training loss: 0.001451411982998252\n",
      "epoch: 7 | 77920 / 114272 | training loss: 0.000939022924285382\n",
      "epoch: 7 | 77952 / 114272 | training loss: 0.002102904487401247\n",
      "epoch: 7 | 77984 / 114272 | training loss: 0.001066582277417183\n",
      "epoch: 7 | 78016 / 114272 | training loss: 0.0014824038371443748\n",
      "epoch: 7 | 78048 / 114272 | training loss: 0.02147376537322998\n",
      "epoch: 7 | 78080 / 114272 | training loss: 0.0016123662935569882\n",
      "epoch: 7 | 78112 / 114272 | training loss: 0.00144259724766016\n",
      "epoch: 7 | 78144 / 114272 | training loss: 0.0014623062452301383\n",
      "epoch: 7 | 78176 / 114272 | training loss: 0.0025674114003777504\n",
      "epoch: 7 | 78208 / 114272 | training loss: 0.0016088136471807957\n",
      "epoch: 7 | 78240 / 114272 | training loss: 0.004800307564437389\n",
      "epoch: 7 | 78272 / 114272 | training loss: 0.004585491493344307\n",
      "epoch: 7 | 78304 / 114272 | training loss: 0.0012525918427854776\n",
      "epoch: 7 | 78336 / 114272 | training loss: 0.0010043284855782986\n",
      "epoch: 7 | 78368 / 114272 | training loss: 0.12986470758914948\n",
      "epoch: 7 | 78400 / 114272 | training loss: 0.001016354886814952\n",
      "epoch: 7 | 78432 / 114272 | training loss: 0.0025799458380788565\n",
      "epoch: 7 | 78464 / 114272 | training loss: 0.0006394886877387762\n",
      "epoch: 7 | 78496 / 114272 | training loss: 0.000950931163970381\n",
      "epoch: 7 | 78528 / 114272 | training loss: 0.0009043678292073309\n",
      "epoch: 7 | 78560 / 114272 | training loss: 0.0005644981865771115\n",
      "epoch: 7 | 78592 / 114272 | training loss: 0.003517366014420986\n",
      "epoch: 7 | 78624 / 114272 | training loss: 0.004321728367358446\n",
      "epoch: 7 | 78656 / 114272 | training loss: 0.0011426695855334401\n",
      "epoch: 7 | 78688 / 114272 | training loss: 0.002477586269378662\n",
      "epoch: 7 | 78720 / 114272 | training loss: 0.0005122569855302572\n",
      "epoch: 7 | 78752 / 114272 | training loss: 0.0030623897910118103\n",
      "epoch: 7 | 78784 / 114272 | training loss: 0.06634471565485\n",
      "epoch: 7 | 78816 / 114272 | training loss: 0.0010208168532699347\n",
      "epoch: 7 | 78848 / 114272 | training loss: 0.0036669413093477488\n",
      "epoch: 7 | 78880 / 114272 | training loss: 0.0006000003195367754\n",
      "epoch: 7 | 78912 / 114272 | training loss: 0.0008531674393452704\n",
      "epoch: 7 | 78944 / 114272 | training loss: 0.0011217829305678606\n",
      "epoch: 7 | 78976 / 114272 | training loss: 0.0005479984101839364\n",
      "epoch: 7 | 79008 / 114272 | training loss: 0.030359862372279167\n",
      "epoch: 7 | 79040 / 114272 | training loss: 0.00033187191002070904\n",
      "epoch: 7 | 79072 / 114272 | training loss: 0.07935625314712524\n",
      "epoch: 7 | 79104 / 114272 | training loss: 0.11629641056060791\n",
      "epoch: 7 | 79136 / 114272 | training loss: 0.0008607924683019519\n",
      "epoch: 7 | 79168 / 114272 | training loss: 0.0013387510553002357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 79200 / 114272 | training loss: 0.0024198482278734446\n",
      "epoch: 7 | 79232 / 114272 | training loss: 0.0005285891820676625\n",
      "epoch: 7 | 79264 / 114272 | training loss: 0.003354591317474842\n",
      "epoch: 7 | 79296 / 114272 | training loss: 0.0010497201001271605\n",
      "epoch: 7 | 79328 / 114272 | training loss: 0.0006889636279083788\n",
      "epoch: 7 | 79360 / 114272 | training loss: 0.00017833909078035504\n",
      "epoch: 7 | 79392 / 114272 | training loss: 0.0016861462499946356\n",
      "epoch: 7 | 79424 / 114272 | training loss: 0.0011719345347955823\n",
      "epoch: 7 | 79456 / 114272 | training loss: 0.07676587998867035\n",
      "epoch: 7 | 79488 / 114272 | training loss: 0.0004272273217793554\n",
      "epoch: 7 | 79520 / 114272 | training loss: 0.002900114981457591\n",
      "epoch: 7 | 79552 / 114272 | training loss: 0.0032991101033985615\n",
      "epoch: 7 | 79584 / 114272 | training loss: 0.0010660759871825576\n",
      "epoch: 7 | 79616 / 114272 | training loss: 0.002908139256760478\n",
      "epoch: 7 | 79648 / 114272 | training loss: 0.0007591921021230519\n",
      "epoch: 7 | 79680 / 114272 | training loss: 0.0005415588966570795\n",
      "epoch: 7 | 79712 / 114272 | training loss: 0.0040886481292545795\n",
      "epoch: 7 | 79744 / 114272 | training loss: 0.0006919947918504477\n",
      "epoch: 7 | 79776 / 114272 | training loss: 0.0006374550284817815\n",
      "epoch: 7 | 79808 / 114272 | training loss: 0.0012834621593356133\n",
      "epoch: 7 | 79840 / 114272 | training loss: 0.0008018523803912103\n",
      "epoch: 7 | 79872 / 114272 | training loss: 0.018900660797953606\n",
      "epoch: 7 | 79904 / 114272 | training loss: 0.0002029878960456699\n",
      "epoch: 7 | 79936 / 114272 | training loss: 0.0016860407777130604\n",
      "epoch: 7 | 79968 / 114272 | training loss: 0.0004821105394512415\n",
      "epoch: 7 | 80000 / 114272 | training loss: 0.0010761735029518604\n",
      "epoch: 7 | 80032 / 114272 | training loss: 0.0030976494308561087\n",
      "epoch: 7 | 80064 / 114272 | training loss: 0.0011329681146889925\n",
      "epoch: 7 | 80096 / 114272 | training loss: 0.0005764596280641854\n",
      "epoch: 7 | 80128 / 114272 | training loss: 0.0010610807221382856\n",
      "epoch: 7 | 80160 / 114272 | training loss: 0.00066601880826056\n",
      "epoch: 7 | 80192 / 114272 | training loss: 0.00026810026611201465\n",
      "epoch: 7 | 80224 / 114272 | training loss: 0.00037681940011680126\n",
      "epoch: 7 | 80256 / 114272 | training loss: 0.0002415831695543602\n",
      "epoch: 7 | 80288 / 114272 | training loss: 0.0002448114100843668\n",
      "epoch: 7 | 80320 / 114272 | training loss: 0.00025484859361313283\n",
      "epoch: 7 | 80352 / 114272 | training loss: 0.0004325947957113385\n",
      "epoch: 7 | 80384 / 114272 | training loss: 0.00017492780170869082\n",
      "epoch: 7 | 80416 / 114272 | training loss: 0.003284123260527849\n",
      "epoch: 7 | 80448 / 114272 | training loss: 0.007759299129247665\n",
      "epoch: 7 | 80480 / 114272 | training loss: 0.00026189297204837203\n",
      "epoch: 7 | 80512 / 114272 | training loss: 0.0014711541589349508\n",
      "epoch: 7 | 80544 / 114272 | training loss: 0.0031182137317955494\n",
      "epoch: 7 | 80576 / 114272 | training loss: 0.0006038633291609585\n",
      "epoch: 7 | 80608 / 114272 | training loss: 0.00027532692183740437\n",
      "epoch: 7 | 80640 / 114272 | training loss: 0.0005650026723742485\n",
      "epoch: 7 | 80672 / 114272 | training loss: 0.00041717884596437216\n",
      "epoch: 7 | 80704 / 114272 | training loss: 0.00038290611701086164\n",
      "epoch: 7 | 80736 / 114272 | training loss: 0.00041411910206079483\n",
      "epoch: 7 | 80768 / 114272 | training loss: 0.00034420922747813165\n",
      "epoch: 7 | 80800 / 114272 | training loss: 0.00042678386671468616\n",
      "epoch: 7 | 80832 / 114272 | training loss: 0.0005980543210171163\n",
      "epoch: 7 | 80864 / 114272 | training loss: 0.00026789784897118807\n",
      "epoch: 7 | 80896 / 114272 | training loss: 0.0007932949811220169\n",
      "epoch: 7 | 80928 / 114272 | training loss: 0.0008053128840401769\n",
      "epoch: 7 | 80960 / 114272 | training loss: 0.0005556303076446056\n",
      "epoch: 7 | 80992 / 114272 | training loss: 0.00045273377327248454\n",
      "epoch: 7 | 81024 / 114272 | training loss: 0.0005352718872018158\n",
      "epoch: 7 | 81056 / 114272 | training loss: 0.0004601265536621213\n",
      "epoch: 7 | 81088 / 114272 | training loss: 0.0014432191383093596\n",
      "epoch: 7 | 81120 / 114272 | training loss: 0.00044814261491410434\n",
      "epoch: 7 | 81152 / 114272 | training loss: 0.00038390871486626565\n",
      "epoch: 7 | 81184 / 114272 | training loss: 0.0004490697174333036\n",
      "epoch: 7 | 81216 / 114272 | training loss: 0.0010335498955100775\n",
      "epoch: 7 | 81248 / 114272 | training loss: 0.00045496251550503075\n",
      "epoch: 7 | 81280 / 114272 | training loss: 0.00023345185036305338\n",
      "epoch: 7 | 81312 / 114272 | training loss: 0.01063306163996458\n",
      "epoch: 7 | 81344 / 114272 | training loss: 0.0012004212476313114\n",
      "epoch: 7 | 81376 / 114272 | training loss: 0.010019081644713879\n",
      "epoch: 7 | 81408 / 114272 | training loss: 0.1197294220328331\n",
      "epoch: 7 | 81440 / 114272 | training loss: 0.00031946960370987654\n",
      "epoch: 7 | 81472 / 114272 | training loss: 0.00024293184105772525\n",
      "epoch: 7 | 81504 / 114272 | training loss: 0.0006148674874566495\n",
      "epoch: 7 | 81536 / 114272 | training loss: 0.0002939292462542653\n",
      "epoch: 7 | 81568 / 114272 | training loss: 0.0004806224605999887\n",
      "epoch: 7 | 81600 / 114272 | training loss: 0.00020088702149223536\n",
      "epoch: 7 | 81632 / 114272 | training loss: 0.00040986662497743964\n",
      "epoch: 7 | 81664 / 114272 | training loss: 0.0005648602964356542\n",
      "epoch: 7 | 81696 / 114272 | training loss: 0.0003769344184547663\n",
      "epoch: 7 | 81728 / 114272 | training loss: 0.0004135041090194136\n",
      "epoch: 7 | 81760 / 114272 | training loss: 0.00042853495688177645\n",
      "epoch: 7 | 81792 / 114272 | training loss: 0.0003219765785615891\n",
      "epoch: 7 | 81824 / 114272 | training loss: 0.00032835736055858433\n",
      "epoch: 7 | 81856 / 114272 | training loss: 0.0004756029520649463\n",
      "epoch: 7 | 81888 / 114272 | training loss: 0.0004868924443144351\n",
      "epoch: 7 | 81920 / 114272 | training loss: 0.00015359069220721722\n",
      "epoch: 7 | 81952 / 114272 | training loss: 0.0003383471630513668\n",
      "epoch: 7 | 81984 / 114272 | training loss: 0.00024227479298133403\n",
      "epoch: 7 | 82016 / 114272 | training loss: 0.000434109999332577\n",
      "epoch: 7 | 82048 / 114272 | training loss: 0.00042558490531519055\n",
      "epoch: 7 | 82080 / 114272 | training loss: 0.0003311456530354917\n",
      "epoch: 7 | 82112 / 114272 | training loss: 0.000491051934659481\n",
      "epoch: 7 | 82144 / 114272 | training loss: 0.0003911927342414856\n",
      "epoch: 7 | 82176 / 114272 | training loss: 0.0002774492895696312\n",
      "epoch: 7 | 82208 / 114272 | training loss: 0.0005355861503630877\n",
      "epoch: 7 | 82240 / 114272 | training loss: 0.0005027372972108424\n",
      "epoch: 7 | 82272 / 114272 | training loss: 0.00037576796603389084\n",
      "epoch: 7 | 82304 / 114272 | training loss: 0.00026728041120804846\n",
      "epoch: 7 | 82336 / 114272 | training loss: 0.003912134096026421\n",
      "epoch: 7 | 82368 / 114272 | training loss: 0.00044696673285216093\n",
      "epoch: 7 | 82400 / 114272 | training loss: 0.0003576867748051882\n",
      "epoch: 7 | 82432 / 114272 | training loss: 0.0016599834198132157\n",
      "epoch: 7 | 82464 / 114272 | training loss: 0.0004710810899268836\n",
      "epoch: 7 | 82496 / 114272 | training loss: 0.00030028301989659667\n",
      "epoch: 7 | 82528 / 114272 | training loss: 0.0003653387539088726\n",
      "epoch: 7 | 82560 / 114272 | training loss: 0.0003844529564958066\n",
      "epoch: 7 | 82592 / 114272 | training loss: 0.0002410851593594998\n",
      "epoch: 7 | 82624 / 114272 | training loss: 0.0004831870610360056\n",
      "epoch: 7 | 82656 / 114272 | training loss: 0.00020707296789623797\n",
      "epoch: 7 | 82688 / 114272 | training loss: 0.0002824427210725844\n",
      "epoch: 7 | 82720 / 114272 | training loss: 0.0003870287910103798\n",
      "epoch: 7 | 82752 / 114272 | training loss: 0.0003560343466233462\n",
      "epoch: 7 | 82784 / 114272 | training loss: 0.00039508604095317423\n",
      "epoch: 7 | 82816 / 114272 | training loss: 0.00024775118799880147\n",
      "epoch: 7 | 82848 / 114272 | training loss: 0.00039219524478539824\n",
      "epoch: 7 | 82880 / 114272 | training loss: 0.0006440759170800447\n",
      "epoch: 7 | 82912 / 114272 | training loss: 0.0004170284082647413\n",
      "epoch: 7 | 82944 / 114272 | training loss: 0.00029057261417619884\n",
      "epoch: 7 | 82976 / 114272 | training loss: 0.0002105982566718012\n",
      "epoch: 7 | 83008 / 114272 | training loss: 0.00015160557813942432\n",
      "epoch: 7 | 83040 / 114272 | training loss: 0.11815931648015976\n",
      "epoch: 7 | 83072 / 114272 | training loss: 0.0006601004861295223\n",
      "epoch: 7 | 83104 / 114272 | training loss: 0.00034938225871883333\n",
      "epoch: 7 | 83136 / 114272 | training loss: 0.00033756232005544007\n",
      "epoch: 7 | 83168 / 114272 | training loss: 0.00029366477974690497\n",
      "epoch: 7 | 83200 / 114272 | training loss: 0.0006389952613972127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 83232 / 114272 | training loss: 0.00038240093272179365\n",
      "epoch: 7 | 83264 / 114272 | training loss: 0.06765588372945786\n",
      "epoch: 7 | 83296 / 114272 | training loss: 0.0002086834138026461\n",
      "epoch: 7 | 83328 / 114272 | training loss: 0.12218010425567627\n",
      "epoch: 7 | 83360 / 114272 | training loss: 0.007086263038218021\n",
      "epoch: 7 | 83392 / 114272 | training loss: 0.048950519412755966\n",
      "epoch: 7 | 83424 / 114272 | training loss: 0.0004062241059727967\n",
      "epoch: 7 | 83456 / 114272 | training loss: 0.0003274790942668915\n",
      "epoch: 7 | 83488 / 114272 | training loss: 0.0002528748009353876\n",
      "epoch: 7 | 83520 / 114272 | training loss: 0.0005065990844741464\n",
      "epoch: 7 | 83552 / 114272 | training loss: 0.0002470273175276816\n",
      "epoch: 7 | 83584 / 114272 | training loss: 0.26924070715904236\n",
      "epoch: 7 | 83616 / 114272 | training loss: 0.00026082390104420483\n",
      "epoch: 7 | 83648 / 114272 | training loss: 0.00038639717968180776\n",
      "epoch: 7 | 83680 / 114272 | training loss: 0.0009186138049699366\n",
      "epoch: 7 | 83712 / 114272 | training loss: 0.006636638659983873\n",
      "epoch: 7 | 83744 / 114272 | training loss: 0.0004601862747222185\n",
      "epoch: 7 | 83776 / 114272 | training loss: 0.0002855324710253626\n",
      "epoch: 7 | 83808 / 114272 | training loss: 0.00036965851904824376\n",
      "epoch: 7 | 83840 / 114272 | training loss: 0.0005381694645620883\n",
      "epoch: 7 | 83872 / 114272 | training loss: 0.0022748615592718124\n",
      "epoch: 7 | 83904 / 114272 | training loss: 0.0004312451637815684\n",
      "epoch: 7 | 83936 / 114272 | training loss: 0.00032720918534323573\n",
      "epoch: 7 | 83968 / 114272 | training loss: 0.0002478421956766397\n",
      "epoch: 7 | 84000 / 114272 | training loss: 0.0002647348155733198\n",
      "epoch: 7 | 84032 / 114272 | training loss: 0.0005113922525197268\n",
      "epoch: 7 | 84064 / 114272 | training loss: 0.03549591079354286\n",
      "epoch: 7 | 84096 / 114272 | training loss: 0.0014328781981021166\n",
      "epoch: 7 | 84128 / 114272 | training loss: 0.0013961369404569268\n",
      "epoch: 7 | 84160 / 114272 | training loss: 0.00027382918051443994\n",
      "epoch: 7 | 84192 / 114272 | training loss: 0.000320702645694837\n",
      "epoch: 7 | 84224 / 114272 | training loss: 0.0002712415298447013\n",
      "epoch: 7 | 84256 / 114272 | training loss: 0.00028038519667461514\n",
      "epoch: 7 | 84288 / 114272 | training loss: 0.00027308359858579934\n",
      "epoch: 7 | 84320 / 114272 | training loss: 0.00035289235529489815\n",
      "epoch: 7 | 84352 / 114272 | training loss: 0.0007686350145377219\n",
      "epoch: 7 | 84384 / 114272 | training loss: 0.0003580115153454244\n",
      "epoch: 7 | 84416 / 114272 | training loss: 0.0006201803917065263\n",
      "epoch: 7 | 84448 / 114272 | training loss: 0.0020617367699742317\n",
      "epoch: 7 | 84480 / 114272 | training loss: 0.0018956681014969945\n",
      "epoch: 7 | 84512 / 114272 | training loss: 0.0009119219612330198\n",
      "epoch: 7 | 84544 / 114272 | training loss: 0.22112616896629333\n",
      "epoch: 7 | 84576 / 114272 | training loss: 0.00039037212263792753\n",
      "epoch: 7 | 84608 / 114272 | training loss: 0.0007025452796369791\n",
      "epoch: 7 | 84640 / 114272 | training loss: 0.0006778857787139714\n",
      "epoch: 7 | 84672 / 114272 | training loss: 0.0005528618930839002\n",
      "epoch: 7 | 84704 / 114272 | training loss: 0.0005259460886009037\n",
      "epoch: 7 | 84736 / 114272 | training loss: 0.00046775719965808094\n",
      "epoch: 7 | 84768 / 114272 | training loss: 0.0007502577500417829\n",
      "epoch: 7 | 84800 / 114272 | training loss: 0.00046547254896722734\n",
      "epoch: 7 | 84832 / 114272 | training loss: 0.0004135118215344846\n",
      "epoch: 7 | 84864 / 114272 | training loss: 0.0007516428595408797\n",
      "epoch: 7 | 84896 / 114272 | training loss: 0.0012049400247633457\n",
      "epoch: 7 | 84928 / 114272 | training loss: 0.0005095576634630561\n",
      "epoch: 7 | 84960 / 114272 | training loss: 0.0007796778809279203\n",
      "epoch: 7 | 84992 / 114272 | training loss: 0.0002813529863487929\n",
      "epoch: 7 | 85024 / 114272 | training loss: 0.0004919226048514247\n",
      "epoch: 7 | 85056 / 114272 | training loss: 0.0004476320755202323\n",
      "epoch: 7 | 85088 / 114272 | training loss: 0.0005669495440088212\n",
      "epoch: 7 | 85120 / 114272 | training loss: 0.0021549363154917955\n",
      "epoch: 7 | 85152 / 114272 | training loss: 0.000624637643340975\n",
      "epoch: 7 | 85184 / 114272 | training loss: 0.12369765341281891\n",
      "epoch: 7 | 85216 / 114272 | training loss: 0.0005709422403015196\n",
      "epoch: 7 | 85248 / 114272 | training loss: 0.000501192407682538\n",
      "epoch: 7 | 85280 / 114272 | training loss: 0.0004546842537820339\n",
      "epoch: 7 | 85312 / 114272 | training loss: 0.0005016163340769708\n",
      "epoch: 7 | 85344 / 114272 | training loss: 0.00047365319915115833\n",
      "epoch: 7 | 85376 / 114272 | training loss: 0.00036789433215744793\n",
      "epoch: 7 | 85408 / 114272 | training loss: 0.13072164356708527\n",
      "epoch: 7 | 85440 / 114272 | training loss: 0.00039756394107826054\n",
      "epoch: 7 | 85472 / 114272 | training loss: 0.0020229932852089405\n",
      "epoch: 7 | 85504 / 114272 | training loss: 0.0023076809011399746\n",
      "epoch: 7 | 85536 / 114272 | training loss: 0.00046817550901323557\n",
      "epoch: 7 | 85568 / 114272 | training loss: 0.000489064259454608\n",
      "epoch: 7 | 85600 / 114272 | training loss: 0.025023840367794037\n",
      "epoch: 7 | 85632 / 114272 | training loss: 0.0004397440643515438\n",
      "epoch: 7 | 85664 / 114272 | training loss: 0.0009579519391991198\n",
      "epoch: 7 | 85696 / 114272 | training loss: 0.0003290051536168903\n",
      "epoch: 7 | 85728 / 114272 | training loss: 0.0003911468375008553\n",
      "epoch: 7 | 85760 / 114272 | training loss: 0.000381426012609154\n",
      "epoch: 7 | 85792 / 114272 | training loss: 0.0019091510912403464\n",
      "epoch: 7 | 85824 / 114272 | training loss: 0.0004441746277734637\n",
      "epoch: 7 | 85856 / 114272 | training loss: 0.00033366953721269965\n",
      "epoch: 7 | 85888 / 114272 | training loss: 0.23367692530155182\n",
      "epoch: 7 | 85920 / 114272 | training loss: 0.0003870859800372273\n",
      "epoch: 7 | 85952 / 114272 | training loss: 0.0003388429176993668\n",
      "epoch: 7 | 85984 / 114272 | training loss: 0.0003263162507209927\n",
      "epoch: 7 | 86016 / 114272 | training loss: 0.09451133012771606\n",
      "epoch: 7 | 86048 / 114272 | training loss: 0.00022264984727371484\n",
      "epoch: 7 | 86080 / 114272 | training loss: 0.0005439201486296952\n",
      "epoch: 7 | 86112 / 114272 | training loss: 0.0005277454620227218\n",
      "epoch: 7 | 86144 / 114272 | training loss: 0.0003932733088731766\n",
      "epoch: 7 | 86176 / 114272 | training loss: 0.000730037281755358\n",
      "epoch: 7 | 86208 / 114272 | training loss: 0.000644301762804389\n",
      "epoch: 7 | 86240 / 114272 | training loss: 0.0003781118430197239\n",
      "epoch: 7 | 86272 / 114272 | training loss: 0.0038433223962783813\n",
      "epoch: 7 | 86304 / 114272 | training loss: 0.00035760688479058444\n",
      "epoch: 7 | 86336 / 114272 | training loss: 0.0003544949577189982\n",
      "epoch: 7 | 86368 / 114272 | training loss: 0.0005259947502054274\n",
      "epoch: 7 | 86400 / 114272 | training loss: 0.0006029378855600953\n",
      "epoch: 7 | 86432 / 114272 | training loss: 0.0006703873514197767\n",
      "epoch: 7 | 86464 / 114272 | training loss: 0.0004575341590680182\n",
      "epoch: 7 | 86496 / 114272 | training loss: 0.0004073500749655068\n",
      "epoch: 7 | 86528 / 114272 | training loss: 0.0010076735634356737\n",
      "epoch: 7 | 86560 / 114272 | training loss: 0.0004883010988123715\n",
      "epoch: 7 | 86592 / 114272 | training loss: 0.0003083270275965333\n",
      "epoch: 7 | 86624 / 114272 | training loss: 0.00030491664074361324\n",
      "epoch: 7 | 86656 / 114272 | training loss: 0.0013618532102555037\n",
      "epoch: 7 | 86688 / 114272 | training loss: 0.0006017499836161733\n",
      "epoch: 7 | 86720 / 114272 | training loss: 0.00776092940941453\n",
      "epoch: 7 | 86752 / 114272 | training loss: 0.0010814059060066938\n",
      "epoch: 7 | 86784 / 114272 | training loss: 0.00031939963810145855\n",
      "epoch: 7 | 86816 / 114272 | training loss: 0.0003814786032307893\n",
      "epoch: 7 | 86848 / 114272 | training loss: 0.0004241176648065448\n",
      "epoch: 7 | 86880 / 114272 | training loss: 0.0005246206419542432\n",
      "epoch: 7 | 86912 / 114272 | training loss: 0.0004193815984763205\n",
      "epoch: 7 | 86944 / 114272 | training loss: 0.0006659262580797076\n",
      "epoch: 7 | 86976 / 114272 | training loss: 0.0003903598408214748\n",
      "epoch: 7 | 87008 / 114272 | training loss: 0.0002376065094722435\n",
      "epoch: 7 | 87040 / 114272 | training loss: 0.000150361520354636\n",
      "epoch: 7 | 87072 / 114272 | training loss: 0.0003376473323442042\n",
      "epoch: 7 | 87104 / 114272 | training loss: 0.000334879441652447\n",
      "epoch: 7 | 87136 / 114272 | training loss: 0.0004870417760685086\n",
      "epoch: 7 | 87168 / 114272 | training loss: 0.00039587795617990196\n",
      "epoch: 7 | 87200 / 114272 | training loss: 0.1186860129237175\n",
      "epoch: 7 | 87232 / 114272 | training loss: 0.0002976216492243111\n",
      "epoch: 7 | 87264 / 114272 | training loss: 0.0003856733092106879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 87296 / 114272 | training loss: 0.0004270917852409184\n",
      "epoch: 7 | 87328 / 114272 | training loss: 0.0025612651370465755\n",
      "epoch: 7 | 87360 / 114272 | training loss: 0.00032753043342381716\n",
      "epoch: 7 | 87392 / 114272 | training loss: 0.00029268962680362165\n",
      "epoch: 7 | 87424 / 114272 | training loss: 0.0006486066267825663\n",
      "epoch: 7 | 87456 / 114272 | training loss: 0.0005024472484365106\n",
      "epoch: 7 | 87488 / 114272 | training loss: 0.0012632125290110707\n",
      "epoch: 7 | 87520 / 114272 | training loss: 0.0006580821936950088\n",
      "epoch: 7 | 87552 / 114272 | training loss: 0.0005873843911103904\n",
      "epoch: 7 | 87584 / 114272 | training loss: 0.0007884177612140775\n",
      "epoch: 7 | 87616 / 114272 | training loss: 0.0004547203134279698\n",
      "epoch: 7 | 87648 / 114272 | training loss: 0.0002993732923641801\n",
      "epoch: 7 | 87680 / 114272 | training loss: 0.20782339572906494\n",
      "epoch: 7 | 87712 / 114272 | training loss: 0.0007075761677697301\n",
      "epoch: 7 | 87744 / 114272 | training loss: 0.0003859159187413752\n",
      "epoch: 7 | 87776 / 114272 | training loss: 0.0005235546850599349\n",
      "epoch: 7 | 87808 / 114272 | training loss: 0.0002734364243224263\n",
      "epoch: 7 | 87840 / 114272 | training loss: 0.00029576377710327506\n",
      "epoch: 7 | 87872 / 114272 | training loss: 0.0021240392234176397\n",
      "epoch: 7 | 87904 / 114272 | training loss: 0.0005384875694289804\n",
      "epoch: 7 | 87936 / 114272 | training loss: 0.0007780095911584795\n",
      "epoch: 7 | 87968 / 114272 | training loss: 0.00033103738678619266\n",
      "epoch: 7 | 88000 / 114272 | training loss: 0.00037522552884183824\n",
      "epoch: 7 | 88032 / 114272 | training loss: 0.00046513904817402363\n",
      "epoch: 7 | 88064 / 114272 | training loss: 0.001888052560389042\n",
      "epoch: 7 | 88096 / 114272 | training loss: 0.000431591848609969\n",
      "epoch: 7 | 88128 / 114272 | training loss: 0.19436165690422058\n",
      "epoch: 7 | 88160 / 114272 | training loss: 0.0012064874172210693\n",
      "epoch: 7 | 88192 / 114272 | training loss: 0.0016525438986718655\n",
      "epoch: 7 | 88224 / 114272 | training loss: 0.000575753569137305\n",
      "epoch: 7 | 88256 / 114272 | training loss: 0.2185731679201126\n",
      "epoch: 7 | 88288 / 114272 | training loss: 0.00107535719871521\n",
      "epoch: 7 | 88320 / 114272 | training loss: 0.00047185481525957584\n",
      "epoch: 7 | 88352 / 114272 | training loss: 0.0004531865706667304\n",
      "epoch: 7 | 88384 / 114272 | training loss: 0.0005395952030085027\n",
      "epoch: 7 | 88416 / 114272 | training loss: 0.0005787487607449293\n",
      "epoch: 7 | 88448 / 114272 | training loss: 0.00047417785390280187\n",
      "epoch: 7 | 88480 / 114272 | training loss: 0.0015142192132771015\n",
      "epoch: 7 | 88512 / 114272 | training loss: 0.0019266880117356777\n",
      "epoch: 7 | 88544 / 114272 | training loss: 0.0005803494714200497\n",
      "epoch: 7 | 88576 / 114272 | training loss: 0.0025219519156962633\n",
      "epoch: 7 | 88608 / 114272 | training loss: 0.0013660079566761851\n",
      "epoch: 7 | 88640 / 114272 | training loss: 0.0005047058803029358\n",
      "epoch: 7 | 88672 / 114272 | training loss: 0.0003826444153673947\n",
      "epoch: 7 | 88704 / 114272 | training loss: 0.0004179634270258248\n",
      "epoch: 7 | 88736 / 114272 | training loss: 0.00035605052835308015\n",
      "epoch: 7 | 88768 / 114272 | training loss: 0.00024877279065549374\n",
      "epoch: 7 | 88800 / 114272 | training loss: 0.001246019033715129\n",
      "epoch: 7 | 88832 / 114272 | training loss: 0.0008135430398397148\n",
      "epoch: 7 | 88864 / 114272 | training loss: 0.07136855274438858\n",
      "epoch: 7 | 88896 / 114272 | training loss: 0.00040065537905320525\n",
      "epoch: 7 | 88928 / 114272 | training loss: 0.0007325069163925946\n",
      "epoch: 7 | 88960 / 114272 | training loss: 0.3058726489543915\n",
      "epoch: 7 | 88992 / 114272 | training loss: 0.0025886036455631256\n",
      "epoch: 7 | 89024 / 114272 | training loss: 0.0006601019995287061\n",
      "epoch: 7 | 89056 / 114272 | training loss: 0.008667037822306156\n",
      "epoch: 7 | 89088 / 114272 | training loss: 0.00048582503222860396\n",
      "epoch: 7 | 89120 / 114272 | training loss: 0.000392149668186903\n",
      "epoch: 7 | 89152 / 114272 | training loss: 0.0006789746694266796\n",
      "epoch: 7 | 89184 / 114272 | training loss: 0.0003678218345157802\n",
      "epoch: 7 | 89216 / 114272 | training loss: 0.0002743747318163514\n",
      "epoch: 7 | 89248 / 114272 | training loss: 0.00045740095083601773\n",
      "epoch: 7 | 89280 / 114272 | training loss: 0.0011651236563920975\n",
      "epoch: 7 | 89312 / 114272 | training loss: 0.00039810227463021874\n",
      "epoch: 7 | 89344 / 114272 | training loss: 0.2387719750404358\n",
      "epoch: 7 | 89376 / 114272 | training loss: 0.00046791104250587523\n",
      "epoch: 7 | 89408 / 114272 | training loss: 0.12345506250858307\n",
      "epoch: 7 | 89440 / 114272 | training loss: 0.00034273366327397525\n",
      "epoch: 7 | 89472 / 114272 | training loss: 0.0020300543401390314\n",
      "epoch: 7 | 89504 / 114272 | training loss: 0.0003351921623107046\n",
      "epoch: 7 | 89536 / 114272 | training loss: 0.0005969813209958375\n",
      "epoch: 7 | 89568 / 114272 | training loss: 0.0021992656402289867\n",
      "epoch: 7 | 89600 / 114272 | training loss: 0.0002878718078136444\n",
      "epoch: 7 | 89632 / 114272 | training loss: 0.0002690382534638047\n",
      "epoch: 7 | 89664 / 114272 | training loss: 0.00035811038105748594\n",
      "epoch: 7 | 89696 / 114272 | training loss: 0.0004782391188200563\n",
      "epoch: 7 | 89728 / 114272 | training loss: 0.0003588547115214169\n",
      "epoch: 7 | 89760 / 114272 | training loss: 0.0004559124063234776\n",
      "epoch: 7 | 89792 / 114272 | training loss: 0.20729558169841766\n",
      "epoch: 7 | 89824 / 114272 | training loss: 0.0006121723563410342\n",
      "epoch: 7 | 89856 / 114272 | training loss: 0.00044846904347650707\n",
      "epoch: 7 | 89888 / 114272 | training loss: 0.0004884798545390368\n",
      "epoch: 7 | 89920 / 114272 | training loss: 0.0008451085886918008\n",
      "epoch: 7 | 89952 / 114272 | training loss: 0.001969636417925358\n",
      "epoch: 7 | 89984 / 114272 | training loss: 0.00038159932591952384\n",
      "epoch: 7 | 90016 / 114272 | training loss: 0.0003974798892159015\n",
      "epoch: 7 | 90048 / 114272 | training loss: 0.0002784194075502455\n",
      "epoch: 7 | 90080 / 114272 | training loss: 0.0002750297717284411\n",
      "epoch: 7 | 90112 / 114272 | training loss: 0.0002687307132873684\n",
      "epoch: 7 | 90144 / 114272 | training loss: 0.00022876339789945632\n",
      "epoch: 7 | 90176 / 114272 | training loss: 0.00037587888073176146\n",
      "epoch: 7 | 90208 / 114272 | training loss: 0.0007234560907818377\n",
      "epoch: 7 | 90240 / 114272 | training loss: 0.00033346371492370963\n",
      "epoch: 7 | 90272 / 114272 | training loss: 0.00038013822631910443\n",
      "epoch: 7 | 90304 / 114272 | training loss: 0.08790259063243866\n",
      "epoch: 7 | 90336 / 114272 | training loss: 0.0025336863473057747\n",
      "epoch: 7 | 90368 / 114272 | training loss: 0.0005559624405577779\n",
      "epoch: 7 | 90400 / 114272 | training loss: 0.0005741868517361581\n",
      "epoch: 7 | 90432 / 114272 | training loss: 0.0005647712387144566\n",
      "epoch: 7 | 90464 / 114272 | training loss: 0.000453420594567433\n",
      "epoch: 7 | 90496 / 114272 | training loss: 0.00040448218351230025\n",
      "epoch: 7 | 90528 / 114272 | training loss: 0.06026741489768028\n",
      "epoch: 7 | 90560 / 114272 | training loss: 0.0007982013048604131\n",
      "epoch: 7 | 90592 / 114272 | training loss: 0.0002885157009586692\n",
      "epoch: 7 | 90624 / 114272 | training loss: 0.04301396384835243\n",
      "epoch: 7 | 90656 / 114272 | training loss: 0.0005344845121726394\n",
      "epoch: 7 | 90688 / 114272 | training loss: 0.0004582793917506933\n",
      "epoch: 7 | 90720 / 114272 | training loss: 0.0008655408746562898\n",
      "epoch: 7 | 90752 / 114272 | training loss: 0.002965801628306508\n",
      "epoch: 7 | 90784 / 114272 | training loss: 0.00035420883796177804\n",
      "epoch: 7 | 90816 / 114272 | training loss: 0.0006415561656467617\n",
      "epoch: 7 | 90848 / 114272 | training loss: 0.009545380249619484\n",
      "epoch: 7 | 90880 / 114272 | training loss: 0.0021228049881756306\n",
      "epoch: 7 | 90912 / 114272 | training loss: 0.00042166581260971725\n",
      "epoch: 7 | 90944 / 114272 | training loss: 0.0005352850421331823\n",
      "epoch: 7 | 90976 / 114272 | training loss: 0.2891869843006134\n",
      "epoch: 7 | 91008 / 114272 | training loss: 0.0003381484712008387\n",
      "epoch: 7 | 91040 / 114272 | training loss: 0.0003780251136049628\n",
      "epoch: 7 | 91072 / 114272 | training loss: 0.04032463952898979\n",
      "epoch: 7 | 91104 / 114272 | training loss: 0.0002965788880828768\n",
      "epoch: 7 | 91136 / 114272 | training loss: 0.00041764468187466264\n",
      "epoch: 7 | 91168 / 114272 | training loss: 0.0005617337301373482\n",
      "epoch: 7 | 91200 / 114272 | training loss: 0.0005583231686614454\n",
      "epoch: 7 | 91232 / 114272 | training loss: 0.07145658880472183\n",
      "epoch: 7 | 91264 / 114272 | training loss: 0.0005926839075982571\n",
      "epoch: 7 | 91296 / 114272 | training loss: 0.0003401431313250214\n",
      "epoch: 7 | 91328 / 114272 | training loss: 0.0003521916805766523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 91360 / 114272 | training loss: 0.00044697089469991624\n",
      "epoch: 7 | 91392 / 114272 | training loss: 0.0003719534142874181\n",
      "epoch: 7 | 91424 / 114272 | training loss: 0.0007557417848147452\n",
      "epoch: 7 | 91456 / 114272 | training loss: 0.00040871312376111746\n",
      "epoch: 7 | 91488 / 114272 | training loss: 0.0005569583154283464\n",
      "epoch: 7 | 91520 / 114272 | training loss: 0.0003985770745202899\n",
      "epoch: 7 | 91552 / 114272 | training loss: 0.0012556114234030247\n",
      "epoch: 7 | 91584 / 114272 | training loss: 0.0003759860701393336\n",
      "epoch: 7 | 91616 / 114272 | training loss: 0.0008872188627719879\n",
      "epoch: 7 | 91648 / 114272 | training loss: 0.00043370749335736036\n",
      "epoch: 7 | 91680 / 114272 | training loss: 0.0003156897146254778\n",
      "epoch: 7 | 91712 / 114272 | training loss: 0.013026248663663864\n",
      "epoch: 7 | 91744 / 114272 | training loss: 0.0007883204962126911\n",
      "epoch: 7 | 91776 / 114272 | training loss: 0.00027665929519571364\n",
      "epoch: 7 | 91808 / 114272 | training loss: 0.0005102681461721659\n",
      "epoch: 7 | 91840 / 114272 | training loss: 0.0006521627074107528\n",
      "epoch: 7 | 91872 / 114272 | training loss: 0.0005039354437030852\n",
      "epoch: 7 | 91904 / 114272 | training loss: 0.00037887782673351467\n",
      "epoch: 7 | 91936 / 114272 | training loss: 0.00034460306051187217\n",
      "epoch: 7 | 91968 / 114272 | training loss: 0.007619448937475681\n",
      "epoch: 7 | 92000 / 114272 | training loss: 0.00046972374548204243\n",
      "epoch: 7 | 92032 / 114272 | training loss: 0.0002882386033888906\n",
      "epoch: 7 | 92064 / 114272 | training loss: 0.0002928096801042557\n",
      "epoch: 7 | 92096 / 114272 | training loss: 0.0004003555222880095\n",
      "epoch: 7 | 92128 / 114272 | training loss: 0.0003766138106584549\n",
      "epoch: 7 | 92160 / 114272 | training loss: 0.0002099818957503885\n",
      "epoch: 7 | 92192 / 114272 | training loss: 0.00040583338704891503\n",
      "epoch: 7 | 92224 / 114272 | training loss: 0.00040628621354699135\n",
      "epoch: 7 | 92256 / 114272 | training loss: 0.000400740624172613\n",
      "epoch: 7 | 92288 / 114272 | training loss: 0.00024658485199324787\n",
      "epoch: 7 | 92320 / 114272 | training loss: 0.0005464499699883163\n",
      "epoch: 7 | 92352 / 114272 | training loss: 0.0006178257754072547\n",
      "epoch: 7 | 92384 / 114272 | training loss: 0.25359058380126953\n",
      "epoch: 7 | 92416 / 114272 | training loss: 0.0003170256386511028\n",
      "epoch: 7 | 92448 / 114272 | training loss: 0.0002555975515861064\n",
      "epoch: 7 | 92480 / 114272 | training loss: 0.00027509304345585406\n",
      "epoch: 7 | 92512 / 114272 | training loss: 0.0004927024128846824\n",
      "epoch: 7 | 92544 / 114272 | training loss: 0.0003252706374041736\n",
      "epoch: 7 | 92576 / 114272 | training loss: 0.23365145921707153\n",
      "epoch: 7 | 92608 / 114272 | training loss: 0.000765798322390765\n",
      "epoch: 7 | 92640 / 114272 | training loss: 0.00032410191488452256\n",
      "epoch: 7 | 92672 / 114272 | training loss: 0.000497114029712975\n",
      "epoch: 7 | 92704 / 114272 | training loss: 0.0005061359843239188\n",
      "epoch: 7 | 92736 / 114272 | training loss: 0.0003775431541725993\n",
      "epoch: 7 | 92768 / 114272 | training loss: 0.00038355629658326507\n",
      "epoch: 7 | 92800 / 114272 | training loss: 0.0004381413455121219\n",
      "epoch: 7 | 92832 / 114272 | training loss: 0.00026277132565155625\n",
      "epoch: 7 | 92864 / 114272 | training loss: 0.00041954178595915437\n",
      "epoch: 7 | 92896 / 114272 | training loss: 0.0003371077182237059\n",
      "epoch: 7 | 92928 / 114272 | training loss: 0.00032051964080892503\n",
      "epoch: 7 | 92960 / 114272 | training loss: 0.12795037031173706\n",
      "epoch: 7 | 92992 / 114272 | training loss: 0.00037390817306004465\n",
      "epoch: 7 | 93024 / 114272 | training loss: 0.00048367836279794574\n",
      "epoch: 7 | 93056 / 114272 | training loss: 0.00260446616448462\n",
      "epoch: 7 | 93088 / 114272 | training loss: 0.0002480065159033984\n",
      "epoch: 7 | 93120 / 114272 | training loss: 0.0008097137906588614\n",
      "epoch: 7 | 93152 / 114272 | training loss: 0.002965944353491068\n",
      "epoch: 7 | 93184 / 114272 | training loss: 0.000801756395958364\n",
      "epoch: 7 | 93216 / 114272 | training loss: 0.000622011604718864\n",
      "epoch: 7 | 93248 / 114272 | training loss: 0.0013840229948982596\n",
      "epoch: 7 | 93280 / 114272 | training loss: 0.07804852724075317\n",
      "epoch: 7 | 93312 / 114272 | training loss: 0.0004861272464040667\n",
      "epoch: 7 | 93344 / 114272 | training loss: 0.12372337281703949\n",
      "epoch: 7 | 93376 / 114272 | training loss: 0.0004260544665157795\n",
      "epoch: 7 | 93408 / 114272 | training loss: 0.00031648113508708775\n",
      "epoch: 7 | 93440 / 114272 | training loss: 0.00039329309947788715\n",
      "epoch: 7 | 93472 / 114272 | training loss: 0.3849372863769531\n",
      "epoch: 7 | 93504 / 114272 | training loss: 0.000159367365995422\n",
      "epoch: 7 | 93536 / 114272 | training loss: 0.001273412024602294\n",
      "epoch: 7 | 93568 / 114272 | training loss: 0.0004249733465258032\n",
      "epoch: 7 | 93600 / 114272 | training loss: 0.0005013387999497354\n",
      "epoch: 7 | 93632 / 114272 | training loss: 0.00034238878288306296\n",
      "epoch: 7 | 93664 / 114272 | training loss: 0.0003222365630790591\n",
      "epoch: 7 | 93696 / 114272 | training loss: 0.0003250039299018681\n",
      "epoch: 7 | 93728 / 114272 | training loss: 0.0004981604870408773\n",
      "epoch: 7 | 93760 / 114272 | training loss: 0.19362382590770721\n",
      "epoch: 7 | 93792 / 114272 | training loss: 0.0004058521008118987\n",
      "epoch: 7 | 93824 / 114272 | training loss: 0.2114451378583908\n",
      "epoch: 7 | 93856 / 114272 | training loss: 0.0005082364077679813\n",
      "epoch: 7 | 93888 / 114272 | training loss: 0.0006968420348130167\n",
      "epoch: 7 | 93920 / 114272 | training loss: 0.00047436816385015845\n",
      "epoch: 7 | 93952 / 114272 | training loss: 0.00043373688822612166\n",
      "epoch: 7 | 93984 / 114272 | training loss: 0.15809045732021332\n",
      "epoch: 7 | 94016 / 114272 | training loss: 0.00039236500742845237\n",
      "epoch: 7 | 94048 / 114272 | training loss: 0.23857806622982025\n",
      "epoch: 7 | 94080 / 114272 | training loss: 0.0005865977145731449\n",
      "epoch: 7 | 94112 / 114272 | training loss: 0.0004229917249176651\n",
      "epoch: 7 | 94144 / 114272 | training loss: 0.0005401641828939319\n",
      "epoch: 7 | 94176 / 114272 | training loss: 0.00033045507734641433\n",
      "epoch: 7 | 94208 / 114272 | training loss: 0.0008079693652689457\n",
      "epoch: 7 | 94240 / 114272 | training loss: 0.0003018811985384673\n",
      "epoch: 7 | 94272 / 114272 | training loss: 0.0005238543963059783\n",
      "epoch: 7 | 94304 / 114272 | training loss: 0.0006661044899374247\n",
      "epoch: 7 | 94336 / 114272 | training loss: 0.0005186576163396239\n",
      "epoch: 7 | 94368 / 114272 | training loss: 0.011227032169699669\n",
      "epoch: 7 | 94400 / 114272 | training loss: 0.0004651023482438177\n",
      "epoch: 7 | 94432 / 114272 | training loss: 0.0004236839304212481\n",
      "epoch: 7 | 94464 / 114272 | training loss: 0.00045838317601010203\n",
      "epoch: 7 | 94496 / 114272 | training loss: 0.0005371737061068416\n",
      "epoch: 7 | 94528 / 114272 | training loss: 0.0004800709430128336\n",
      "epoch: 7 | 94560 / 114272 | training loss: 0.0004055879544466734\n",
      "epoch: 7 | 94592 / 114272 | training loss: 0.0003672335878945887\n",
      "epoch: 7 | 94624 / 114272 | training loss: 0.00039726184331811965\n",
      "epoch: 7 | 94656 / 114272 | training loss: 0.0008812039159238338\n",
      "epoch: 7 | 94688 / 114272 | training loss: 0.0004629550385288894\n",
      "epoch: 7 | 94720 / 114272 | training loss: 0.2277200073003769\n",
      "epoch: 7 | 94752 / 114272 | training loss: 0.0005798850324936211\n",
      "epoch: 7 | 94784 / 114272 | training loss: 0.0004907715483568609\n",
      "epoch: 7 | 94816 / 114272 | training loss: 0.0005098386900499463\n",
      "epoch: 7 | 94848 / 114272 | training loss: 0.0010236517991870642\n",
      "epoch: 7 | 94880 / 114272 | training loss: 0.12185341119766235\n",
      "epoch: 7 | 94912 / 114272 | training loss: 0.0005474106292240322\n",
      "epoch: 7 | 94944 / 114272 | training loss: 0.00042981564183719456\n",
      "epoch: 7 | 94976 / 114272 | training loss: 0.00039777965866960585\n",
      "epoch: 7 | 95008 / 114272 | training loss: 0.14238901436328888\n",
      "epoch: 7 | 95040 / 114272 | training loss: 0.0004830016987398267\n",
      "epoch: 7 | 95072 / 114272 | training loss: 0.00028852076502516866\n",
      "epoch: 7 | 95104 / 114272 | training loss: 0.09722573310136795\n",
      "epoch: 7 | 95136 / 114272 | training loss: 0.0007270678179338574\n",
      "epoch: 7 | 95168 / 114272 | training loss: 0.1392182856798172\n",
      "epoch: 7 | 95200 / 114272 | training loss: 0.00047183039714582264\n",
      "epoch: 7 | 95232 / 114272 | training loss: 0.001401356770657003\n",
      "epoch: 7 | 95264 / 114272 | training loss: 0.0003471971722319722\n",
      "epoch: 7 | 95296 / 114272 | training loss: 0.0016491231508553028\n",
      "epoch: 7 | 95328 / 114272 | training loss: 0.0004967320710420609\n",
      "epoch: 7 | 95360 / 114272 | training loss: 0.0007535545155405998\n",
      "epoch: 7 | 95392 / 114272 | training loss: 0.00069392379373312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 95424 / 114272 | training loss: 0.00041758138104341924\n",
      "epoch: 7 | 95456 / 114272 | training loss: 0.0011592441005632281\n",
      "epoch: 7 | 95488 / 114272 | training loss: 0.00038680859142914414\n",
      "epoch: 7 | 95520 / 114272 | training loss: 0.0004378198937047273\n",
      "epoch: 7 | 95552 / 114272 | training loss: 0.00037578310002572834\n",
      "epoch: 7 | 95584 / 114272 | training loss: 0.0006471267552115023\n",
      "epoch: 7 | 95616 / 114272 | training loss: 0.0021896257530897856\n",
      "epoch: 7 | 95648 / 114272 | training loss: 0.0017381191719323397\n",
      "epoch: 7 | 95680 / 114272 | training loss: 0.17374902963638306\n",
      "epoch: 7 | 95712 / 114272 | training loss: 0.0008694620337337255\n",
      "epoch: 7 | 95744 / 114272 | training loss: 0.0009320232202298939\n",
      "epoch: 7 | 95776 / 114272 | training loss: 0.06696625053882599\n",
      "epoch: 7 | 95808 / 114272 | training loss: 0.0017416269984096289\n",
      "epoch: 7 | 95840 / 114272 | training loss: 0.0015091089298948646\n",
      "epoch: 7 | 95872 / 114272 | training loss: 0.001198106911033392\n",
      "epoch: 7 | 95904 / 114272 | training loss: 0.0002471189945936203\n",
      "epoch: 7 | 95936 / 114272 | training loss: 0.01003576535731554\n",
      "epoch: 7 | 95968 / 114272 | training loss: 0.0036449707113206387\n",
      "epoch: 7 | 96000 / 114272 | training loss: 0.002398847369477153\n",
      "epoch: 7 | 96032 / 114272 | training loss: 0.0008503712015226483\n",
      "epoch: 7 | 96064 / 114272 | training loss: 0.0137156518176198\n",
      "epoch: 7 | 96096 / 114272 | training loss: 0.0005804693209938705\n",
      "epoch: 7 | 96128 / 114272 | training loss: 0.0010002430062741041\n",
      "epoch: 7 | 96160 / 114272 | training loss: 0.0003653994353953749\n",
      "epoch: 7 | 96192 / 114272 | training loss: 0.0017150611383840442\n",
      "epoch: 7 | 96224 / 114272 | training loss: 0.00037977451574988663\n",
      "epoch: 7 | 96256 / 114272 | training loss: 0.000578336592298001\n",
      "epoch: 7 | 96288 / 114272 | training loss: 0.0007407232187688351\n",
      "epoch: 7 | 96320 / 114272 | training loss: 0.0005263613420538604\n",
      "epoch: 7 | 96352 / 114272 | training loss: 0.0009221879299730062\n",
      "epoch: 7 | 96384 / 114272 | training loss: 0.12470018863677979\n",
      "epoch: 7 | 96416 / 114272 | training loss: 0.000752395368181169\n",
      "epoch: 7 | 96448 / 114272 | training loss: 0.23059600591659546\n",
      "epoch: 7 | 96480 / 114272 | training loss: 0.00035480575752444565\n",
      "epoch: 7 | 96512 / 114272 | training loss: 0.0011358914198353887\n",
      "epoch: 7 | 96544 / 114272 | training loss: 0.0005073886131867766\n",
      "epoch: 7 | 96576 / 114272 | training loss: 0.0008089889888651669\n",
      "epoch: 7 | 96608 / 114272 | training loss: 0.0007295204559341073\n",
      "epoch: 7 | 96640 / 114272 | training loss: 0.0008682803018018603\n",
      "epoch: 7 | 96672 / 114272 | training loss: 0.18366537988185883\n",
      "epoch: 7 | 96704 / 114272 | training loss: 0.21455702185630798\n",
      "epoch: 7 | 96736 / 114272 | training loss: 0.003872921224683523\n",
      "epoch: 7 | 96768 / 114272 | training loss: 0.0005531784263439476\n",
      "epoch: 7 | 96800 / 114272 | training loss: 0.0004677334800362587\n",
      "epoch: 7 | 96832 / 114272 | training loss: 0.00048228769446723163\n",
      "epoch: 7 | 96864 / 114272 | training loss: 0.0015373807400465012\n",
      "epoch: 7 | 96896 / 114272 | training loss: 0.0005347636179067194\n",
      "epoch: 7 | 96928 / 114272 | training loss: 0.05699915811419487\n",
      "epoch: 7 | 96960 / 114272 | training loss: 0.00037335214437916875\n",
      "epoch: 7 | 96992 / 114272 | training loss: 0.00037315001827664673\n",
      "epoch: 7 | 97024 / 114272 | training loss: 0.0009134938009083271\n",
      "epoch: 7 | 97056 / 114272 | training loss: 0.0006579432520084083\n",
      "epoch: 7 | 97088 / 114272 | training loss: 0.4190202057361603\n",
      "epoch: 7 | 97120 / 114272 | training loss: 0.00048496367526240647\n",
      "epoch: 7 | 97152 / 114272 | training loss: 0.00037138513289391994\n",
      "epoch: 7 | 97184 / 114272 | training loss: 0.0005310935666784644\n",
      "epoch: 7 | 97216 / 114272 | training loss: 0.0019333172822371125\n",
      "epoch: 7 | 97248 / 114272 | training loss: 0.000813017541076988\n",
      "epoch: 7 | 97280 / 114272 | training loss: 0.0008968178299255669\n",
      "epoch: 7 | 97312 / 114272 | training loss: 0.0007011429406702518\n",
      "epoch: 7 | 97344 / 114272 | training loss: 0.0005623130709864199\n",
      "epoch: 7 | 97376 / 114272 | training loss: 0.0009763280977495015\n",
      "epoch: 7 | 97408 / 114272 | training loss: 0.0004596178187057376\n",
      "epoch: 7 | 97440 / 114272 | training loss: 0.12746749818325043\n",
      "epoch: 7 | 97472 / 114272 | training loss: 0.0011317298049107194\n",
      "epoch: 7 | 97504 / 114272 | training loss: 0.00046396927791647613\n",
      "epoch: 7 | 97536 / 114272 | training loss: 0.0011675279820337892\n",
      "epoch: 7 | 97568 / 114272 | training loss: 0.00037591648288071156\n",
      "epoch: 7 | 97600 / 114272 | training loss: 0.0006810870254412293\n",
      "epoch: 7 | 97632 / 114272 | training loss: 0.0004230649792589247\n",
      "epoch: 7 | 97664 / 114272 | training loss: 0.03617989271879196\n",
      "epoch: 7 | 97696 / 114272 | training loss: 0.0006602283101528883\n",
      "epoch: 7 | 97728 / 114272 | training loss: 0.00039896205998957157\n",
      "epoch: 7 | 97760 / 114272 | training loss: 0.0005037272348999977\n",
      "epoch: 7 | 97792 / 114272 | training loss: 0.000350461428752169\n",
      "epoch: 7 | 97824 / 114272 | training loss: 0.001053995336405933\n",
      "epoch: 7 | 97856 / 114272 | training loss: 0.0004924209206365049\n",
      "epoch: 7 | 97888 / 114272 | training loss: 0.0014053229242563248\n",
      "epoch: 7 | 97920 / 114272 | training loss: 0.00028061697958037257\n",
      "epoch: 7 | 97952 / 114272 | training loss: 0.00043631106382235885\n",
      "epoch: 7 | 97984 / 114272 | training loss: 0.0004199076793156564\n",
      "epoch: 7 | 98016 / 114272 | training loss: 0.000577307480853051\n",
      "epoch: 7 | 98048 / 114272 | training loss: 0.000682815327309072\n",
      "epoch: 7 | 98080 / 114272 | training loss: 0.00045539505663327873\n",
      "epoch: 7 | 98112 / 114272 | training loss: 0.0008068073075264692\n",
      "epoch: 7 | 98144 / 114272 | training loss: 0.0004528093559201807\n",
      "epoch: 7 | 98176 / 114272 | training loss: 0.0005003829719498754\n",
      "epoch: 7 | 98208 / 114272 | training loss: 0.03806852549314499\n",
      "epoch: 7 | 98240 / 114272 | training loss: 0.000516227213665843\n",
      "epoch: 7 | 98272 / 114272 | training loss: 0.0046288189478218555\n",
      "epoch: 7 | 98304 / 114272 | training loss: 0.004446645732969046\n",
      "epoch: 7 | 98336 / 114272 | training loss: 0.0016895189182832837\n",
      "epoch: 7 | 98368 / 114272 | training loss: 0.00033567173522897065\n",
      "epoch: 7 | 98400 / 114272 | training loss: 0.0006442040321417153\n",
      "epoch: 7 | 98432 / 114272 | training loss: 0.0005464437999762595\n",
      "epoch: 7 | 98464 / 114272 | training loss: 0.0005160163855180144\n",
      "epoch: 7 | 98496 / 114272 | training loss: 0.0005992305232211947\n",
      "epoch: 7 | 98528 / 114272 | training loss: 0.0006250398582778871\n",
      "epoch: 7 | 98560 / 114272 | training loss: 0.0002886706788558513\n",
      "epoch: 7 | 98592 / 114272 | training loss: 0.0006168491090647876\n",
      "epoch: 7 | 98624 / 114272 | training loss: 0.0006593161961063743\n",
      "epoch: 7 | 98656 / 114272 | training loss: 0.00028206256683915854\n",
      "epoch: 7 | 98688 / 114272 | training loss: 0.00037006792263127863\n",
      "epoch: 7 | 98720 / 114272 | training loss: 0.0009581418125890195\n",
      "epoch: 7 | 98752 / 114272 | training loss: 0.000464560987893492\n",
      "epoch: 7 | 98784 / 114272 | training loss: 0.23242059350013733\n",
      "epoch: 7 | 98816 / 114272 | training loss: 0.03418285772204399\n",
      "epoch: 7 | 98848 / 114272 | training loss: 0.0014979209518060088\n",
      "epoch: 7 | 98880 / 114272 | training loss: 0.000546150840818882\n",
      "epoch: 7 | 98912 / 114272 | training loss: 0.13609930872917175\n",
      "epoch: 7 | 98944 / 114272 | training loss: 0.0006328713498078287\n",
      "epoch: 7 | 98976 / 114272 | training loss: 0.00040240513044409454\n",
      "epoch: 7 | 99008 / 114272 | training loss: 0.22586970031261444\n",
      "epoch: 7 | 99040 / 114272 | training loss: 0.00047313974937424064\n",
      "epoch: 7 | 99072 / 114272 | training loss: 0.0004963907995261252\n",
      "epoch: 7 | 99104 / 114272 | training loss: 0.0010576447239145637\n",
      "epoch: 7 | 99136 / 114272 | training loss: 0.00048304334632121027\n",
      "epoch: 7 | 99168 / 114272 | training loss: 0.0004978732322342694\n",
      "epoch: 7 | 99200 / 114272 | training loss: 0.00032018517958931625\n",
      "epoch: 7 | 99232 / 114272 | training loss: 0.0004132123140152544\n",
      "epoch: 7 | 99264 / 114272 | training loss: 0.000680037250276655\n",
      "epoch: 7 | 99296 / 114272 | training loss: 0.000831306038890034\n",
      "epoch: 7 | 99328 / 114272 | training loss: 0.0006678103818558156\n",
      "epoch: 7 | 99360 / 114272 | training loss: 0.00028373164241202176\n",
      "epoch: 7 | 99392 / 114272 | training loss: 0.0002985586179420352\n",
      "epoch: 7 | 99424 / 114272 | training loss: 0.0005031792097724974\n",
      "epoch: 7 | 99456 / 114272 | training loss: 0.0003781112318392843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 99488 / 114272 | training loss: 0.09857155382633209\n",
      "epoch: 7 | 99520 / 114272 | training loss: 0.0009615348535589874\n",
      "epoch: 7 | 99552 / 114272 | training loss: 0.0005163043970242143\n",
      "epoch: 7 | 99584 / 114272 | training loss: 0.1529056429862976\n",
      "epoch: 7 | 99616 / 114272 | training loss: 0.0016396581195294857\n",
      "epoch: 7 | 99648 / 114272 | training loss: 0.08711953461170197\n",
      "epoch: 7 | 99680 / 114272 | training loss: 0.0007255825912579894\n",
      "epoch: 7 | 99712 / 114272 | training loss: 0.00037561269709840417\n",
      "epoch: 7 | 99744 / 114272 | training loss: 0.0005898435483686626\n",
      "epoch: 7 | 99776 / 114272 | training loss: 0.001663238974288106\n",
      "epoch: 7 | 99808 / 114272 | training loss: 0.00044922492816112936\n",
      "epoch: 7 | 99840 / 114272 | training loss: 0.0003310189349576831\n",
      "epoch: 7 | 99872 / 114272 | training loss: 0.0005760256317444146\n",
      "epoch: 7 | 99904 / 114272 | training loss: 0.0049894279800355434\n",
      "epoch: 7 | 99936 / 114272 | training loss: 0.0006904105539433658\n",
      "epoch: 7 | 99968 / 114272 | training loss: 0.0011300353799015284\n",
      "epoch: 7 | 100000 / 114272 | training loss: 0.301195353269577\n",
      "epoch: 7 | 100032 / 114272 | training loss: 0.05367354303598404\n",
      "epoch: 7 | 100064 / 114272 | training loss: 0.0003542437043506652\n",
      "epoch: 7 | 100096 / 114272 | training loss: 0.0005914135253988206\n",
      "epoch: 7 | 100128 / 114272 | training loss: 0.00047690002247691154\n",
      "epoch: 7 | 100160 / 114272 | training loss: 0.0005235478165559471\n",
      "epoch: 7 | 100192 / 114272 | training loss: 0.00033735879696905613\n",
      "epoch: 7 | 100224 / 114272 | training loss: 0.0005701824557036161\n",
      "epoch: 7 | 100256 / 114272 | training loss: 0.0032807437237352133\n",
      "epoch: 7 | 100288 / 114272 | training loss: 0.0005899138632230461\n",
      "epoch: 7 | 100320 / 114272 | training loss: 0.06905409693717957\n",
      "epoch: 7 | 100352 / 114272 | training loss: 0.0004993081674911082\n",
      "epoch: 7 | 100384 / 114272 | training loss: 0.0009256721823476255\n",
      "epoch: 7 | 100416 / 114272 | training loss: 0.0006462425226345658\n",
      "epoch: 7 | 100448 / 114272 | training loss: 0.00048337678890675306\n",
      "epoch: 7 | 100480 / 114272 | training loss: 0.0010855640284717083\n",
      "epoch: 7 | 100512 / 114272 | training loss: 0.00413714163005352\n",
      "epoch: 7 | 100544 / 114272 | training loss: 0.0005978035042062402\n",
      "epoch: 7 | 100576 / 114272 | training loss: 0.0006494181579910219\n",
      "epoch: 7 | 100608 / 114272 | training loss: 0.0006535910069942474\n",
      "epoch: 7 | 100640 / 114272 | training loss: 0.0004780674062203616\n",
      "epoch: 7 | 100672 / 114272 | training loss: 0.001520646153949201\n",
      "epoch: 7 | 100704 / 114272 | training loss: 0.051861077547073364\n",
      "epoch: 7 | 100736 / 114272 | training loss: 0.11336071789264679\n",
      "epoch: 7 | 100768 / 114272 | training loss: 0.0003736375365406275\n",
      "epoch: 7 | 100800 / 114272 | training loss: 0.0015005012974143028\n",
      "epoch: 7 | 100832 / 114272 | training loss: 0.0007199837127700448\n",
      "epoch: 7 | 100864 / 114272 | training loss: 0.0008238837472163141\n",
      "epoch: 7 | 100896 / 114272 | training loss: 0.0012113368138670921\n",
      "epoch: 7 | 100928 / 114272 | training loss: 0.1171390637755394\n",
      "epoch: 7 | 100960 / 114272 | training loss: 0.001052387524396181\n",
      "epoch: 7 | 100992 / 114272 | training loss: 0.0009188755648210645\n",
      "epoch: 7 | 101024 / 114272 | training loss: 0.0006894031539559364\n",
      "epoch: 7 | 101056 / 114272 | training loss: 0.022310685366392136\n",
      "epoch: 7 | 101088 / 114272 | training loss: 0.11440085619688034\n",
      "epoch: 7 | 101120 / 114272 | training loss: 0.013151916675269604\n",
      "epoch: 7 | 101152 / 114272 | training loss: 0.000678188051097095\n",
      "epoch: 7 | 101184 / 114272 | training loss: 0.0023684233892709017\n",
      "epoch: 7 | 101216 / 114272 | training loss: 0.0005347244441509247\n",
      "epoch: 7 | 101248 / 114272 | training loss: 0.0012182951904833317\n",
      "epoch: 7 | 101280 / 114272 | training loss: 0.0006324404384940863\n",
      "epoch: 7 | 101312 / 114272 | training loss: 0.0007151426980271935\n",
      "epoch: 7 | 101344 / 114272 | training loss: 0.0006704878178425133\n",
      "epoch: 7 | 101376 / 114272 | training loss: 0.0021637175232172012\n",
      "epoch: 7 | 101408 / 114272 | training loss: 0.11050142347812653\n",
      "epoch: 7 | 101440 / 114272 | training loss: 0.0005440376116894186\n",
      "epoch: 7 | 101472 / 114272 | training loss: 0.0005270998808555305\n",
      "epoch: 7 | 101504 / 114272 | training loss: 0.011831223033368587\n",
      "epoch: 7 | 101536 / 114272 | training loss: 0.0005626778001897037\n",
      "epoch: 7 | 101568 / 114272 | training loss: 0.0013033212162554264\n",
      "epoch: 7 | 101600 / 114272 | training loss: 0.00040188233833760023\n",
      "epoch: 7 | 101632 / 114272 | training loss: 0.0004114339826628566\n",
      "epoch: 7 | 101664 / 114272 | training loss: 0.0008320003980770707\n",
      "epoch: 7 | 101696 / 114272 | training loss: 0.005859950557351112\n",
      "epoch: 7 | 101728 / 114272 | training loss: 0.0016356740379706025\n",
      "epoch: 7 | 101760 / 114272 | training loss: 0.0004314401594456285\n",
      "epoch: 7 | 101792 / 114272 | training loss: 0.0005064599099569023\n",
      "epoch: 7 | 101824 / 114272 | training loss: 0.0006147484527900815\n",
      "epoch: 7 | 101856 / 114272 | training loss: 0.20497716963291168\n",
      "epoch: 7 | 101888 / 114272 | training loss: 0.000475588021799922\n",
      "epoch: 7 | 101920 / 114272 | training loss: 0.0005167308263480663\n",
      "epoch: 7 | 101952 / 114272 | training loss: 0.0006118444725871086\n",
      "epoch: 7 | 101984 / 114272 | training loss: 0.0006304144044406712\n",
      "epoch: 7 | 102016 / 114272 | training loss: 0.0010191708570346236\n",
      "epoch: 7 | 102048 / 114272 | training loss: 0.0005578114651143551\n",
      "epoch: 7 | 102080 / 114272 | training loss: 0.0005410873563960195\n",
      "epoch: 7 | 102112 / 114272 | training loss: 0.0012394551886245608\n",
      "epoch: 7 | 102144 / 114272 | training loss: 0.0005808259593322873\n",
      "epoch: 7 | 102176 / 114272 | training loss: 0.00036408769665285945\n",
      "epoch: 7 | 102208 / 114272 | training loss: 0.0004491785657592118\n",
      "epoch: 7 | 102240 / 114272 | training loss: 0.0005041617550887167\n",
      "epoch: 7 | 102272 / 114272 | training loss: 0.00045810022857040167\n",
      "epoch: 7 | 102304 / 114272 | training loss: 0.0008797641494311392\n",
      "epoch: 7 | 102336 / 114272 | training loss: 0.00035261077573522925\n",
      "epoch: 7 | 102368 / 114272 | training loss: 0.12527185678482056\n",
      "epoch: 7 | 102400 / 114272 | training loss: 0.00044351289398036897\n",
      "epoch: 7 | 102432 / 114272 | training loss: 0.0004794521373696625\n",
      "epoch: 7 | 102464 / 114272 | training loss: 0.0014583209995180368\n",
      "epoch: 7 | 102496 / 114272 | training loss: 0.00039341553929261863\n",
      "epoch: 7 | 102528 / 114272 | training loss: 0.0006121588521637022\n",
      "epoch: 7 | 102560 / 114272 | training loss: 0.049473341554403305\n",
      "epoch: 7 | 102592 / 114272 | training loss: 0.020549464970827103\n",
      "epoch: 7 | 102624 / 114272 | training loss: 0.00042966927867382765\n",
      "epoch: 7 | 102656 / 114272 | training loss: 0.0006007780320942402\n",
      "epoch: 7 | 102688 / 114272 | training loss: 0.00041075574699789286\n",
      "epoch: 7 | 102720 / 114272 | training loss: 0.05858874320983887\n",
      "epoch: 7 | 102752 / 114272 | training loss: 0.00046082286280579865\n",
      "epoch: 7 | 102784 / 114272 | training loss: 0.0005650007515214384\n",
      "epoch: 7 | 102816 / 114272 | training loss: 0.14101140201091766\n",
      "epoch: 7 | 102848 / 114272 | training loss: 0.00038852953002788126\n",
      "epoch: 7 | 102880 / 114272 | training loss: 0.002430928172543645\n",
      "epoch: 7 | 102912 / 114272 | training loss: 0.0006018485873937607\n",
      "epoch: 7 | 102944 / 114272 | training loss: 0.007632032036781311\n",
      "epoch: 7 | 102976 / 114272 | training loss: 0.0005107730394229293\n",
      "epoch: 7 | 103008 / 114272 | training loss: 0.0006712990580126643\n",
      "epoch: 7 | 103040 / 114272 | training loss: 0.26223915815353394\n",
      "epoch: 7 | 103072 / 114272 | training loss: 0.0011503271525725722\n",
      "epoch: 7 | 103104 / 114272 | training loss: 0.0005692556733265519\n",
      "epoch: 7 | 103136 / 114272 | training loss: 0.0019200531532987952\n",
      "epoch: 7 | 103168 / 114272 | training loss: 0.0005637682043015957\n",
      "epoch: 7 | 103200 / 114272 | training loss: 0.001363852759823203\n",
      "epoch: 7 | 103232 / 114272 | training loss: 0.0030193047132343054\n",
      "epoch: 7 | 103264 / 114272 | training loss: 0.0006977256271056831\n",
      "epoch: 7 | 103296 / 114272 | training loss: 0.0007769350195303559\n",
      "epoch: 7 | 103328 / 114272 | training loss: 0.003559211501851678\n",
      "epoch: 7 | 103360 / 114272 | training loss: 0.05414693057537079\n",
      "epoch: 7 | 103392 / 114272 | training loss: 0.0006984913488849998\n",
      "epoch: 7 | 103424 / 114272 | training loss: 0.0004582236579153687\n",
      "epoch: 7 | 103456 / 114272 | training loss: 0.0005335102905519307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 103488 / 114272 | training loss: 0.0004390488611534238\n",
      "epoch: 7 | 103520 / 114272 | training loss: 0.0011818482307717204\n",
      "epoch: 7 | 103552 / 114272 | training loss: 0.0007696676766499877\n",
      "epoch: 7 | 103584 / 114272 | training loss: 0.0007810090901330113\n",
      "epoch: 7 | 103616 / 114272 | training loss: 0.0006362408166751266\n",
      "epoch: 7 | 103648 / 114272 | training loss: 0.0008137181284837425\n",
      "epoch: 7 | 103680 / 114272 | training loss: 0.0005588078056462109\n",
      "epoch: 7 | 103712 / 114272 | training loss: 0.0005773192388005555\n",
      "epoch: 7 | 103744 / 114272 | training loss: 0.0008160933502949774\n",
      "epoch: 7 | 103776 / 114272 | training loss: 0.0004230391059536487\n",
      "epoch: 7 | 103808 / 114272 | training loss: 0.11292126029729843\n",
      "epoch: 7 | 103840 / 114272 | training loss: 0.000625386368483305\n",
      "epoch: 7 | 103872 / 114272 | training loss: 0.0007604179554618895\n",
      "epoch: 7 | 103904 / 114272 | training loss: 0.0009032959351316094\n",
      "epoch: 7 | 103936 / 114272 | training loss: 0.009930039756000042\n",
      "epoch: 7 | 103968 / 114272 | training loss: 0.007506840396672487\n",
      "epoch: 7 | 104000 / 114272 | training loss: 0.0004578777588903904\n",
      "epoch: 7 | 104032 / 114272 | training loss: 0.0005852758185938001\n",
      "epoch: 7 | 104064 / 114272 | training loss: 0.0011545575689524412\n",
      "epoch: 7 | 104096 / 114272 | training loss: 0.40109243988990784\n",
      "epoch: 7 | 104128 / 114272 | training loss: 0.00029620114946737885\n",
      "epoch: 7 | 104160 / 114272 | training loss: 0.0004085224063601345\n",
      "epoch: 7 | 104192 / 114272 | training loss: 0.0017107679741457105\n",
      "epoch: 7 | 104224 / 114272 | training loss: 0.0005378891946747899\n",
      "epoch: 7 | 104256 / 114272 | training loss: 0.11697027087211609\n",
      "epoch: 7 | 104288 / 114272 | training loss: 0.002043507294729352\n",
      "epoch: 7 | 104320 / 114272 | training loss: 0.0004932211013510823\n",
      "epoch: 7 | 104352 / 114272 | training loss: 0.002149708801880479\n",
      "epoch: 7 | 104384 / 114272 | training loss: 0.002426950028166175\n",
      "epoch: 7 | 104416 / 114272 | training loss: 0.17880253493785858\n",
      "epoch: 7 | 104448 / 114272 | training loss: 0.0006102383485995233\n",
      "epoch: 7 | 104480 / 114272 | training loss: 0.0007517175399698317\n",
      "epoch: 7 | 104512 / 114272 | training loss: 0.0010114909382537007\n",
      "epoch: 7 | 104544 / 114272 | training loss: 0.0010779574513435364\n",
      "epoch: 7 | 104576 / 114272 | training loss: 0.000647895853035152\n",
      "epoch: 7 | 104608 / 114272 | training loss: 0.0004979765508323908\n",
      "epoch: 7 | 104640 / 114272 | training loss: 0.0005876532522961497\n",
      "epoch: 7 | 104672 / 114272 | training loss: 0.0010124137625098228\n",
      "epoch: 7 | 104704 / 114272 | training loss: 0.0006544613861478865\n",
      "epoch: 7 | 104736 / 114272 | training loss: 0.0009493987308815122\n",
      "epoch: 7 | 104768 / 114272 | training loss: 0.04976609721779823\n",
      "epoch: 7 | 104800 / 114272 | training loss: 0.0016301680589094758\n",
      "epoch: 7 | 104832 / 114272 | training loss: 0.0006815333617851138\n",
      "epoch: 7 | 104864 / 114272 | training loss: 0.0005531601491384208\n",
      "epoch: 7 | 104896 / 114272 | training loss: 0.008568515069782734\n",
      "epoch: 7 | 104928 / 114272 | training loss: 0.011868535540997982\n",
      "epoch: 7 | 104960 / 114272 | training loss: 0.0013755180407315493\n",
      "epoch: 7 | 104992 / 114272 | training loss: 0.0004711564688477665\n",
      "epoch: 7 | 105024 / 114272 | training loss: 0.06620214134454727\n",
      "epoch: 7 | 105056 / 114272 | training loss: 0.0007494517485611141\n",
      "epoch: 7 | 105088 / 114272 | training loss: 0.0005026124999858439\n",
      "epoch: 7 | 105120 / 114272 | training loss: 0.00045921900891698897\n",
      "epoch: 7 | 105152 / 114272 | training loss: 0.0003488307411316782\n",
      "epoch: 7 | 105184 / 114272 | training loss: 0.00036933121737092733\n",
      "epoch: 7 | 105216 / 114272 | training loss: 0.0007777222199365497\n",
      "epoch: 7 | 105248 / 114272 | training loss: 0.0009024140890687704\n",
      "epoch: 7 | 105280 / 114272 | training loss: 0.0021650008857250214\n",
      "epoch: 7 | 105312 / 114272 | training loss: 0.0006952792755328119\n",
      "epoch: 7 | 105344 / 114272 | training loss: 0.0006090544047765434\n",
      "epoch: 7 | 105376 / 114272 | training loss: 0.18538512289524078\n",
      "epoch: 7 | 105408 / 114272 | training loss: 0.0007520997314713895\n",
      "epoch: 7 | 105440 / 114272 | training loss: 0.0006181090720929205\n",
      "epoch: 7 | 105472 / 114272 | training loss: 0.01082832831889391\n",
      "epoch: 7 | 105504 / 114272 | training loss: 0.0005672966944985092\n",
      "epoch: 7 | 105536 / 114272 | training loss: 0.0011154233943670988\n",
      "epoch: 7 | 105568 / 114272 | training loss: 0.0005171199445612729\n",
      "epoch: 7 | 105600 / 114272 | training loss: 0.00031164917163550854\n",
      "epoch: 7 | 105632 / 114272 | training loss: 0.0007069758721627295\n",
      "epoch: 7 | 105664 / 114272 | training loss: 0.2547609210014343\n",
      "epoch: 7 | 105696 / 114272 | training loss: 0.0007509421557188034\n",
      "epoch: 7 | 105728 / 114272 | training loss: 0.0004051965370308608\n",
      "epoch: 7 | 105760 / 114272 | training loss: 0.0006502965115942061\n",
      "epoch: 7 | 105792 / 114272 | training loss: 0.09948457032442093\n",
      "epoch: 7 | 105824 / 114272 | training loss: 0.0005842120153829455\n",
      "epoch: 7 | 105856 / 114272 | training loss: 0.0011456848587840796\n",
      "epoch: 7 | 105888 / 114272 | training loss: 0.0005036911461502314\n",
      "epoch: 7 | 105920 / 114272 | training loss: 0.0008559865527786314\n",
      "epoch: 7 | 105952 / 114272 | training loss: 0.0006746886647306383\n",
      "epoch: 7 | 105984 / 114272 | training loss: 0.00037522357888519764\n",
      "epoch: 7 | 106016 / 114272 | training loss: 0.049092382192611694\n",
      "epoch: 7 | 106048 / 114272 | training loss: 0.00039767182897776365\n",
      "epoch: 7 | 106080 / 114272 | training loss: 0.00041742424946278334\n",
      "epoch: 7 | 106112 / 114272 | training loss: 0.00065759161952883\n",
      "epoch: 7 | 106144 / 114272 | training loss: 0.0005043906276114285\n",
      "epoch: 7 | 106176 / 114272 | training loss: 0.0005135987885296345\n",
      "epoch: 7 | 106208 / 114272 | training loss: 0.005320757161825895\n",
      "epoch: 7 | 106240 / 114272 | training loss: 0.0005555037641897798\n",
      "epoch: 7 | 106272 / 114272 | training loss: 0.0004804307536687702\n",
      "epoch: 7 | 106304 / 114272 | training loss: 0.0004502966476138681\n",
      "epoch: 7 | 106336 / 114272 | training loss: 0.0033760350197553635\n",
      "epoch: 7 | 106368 / 114272 | training loss: 0.0009744993876665831\n",
      "epoch: 7 | 106400 / 114272 | training loss: 0.00092287891311571\n",
      "epoch: 7 | 106432 / 114272 | training loss: 0.0032235714606940746\n",
      "epoch: 7 | 106464 / 114272 | training loss: 0.13945713639259338\n",
      "epoch: 7 | 106496 / 114272 | training loss: 0.0003341976262163371\n",
      "epoch: 7 | 106528 / 114272 | training loss: 0.0004897507606074214\n",
      "epoch: 7 | 106560 / 114272 | training loss: 0.0006701871752738953\n",
      "epoch: 7 | 106592 / 114272 | training loss: 0.00048201053868979216\n",
      "epoch: 7 | 106624 / 114272 | training loss: 0.000856787315569818\n",
      "epoch: 7 | 106656 / 114272 | training loss: 0.0004554009938146919\n",
      "epoch: 7 | 106688 / 114272 | training loss: 0.0007120187510736287\n",
      "epoch: 7 | 106720 / 114272 | training loss: 0.05587344616651535\n",
      "epoch: 7 | 106752 / 114272 | training loss: 0.0006255526095628738\n",
      "epoch: 7 | 106784 / 114272 | training loss: 0.0004905321402475238\n",
      "epoch: 7 | 106816 / 114272 | training loss: 0.0010483948281034827\n",
      "epoch: 7 | 106848 / 114272 | training loss: 0.00032235251273959875\n",
      "epoch: 7 | 106880 / 114272 | training loss: 0.000626213091891259\n",
      "epoch: 7 | 106912 / 114272 | training loss: 0.06320202350616455\n",
      "epoch: 7 | 106944 / 114272 | training loss: 0.00039295954047702253\n",
      "epoch: 7 | 106976 / 114272 | training loss: 0.00040946531225927174\n",
      "epoch: 7 | 107008 / 114272 | training loss: 0.00048578757559880614\n",
      "epoch: 7 | 107040 / 114272 | training loss: 0.0004176833026576787\n",
      "epoch: 7 | 107072 / 114272 | training loss: 0.00042503682198002934\n",
      "epoch: 7 | 107104 / 114272 | training loss: 0.0006013501551933587\n",
      "epoch: 7 | 107136 / 114272 | training loss: 0.0011239604791626334\n",
      "epoch: 7 | 107168 / 114272 | training loss: 0.06767573952674866\n",
      "epoch: 7 | 107200 / 114272 | training loss: 0.12123255431652069\n",
      "epoch: 7 | 107232 / 114272 | training loss: 0.0006064765038900077\n",
      "epoch: 7 | 107264 / 114272 | training loss: 0.002355462871491909\n",
      "epoch: 7 | 107296 / 114272 | training loss: 0.0016071428544819355\n",
      "epoch: 7 | 107328 / 114272 | training loss: 0.00035072449827566743\n",
      "epoch: 7 | 107360 / 114272 | training loss: 0.0006985361687839031\n",
      "epoch: 7 | 107392 / 114272 | training loss: 0.0005181275773793459\n",
      "epoch: 7 | 107424 / 114272 | training loss: 0.003316734917461872\n",
      "epoch: 7 | 107456 / 114272 | training loss: 0.000457668153103441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 107488 / 114272 | training loss: 0.0026214965619146824\n",
      "epoch: 7 | 107520 / 114272 | training loss: 0.07539860159158707\n",
      "epoch: 7 | 107552 / 114272 | training loss: 0.0003696903877425939\n",
      "epoch: 7 | 107584 / 114272 | training loss: 0.0019521977519616485\n",
      "epoch: 7 | 107616 / 114272 | training loss: 0.00047683733282610774\n",
      "epoch: 7 | 107648 / 114272 | training loss: 0.0003827569598797709\n",
      "epoch: 7 | 107680 / 114272 | training loss: 0.000475273554911837\n",
      "epoch: 7 | 107712 / 114272 | training loss: 0.0004394219140522182\n",
      "epoch: 7 | 107744 / 114272 | training loss: 0.00048250294639728963\n",
      "epoch: 7 | 107776 / 114272 | training loss: 0.0004133709880989045\n",
      "epoch: 7 | 107808 / 114272 | training loss: 0.00038173090433701873\n",
      "epoch: 7 | 107840 / 114272 | training loss: 0.0005419269436970353\n",
      "epoch: 7 | 107872 / 114272 | training loss: 0.03633316978812218\n",
      "epoch: 7 | 107904 / 114272 | training loss: 0.0004888522089459002\n",
      "epoch: 7 | 107936 / 114272 | training loss: 0.00044170606997795403\n",
      "epoch: 7 | 107968 / 114272 | training loss: 0.050589438527822495\n",
      "epoch: 7 | 108000 / 114272 | training loss: 0.0005191643140278757\n",
      "epoch: 7 | 108032 / 114272 | training loss: 0.00038573271012865007\n",
      "epoch: 7 | 108064 / 114272 | training loss: 0.0020374704618006945\n",
      "epoch: 7 | 108096 / 114272 | training loss: 0.1710914969444275\n",
      "epoch: 7 | 108128 / 114272 | training loss: 0.04537343606352806\n",
      "epoch: 7 | 108160 / 114272 | training loss: 0.09762895107269287\n",
      "epoch: 7 | 108192 / 114272 | training loss: 0.001145852729678154\n",
      "epoch: 7 | 108224 / 114272 | training loss: 0.005624693818390369\n",
      "epoch: 7 | 108256 / 114272 | training loss: 0.00037540827179327607\n",
      "epoch: 7 | 108288 / 114272 | training loss: 0.0094467056915164\n",
      "epoch: 7 | 108320 / 114272 | training loss: 0.009946349076926708\n",
      "epoch: 7 | 108352 / 114272 | training loss: 0.006479313131421804\n",
      "epoch: 7 | 108384 / 114272 | training loss: 0.00028829267830587924\n",
      "epoch: 7 | 108416 / 114272 | training loss: 0.00038252750528045\n",
      "epoch: 7 | 108448 / 114272 | training loss: 0.009131696075201035\n",
      "epoch: 7 | 108480 / 114272 | training loss: 0.00028646778082475066\n",
      "epoch: 7 | 108512 / 114272 | training loss: 0.0003722083056345582\n",
      "epoch: 7 | 108544 / 114272 | training loss: 0.0004026945971418172\n",
      "epoch: 7 | 108576 / 114272 | training loss: 0.0004688577027991414\n",
      "epoch: 7 | 108608 / 114272 | training loss: 0.0004336985875852406\n",
      "epoch: 7 | 108640 / 114272 | training loss: 0.0038899413775652647\n",
      "epoch: 7 | 108672 / 114272 | training loss: 0.00048110485658980906\n",
      "epoch: 7 | 108704 / 114272 | training loss: 0.016466790810227394\n",
      "epoch: 7 | 108736 / 114272 | training loss: 0.0004143451515119523\n",
      "epoch: 7 | 108768 / 114272 | training loss: 0.0004458427429199219\n",
      "epoch: 7 | 108800 / 114272 | training loss: 0.0003467474889475852\n",
      "epoch: 7 | 108832 / 114272 | training loss: 0.0004302164597902447\n",
      "epoch: 7 | 108864 / 114272 | training loss: 0.0010324931936338544\n",
      "epoch: 7 | 108896 / 114272 | training loss: 0.00032765124342404306\n",
      "epoch: 7 | 108928 / 114272 | training loss: 0.00038163503631949425\n",
      "epoch: 7 | 108960 / 114272 | training loss: 0.00036292566801421344\n",
      "epoch: 7 | 108992 / 114272 | training loss: 0.0004011288983747363\n",
      "epoch: 7 | 109024 / 114272 | training loss: 0.0013806906063109636\n",
      "epoch: 7 | 109056 / 114272 | training loss: 0.0003304994315840304\n",
      "epoch: 7 | 109088 / 114272 | training loss: 0.0005079989205114543\n",
      "epoch: 7 | 109120 / 114272 | training loss: 0.000682929705362767\n",
      "epoch: 7 | 109152 / 114272 | training loss: 0.0003332900523673743\n",
      "epoch: 7 | 109184 / 114272 | training loss: 0.00043047370854765177\n",
      "epoch: 7 | 109216 / 114272 | training loss: 0.0039256298914551735\n",
      "epoch: 7 | 109248 / 114272 | training loss: 0.0006790749612264335\n",
      "epoch: 7 | 109280 / 114272 | training loss: 0.0004970223526470363\n",
      "epoch: 7 | 109312 / 114272 | training loss: 0.0002858027582988143\n",
      "epoch: 7 | 109344 / 114272 | training loss: 0.001941981608979404\n",
      "epoch: 7 | 109376 / 114272 | training loss: 0.000577433907892555\n",
      "epoch: 7 | 109408 / 114272 | training loss: 0.001584120444022119\n",
      "epoch: 7 | 109440 / 114272 | training loss: 0.00033321790397167206\n",
      "epoch: 7 | 109472 / 114272 | training loss: 0.00020797370234504342\n",
      "epoch: 7 | 109504 / 114272 | training loss: 0.001954180421307683\n",
      "epoch: 7 | 109536 / 114272 | training loss: 0.001577617833390832\n",
      "epoch: 7 | 109568 / 114272 | training loss: 0.00035653598024509847\n",
      "epoch: 7 | 109600 / 114272 | training loss: 0.0003112710255663842\n",
      "epoch: 7 | 109632 / 114272 | training loss: 0.00043905817437916994\n",
      "epoch: 7 | 109664 / 114272 | training loss: 0.04365454241633415\n",
      "epoch: 7 | 109696 / 114272 | training loss: 0.0005249535897746682\n",
      "epoch: 7 | 109728 / 114272 | training loss: 0.0004185121215414256\n",
      "epoch: 7 | 109760 / 114272 | training loss: 0.0002655366843100637\n",
      "epoch: 7 | 109792 / 114272 | training loss: 0.009607560932636261\n",
      "epoch: 7 | 109824 / 114272 | training loss: 0.00030217826133593917\n",
      "epoch: 7 | 109856 / 114272 | training loss: 0.00028736557578668\n",
      "epoch: 7 | 109888 / 114272 | training loss: 0.0002254864084534347\n",
      "epoch: 7 | 109920 / 114272 | training loss: 0.001519186538644135\n",
      "epoch: 7 | 109952 / 114272 | training loss: 0.00046301723341457546\n",
      "epoch: 7 | 109984 / 114272 | training loss: 0.00036609379458241165\n",
      "epoch: 7 | 110016 / 114272 | training loss: 0.0003775430959649384\n",
      "epoch: 7 | 110048 / 114272 | training loss: 0.0003145718656014651\n",
      "epoch: 7 | 110080 / 114272 | training loss: 0.0013236089143902063\n",
      "epoch: 7 | 110112 / 114272 | training loss: 0.0003171512798871845\n",
      "epoch: 7 | 110144 / 114272 | training loss: 0.00025354113313369453\n",
      "epoch: 7 | 110176 / 114272 | training loss: 0.00022645675926469266\n",
      "epoch: 7 | 110208 / 114272 | training loss: 0.0002649403177201748\n",
      "epoch: 7 | 110240 / 114272 | training loss: 0.003685184521600604\n",
      "epoch: 7 | 110272 / 114272 | training loss: 0.00035596912493929267\n",
      "epoch: 7 | 110304 / 114272 | training loss: 0.0002420309465378523\n",
      "epoch: 7 | 110336 / 114272 | training loss: 0.19138894975185394\n",
      "epoch: 7 | 110368 / 114272 | training loss: 0.00028742599533870816\n",
      "epoch: 7 | 110400 / 114272 | training loss: 0.0016881474293768406\n",
      "epoch: 7 | 110432 / 114272 | training loss: 0.0005326525424607098\n",
      "epoch: 7 | 110464 / 114272 | training loss: 0.0004003167850896716\n",
      "epoch: 7 | 110496 / 114272 | training loss: 0.22057411074638367\n",
      "epoch: 7 | 110528 / 114272 | training loss: 0.000266792019829154\n",
      "epoch: 7 | 110560 / 114272 | training loss: 0.01859503611922264\n",
      "epoch: 7 | 110592 / 114272 | training loss: 0.0008868651348166168\n",
      "epoch: 7 | 110624 / 114272 | training loss: 0.0003445102192927152\n",
      "epoch: 7 | 110656 / 114272 | training loss: 0.0022937278263270855\n",
      "epoch: 7 | 110688 / 114272 | training loss: 0.0005693254061043262\n",
      "epoch: 7 | 110720 / 114272 | training loss: 0.00025750804343260825\n",
      "epoch: 7 | 110752 / 114272 | training loss: 0.0005389082944020629\n",
      "epoch: 7 | 110784 / 114272 | training loss: 0.00505934190005064\n",
      "epoch: 7 | 110816 / 114272 | training loss: 0.00023447792045772076\n",
      "epoch: 7 | 110848 / 114272 | training loss: 0.0004427761014085263\n",
      "epoch: 7 | 110880 / 114272 | training loss: 0.0005787342670373619\n",
      "epoch: 7 | 110912 / 114272 | training loss: 0.0003826115280389786\n",
      "epoch: 7 | 110944 / 114272 | training loss: 0.000928403518628329\n",
      "epoch: 7 | 110976 / 114272 | training loss: 0.00033319275826215744\n",
      "epoch: 7 | 111008 / 114272 | training loss: 0.0004429012187756598\n",
      "epoch: 7 | 111040 / 114272 | training loss: 0.00027840686379931867\n",
      "epoch: 7 | 111072 / 114272 | training loss: 0.0003217743069399148\n",
      "epoch: 7 | 111104 / 114272 | training loss: 0.0004298196581657976\n",
      "epoch: 7 | 111136 / 114272 | training loss: 0.0003772175987251103\n",
      "epoch: 7 | 111168 / 114272 | training loss: 0.0004979328368790448\n",
      "epoch: 7 | 111200 / 114272 | training loss: 0.00024747324641793966\n",
      "epoch: 7 | 111232 / 114272 | training loss: 0.0003583819780033082\n",
      "epoch: 7 | 111264 / 114272 | training loss: 0.0009018033160828054\n",
      "epoch: 7 | 111296 / 114272 | training loss: 0.006561174523085356\n",
      "epoch: 7 | 111328 / 114272 | training loss: 0.18528951704502106\n",
      "epoch: 7 | 111360 / 114272 | training loss: 0.00024494618992321193\n",
      "epoch: 7 | 111392 / 114272 | training loss: 0.00039370101876556873\n",
      "epoch: 7 | 111424 / 114272 | training loss: 0.0003955235006287694\n",
      "epoch: 7 | 111456 / 114272 | training loss: 0.10917366296052933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | 111488 / 114272 | training loss: 0.000635006173979491\n",
      "epoch: 7 | 111520 / 114272 | training loss: 0.00041208791662938893\n",
      "epoch: 7 | 111552 / 114272 | training loss: 0.000430754735134542\n",
      "epoch: 7 | 111584 / 114272 | training loss: 0.0002778897760435939\n",
      "epoch: 7 | 111616 / 114272 | training loss: 0.19148685038089752\n",
      "epoch: 7 | 111648 / 114272 | training loss: 0.0002710841363295913\n",
      "epoch: 7 | 111680 / 114272 | training loss: 0.0008398770587518811\n",
      "epoch: 7 | 111712 / 114272 | training loss: 0.0005163839086890221\n",
      "epoch: 7 | 111744 / 114272 | training loss: 0.00037771082133986056\n",
      "epoch: 7 | 111776 / 114272 | training loss: 0.000775253982283175\n",
      "epoch: 7 | 111808 / 114272 | training loss: 0.00020856305491179228\n",
      "epoch: 7 | 111840 / 114272 | training loss: 0.00039574463153257966\n",
      "epoch: 7 | 111872 / 114272 | training loss: 0.00034744341974146664\n",
      "epoch: 7 | 111904 / 114272 | training loss: 0.0005610750522464514\n",
      "epoch: 7 | 111936 / 114272 | training loss: 0.00027977782883681357\n",
      "epoch: 7 | 111968 / 114272 | training loss: 0.0002791533770505339\n",
      "epoch: 7 | 112000 / 114272 | training loss: 0.0535782165825367\n",
      "epoch: 7 | 112032 / 114272 | training loss: 0.003092036582529545\n",
      "epoch: 7 | 112064 / 114272 | training loss: 0.00044442006037570536\n",
      "epoch: 7 | 112096 / 114272 | training loss: 0.00037868350045755506\n",
      "epoch: 7 | 112128 / 114272 | training loss: 0.000657089170999825\n",
      "epoch: 7 | 112160 / 114272 | training loss: 0.0008033823687583208\n",
      "epoch: 7 | 112192 / 114272 | training loss: 0.0008788208942860365\n",
      "epoch: 7 | 112224 / 114272 | training loss: 0.0033201780170202255\n",
      "epoch: 7 | 112256 / 114272 | training loss: 0.0006632645963691175\n",
      "epoch: 7 | 112288 / 114272 | training loss: 0.0007327713537961245\n",
      "epoch: 7 | 112320 / 114272 | training loss: 0.00031497262534685433\n",
      "epoch: 7 | 112352 / 114272 | training loss: 0.000511628168169409\n",
      "epoch: 7 | 112384 / 114272 | training loss: 0.002261111279949546\n",
      "epoch: 7 | 112416 / 114272 | training loss: 0.000266562303295359\n",
      "epoch: 7 | 112448 / 114272 | training loss: 0.00024479677085764706\n",
      "epoch: 7 | 112480 / 114272 | training loss: 0.00022643744887318462\n",
      "epoch: 7 | 112512 / 114272 | training loss: 0.0005262534250505269\n",
      "epoch: 7 | 112544 / 114272 | training loss: 0.31460753083229065\n",
      "epoch: 7 | 112576 / 114272 | training loss: 0.0006103671039454639\n",
      "epoch: 7 | 112608 / 114272 | training loss: 0.0016186952125281096\n",
      "epoch: 7 | 112640 / 114272 | training loss: 0.00034534209407866\n",
      "epoch: 7 | 112672 / 114272 | training loss: 0.10844394564628601\n",
      "epoch: 7 | 112704 / 114272 | training loss: 0.00038731281529180706\n",
      "epoch: 7 | 112736 / 114272 | training loss: 0.0004521723312791437\n",
      "epoch: 7 | 112768 / 114272 | training loss: 0.0005381835508160293\n",
      "epoch: 7 | 112800 / 114272 | training loss: 0.0015920089790597558\n",
      "epoch: 7 | 112832 / 114272 | training loss: 0.00047987469588406384\n",
      "epoch: 7 | 112864 / 114272 | training loss: 0.0005799197242595255\n",
      "epoch: 7 | 112896 / 114272 | training loss: 0.0006031716475263238\n",
      "epoch: 7 | 112928 / 114272 | training loss: 0.0003814679803326726\n",
      "epoch: 7 | 112960 / 114272 | training loss: 0.0003321851836517453\n",
      "epoch: 7 | 112992 / 114272 | training loss: 0.001117847627028823\n",
      "epoch: 7 | 113024 / 114272 | training loss: 0.002838611602783203\n",
      "epoch: 7 | 113056 / 114272 | training loss: 0.12956972420215607\n",
      "epoch: 7 | 113088 / 114272 | training loss: 0.09117372334003448\n",
      "epoch: 7 | 113120 / 114272 | training loss: 0.00046300102258101106\n",
      "epoch: 7 | 113152 / 114272 | training loss: 0.15391311049461365\n",
      "epoch: 7 | 113184 / 114272 | training loss: 0.00037379778223112226\n",
      "epoch: 7 | 113216 / 114272 | training loss: 0.0003353008069097996\n",
      "epoch: 7 | 113248 / 114272 | training loss: 0.0011906075524166226\n",
      "epoch: 7 | 113280 / 114272 | training loss: 0.00039336850750260055\n",
      "epoch: 7 | 113312 / 114272 | training loss: 0.00045485535520128906\n",
      "epoch: 7 | 113344 / 114272 | training loss: 0.0008434017072431743\n",
      "epoch: 7 | 113376 / 114272 | training loss: 0.0003303879057057202\n",
      "epoch: 7 | 113408 / 114272 | training loss: 0.0002923539141193032\n",
      "epoch: 7 | 113440 / 114272 | training loss: 0.00046313158236443996\n",
      "epoch: 7 | 113472 / 114272 | training loss: 0.00041391816921532154\n",
      "epoch: 7 | 113504 / 114272 | training loss: 0.00034440410672686994\n",
      "epoch: 7 | 113536 / 114272 | training loss: 0.00037541103665716946\n",
      "epoch: 7 | 113568 / 114272 | training loss: 0.0005232407129369676\n",
      "epoch: 7 | 113600 / 114272 | training loss: 0.0001777384168235585\n",
      "epoch: 7 | 113632 / 114272 | training loss: 0.0005446203867904842\n",
      "epoch: 7 | 113664 / 114272 | training loss: 0.0007506046677008271\n",
      "epoch: 7 | 113696 / 114272 | training loss: 0.0003586585517041385\n",
      "epoch: 7 | 113728 / 114272 | training loss: 0.0003264289698563516\n",
      "epoch: 7 | 113760 / 114272 | training loss: 0.0036058761179447174\n",
      "epoch: 7 | 113792 / 114272 | training loss: 0.002686671447008848\n",
      "epoch: 7 | 113824 / 114272 | training loss: 0.08423783630132675\n",
      "epoch: 7 | 113856 / 114272 | training loss: 0.001268579508177936\n",
      "epoch: 7 | 113888 / 114272 | training loss: 0.00037649981095455587\n",
      "epoch: 7 | 113920 / 114272 | training loss: 0.000376378302462399\n",
      "epoch: 7 | 113952 / 114272 | training loss: 0.00031015233253128827\n",
      "epoch: 7 | 113984 / 114272 | training loss: 0.00045498611871153116\n",
      "epoch: 7 | 114016 / 114272 | training loss: 0.0009556532604619861\n",
      "epoch: 7 | 114048 / 114272 | training loss: 0.000315738347126171\n",
      "epoch: 7 | 114080 / 114272 | training loss: 0.0006728682783432305\n",
      "epoch: 7 | 114112 / 114272 | training loss: 0.00036805393756367266\n",
      "epoch: 7 | 114144 / 114272 | training loss: 0.0007052213186398149\n",
      "epoch: 7 | 114176 / 114272 | training loss: 0.0003490088856779039\n",
      "epoch: 7 | 114208 / 114272 | training loss: 0.0005594397080130875\n",
      "epoch: 7 | 114240 / 114272 | training loss: 0.0002691329282242805\n",
      "Training epoch 7 done! Average loss: 0.01532122124479251. Accuracy: 0.9964033183982078\n",
      "Validation epoch 7 done! Average loss: 0.25155755282025233. Accurage: 0.9576342281879194\n",
      "Epoch 9 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 8 | 0 / 114272 | training loss: 0.030049460008740425\n",
      "epoch: 8 | 32 / 114272 | training loss: 0.0003273520269431174\n",
      "epoch: 8 | 64 / 114272 | training loss: 0.0003484839980956167\n",
      "epoch: 8 | 96 / 114272 | training loss: 0.0013030371628701687\n",
      "epoch: 8 | 128 / 114272 | training loss: 0.0004164435958955437\n",
      "epoch: 8 | 160 / 114272 | training loss: 0.00034304417204111814\n",
      "epoch: 8 | 192 / 114272 | training loss: 0.001497706980444491\n",
      "epoch: 8 | 224 / 114272 | training loss: 0.0004342622705735266\n",
      "epoch: 8 | 256 / 114272 | training loss: 0.0035089338198304176\n",
      "epoch: 8 | 288 / 114272 | training loss: 0.0019754914101213217\n",
      "epoch: 8 | 320 / 114272 | training loss: 0.00024813812342472374\n",
      "epoch: 8 | 352 / 114272 | training loss: 0.0002978954871650785\n",
      "epoch: 8 | 384 / 114272 | training loss: 0.0002977860567625612\n",
      "epoch: 8 | 416 / 114272 | training loss: 0.00024244214000646025\n",
      "epoch: 8 | 448 / 114272 | training loss: 0.2060476541519165\n",
      "epoch: 8 | 480 / 114272 | training loss: 0.0004127811698708683\n",
      "epoch: 8 | 512 / 114272 | training loss: 0.0002495146472938359\n",
      "epoch: 8 | 544 / 114272 | training loss: 0.00032090285094454885\n",
      "epoch: 8 | 576 / 114272 | training loss: 0.0021724682301282883\n",
      "epoch: 8 | 608 / 114272 | training loss: 0.00026391990832053125\n",
      "epoch: 8 | 640 / 114272 | training loss: 0.004139524418860674\n",
      "epoch: 8 | 672 / 114272 | training loss: 0.00027520066942088306\n",
      "epoch: 8 | 704 / 114272 | training loss: 0.00037236674688756466\n",
      "epoch: 8 | 736 / 114272 | training loss: 0.0003836415708065033\n",
      "epoch: 8 | 768 / 114272 | training loss: 0.0003260416560806334\n",
      "epoch: 8 | 800 / 114272 | training loss: 0.0007983854156918824\n",
      "epoch: 8 | 832 / 114272 | training loss: 0.00028210500022396445\n",
      "epoch: 8 | 864 / 114272 | training loss: 0.0002271578850923106\n",
      "epoch: 8 | 896 / 114272 | training loss: 0.0011031684698536992\n",
      "epoch: 8 | 928 / 114272 | training loss: 0.0005267276428639889\n",
      "epoch: 8 | 960 / 114272 | training loss: 0.0006143493228591979\n",
      "epoch: 8 | 992 / 114272 | training loss: 0.00030520878499373794\n",
      "epoch: 8 | 1024 / 114272 | training loss: 0.0004669266636483371\n",
      "epoch: 8 | 1056 / 114272 | training loss: 0.000421983830165118\n",
      "epoch: 8 | 1088 / 114272 | training loss: 0.0003039295261260122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 1120 / 114272 | training loss: 0.0007326725753955543\n",
      "epoch: 8 | 1152 / 114272 | training loss: 0.0036889498587697744\n",
      "epoch: 8 | 1184 / 114272 | training loss: 0.0003394758387003094\n",
      "epoch: 8 | 1216 / 114272 | training loss: 0.00031840376323089004\n",
      "epoch: 8 | 1248 / 114272 | training loss: 0.0005990041536279023\n",
      "epoch: 8 | 1280 / 114272 | training loss: 0.00036799703957512975\n",
      "epoch: 8 | 1312 / 114272 | training loss: 0.0003323107666801661\n",
      "epoch: 8 | 1344 / 114272 | training loss: 0.0002751651336438954\n",
      "epoch: 8 | 1376 / 114272 | training loss: 0.0026308230590075254\n",
      "epoch: 8 | 1408 / 114272 | training loss: 0.04899649694561958\n",
      "epoch: 8 | 1440 / 114272 | training loss: 0.00017995332018472254\n",
      "epoch: 8 | 1472 / 114272 | training loss: 0.00042750456486828625\n",
      "epoch: 8 | 1504 / 114272 | training loss: 0.0046035912819206715\n",
      "epoch: 8 | 1536 / 114272 | training loss: 0.04392337426543236\n",
      "epoch: 8 | 1568 / 114272 | training loss: 0.0002498907269909978\n",
      "epoch: 8 | 1600 / 114272 | training loss: 0.18486890196800232\n",
      "epoch: 8 | 1632 / 114272 | training loss: 0.0002262843045173213\n",
      "epoch: 8 | 1664 / 114272 | training loss: 0.0004981691017746925\n",
      "epoch: 8 | 1696 / 114272 | training loss: 0.0004826330114156008\n",
      "epoch: 8 | 1728 / 114272 | training loss: 0.06049026921391487\n",
      "epoch: 8 | 1760 / 114272 | training loss: 0.00019233234343118966\n",
      "epoch: 8 | 1792 / 114272 | training loss: 0.00042454374488443136\n",
      "epoch: 8 | 1824 / 114272 | training loss: 0.0004361746250651777\n",
      "epoch: 8 | 1856 / 114272 | training loss: 0.0006055373232811689\n",
      "epoch: 8 | 1888 / 114272 | training loss: 0.0004145465500187129\n",
      "epoch: 8 | 1920 / 114272 | training loss: 0.0004261706199031323\n",
      "epoch: 8 | 1952 / 114272 | training loss: 0.0005519621190614998\n",
      "epoch: 8 | 1984 / 114272 | training loss: 0.00043037100112996995\n",
      "epoch: 8 | 2016 / 114272 | training loss: 0.00035067679709754884\n",
      "epoch: 8 | 2048 / 114272 | training loss: 0.00033472187351435423\n",
      "epoch: 8 | 2080 / 114272 | training loss: 0.0030185107607394457\n",
      "epoch: 8 | 2112 / 114272 | training loss: 0.04312952980399132\n",
      "epoch: 8 | 2144 / 114272 | training loss: 0.00029892087331973016\n",
      "epoch: 8 | 2176 / 114272 | training loss: 0.0003448096686042845\n",
      "epoch: 8 | 2208 / 114272 | training loss: 0.0009572871495038271\n",
      "epoch: 8 | 2240 / 114272 | training loss: 0.00040866032941266894\n",
      "epoch: 8 | 2272 / 114272 | training loss: 0.0938110426068306\n",
      "epoch: 8 | 2304 / 114272 | training loss: 0.00038529865560121834\n",
      "epoch: 8 | 2336 / 114272 | training loss: 0.00037882616743445396\n",
      "epoch: 8 | 2368 / 114272 | training loss: 0.0003494829870760441\n",
      "epoch: 8 | 2400 / 114272 | training loss: 0.0005959849222563207\n",
      "epoch: 8 | 2432 / 114272 | training loss: 0.0003058461006730795\n",
      "epoch: 8 | 2464 / 114272 | training loss: 0.0006523487973026931\n",
      "epoch: 8 | 2496 / 114272 | training loss: 0.0005703958449885249\n",
      "epoch: 8 | 2528 / 114272 | training loss: 0.000356403790647164\n",
      "epoch: 8 | 2560 / 114272 | training loss: 0.07224515825510025\n",
      "epoch: 8 | 2592 / 114272 | training loss: 0.00044506750418804586\n",
      "epoch: 8 | 2624 / 114272 | training loss: 0.00033755446202121675\n",
      "epoch: 8 | 2656 / 114272 | training loss: 0.0014081327244639397\n",
      "epoch: 8 | 2688 / 114272 | training loss: 0.00032082738471217453\n",
      "epoch: 8 | 2720 / 114272 | training loss: 0.012225151993334293\n",
      "epoch: 8 | 2752 / 114272 | training loss: 0.00022776368132326752\n",
      "epoch: 8 | 2784 / 114272 | training loss: 0.00039324015961028636\n",
      "epoch: 8 | 2816 / 114272 | training loss: 0.00031086851959116757\n",
      "epoch: 8 | 2848 / 114272 | training loss: 0.0004256802494637668\n",
      "epoch: 8 | 2880 / 114272 | training loss: 0.0003433942038100213\n",
      "epoch: 8 | 2912 / 114272 | training loss: 0.00023333419812843204\n",
      "epoch: 8 | 2944 / 114272 | training loss: 0.0005137230036780238\n",
      "epoch: 8 | 2976 / 114272 | training loss: 0.00018376177467871457\n",
      "epoch: 8 | 3008 / 114272 | training loss: 0.00030843380955047905\n",
      "epoch: 8 | 3040 / 114272 | training loss: 0.0005860928795300424\n",
      "epoch: 8 | 3072 / 114272 | training loss: 0.0003956845321226865\n",
      "epoch: 8 | 3104 / 114272 | training loss: 0.00041928407154046\n",
      "epoch: 8 | 3136 / 114272 | training loss: 0.0002613126707728952\n",
      "epoch: 8 | 3168 / 114272 | training loss: 0.0011796358739957213\n",
      "epoch: 8 | 3200 / 114272 | training loss: 0.000261786364717409\n",
      "epoch: 8 | 3232 / 114272 | training loss: 0.0004151758912485093\n",
      "epoch: 8 | 3264 / 114272 | training loss: 0.0003444839676376432\n",
      "epoch: 8 | 3296 / 114272 | training loss: 0.0009756310610100627\n",
      "epoch: 8 | 3328 / 114272 | training loss: 0.0002986402250826359\n",
      "epoch: 8 | 3360 / 114272 | training loss: 0.00018094955885317177\n",
      "epoch: 8 | 3392 / 114272 | training loss: 0.0005903203273192048\n",
      "epoch: 8 | 3424 / 114272 | training loss: 0.00029311355319805443\n",
      "epoch: 8 | 3456 / 114272 | training loss: 0.0008919916581362486\n",
      "epoch: 8 | 3488 / 114272 | training loss: 0.0005874191992916167\n",
      "epoch: 8 | 3520 / 114272 | training loss: 0.0002994966343976557\n",
      "epoch: 8 | 3552 / 114272 | training loss: 0.0017245184862986207\n",
      "epoch: 8 | 3584 / 114272 | training loss: 0.0004981740494258702\n",
      "epoch: 8 | 3616 / 114272 | training loss: 0.0003844747843686491\n",
      "epoch: 8 | 3648 / 114272 | training loss: 0.000644120795186609\n",
      "epoch: 8 | 3680 / 114272 | training loss: 0.0004974245093762875\n",
      "epoch: 8 | 3712 / 114272 | training loss: 0.0013527320697903633\n",
      "epoch: 8 | 3744 / 114272 | training loss: 0.0002611449162941426\n",
      "epoch: 8 | 3776 / 114272 | training loss: 0.00031960196793079376\n",
      "epoch: 8 | 3808 / 114272 | training loss: 0.0002758159243967384\n",
      "epoch: 8 | 3840 / 114272 | training loss: 0.0003221717197448015\n",
      "epoch: 8 | 3872 / 114272 | training loss: 0.00043776980601251125\n",
      "epoch: 8 | 3904 / 114272 | training loss: 0.000494997133500874\n",
      "epoch: 8 | 3936 / 114272 | training loss: 0.0002637086436152458\n",
      "epoch: 8 | 3968 / 114272 | training loss: 0.0005530276685021818\n",
      "epoch: 8 | 4000 / 114272 | training loss: 0.00046304473653435707\n",
      "epoch: 8 | 4032 / 114272 | training loss: 0.0002991842047777027\n",
      "epoch: 8 | 4064 / 114272 | training loss: 0.00016882801719475538\n",
      "epoch: 8 | 4096 / 114272 | training loss: 0.0003165740054100752\n",
      "epoch: 8 | 4128 / 114272 | training loss: 0.0004200676630716771\n",
      "epoch: 8 | 4160 / 114272 | training loss: 0.0006008801865391433\n",
      "epoch: 8 | 4192 / 114272 | training loss: 0.000348537287209183\n",
      "epoch: 8 | 4224 / 114272 | training loss: 0.00048605038318783045\n",
      "epoch: 8 | 4256 / 114272 | training loss: 0.00029122288106009364\n",
      "epoch: 8 | 4288 / 114272 | training loss: 0.00023042337852530181\n",
      "epoch: 8 | 4320 / 114272 | training loss: 0.00024878294789232314\n",
      "epoch: 8 | 4352 / 114272 | training loss: 0.0001950056612258777\n",
      "epoch: 8 | 4384 / 114272 | training loss: 0.20764300227165222\n",
      "epoch: 8 | 4416 / 114272 | training loss: 0.0009848583722487092\n",
      "epoch: 8 | 4448 / 114272 | training loss: 0.0002932533388957381\n",
      "epoch: 8 | 4480 / 114272 | training loss: 0.0002610826340969652\n",
      "epoch: 8 | 4512 / 114272 | training loss: 0.0004372942785266787\n",
      "epoch: 8 | 4544 / 114272 | training loss: 0.0002267449744977057\n",
      "epoch: 8 | 4576 / 114272 | training loss: 0.0002427069703117013\n",
      "epoch: 8 | 4608 / 114272 | training loss: 0.00025848456425592303\n",
      "epoch: 8 | 4640 / 114272 | training loss: 0.00026021673693321645\n",
      "epoch: 8 | 4672 / 114272 | training loss: 0.00032493044272996485\n",
      "epoch: 8 | 4704 / 114272 | training loss: 0.0007609524764120579\n",
      "epoch: 8 | 4736 / 114272 | training loss: 0.000512727361638099\n",
      "epoch: 8 | 4768 / 114272 | training loss: 0.00032352653215639293\n",
      "epoch: 8 | 4800 / 114272 | training loss: 0.0002727771061472595\n",
      "epoch: 8 | 4832 / 114272 | training loss: 0.21835623681545258\n",
      "epoch: 8 | 4864 / 114272 | training loss: 0.0003605955862440169\n",
      "epoch: 8 | 4896 / 114272 | training loss: 0.0003407907788641751\n",
      "epoch: 8 | 4928 / 114272 | training loss: 0.0001635616208659485\n",
      "epoch: 8 | 4960 / 114272 | training loss: 0.0004133438051212579\n",
      "epoch: 8 | 4992 / 114272 | training loss: 0.0005567133193835616\n",
      "epoch: 8 | 5024 / 114272 | training loss: 0.0003656288899946958\n",
      "epoch: 8 | 5056 / 114272 | training loss: 0.20119282603263855\n",
      "epoch: 8 | 5088 / 114272 | training loss: 0.0005828606663271785\n",
      "epoch: 8 | 5120 / 114272 | training loss: 0.00024917861446738243\n",
      "epoch: 8 | 5152 / 114272 | training loss: 0.0002340568316867575\n",
      "epoch: 8 | 5184 / 114272 | training loss: 0.07703464478254318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 5216 / 114272 | training loss: 0.0004087959532625973\n",
      "epoch: 8 | 5248 / 114272 | training loss: 0.0005740791675634682\n",
      "epoch: 8 | 5280 / 114272 | training loss: 0.00013914777082391083\n",
      "epoch: 8 | 5312 / 114272 | training loss: 0.0002770794671960175\n",
      "epoch: 8 | 5344 / 114272 | training loss: 0.0003025417390745133\n",
      "epoch: 8 | 5376 / 114272 | training loss: 0.0005600462900474668\n",
      "epoch: 8 | 5408 / 114272 | training loss: 0.0004817840817850083\n",
      "epoch: 8 | 5440 / 114272 | training loss: 0.0003077971050515771\n",
      "epoch: 8 | 5472 / 114272 | training loss: 0.00043100951006636024\n",
      "epoch: 8 | 5504 / 114272 | training loss: 0.00035385077353566885\n",
      "epoch: 8 | 5536 / 114272 | training loss: 0.00029468655702658\n",
      "epoch: 8 | 5568 / 114272 | training loss: 0.0009867671178653836\n",
      "epoch: 8 | 5600 / 114272 | training loss: 0.000323722226312384\n",
      "epoch: 8 | 5632 / 114272 | training loss: 0.0004617365193553269\n",
      "epoch: 8 | 5664 / 114272 | training loss: 0.000635732663795352\n",
      "epoch: 8 | 5696 / 114272 | training loss: 0.00033178244484588504\n",
      "epoch: 8 | 5728 / 114272 | training loss: 0.00029327644733712077\n",
      "epoch: 8 | 5760 / 114272 | training loss: 0.0003917710273526609\n",
      "epoch: 8 | 5792 / 114272 | training loss: 0.0002786032564472407\n",
      "epoch: 8 | 5824 / 114272 | training loss: 0.0010144658153876662\n",
      "epoch: 8 | 5856 / 114272 | training loss: 0.0005358551279641688\n",
      "epoch: 8 | 5888 / 114272 | training loss: 0.00037755395169369876\n",
      "epoch: 8 | 5920 / 114272 | training loss: 0.0003435049147810787\n",
      "epoch: 8 | 5952 / 114272 | training loss: 0.0003111003607045859\n",
      "epoch: 8 | 5984 / 114272 | training loss: 0.15703070163726807\n",
      "epoch: 8 | 6016 / 114272 | training loss: 0.00030060310382395983\n",
      "epoch: 8 | 6048 / 114272 | training loss: 0.0012234067544341087\n",
      "epoch: 8 | 6080 / 114272 | training loss: 0.02143869362771511\n",
      "epoch: 8 | 6112 / 114272 | training loss: 0.0003620856150519103\n",
      "epoch: 8 | 6144 / 114272 | training loss: 0.0023379395715892315\n",
      "epoch: 8 | 6176 / 114272 | training loss: 0.00036096249823458493\n",
      "epoch: 8 | 6208 / 114272 | training loss: 0.0006541424081660807\n",
      "epoch: 8 | 6240 / 114272 | training loss: 0.00038492170278914273\n",
      "epoch: 8 | 6272 / 114272 | training loss: 0.0005668832454830408\n",
      "epoch: 8 | 6304 / 114272 | training loss: 0.0006255828775465488\n",
      "epoch: 8 | 6336 / 114272 | training loss: 0.09910345822572708\n",
      "epoch: 8 | 6368 / 114272 | training loss: 0.0011773556470870972\n",
      "epoch: 8 | 6400 / 114272 | training loss: 0.0007440831395797431\n",
      "epoch: 8 | 6432 / 114272 | training loss: 0.0004411879926919937\n",
      "epoch: 8 | 6464 / 114272 | training loss: 0.0006807619938626885\n",
      "epoch: 8 | 6496 / 114272 | training loss: 0.06848157942295074\n",
      "epoch: 8 | 6528 / 114272 | training loss: 0.00025301973801106215\n",
      "epoch: 8 | 6560 / 114272 | training loss: 0.00038510444574058056\n",
      "epoch: 8 | 6592 / 114272 | training loss: 0.0006165876402519643\n",
      "epoch: 8 | 6624 / 114272 | training loss: 0.0003992668935097754\n",
      "epoch: 8 | 6656 / 114272 | training loss: 0.0003934665000997484\n",
      "epoch: 8 | 6688 / 114272 | training loss: 0.000560958287678659\n",
      "epoch: 8 | 6720 / 114272 | training loss: 0.00023678212892264128\n",
      "epoch: 8 | 6752 / 114272 | training loss: 0.00981107633560896\n",
      "epoch: 8 | 6784 / 114272 | training loss: 0.0002669624809641391\n",
      "epoch: 8 | 6816 / 114272 | training loss: 0.00038630739436484873\n",
      "epoch: 8 | 6848 / 114272 | training loss: 0.0008741733036004007\n",
      "epoch: 8 | 6880 / 114272 | training loss: 0.0004112537717446685\n",
      "epoch: 8 | 6912 / 114272 | training loss: 0.12525445222854614\n",
      "epoch: 8 | 6944 / 114272 | training loss: 0.00042506898171268404\n",
      "epoch: 8 | 6976 / 114272 | training loss: 0.0002975293027702719\n",
      "epoch: 8 | 7008 / 114272 | training loss: 0.0003537754528224468\n",
      "epoch: 8 | 7040 / 114272 | training loss: 0.0004931844887323678\n",
      "epoch: 8 | 7072 / 114272 | training loss: 0.0006472968379966915\n",
      "epoch: 8 | 7104 / 114272 | training loss: 0.0005968224140815437\n",
      "epoch: 8 | 7136 / 114272 | training loss: 0.0003614006272982806\n",
      "epoch: 8 | 7168 / 114272 | training loss: 0.0003567406674847007\n",
      "epoch: 8 | 7200 / 114272 | training loss: 0.0005621506134048104\n",
      "epoch: 8 | 7232 / 114272 | training loss: 0.00026514852652326226\n",
      "epoch: 8 | 7264 / 114272 | training loss: 0.00013141351519152522\n",
      "epoch: 8 | 7296 / 114272 | training loss: 0.00030263952794484794\n",
      "epoch: 8 | 7328 / 114272 | training loss: 0.14658218622207642\n",
      "epoch: 8 | 7360 / 114272 | training loss: 0.0002794020692817867\n",
      "epoch: 8 | 7392 / 114272 | training loss: 0.0002963572333101183\n",
      "epoch: 8 | 7424 / 114272 | training loss: 0.00035131076583638787\n",
      "epoch: 8 | 7456 / 114272 | training loss: 0.00032786940573714674\n",
      "epoch: 8 | 7488 / 114272 | training loss: 0.00032609314075671136\n",
      "epoch: 8 | 7520 / 114272 | training loss: 0.061199720948934555\n",
      "epoch: 8 | 7552 / 114272 | training loss: 0.00035014230525121093\n",
      "epoch: 8 | 7584 / 114272 | training loss: 0.0007730840006843209\n",
      "epoch: 8 | 7616 / 114272 | training loss: 0.008304211311042309\n",
      "epoch: 8 | 7648 / 114272 | training loss: 0.0007477877079509199\n",
      "epoch: 8 | 7680 / 114272 | training loss: 0.00032444842508994043\n",
      "epoch: 8 | 7712 / 114272 | training loss: 0.0004404540522955358\n",
      "epoch: 8 | 7744 / 114272 | training loss: 0.0006872341618873179\n",
      "epoch: 8 | 7776 / 114272 | training loss: 0.0003855557879433036\n",
      "epoch: 8 | 7808 / 114272 | training loss: 0.00046171032590791583\n",
      "epoch: 8 | 7840 / 114272 | training loss: 0.0009651041473262012\n",
      "epoch: 8 | 7872 / 114272 | training loss: 0.0005731137352995574\n",
      "epoch: 8 | 7904 / 114272 | training loss: 0.0010881581110879779\n",
      "epoch: 8 | 7936 / 114272 | training loss: 0.0003992802812717855\n",
      "epoch: 8 | 7968 / 114272 | training loss: 0.00040079804603010416\n",
      "epoch: 8 | 8000 / 114272 | training loss: 0.0004510146100074053\n",
      "epoch: 8 | 8032 / 114272 | training loss: 0.0006895091501064599\n",
      "epoch: 8 | 8064 / 114272 | training loss: 0.01110098697245121\n",
      "epoch: 8 | 8096 / 114272 | training loss: 0.00029089857707731426\n",
      "epoch: 8 | 8128 / 114272 | training loss: 0.00045015031355433166\n",
      "epoch: 8 | 8160 / 114272 | training loss: 0.0005704769282601774\n",
      "epoch: 8 | 8192 / 114272 | training loss: 0.0003139532345812768\n",
      "epoch: 8 | 8224 / 114272 | training loss: 0.00024259848578367382\n",
      "epoch: 8 | 8256 / 114272 | training loss: 0.0004944396205246449\n",
      "epoch: 8 | 8288 / 114272 | training loss: 0.000765323406085372\n",
      "epoch: 8 | 8320 / 114272 | training loss: 0.0007882959325797856\n",
      "epoch: 8 | 8352 / 114272 | training loss: 0.0005048456368967891\n",
      "epoch: 8 | 8384 / 114272 | training loss: 0.0006318940431810915\n",
      "epoch: 8 | 8416 / 114272 | training loss: 0.0002731823187787086\n",
      "epoch: 8 | 8448 / 114272 | training loss: 0.000297640566714108\n",
      "epoch: 8 | 8480 / 114272 | training loss: 0.0003660578222479671\n",
      "epoch: 8 | 8512 / 114272 | training loss: 0.00032157820533029735\n",
      "epoch: 8 | 8544 / 114272 | training loss: 0.0008367801201529801\n",
      "epoch: 8 | 8576 / 114272 | training loss: 0.19900086522102356\n",
      "epoch: 8 | 8608 / 114272 | training loss: 0.05102836713194847\n",
      "epoch: 8 | 8640 / 114272 | training loss: 0.0002792629529722035\n",
      "epoch: 8 | 8672 / 114272 | training loss: 0.0002759559138212353\n",
      "epoch: 8 | 8704 / 114272 | training loss: 0.0003473194083198905\n",
      "epoch: 8 | 8736 / 114272 | training loss: 0.0002833303587976843\n",
      "epoch: 8 | 8768 / 114272 | training loss: 0.0005800813669338822\n",
      "epoch: 8 | 8800 / 114272 | training loss: 0.00025023205671459436\n",
      "epoch: 8 | 8832 / 114272 | training loss: 0.0004457362520042807\n",
      "epoch: 8 | 8864 / 114272 | training loss: 0.0003614726010710001\n",
      "epoch: 8 | 8896 / 114272 | training loss: 0.0003988505632150918\n",
      "epoch: 8 | 8928 / 114272 | training loss: 0.002694919938221574\n",
      "epoch: 8 | 8960 / 114272 | training loss: 0.0003183110966347158\n",
      "epoch: 8 | 8992 / 114272 | training loss: 0.0006385229644365609\n",
      "epoch: 8 | 9024 / 114272 | training loss: 0.0004761620657518506\n",
      "epoch: 8 | 9056 / 114272 | training loss: 0.0030430639162659645\n",
      "epoch: 8 | 9088 / 114272 | training loss: 0.003963551949709654\n",
      "epoch: 8 | 9120 / 114272 | training loss: 0.01195000670850277\n",
      "epoch: 8 | 9152 / 114272 | training loss: 0.0004321433661971241\n",
      "epoch: 8 | 9184 / 114272 | training loss: 0.15124550461769104\n",
      "epoch: 8 | 9216 / 114272 | training loss: 0.0002966807223856449\n",
      "epoch: 8 | 9248 / 114272 | training loss: 0.005358873400837183\n",
      "epoch: 8 | 9280 / 114272 | training loss: 0.000472771906061098\n",
      "epoch: 8 | 9312 / 114272 | training loss: 0.00046893145190551877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 9344 / 114272 | training loss: 0.0003241261001676321\n",
      "epoch: 8 | 9376 / 114272 | training loss: 0.0003218210767954588\n",
      "epoch: 8 | 9408 / 114272 | training loss: 0.00021579916938208044\n",
      "epoch: 8 | 9440 / 114272 | training loss: 0.000374661001842469\n",
      "epoch: 8 | 9472 / 114272 | training loss: 0.13433299958705902\n",
      "epoch: 8 | 9504 / 114272 | training loss: 0.0006448224885389209\n",
      "epoch: 8 | 9536 / 114272 | training loss: 0.0007694457308389246\n",
      "epoch: 8 | 9568 / 114272 | training loss: 0.00015943942707963288\n",
      "epoch: 8 | 9600 / 114272 | training loss: 0.00038821398629806936\n",
      "epoch: 8 | 9632 / 114272 | training loss: 0.0005205979105085135\n",
      "epoch: 8 | 9664 / 114272 | training loss: 0.0002710623375605792\n",
      "epoch: 8 | 9696 / 114272 | training loss: 0.0003843343583866954\n",
      "epoch: 8 | 9728 / 114272 | training loss: 0.001454434241168201\n",
      "epoch: 8 | 9760 / 114272 | training loss: 0.000306729634758085\n",
      "epoch: 8 | 9792 / 114272 | training loss: 0.0005327700055204332\n",
      "epoch: 8 | 9824 / 114272 | training loss: 0.00032733374973759055\n",
      "epoch: 8 | 9856 / 114272 | training loss: 0.00047531211748719215\n",
      "epoch: 8 | 9888 / 114272 | training loss: 0.0003627496771514416\n",
      "epoch: 8 | 9920 / 114272 | training loss: 0.0003624015662353486\n",
      "epoch: 8 | 9952 / 114272 | training loss: 0.0007144955452531576\n",
      "epoch: 8 | 9984 / 114272 | training loss: 0.00026199957937933505\n",
      "epoch: 8 | 10016 / 114272 | training loss: 0.0027062317822128534\n",
      "epoch: 8 | 10048 / 114272 | training loss: 0.00025399462901987135\n",
      "epoch: 8 | 10080 / 114272 | training loss: 0.00037006111233495176\n",
      "epoch: 8 | 10112 / 114272 | training loss: 0.000217436405364424\n",
      "epoch: 8 | 10144 / 114272 | training loss: 0.0007416392909362912\n",
      "epoch: 8 | 10176 / 114272 | training loss: 0.00039059072150848806\n",
      "epoch: 8 | 10208 / 114272 | training loss: 0.00041073732427321374\n",
      "epoch: 8 | 10240 / 114272 | training loss: 0.00046129210386425257\n",
      "epoch: 8 | 10272 / 114272 | training loss: 0.0006202741642482579\n",
      "epoch: 8 | 10304 / 114272 | training loss: 0.0004796008870471269\n",
      "epoch: 8 | 10336 / 114272 | training loss: 0.0003136316081508994\n",
      "epoch: 8 | 10368 / 114272 | training loss: 0.039565712213516235\n",
      "epoch: 8 | 10400 / 114272 | training loss: 0.00024907977785915136\n",
      "epoch: 8 | 10432 / 114272 | training loss: 0.09477882832288742\n",
      "epoch: 8 | 10464 / 114272 | training loss: 0.0002499164256732911\n",
      "epoch: 8 | 10496 / 114272 | training loss: 0.0005686372751370072\n",
      "epoch: 8 | 10528 / 114272 | training loss: 0.00039641186594963074\n",
      "epoch: 8 | 10560 / 114272 | training loss: 0.00021625214139930904\n",
      "epoch: 8 | 10592 / 114272 | training loss: 0.00020462919201236218\n",
      "epoch: 8 | 10624 / 114272 | training loss: 0.00031186209525913\n",
      "epoch: 8 | 10656 / 114272 | training loss: 0.00032865768298506737\n",
      "epoch: 8 | 10688 / 114272 | training loss: 0.000780428119469434\n",
      "epoch: 8 | 10720 / 114272 | training loss: 0.00033782669925130904\n",
      "epoch: 8 | 10752 / 114272 | training loss: 0.0003924565389752388\n",
      "epoch: 8 | 10784 / 114272 | training loss: 0.00028683390701189637\n",
      "epoch: 8 | 10816 / 114272 | training loss: 0.00018172725685872138\n",
      "epoch: 8 | 10848 / 114272 | training loss: 0.00033708964474499226\n",
      "epoch: 8 | 10880 / 114272 | training loss: 0.0003038537106476724\n",
      "epoch: 8 | 10912 / 114272 | training loss: 0.0014772359281778336\n",
      "epoch: 8 | 10944 / 114272 | training loss: 0.00024638159084133804\n",
      "epoch: 8 | 10976 / 114272 | training loss: 0.00033810228342190385\n",
      "epoch: 8 | 11008 / 114272 | training loss: 0.00036493557854555547\n",
      "epoch: 8 | 11040 / 114272 | training loss: 0.0003001866862177849\n",
      "epoch: 8 | 11072 / 114272 | training loss: 0.000387052190490067\n",
      "epoch: 8 | 11104 / 114272 | training loss: 0.0003412133955862373\n",
      "epoch: 8 | 11136 / 114272 | training loss: 0.20464619994163513\n",
      "epoch: 8 | 11168 / 114272 | training loss: 0.0023185566533356905\n",
      "epoch: 8 | 11200 / 114272 | training loss: 0.0004589612944982946\n",
      "epoch: 8 | 11232 / 114272 | training loss: 0.001578209106810391\n",
      "epoch: 8 | 11264 / 114272 | training loss: 0.000557507446501404\n",
      "epoch: 8 | 11296 / 114272 | training loss: 0.00030908227199688554\n",
      "epoch: 8 | 11328 / 114272 | training loss: 0.00032701317104510963\n",
      "epoch: 8 | 11360 / 114272 | training loss: 0.005529912654310465\n",
      "epoch: 8 | 11392 / 114272 | training loss: 0.0010507608531042933\n",
      "epoch: 8 | 11424 / 114272 | training loss: 0.0002373043680563569\n",
      "epoch: 8 | 11456 / 114272 | training loss: 0.0003651083097793162\n",
      "epoch: 8 | 11488 / 114272 | training loss: 0.0003141351626254618\n",
      "epoch: 8 | 11520 / 114272 | training loss: 0.0003777506644837558\n",
      "epoch: 8 | 11552 / 114272 | training loss: 0.0006368340691551566\n",
      "epoch: 8 | 11584 / 114272 | training loss: 0.000647682580165565\n",
      "epoch: 8 | 11616 / 114272 | training loss: 0.01210669707506895\n",
      "epoch: 8 | 11648 / 114272 | training loss: 0.0004898605984635651\n",
      "epoch: 8 | 11680 / 114272 | training loss: 0.0001820868201320991\n",
      "epoch: 8 | 11712 / 114272 | training loss: 0.000272085570031777\n",
      "epoch: 8 | 11744 / 114272 | training loss: 0.0006116777658462524\n",
      "epoch: 8 | 11776 / 114272 | training loss: 0.0003136226732749492\n",
      "epoch: 8 | 11808 / 114272 | training loss: 0.00035077272332273424\n",
      "epoch: 8 | 11840 / 114272 | training loss: 0.00032427182304672897\n",
      "epoch: 8 | 11872 / 114272 | training loss: 0.0007350133964791894\n",
      "epoch: 8 | 11904 / 114272 | training loss: 0.00040702533442527056\n",
      "epoch: 8 | 11936 / 114272 | training loss: 0.00036009555333293974\n",
      "epoch: 8 | 11968 / 114272 | training loss: 0.0005011916509829462\n",
      "epoch: 8 | 12000 / 114272 | training loss: 0.0004119826480746269\n",
      "epoch: 8 | 12032 / 114272 | training loss: 0.00012400231207720935\n",
      "epoch: 8 | 12064 / 114272 | training loss: 0.0010830459650605917\n",
      "epoch: 8 | 12096 / 114272 | training loss: 0.0007633008062839508\n",
      "epoch: 8 | 12128 / 114272 | training loss: 0.004110039211809635\n",
      "epoch: 8 | 12160 / 114272 | training loss: 0.0003426065668463707\n",
      "epoch: 8 | 12192 / 114272 | training loss: 0.00023222397430799901\n",
      "epoch: 8 | 12224 / 114272 | training loss: 0.00033721383078955114\n",
      "epoch: 8 | 12256 / 114272 | training loss: 0.26489371061325073\n",
      "epoch: 8 | 12288 / 114272 | training loss: 0.0006000364664942026\n",
      "epoch: 8 | 12320 / 114272 | training loss: 0.0002754467132035643\n",
      "epoch: 8 | 12352 / 114272 | training loss: 0.0003482690663076937\n",
      "epoch: 8 | 12384 / 114272 | training loss: 0.0003039069124497473\n",
      "epoch: 8 | 12416 / 114272 | training loss: 0.00035354014835320413\n",
      "epoch: 8 | 12448 / 114272 | training loss: 0.0010750774526968598\n",
      "epoch: 8 | 12480 / 114272 | training loss: 0.0003410000936128199\n",
      "epoch: 8 | 12512 / 114272 | training loss: 0.0004655783122871071\n",
      "epoch: 8 | 12544 / 114272 | training loss: 0.0004745642945636064\n",
      "epoch: 8 | 12576 / 114272 | training loss: 0.0006009376957081258\n",
      "epoch: 8 | 12608 / 114272 | training loss: 0.00033299095230177045\n",
      "epoch: 8 | 12640 / 114272 | training loss: 0.23140157759189606\n",
      "epoch: 8 | 12672 / 114272 | training loss: 0.0004392805276438594\n",
      "epoch: 8 | 12704 / 114272 | training loss: 0.0003645863034762442\n",
      "epoch: 8 | 12736 / 114272 | training loss: 0.00018223801453132182\n",
      "epoch: 8 | 12768 / 114272 | training loss: 0.00026866485131904483\n",
      "epoch: 8 | 12800 / 114272 | training loss: 0.00026269431691616774\n",
      "epoch: 8 | 12832 / 114272 | training loss: 0.0003198017366230488\n",
      "epoch: 8 | 12864 / 114272 | training loss: 0.0004686327592935413\n",
      "epoch: 8 | 12896 / 114272 | training loss: 0.0004094637115485966\n",
      "epoch: 8 | 12928 / 114272 | training loss: 0.0008879626402631402\n",
      "epoch: 8 | 12960 / 114272 | training loss: 0.0003793938085436821\n",
      "epoch: 8 | 12992 / 114272 | training loss: 0.21692632138729095\n",
      "epoch: 8 | 13024 / 114272 | training loss: 0.0002914135984610766\n",
      "epoch: 8 | 13056 / 114272 | training loss: 0.000708211911842227\n",
      "epoch: 8 | 13088 / 114272 | training loss: 0.0014171559596434236\n",
      "epoch: 8 | 13120 / 114272 | training loss: 0.00040391526999883354\n",
      "epoch: 8 | 13152 / 114272 | training loss: 0.0011632783571258187\n",
      "epoch: 8 | 13184 / 114272 | training loss: 0.0001680205314187333\n",
      "epoch: 8 | 13216 / 114272 | training loss: 0.0016530235297977924\n",
      "epoch: 8 | 13248 / 114272 | training loss: 0.0003553466231096536\n",
      "epoch: 8 | 13280 / 114272 | training loss: 0.0003109713434241712\n",
      "epoch: 8 | 13312 / 114272 | training loss: 0.0004641388077288866\n",
      "epoch: 8 | 13344 / 114272 | training loss: 0.0016210840549319983\n",
      "epoch: 8 | 13376 / 114272 | training loss: 0.0003686943673528731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 13408 / 114272 | training loss: 0.0006184237427078187\n",
      "epoch: 8 | 13440 / 114272 | training loss: 0.00019823807815555483\n",
      "epoch: 8 | 13472 / 114272 | training loss: 0.0018087225034832954\n",
      "epoch: 8 | 13504 / 114272 | training loss: 0.0003530681715346873\n",
      "epoch: 8 | 13536 / 114272 | training loss: 0.00019453346612863243\n",
      "epoch: 8 | 13568 / 114272 | training loss: 0.0003171589632984251\n",
      "epoch: 8 | 13600 / 114272 | training loss: 0.0002505835145711899\n",
      "epoch: 8 | 13632 / 114272 | training loss: 0.0004326911293901503\n",
      "epoch: 8 | 13664 / 114272 | training loss: 0.00043575986637733877\n",
      "epoch: 8 | 13696 / 114272 | training loss: 0.0004662247665692121\n",
      "epoch: 8 | 13728 / 114272 | training loss: 0.0006506238714791834\n",
      "epoch: 8 | 13760 / 114272 | training loss: 0.02740361914038658\n",
      "epoch: 8 | 13792 / 114272 | training loss: 0.00044422110659070313\n",
      "epoch: 8 | 13824 / 114272 | training loss: 0.0005204442422837019\n",
      "epoch: 8 | 13856 / 114272 | training loss: 0.0008686247747391462\n",
      "epoch: 8 | 13888 / 114272 | training loss: 0.0002538276312407106\n",
      "epoch: 8 | 13920 / 114272 | training loss: 0.0003074656706303358\n",
      "epoch: 8 | 13952 / 114272 | training loss: 0.09914307296276093\n",
      "epoch: 8 | 13984 / 114272 | training loss: 0.00042821653187274933\n",
      "epoch: 8 | 14016 / 114272 | training loss: 0.0005073502543382347\n",
      "epoch: 8 | 14048 / 114272 | training loss: 0.00026746769435703754\n",
      "epoch: 8 | 14080 / 114272 | training loss: 0.0003559085016604513\n",
      "epoch: 8 | 14112 / 114272 | training loss: 0.0003363226423971355\n",
      "epoch: 8 | 14144 / 114272 | training loss: 0.038829028606414795\n",
      "epoch: 8 | 14176 / 114272 | training loss: 0.00045246939407661557\n",
      "epoch: 8 | 14208 / 114272 | training loss: 0.00028538296464830637\n",
      "epoch: 8 | 14240 / 114272 | training loss: 0.00023455789778381586\n",
      "epoch: 8 | 14272 / 114272 | training loss: 0.00038860979839228094\n",
      "epoch: 8 | 14304 / 114272 | training loss: 0.0012478448916226625\n",
      "epoch: 8 | 14336 / 114272 | training loss: 0.0005263038328848779\n",
      "epoch: 8 | 14368 / 114272 | training loss: 0.0004369659291114658\n",
      "epoch: 8 | 14400 / 114272 | training loss: 0.0003207967965863645\n",
      "epoch: 8 | 14432 / 114272 | training loss: 0.000364547100616619\n",
      "epoch: 8 | 14464 / 114272 | training loss: 0.00031070533441379666\n",
      "epoch: 8 | 14496 / 114272 | training loss: 0.0007887402316555381\n",
      "epoch: 8 | 14528 / 114272 | training loss: 0.0004284023307263851\n",
      "epoch: 8 | 14560 / 114272 | training loss: 0.00015016876568552107\n",
      "epoch: 8 | 14592 / 114272 | training loss: 0.10424267500638962\n",
      "epoch: 8 | 14624 / 114272 | training loss: 0.00029179209377616644\n",
      "epoch: 8 | 14656 / 114272 | training loss: 0.0003005333710461855\n",
      "epoch: 8 | 14688 / 114272 | training loss: 0.2133556306362152\n",
      "epoch: 8 | 14720 / 114272 | training loss: 0.00038223955198191106\n",
      "epoch: 8 | 14752 / 114272 | training loss: 0.0005344988312572241\n",
      "epoch: 8 | 14784 / 114272 | training loss: 0.00032928603468462825\n",
      "epoch: 8 | 14816 / 114272 | training loss: 0.00022996871848590672\n",
      "epoch: 8 | 14848 / 114272 | training loss: 0.000414479203755036\n",
      "epoch: 8 | 14880 / 114272 | training loss: 0.0005133788217790425\n",
      "epoch: 8 | 14912 / 114272 | training loss: 0.00020169604977127165\n",
      "epoch: 8 | 14944 / 114272 | training loss: 0.0007431217236444354\n",
      "epoch: 8 | 14976 / 114272 | training loss: 0.00023024696565698832\n",
      "epoch: 8 | 15008 / 114272 | training loss: 0.00025839885347522795\n",
      "epoch: 8 | 15040 / 114272 | training loss: 0.00032824641675688326\n",
      "epoch: 8 | 15072 / 114272 | training loss: 0.0918564572930336\n",
      "epoch: 8 | 15104 / 114272 | training loss: 0.00023947471345309168\n",
      "epoch: 8 | 15136 / 114272 | training loss: 0.0002145701291738078\n",
      "epoch: 8 | 15168 / 114272 | training loss: 0.00036273457226343453\n",
      "epoch: 8 | 15200 / 114272 | training loss: 0.00039847492007538676\n",
      "epoch: 8 | 15232 / 114272 | training loss: 0.0004185970174148679\n",
      "epoch: 8 | 15264 / 114272 | training loss: 0.00036327927955426276\n",
      "epoch: 8 | 15296 / 114272 | training loss: 0.000407281651860103\n",
      "epoch: 8 | 15328 / 114272 | training loss: 0.0003278213262092322\n",
      "epoch: 8 | 15360 / 114272 | training loss: 0.1777276247739792\n",
      "epoch: 8 | 15392 / 114272 | training loss: 0.0003773486823774874\n",
      "epoch: 8 | 15424 / 114272 | training loss: 0.0006201534997671843\n",
      "epoch: 8 | 15456 / 114272 | training loss: 0.011229979805648327\n",
      "epoch: 8 | 15488 / 114272 | training loss: 0.00047264742897823453\n",
      "epoch: 8 | 15520 / 114272 | training loss: 0.0008369047427549958\n",
      "epoch: 8 | 15552 / 114272 | training loss: 0.00029608531622216105\n",
      "epoch: 8 | 15584 / 114272 | training loss: 0.0012988625094294548\n",
      "epoch: 8 | 15616 / 114272 | training loss: 0.0006704642437398434\n",
      "epoch: 8 | 15648 / 114272 | training loss: 0.00031284987926483154\n",
      "epoch: 8 | 15680 / 114272 | training loss: 0.0004013193538412452\n",
      "epoch: 8 | 15712 / 114272 | training loss: 0.0005116894608363509\n",
      "epoch: 8 | 15744 / 114272 | training loss: 0.0003205313114449382\n",
      "epoch: 8 | 15776 / 114272 | training loss: 0.00034533601137809455\n",
      "epoch: 8 | 15808 / 114272 | training loss: 0.09589181840419769\n",
      "epoch: 8 | 15840 / 114272 | training loss: 0.0005462733097374439\n",
      "epoch: 8 | 15872 / 114272 | training loss: 0.0004182151169516146\n",
      "epoch: 8 | 15904 / 114272 | training loss: 0.01796734519302845\n",
      "epoch: 8 | 15936 / 114272 | training loss: 0.00042542663868516684\n",
      "epoch: 8 | 15968 / 114272 | training loss: 0.00023336164304055274\n",
      "epoch: 8 | 16000 / 114272 | training loss: 0.00035712728276848793\n",
      "epoch: 8 | 16032 / 114272 | training loss: 0.0003757191589102149\n",
      "epoch: 8 | 16064 / 114272 | training loss: 0.00018658553017303348\n",
      "epoch: 8 | 16096 / 114272 | training loss: 0.0002618530415929854\n",
      "epoch: 8 | 16128 / 114272 | training loss: 0.0002996290277224034\n",
      "epoch: 8 | 16160 / 114272 | training loss: 0.0004381166072562337\n",
      "epoch: 8 | 16192 / 114272 | training loss: 0.00025556847685948014\n",
      "epoch: 8 | 16224 / 114272 | training loss: 0.09593886137008667\n",
      "epoch: 8 | 16256 / 114272 | training loss: 0.00044976058416068554\n",
      "epoch: 8 | 16288 / 114272 | training loss: 0.00021089798246975988\n",
      "epoch: 8 | 16320 / 114272 | training loss: 0.0003670023870654404\n",
      "epoch: 8 | 16352 / 114272 | training loss: 0.00044154500938020647\n",
      "epoch: 8 | 16384 / 114272 | training loss: 0.0002182478638133034\n",
      "epoch: 8 | 16416 / 114272 | training loss: 0.00021587216178886592\n",
      "epoch: 8 | 16448 / 114272 | training loss: 0.10314485430717468\n",
      "epoch: 8 | 16480 / 114272 | training loss: 0.0004256869142409414\n",
      "epoch: 8 | 16512 / 114272 | training loss: 0.21541471779346466\n",
      "epoch: 8 | 16544 / 114272 | training loss: 0.00026978758978657424\n",
      "epoch: 8 | 16576 / 114272 | training loss: 0.00034042572951875627\n",
      "epoch: 8 | 16608 / 114272 | training loss: 0.0008689647074788809\n",
      "epoch: 8 | 16640 / 114272 | training loss: 0.0002981150755658746\n",
      "epoch: 8 | 16672 / 114272 | training loss: 0.0006166884559206665\n",
      "epoch: 8 | 16704 / 114272 | training loss: 0.00044023769441992044\n",
      "epoch: 8 | 16736 / 114272 | training loss: 0.00023192857042886317\n",
      "epoch: 8 | 16768 / 114272 | training loss: 0.00026885190163739026\n",
      "epoch: 8 | 16800 / 114272 | training loss: 0.00043451765668578446\n",
      "epoch: 8 | 16832 / 114272 | training loss: 0.0034473217092454433\n",
      "epoch: 8 | 16864 / 114272 | training loss: 0.0004600479151122272\n",
      "epoch: 8 | 16896 / 114272 | training loss: 0.029704494401812553\n",
      "epoch: 8 | 16928 / 114272 | training loss: 0.0004276169929653406\n",
      "epoch: 8 | 16960 / 114272 | training loss: 0.0006189955165609717\n",
      "epoch: 8 | 16992 / 114272 | training loss: 0.00038999554817564785\n",
      "epoch: 8 | 17024 / 114272 | training loss: 0.0004221373237669468\n",
      "epoch: 8 | 17056 / 114272 | training loss: 0.0002858080551959574\n",
      "epoch: 8 | 17088 / 114272 | training loss: 0.00021520548034459352\n",
      "epoch: 8 | 17120 / 114272 | training loss: 0.0003624462697189301\n",
      "epoch: 8 | 17152 / 114272 | training loss: 0.0003790066111832857\n",
      "epoch: 8 | 17184 / 114272 | training loss: 0.000610757851973176\n",
      "epoch: 8 | 17216 / 114272 | training loss: 0.00042521144496276975\n",
      "epoch: 8 | 17248 / 114272 | training loss: 0.0014368480769917369\n",
      "epoch: 8 | 17280 / 114272 | training loss: 0.0004072440497111529\n",
      "epoch: 8 | 17312 / 114272 | training loss: 0.0005383221432566643\n",
      "epoch: 8 | 17344 / 114272 | training loss: 0.000227678261580877\n",
      "epoch: 8 | 17376 / 114272 | training loss: 0.00045384286204352975\n",
      "epoch: 8 | 17408 / 114272 | training loss: 0.0004917453625239432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 17440 / 114272 | training loss: 0.0005557840340770781\n",
      "epoch: 8 | 17472 / 114272 | training loss: 0.00035689331707544625\n",
      "epoch: 8 | 17504 / 114272 | training loss: 0.00046416796976700425\n",
      "epoch: 8 | 17536 / 114272 | training loss: 0.04589686915278435\n",
      "epoch: 8 | 17568 / 114272 | training loss: 0.0004093822499271482\n",
      "epoch: 8 | 17600 / 114272 | training loss: 0.0003171483112964779\n",
      "epoch: 8 | 17632 / 114272 | training loss: 0.0005155535764060915\n",
      "epoch: 8 | 17664 / 114272 | training loss: 0.0002742409997154027\n",
      "epoch: 8 | 17696 / 114272 | training loss: 0.0002661078760866076\n",
      "epoch: 8 | 17728 / 114272 | training loss: 0.00045371215674094856\n",
      "epoch: 8 | 17760 / 114272 | training loss: 0.0005178567371331155\n",
      "epoch: 8 | 17792 / 114272 | training loss: 0.0006627468392252922\n",
      "epoch: 8 | 17824 / 114272 | training loss: 0.00033928334596566856\n",
      "epoch: 8 | 17856 / 114272 | training loss: 0.0003605370584409684\n",
      "epoch: 8 | 17888 / 114272 | training loss: 0.00021784048294648528\n",
      "epoch: 8 | 17920 / 114272 | training loss: 0.0003142520145047456\n",
      "epoch: 8 | 17952 / 114272 | training loss: 0.00043918497976846993\n",
      "epoch: 8 | 17984 / 114272 | training loss: 0.00035956851206719875\n",
      "epoch: 8 | 18016 / 114272 | training loss: 0.08554971218109131\n",
      "epoch: 8 | 18048 / 114272 | training loss: 0.0004399919416755438\n",
      "epoch: 8 | 18080 / 114272 | training loss: 0.0007058455375954509\n",
      "epoch: 8 | 18112 / 114272 | training loss: 0.0004526253032963723\n",
      "epoch: 8 | 18144 / 114272 | training loss: 0.000335169694153592\n",
      "epoch: 8 | 18176 / 114272 | training loss: 0.0004611911135725677\n",
      "epoch: 8 | 18208 / 114272 | training loss: 0.00028828176436945796\n",
      "epoch: 8 | 18240 / 114272 | training loss: 0.0004000956832896918\n",
      "epoch: 8 | 18272 / 114272 | training loss: 0.00033247601822949946\n",
      "epoch: 8 | 18304 / 114272 | training loss: 0.00048678944585844874\n",
      "epoch: 8 | 18336 / 114272 | training loss: 0.0012234037276357412\n",
      "epoch: 8 | 18368 / 114272 | training loss: 0.0013806322822347283\n",
      "epoch: 8 | 18400 / 114272 | training loss: 0.0013713926309719682\n",
      "epoch: 8 | 18432 / 114272 | training loss: 0.0004746576596517116\n",
      "epoch: 8 | 18464 / 114272 | training loss: 0.00259215640835464\n",
      "epoch: 8 | 18496 / 114272 | training loss: 0.0002399703807896003\n",
      "epoch: 8 | 18528 / 114272 | training loss: 0.0003735878271982074\n",
      "epoch: 8 | 18560 / 114272 | training loss: 0.0006616853061132133\n",
      "epoch: 8 | 18592 / 114272 | training loss: 0.00048333927406929433\n",
      "epoch: 8 | 18624 / 114272 | training loss: 0.00023364837397821248\n",
      "epoch: 8 | 18656 / 114272 | training loss: 0.0009692408493719995\n",
      "epoch: 8 | 18688 / 114272 | training loss: 0.0005588336498476565\n",
      "epoch: 8 | 18720 / 114272 | training loss: 0.0003181534993927926\n",
      "epoch: 8 | 18752 / 114272 | training loss: 0.0002903734566643834\n",
      "epoch: 8 | 18784 / 114272 | training loss: 0.00019627445726655424\n",
      "epoch: 8 | 18816 / 114272 | training loss: 0.00029870294383727014\n",
      "epoch: 8 | 18848 / 114272 | training loss: 0.0004231884377077222\n",
      "epoch: 8 | 18880 / 114272 | training loss: 0.0004910949501208961\n",
      "epoch: 8 | 18912 / 114272 | training loss: 0.0008602765738032758\n",
      "epoch: 8 | 18944 / 114272 | training loss: 0.00017146716709248722\n",
      "epoch: 8 | 18976 / 114272 | training loss: 0.0002889464085455984\n",
      "epoch: 8 | 19008 / 114272 | training loss: 0.015166210010647774\n",
      "epoch: 8 | 19040 / 114272 | training loss: 0.00047477844054810703\n",
      "epoch: 8 | 19072 / 114272 | training loss: 0.00021641056810040027\n",
      "epoch: 8 | 19104 / 114272 | training loss: 0.00018727524729911238\n",
      "epoch: 8 | 19136 / 114272 | training loss: 0.0003371409547980875\n",
      "epoch: 8 | 19168 / 114272 | training loss: 0.010952316224575043\n",
      "epoch: 8 | 19200 / 114272 | training loss: 0.0004584355338010937\n",
      "epoch: 8 | 19232 / 114272 | training loss: 0.0007114699692465365\n",
      "epoch: 8 | 19264 / 114272 | training loss: 0.00017622178711462766\n",
      "epoch: 8 | 19296 / 114272 | training loss: 0.0012620044872164726\n",
      "epoch: 8 | 19328 / 114272 | training loss: 0.0003167027607560158\n",
      "epoch: 8 | 19360 / 114272 | training loss: 0.00023415272880811244\n",
      "epoch: 8 | 19392 / 114272 | training loss: 0.00037824478931725025\n",
      "epoch: 8 | 19424 / 114272 | training loss: 0.00020469437004067004\n",
      "epoch: 8 | 19456 / 114272 | training loss: 0.0003100403700955212\n",
      "epoch: 8 | 19488 / 114272 | training loss: 0.0002287177339894697\n",
      "epoch: 8 | 19520 / 114272 | training loss: 0.00042194616980850697\n",
      "epoch: 8 | 19552 / 114272 | training loss: 0.00023775488079991192\n",
      "epoch: 8 | 19584 / 114272 | training loss: 0.00037834199611097574\n",
      "epoch: 8 | 19616 / 114272 | training loss: 0.007883954793214798\n",
      "epoch: 8 | 19648 / 114272 | training loss: 0.0006487498758360744\n",
      "epoch: 8 | 19680 / 114272 | training loss: 0.0010726144537329674\n",
      "epoch: 8 | 19712 / 114272 | training loss: 0.00023122166749089956\n",
      "epoch: 8 | 19744 / 114272 | training loss: 0.00024705746909603477\n",
      "epoch: 8 | 19776 / 114272 | training loss: 0.0005976917454972863\n",
      "epoch: 8 | 19808 / 114272 | training loss: 0.0007321414886973798\n",
      "epoch: 8 | 19840 / 114272 | training loss: 0.0003734850324690342\n",
      "epoch: 8 | 19872 / 114272 | training loss: 0.0003369287878740579\n",
      "epoch: 8 | 19904 / 114272 | training loss: 0.00046996172750368714\n",
      "epoch: 8 | 19936 / 114272 | training loss: 0.00022291720961220562\n",
      "epoch: 8 | 19968 / 114272 | training loss: 0.00030294887255877256\n",
      "epoch: 8 | 20000 / 114272 | training loss: 0.0005786679685115814\n",
      "epoch: 8 | 20032 / 114272 | training loss: 0.00017557170940563083\n",
      "epoch: 8 | 20064 / 114272 | training loss: 0.00020383726223371923\n",
      "epoch: 8 | 20096 / 114272 | training loss: 0.0002096883545164019\n",
      "epoch: 8 | 20128 / 114272 | training loss: 0.0017717754235491157\n",
      "epoch: 8 | 20160 / 114272 | training loss: 0.0002954915107693523\n",
      "epoch: 8 | 20192 / 114272 | training loss: 0.0007021112251095474\n",
      "epoch: 8 | 20224 / 114272 | training loss: 0.00036357788485474885\n",
      "epoch: 8 | 20256 / 114272 | training loss: 0.00033887469908222556\n",
      "epoch: 8 | 20288 / 114272 | training loss: 0.0003128760145045817\n",
      "epoch: 8 | 20320 / 114272 | training loss: 0.0002817981003317982\n",
      "epoch: 8 | 20352 / 114272 | training loss: 0.0002511922793928534\n",
      "epoch: 8 | 20384 / 114272 | training loss: 0.03655971214175224\n",
      "epoch: 8 | 20416 / 114272 | training loss: 0.00027393081109039485\n",
      "epoch: 8 | 20448 / 114272 | training loss: 0.0002586935297586024\n",
      "epoch: 8 | 20480 / 114272 | training loss: 0.00031944995862431824\n",
      "epoch: 8 | 20512 / 114272 | training loss: 0.0001734379184199497\n",
      "epoch: 8 | 20544 / 114272 | training loss: 0.0003257406933698803\n",
      "epoch: 8 | 20576 / 114272 | training loss: 0.00023780956689734012\n",
      "epoch: 8 | 20608 / 114272 | training loss: 0.0002746336394920945\n",
      "epoch: 8 | 20640 / 114272 | training loss: 0.00035625346936285496\n",
      "epoch: 8 | 20672 / 114272 | training loss: 0.00042555489926598966\n",
      "epoch: 8 | 20704 / 114272 | training loss: 0.15235808491706848\n",
      "epoch: 8 | 20736 / 114272 | training loss: 0.0016829242231324315\n",
      "epoch: 8 | 20768 / 114272 | training loss: 0.00023238733410835266\n",
      "epoch: 8 | 20800 / 114272 | training loss: 0.000301143154501915\n",
      "epoch: 8 | 20832 / 114272 | training loss: 0.00033918090048246086\n",
      "epoch: 8 | 20864 / 114272 | training loss: 0.00019003890338353813\n",
      "epoch: 8 | 20896 / 114272 | training loss: 0.0004124650149606168\n",
      "epoch: 8 | 20928 / 114272 | training loss: 0.0002397512726020068\n",
      "epoch: 8 | 20960 / 114272 | training loss: 0.00019721379794646055\n",
      "epoch: 8 | 20992 / 114272 | training loss: 0.00018464872846379876\n",
      "epoch: 8 | 21024 / 114272 | training loss: 0.00043945194920524955\n",
      "epoch: 8 | 21056 / 114272 | training loss: 0.001925839576870203\n",
      "epoch: 8 | 21088 / 114272 | training loss: 0.00034734333166852593\n",
      "epoch: 8 | 21120 / 114272 | training loss: 0.0002531672944314778\n",
      "epoch: 8 | 21152 / 114272 | training loss: 0.005779517814517021\n",
      "epoch: 8 | 21184 / 114272 | training loss: 0.00035705784102901816\n",
      "epoch: 8 | 21216 / 114272 | training loss: 0.0002566268085502088\n",
      "epoch: 8 | 21248 / 114272 | training loss: 0.00036638861638493836\n",
      "epoch: 8 | 21280 / 114272 | training loss: 0.0009607037645764649\n",
      "epoch: 8 | 21312 / 114272 | training loss: 0.00033277642796747386\n",
      "epoch: 8 | 21344 / 114272 | training loss: 0.027531452476978302\n",
      "epoch: 8 | 21376 / 114272 | training loss: 0.018551571294665337\n",
      "epoch: 8 | 21408 / 114272 | training loss: 0.00012383336434140801\n",
      "epoch: 8 | 21440 / 114272 | training loss: 0.0001930929283844307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 21472 / 114272 | training loss: 0.0004207803285680711\n",
      "epoch: 8 | 21504 / 114272 | training loss: 0.1666790246963501\n",
      "epoch: 8 | 21536 / 114272 | training loss: 0.00024432301870547235\n",
      "epoch: 8 | 21568 / 114272 | training loss: 0.0002911724732257426\n",
      "epoch: 8 | 21600 / 114272 | training loss: 0.00040504999924451113\n",
      "epoch: 8 | 21632 / 114272 | training loss: 0.029871679842472076\n",
      "epoch: 8 | 21664 / 114272 | training loss: 0.0011603053426370025\n",
      "epoch: 8 | 21696 / 114272 | training loss: 0.0005626404308713973\n",
      "epoch: 8 | 21728 / 114272 | training loss: 0.0002418975782347843\n",
      "epoch: 8 | 21760 / 114272 | training loss: 0.00038824265357106924\n",
      "epoch: 8 | 21792 / 114272 | training loss: 0.0023077744990587234\n",
      "epoch: 8 | 21824 / 114272 | training loss: 0.0003522392944432795\n",
      "epoch: 8 | 21856 / 114272 | training loss: 0.00028659746749326587\n",
      "epoch: 8 | 21888 / 114272 | training loss: 0.00024803593987599015\n",
      "epoch: 8 | 21920 / 114272 | training loss: 0.000403311918489635\n",
      "epoch: 8 | 21952 / 114272 | training loss: 0.00037941758637316525\n",
      "epoch: 8 | 21984 / 114272 | training loss: 0.00016631338803563267\n",
      "epoch: 8 | 22016 / 114272 | training loss: 0.00030294517637230456\n",
      "epoch: 8 | 22048 / 114272 | training loss: 0.27474042773246765\n",
      "epoch: 8 | 22080 / 114272 | training loss: 0.00030307946144603193\n",
      "epoch: 8 | 22112 / 114272 | training loss: 0.0005926013691350818\n",
      "epoch: 8 | 22144 / 114272 | training loss: 0.0003270074084866792\n",
      "epoch: 8 | 22176 / 114272 | training loss: 0.00029047243879176676\n",
      "epoch: 8 | 22208 / 114272 | training loss: 0.0002681625192053616\n",
      "epoch: 8 | 22240 / 114272 | training loss: 0.00020826244144700468\n",
      "epoch: 8 | 22272 / 114272 | training loss: 0.0009291706373915076\n",
      "epoch: 8 | 22304 / 114272 | training loss: 0.00043415784602984786\n",
      "epoch: 8 | 22336 / 114272 | training loss: 0.00023246694763656706\n",
      "epoch: 8 | 22368 / 114272 | training loss: 0.0002756196481641382\n",
      "epoch: 8 | 22400 / 114272 | training loss: 0.00042218915768899024\n",
      "epoch: 8 | 22432 / 114272 | training loss: 0.0008172332309186459\n",
      "epoch: 8 | 22464 / 114272 | training loss: 0.0003784656582865864\n",
      "epoch: 8 | 22496 / 114272 | training loss: 0.0005391003214754164\n",
      "epoch: 8 | 22528 / 114272 | training loss: 0.008135038428008556\n",
      "epoch: 8 | 22560 / 114272 | training loss: 0.0002464592398609966\n",
      "epoch: 8 | 22592 / 114272 | training loss: 0.0046309721656143665\n",
      "epoch: 8 | 22624 / 114272 | training loss: 0.0007196080405265093\n",
      "epoch: 8 | 22656 / 114272 | training loss: 0.00022674705542158335\n",
      "epoch: 8 | 22688 / 114272 | training loss: 0.0005344048258848488\n",
      "epoch: 8 | 22720 / 114272 | training loss: 0.0003431110526435077\n",
      "epoch: 8 | 22752 / 114272 | training loss: 0.00031344726448878646\n",
      "epoch: 8 | 22784 / 114272 | training loss: 0.0002272158453706652\n",
      "epoch: 8 | 22816 / 114272 | training loss: 0.00042135067633353174\n",
      "epoch: 8 | 22848 / 114272 | training loss: 0.00019163306569680572\n",
      "epoch: 8 | 22880 / 114272 | training loss: 0.000313255877699703\n",
      "epoch: 8 | 22912 / 114272 | training loss: 0.1458764523267746\n",
      "epoch: 8 | 22944 / 114272 | training loss: 0.00028018502052873373\n",
      "epoch: 8 | 22976 / 114272 | training loss: 0.0003708149306476116\n",
      "epoch: 8 | 23008 / 114272 | training loss: 0.0003769206814467907\n",
      "epoch: 8 | 23040 / 114272 | training loss: 0.00040391902439296246\n",
      "epoch: 8 | 23072 / 114272 | training loss: 0.0017269208328798413\n",
      "epoch: 8 | 23104 / 114272 | training loss: 0.00027196272276341915\n",
      "epoch: 8 | 23136 / 114272 | training loss: 0.0002338780468562618\n",
      "epoch: 8 | 23168 / 114272 | training loss: 0.0004924169043079019\n",
      "epoch: 8 | 23200 / 114272 | training loss: 0.00031414112891070545\n",
      "epoch: 8 | 23232 / 114272 | training loss: 0.0007284169550985098\n",
      "epoch: 8 | 23264 / 114272 | training loss: 0.009439464658498764\n",
      "epoch: 8 | 23296 / 114272 | training loss: 0.00033052003709599376\n",
      "epoch: 8 | 23328 / 114272 | training loss: 0.0002908497699536383\n",
      "epoch: 8 | 23360 / 114272 | training loss: 0.00030270914430730045\n",
      "epoch: 8 | 23392 / 114272 | training loss: 0.16333284974098206\n",
      "epoch: 8 | 23424 / 114272 | training loss: 0.0003375678206793964\n",
      "epoch: 8 | 23456 / 114272 | training loss: 0.0002974454255308956\n",
      "epoch: 8 | 23488 / 114272 | training loss: 0.00024080595176201314\n",
      "epoch: 8 | 23520 / 114272 | training loss: 0.00035959016531705856\n",
      "epoch: 8 | 23552 / 114272 | training loss: 0.00038457411574199796\n",
      "epoch: 8 | 23584 / 114272 | training loss: 0.00019445919315330684\n",
      "epoch: 8 | 23616 / 114272 | training loss: 0.00029190556961111724\n",
      "epoch: 8 | 23648 / 114272 | training loss: 0.00035464498796500266\n",
      "epoch: 8 | 23680 / 114272 | training loss: 0.00044735168921761215\n",
      "epoch: 8 | 23712 / 114272 | training loss: 0.0006902691093273461\n",
      "epoch: 8 | 23744 / 114272 | training loss: 0.00023546004376839846\n",
      "epoch: 8 | 23776 / 114272 | training loss: 0.00028208873118273914\n",
      "epoch: 8 | 23808 / 114272 | training loss: 0.000265804206719622\n",
      "epoch: 8 | 23840 / 114272 | training loss: 0.0004088850400876254\n",
      "epoch: 8 | 23872 / 114272 | training loss: 0.00019265376613475382\n",
      "epoch: 8 | 23904 / 114272 | training loss: 0.00020067484001629055\n",
      "epoch: 8 | 23936 / 114272 | training loss: 0.0002738316252361983\n",
      "epoch: 8 | 23968 / 114272 | training loss: 0.00040314733632840216\n",
      "epoch: 8 | 24000 / 114272 | training loss: 0.0004546092532109469\n",
      "epoch: 8 | 24032 / 114272 | training loss: 0.0011271974071860313\n",
      "epoch: 8 | 24064 / 114272 | training loss: 0.00038284805486910045\n",
      "epoch: 8 | 24096 / 114272 | training loss: 0.00032906350679695606\n",
      "epoch: 8 | 24128 / 114272 | training loss: 0.00040263982373289764\n",
      "epoch: 8 | 24160 / 114272 | training loss: 0.00023701495956629515\n",
      "epoch: 8 | 24192 / 114272 | training loss: 0.16144950687885284\n",
      "epoch: 8 | 24224 / 114272 | training loss: 0.00021213668514974415\n",
      "epoch: 8 | 24256 / 114272 | training loss: 0.0015733534237369895\n",
      "epoch: 8 | 24288 / 114272 | training loss: 0.0002476906811352819\n",
      "epoch: 8 | 24320 / 114272 | training loss: 0.0004886550013907254\n",
      "epoch: 8 | 24352 / 114272 | training loss: 0.0004645919834729284\n",
      "epoch: 8 | 24384 / 114272 | training loss: 0.0002451088512316346\n",
      "epoch: 8 | 24416 / 114272 | training loss: 0.00027894446975551546\n",
      "epoch: 8 | 24448 / 114272 | training loss: 0.00037765834713354707\n",
      "epoch: 8 | 24480 / 114272 | training loss: 0.0002872833574656397\n",
      "epoch: 8 | 24512 / 114272 | training loss: 0.00029446289408952\n",
      "epoch: 8 | 24544 / 114272 | training loss: 0.0005659908638335764\n",
      "epoch: 8 | 24576 / 114272 | training loss: 0.00032878489582799375\n",
      "epoch: 8 | 24608 / 114272 | training loss: 0.00027767621213570237\n",
      "epoch: 8 | 24640 / 114272 | training loss: 0.004349774215370417\n",
      "epoch: 8 | 24672 / 114272 | training loss: 0.00015363127749878913\n",
      "epoch: 8 | 24704 / 114272 | training loss: 0.00971939880400896\n",
      "epoch: 8 | 24736 / 114272 | training loss: 0.00034093501744791865\n",
      "epoch: 8 | 24768 / 114272 | training loss: 0.00020429662254173309\n",
      "epoch: 8 | 24800 / 114272 | training loss: 0.10298511385917664\n",
      "epoch: 8 | 24832 / 114272 | training loss: 0.01469546277076006\n",
      "epoch: 8 | 24864 / 114272 | training loss: 0.00024911979562602937\n",
      "epoch: 8 | 24896 / 114272 | training loss: 0.00045423052506521344\n",
      "epoch: 8 | 24928 / 114272 | training loss: 0.061708077788352966\n",
      "epoch: 8 | 24960 / 114272 | training loss: 0.0005674712592735887\n",
      "epoch: 8 | 24992 / 114272 | training loss: 0.0002149275242118165\n",
      "epoch: 8 | 25024 / 114272 | training loss: 0.0004924341919831932\n",
      "epoch: 8 | 25056 / 114272 | training loss: 0.00043636863119900227\n",
      "epoch: 8 | 25088 / 114272 | training loss: 0.0009987428784370422\n",
      "epoch: 8 | 25120 / 114272 | training loss: 0.0005059483228251338\n",
      "epoch: 8 | 25152 / 114272 | training loss: 0.0012712946627289057\n",
      "epoch: 8 | 25184 / 114272 | training loss: 0.000373992690583691\n",
      "epoch: 8 | 25216 / 114272 | training loss: 0.0001430363772669807\n",
      "epoch: 8 | 25248 / 114272 | training loss: 0.00047917061601765454\n",
      "epoch: 8 | 25280 / 114272 | training loss: 0.042631134390830994\n",
      "epoch: 8 | 25312 / 114272 | training loss: 0.00024198574828915298\n",
      "epoch: 8 | 25344 / 114272 | training loss: 0.0003855950490105897\n",
      "epoch: 8 | 25376 / 114272 | training loss: 0.00031665453570894897\n",
      "epoch: 8 | 25408 / 114272 | training loss: 0.0005535049131140113\n",
      "epoch: 8 | 25440 / 114272 | training loss: 0.0003294724738225341\n",
      "epoch: 8 | 25472 / 114272 | training loss: 0.0002710848639253527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 25504 / 114272 | training loss: 0.0002197610738221556\n",
      "epoch: 8 | 25536 / 114272 | training loss: 0.0004445726808626205\n",
      "epoch: 8 | 25568 / 114272 | training loss: 0.003716815495863557\n",
      "epoch: 8 | 25600 / 114272 | training loss: 0.00019894400611519814\n",
      "epoch: 8 | 25632 / 114272 | training loss: 0.0003558502357918769\n",
      "epoch: 8 | 25664 / 114272 | training loss: 0.0007186696166172624\n",
      "epoch: 8 | 25696 / 114272 | training loss: 0.00025722404825501144\n",
      "epoch: 8 | 25728 / 114272 | training loss: 0.001262737438082695\n",
      "epoch: 8 | 25760 / 114272 | training loss: 0.0018384776776656508\n",
      "epoch: 8 | 25792 / 114272 | training loss: 0.13554440438747406\n",
      "epoch: 8 | 25824 / 114272 | training loss: 0.00037614593748003244\n",
      "epoch: 8 | 25856 / 114272 | training loss: 0.00031864651828072965\n",
      "epoch: 8 | 25888 / 114272 | training loss: 0.0002570784999988973\n",
      "epoch: 8 | 25920 / 114272 | training loss: 0.0013186994474381208\n",
      "epoch: 8 | 25952 / 114272 | training loss: 0.002119399607181549\n",
      "epoch: 8 | 25984 / 114272 | training loss: 0.00019051307754125446\n",
      "epoch: 8 | 26016 / 114272 | training loss: 0.000257817649981007\n",
      "epoch: 8 | 26048 / 114272 | training loss: 0.00020784835214726627\n",
      "epoch: 8 | 26080 / 114272 | training loss: 0.000518089160323143\n",
      "epoch: 8 | 26112 / 114272 | training loss: 0.00040033890400081873\n",
      "epoch: 8 | 26144 / 114272 | training loss: 0.00024766603019088507\n",
      "epoch: 8 | 26176 / 114272 | training loss: 0.0030752241145819426\n",
      "epoch: 8 | 26208 / 114272 | training loss: 0.0007638706010766327\n",
      "epoch: 8 | 26240 / 114272 | training loss: 0.0006263004615902901\n",
      "epoch: 8 | 26272 / 114272 | training loss: 0.0007472354918718338\n",
      "epoch: 8 | 26304 / 114272 | training loss: 0.00029725211788900197\n",
      "epoch: 8 | 26336 / 114272 | training loss: 0.00022645753051619977\n",
      "epoch: 8 | 26368 / 114272 | training loss: 0.0004492148000281304\n",
      "epoch: 8 | 26400 / 114272 | training loss: 0.005810802802443504\n",
      "epoch: 8 | 26432 / 114272 | training loss: 0.00039648727397434413\n",
      "epoch: 8 | 26464 / 114272 | training loss: 0.000453573273262009\n",
      "epoch: 8 | 26496 / 114272 | training loss: 0.12231666594743729\n",
      "epoch: 8 | 26528 / 114272 | training loss: 0.00028759444830939174\n",
      "epoch: 8 | 26560 / 114272 | training loss: 0.0016138388309627771\n",
      "epoch: 8 | 26592 / 114272 | training loss: 0.02356637828052044\n",
      "epoch: 8 | 26624 / 114272 | training loss: 0.0030484432354569435\n",
      "epoch: 8 | 26656 / 114272 | training loss: 0.00024039848358370364\n",
      "epoch: 8 | 26688 / 114272 | training loss: 0.0005703179049305618\n",
      "epoch: 8 | 26720 / 114272 | training loss: 0.0032744412310421467\n",
      "epoch: 8 | 26752 / 114272 | training loss: 0.00021754858607891947\n",
      "epoch: 8 | 26784 / 114272 | training loss: 0.00020317782764323056\n",
      "epoch: 8 | 26816 / 114272 | training loss: 0.00024141436733771116\n",
      "epoch: 8 | 26848 / 114272 | training loss: 0.03930080682039261\n",
      "epoch: 8 | 26880 / 114272 | training loss: 0.000396615854697302\n",
      "epoch: 8 | 26912 / 114272 | training loss: 0.0005525504238903522\n",
      "epoch: 8 | 26944 / 114272 | training loss: 0.000242815469391644\n",
      "epoch: 8 | 26976 / 114272 | training loss: 0.003971927333623171\n",
      "epoch: 8 | 27008 / 114272 | training loss: 0.046767376363277435\n",
      "epoch: 8 | 27040 / 114272 | training loss: 0.00028011444373987615\n",
      "epoch: 8 | 27072 / 114272 | training loss: 0.00015242729568853974\n",
      "epoch: 8 | 27104 / 114272 | training loss: 0.0003866824845317751\n",
      "epoch: 8 | 27136 / 114272 | training loss: 0.0004052811418659985\n",
      "epoch: 8 | 27168 / 114272 | training loss: 0.00020578833937179297\n",
      "epoch: 8 | 27200 / 114272 | training loss: 0.0002247814554721117\n",
      "epoch: 8 | 27232 / 114272 | training loss: 0.0002338420890737325\n",
      "epoch: 8 | 27264 / 114272 | training loss: 0.0002633419935591519\n",
      "epoch: 8 | 27296 / 114272 | training loss: 0.0002438180090393871\n",
      "epoch: 8 | 27328 / 114272 | training loss: 0.00022624437406193465\n",
      "epoch: 8 | 27360 / 114272 | training loss: 0.000343326450092718\n",
      "epoch: 8 | 27392 / 114272 | training loss: 0.00045688686077483\n",
      "epoch: 8 | 27424 / 114272 | training loss: 0.00018494357937015593\n",
      "epoch: 8 | 27456 / 114272 | training loss: 0.00033596938010305166\n",
      "epoch: 8 | 27488 / 114272 | training loss: 0.00017850649601314217\n",
      "epoch: 8 | 27520 / 114272 | training loss: 0.00023652389063499868\n",
      "epoch: 8 | 27552 / 114272 | training loss: 0.00022557759075425565\n",
      "epoch: 8 | 27584 / 114272 | training loss: 0.0002864492416847497\n",
      "epoch: 8 | 27616 / 114272 | training loss: 0.00021302523964550346\n",
      "epoch: 8 | 27648 / 114272 | training loss: 0.00023682859318796545\n",
      "epoch: 8 | 27680 / 114272 | training loss: 0.00021931403898634017\n",
      "epoch: 8 | 27712 / 114272 | training loss: 0.00032720775925554335\n",
      "epoch: 8 | 27744 / 114272 | training loss: 0.0003182335349265486\n",
      "epoch: 8 | 27776 / 114272 | training loss: 0.000140027332236059\n",
      "epoch: 8 | 27808 / 114272 | training loss: 0.0004163915873505175\n",
      "epoch: 8 | 27840 / 114272 | training loss: 0.0007775190169923007\n",
      "epoch: 8 | 27872 / 114272 | training loss: 0.0005210048984736204\n",
      "epoch: 8 | 27904 / 114272 | training loss: 0.0008882922120392323\n",
      "epoch: 8 | 27936 / 114272 | training loss: 0.0002625300257932395\n",
      "epoch: 8 | 27968 / 114272 | training loss: 0.11275540292263031\n",
      "epoch: 8 | 28000 / 114272 | training loss: 0.00022197866928763688\n",
      "epoch: 8 | 28032 / 114272 | training loss: 0.021564792841672897\n",
      "epoch: 8 | 28064 / 114272 | training loss: 0.00014947853924240917\n",
      "epoch: 8 | 28096 / 114272 | training loss: 0.00031074241269379854\n",
      "epoch: 8 | 28128 / 114272 | training loss: 0.00021467957412824035\n",
      "epoch: 8 | 28160 / 114272 | training loss: 0.000269396259682253\n",
      "epoch: 8 | 28192 / 114272 | training loss: 0.0001647822355153039\n",
      "epoch: 8 | 28224 / 114272 | training loss: 0.00028292194474488497\n",
      "epoch: 8 | 28256 / 114272 | training loss: 0.00022671493934467435\n",
      "epoch: 8 | 28288 / 114272 | training loss: 0.00040862447349354625\n",
      "epoch: 8 | 28320 / 114272 | training loss: 0.00027703630621545017\n",
      "epoch: 8 | 28352 / 114272 | training loss: 0.0007487196126021445\n",
      "epoch: 8 | 28384 / 114272 | training loss: 0.0003442460729274899\n",
      "epoch: 8 | 28416 / 114272 | training loss: 0.00015918372082524002\n",
      "epoch: 8 | 28448 / 114272 | training loss: 0.0002678397868294269\n",
      "epoch: 8 | 28480 / 114272 | training loss: 0.009467302821576595\n",
      "epoch: 8 | 28512 / 114272 | training loss: 0.00039878528332337737\n",
      "epoch: 8 | 28544 / 114272 | training loss: 0.0003123339847661555\n",
      "epoch: 8 | 28576 / 114272 | training loss: 0.0002220719470642507\n",
      "epoch: 8 | 28608 / 114272 | training loss: 0.00017490741447545588\n",
      "epoch: 8 | 28640 / 114272 | training loss: 0.00017808911798056215\n",
      "epoch: 8 | 28672 / 114272 | training loss: 0.0002419672382529825\n",
      "epoch: 8 | 28704 / 114272 | training loss: 0.00026571357739157975\n",
      "epoch: 8 | 28736 / 114272 | training loss: 0.0004874558071605861\n",
      "epoch: 8 | 28768 / 114272 | training loss: 0.00027995090931653976\n",
      "epoch: 8 | 28800 / 114272 | training loss: 0.0003212340525351465\n",
      "epoch: 8 | 28832 / 114272 | training loss: 0.0003415434912312776\n",
      "epoch: 8 | 28864 / 114272 | training loss: 0.00016611783939879388\n",
      "epoch: 8 | 28896 / 114272 | training loss: 0.0002598249993752688\n",
      "epoch: 8 | 28928 / 114272 | training loss: 0.011914066970348358\n",
      "epoch: 8 | 28960 / 114272 | training loss: 0.001100745052099228\n",
      "epoch: 8 | 28992 / 114272 | training loss: 0.00021150057727936655\n",
      "epoch: 8 | 29024 / 114272 | training loss: 0.00023096866789273918\n",
      "epoch: 8 | 29056 / 114272 | training loss: 0.00043343333527445793\n",
      "epoch: 8 | 29088 / 114272 | training loss: 0.0003356553497724235\n",
      "epoch: 8 | 29120 / 114272 | training loss: 0.00040477458969689906\n",
      "epoch: 8 | 29152 / 114272 | training loss: 0.0002989621425513178\n",
      "epoch: 8 | 29184 / 114272 | training loss: 0.012156943790614605\n",
      "epoch: 8 | 29216 / 114272 | training loss: 0.00025446421932429075\n",
      "epoch: 8 | 29248 / 114272 | training loss: 0.0002682181366253644\n",
      "epoch: 8 | 29280 / 114272 | training loss: 0.264431893825531\n",
      "epoch: 8 | 29312 / 114272 | training loss: 0.0002457485534250736\n",
      "epoch: 8 | 29344 / 114272 | training loss: 0.2114970088005066\n",
      "epoch: 8 | 29376 / 114272 | training loss: 0.00048329707351513207\n",
      "epoch: 8 | 29408 / 114272 | training loss: 0.00026143257855437696\n",
      "epoch: 8 | 29440 / 114272 | training loss: 0.0002263850037707016\n",
      "epoch: 8 | 29472 / 114272 | training loss: 0.00032156528322957456\n",
      "epoch: 8 | 29504 / 114272 | training loss: 0.00020959794346708804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 29536 / 114272 | training loss: 0.0004259443376213312\n",
      "epoch: 8 | 29568 / 114272 | training loss: 0.00016456577577628195\n",
      "epoch: 8 | 29600 / 114272 | training loss: 0.0003084354684688151\n",
      "epoch: 8 | 29632 / 114272 | training loss: 0.00028492408455349505\n",
      "epoch: 8 | 29664 / 114272 | training loss: 0.2295728325843811\n",
      "epoch: 8 | 29696 / 114272 | training loss: 0.0002832985483109951\n",
      "epoch: 8 | 29728 / 114272 | training loss: 0.0002561032888479531\n",
      "epoch: 8 | 29760 / 114272 | training loss: 0.0002055294462479651\n",
      "epoch: 8 | 29792 / 114272 | training loss: 0.00035371186095289886\n",
      "epoch: 8 | 29824 / 114272 | training loss: 0.0002755248860921711\n",
      "epoch: 8 | 29856 / 114272 | training loss: 0.00040884737973101437\n",
      "epoch: 8 | 29888 / 114272 | training loss: 0.00022729496413376182\n",
      "epoch: 8 | 29920 / 114272 | training loss: 0.0002349757996853441\n",
      "epoch: 8 | 29952 / 114272 | training loss: 0.0004758181457873434\n",
      "epoch: 8 | 29984 / 114272 | training loss: 0.00022315207752399147\n",
      "epoch: 8 | 30016 / 114272 | training loss: 0.00036896864185109735\n",
      "epoch: 8 | 30048 / 114272 | training loss: 0.0003467789210844785\n",
      "epoch: 8 | 30080 / 114272 | training loss: 0.00017730075342115015\n",
      "epoch: 8 | 30112 / 114272 | training loss: 0.00011776885366998613\n",
      "epoch: 8 | 30144 / 114272 | training loss: 0.00042191820102743804\n",
      "epoch: 8 | 30176 / 114272 | training loss: 0.0002644639171194285\n",
      "epoch: 8 | 30208 / 114272 | training loss: 0.0003765438450500369\n",
      "epoch: 8 | 30240 / 114272 | training loss: 0.00037219037767499685\n",
      "epoch: 8 | 30272 / 114272 | training loss: 0.00031463164486922324\n",
      "epoch: 8 | 30304 / 114272 | training loss: 0.0003658415807876736\n",
      "epoch: 8 | 30336 / 114272 | training loss: 0.0003970716497860849\n",
      "epoch: 8 | 30368 / 114272 | training loss: 0.00042711669811978936\n",
      "epoch: 8 | 30400 / 114272 | training loss: 0.00032834295416250825\n",
      "epoch: 8 | 30432 / 114272 | training loss: 0.000214675601455383\n",
      "epoch: 8 | 30464 / 114272 | training loss: 0.00022799527505412698\n",
      "epoch: 8 | 30496 / 114272 | training loss: 0.0003158998442813754\n",
      "epoch: 8 | 30528 / 114272 | training loss: 0.00047779103624634445\n",
      "epoch: 8 | 30560 / 114272 | training loss: 0.0003222507075406611\n",
      "epoch: 8 | 30592 / 114272 | training loss: 0.00032196007668972015\n",
      "epoch: 8 | 30624 / 114272 | training loss: 0.0003369827172718942\n",
      "epoch: 8 | 30656 / 114272 | training loss: 0.0003608453553169966\n",
      "epoch: 8 | 30688 / 114272 | training loss: 0.0003241743834223598\n",
      "epoch: 8 | 30720 / 114272 | training loss: 0.0003305570862721652\n",
      "epoch: 8 | 30752 / 114272 | training loss: 0.00041650430648587644\n",
      "epoch: 8 | 30784 / 114272 | training loss: 0.003505338216200471\n",
      "epoch: 8 | 30816 / 114272 | training loss: 0.000210031503229402\n",
      "epoch: 8 | 30848 / 114272 | training loss: 0.16162821650505066\n",
      "epoch: 8 | 30880 / 114272 | training loss: 0.0004997069481760263\n",
      "epoch: 8 | 30912 / 114272 | training loss: 0.03430957347154617\n",
      "epoch: 8 | 30944 / 114272 | training loss: 0.00037526575033552945\n",
      "epoch: 8 | 30976 / 114272 | training loss: 0.0019195623463019729\n",
      "epoch: 8 | 31008 / 114272 | training loss: 0.00034778763074427843\n",
      "epoch: 8 | 31040 / 114272 | training loss: 0.0047699143178761005\n",
      "epoch: 8 | 31072 / 114272 | training loss: 0.00029005244141444564\n",
      "epoch: 8 | 31104 / 114272 | training loss: 0.00020029347797390074\n",
      "epoch: 8 | 31136 / 114272 | training loss: 0.0002361890219617635\n",
      "epoch: 8 | 31168 / 114272 | training loss: 0.00015305205306503922\n",
      "epoch: 8 | 31200 / 114272 | training loss: 0.0005772067233920097\n",
      "epoch: 8 | 31232 / 114272 | training loss: 0.0003371663624420762\n",
      "epoch: 8 | 31264 / 114272 | training loss: 0.00025805740733630955\n",
      "epoch: 8 | 31296 / 114272 | training loss: 0.00033657284802757204\n",
      "epoch: 8 | 31328 / 114272 | training loss: 0.00022055910085327923\n",
      "epoch: 8 | 31360 / 114272 | training loss: 0.0006032714154571295\n",
      "epoch: 8 | 31392 / 114272 | training loss: 0.00028636082424782217\n",
      "epoch: 8 | 31424 / 114272 | training loss: 0.001282755401916802\n",
      "epoch: 8 | 31456 / 114272 | training loss: 0.000308134505758062\n",
      "epoch: 8 | 31488 / 114272 | training loss: 0.0003074454434681684\n",
      "epoch: 8 | 31520 / 114272 | training loss: 0.00029584832373075187\n",
      "epoch: 8 | 31552 / 114272 | training loss: 0.0002493552747182548\n",
      "epoch: 8 | 31584 / 114272 | training loss: 0.0003107866214122623\n",
      "epoch: 8 | 31616 / 114272 | training loss: 0.0006248877034522593\n",
      "epoch: 8 | 31648 / 114272 | training loss: 0.0003176079480908811\n",
      "epoch: 8 | 31680 / 114272 | training loss: 0.0002029070456046611\n",
      "epoch: 8 | 31712 / 114272 | training loss: 0.0003938966547138989\n",
      "epoch: 8 | 31744 / 114272 | training loss: 0.0004051014839205891\n",
      "epoch: 8 | 31776 / 114272 | training loss: 0.0003425172180868685\n",
      "epoch: 8 | 31808 / 114272 | training loss: 0.001095866202376783\n",
      "epoch: 8 | 31840 / 114272 | training loss: 0.00036790387821383774\n",
      "epoch: 8 | 31872 / 114272 | training loss: 0.00018562797049526125\n",
      "epoch: 8 | 31904 / 114272 | training loss: 0.00020653939282055944\n",
      "epoch: 8 | 31936 / 114272 | training loss: 0.0029919997323304415\n",
      "epoch: 8 | 31968 / 114272 | training loss: 0.0003545935614965856\n",
      "epoch: 8 | 32000 / 114272 | training loss: 0.00038877082988619804\n",
      "epoch: 8 | 32032 / 114272 | training loss: 0.0002502355200704187\n",
      "epoch: 8 | 32064 / 114272 | training loss: 0.00028063420904800296\n",
      "epoch: 8 | 32096 / 114272 | training loss: 0.00026475335471332073\n",
      "epoch: 8 | 32128 / 114272 | training loss: 0.0002470678009558469\n",
      "epoch: 8 | 32160 / 114272 | training loss: 0.00034505719668231905\n",
      "epoch: 8 | 32192 / 114272 | training loss: 0.0003500476013869047\n",
      "epoch: 8 | 32224 / 114272 | training loss: 0.0004957286291755736\n",
      "epoch: 8 | 32256 / 114272 | training loss: 0.00023477703507523984\n",
      "epoch: 8 | 32288 / 114272 | training loss: 0.00023973558563739061\n",
      "epoch: 8 | 32320 / 114272 | training loss: 0.00026129710022360086\n",
      "epoch: 8 | 32352 / 114272 | training loss: 0.00039752095472067595\n",
      "epoch: 8 | 32384 / 114272 | training loss: 0.00025962680228985846\n",
      "epoch: 8 | 32416 / 114272 | training loss: 0.0018683489179238677\n",
      "epoch: 8 | 32448 / 114272 | training loss: 0.16277508437633514\n",
      "epoch: 8 | 32480 / 114272 | training loss: 0.00022123193775769323\n",
      "epoch: 8 | 32512 / 114272 | training loss: 0.0002540906425565481\n",
      "epoch: 8 | 32544 / 114272 | training loss: 0.011635605245828629\n",
      "epoch: 8 | 32576 / 114272 | training loss: 0.00020974698418285698\n",
      "epoch: 8 | 32608 / 114272 | training loss: 0.0003877393319271505\n",
      "epoch: 8 | 32640 / 114272 | training loss: 0.0002400664088781923\n",
      "epoch: 8 | 32672 / 114272 | training loss: 0.00041064817924052477\n",
      "epoch: 8 | 32704 / 114272 | training loss: 0.0003252921160310507\n",
      "epoch: 8 | 32736 / 114272 | training loss: 0.0002544675371609628\n",
      "epoch: 8 | 32768 / 114272 | training loss: 0.0003468277573119849\n",
      "epoch: 8 | 32800 / 114272 | training loss: 0.0002342167863389477\n",
      "epoch: 8 | 32832 / 114272 | training loss: 0.14931029081344604\n",
      "epoch: 8 | 32864 / 114272 | training loss: 0.00025754948728717864\n",
      "epoch: 8 | 32896 / 114272 | training loss: 0.1217084601521492\n",
      "epoch: 8 | 32928 / 114272 | training loss: 0.0002424251288175583\n",
      "epoch: 8 | 32960 / 114272 | training loss: 0.00019977563351858407\n",
      "epoch: 8 | 32992 / 114272 | training loss: 0.0003844374150503427\n",
      "epoch: 8 | 33024 / 114272 | training loss: 0.00023759181203786284\n",
      "epoch: 8 | 33056 / 114272 | training loss: 0.00035312643740326166\n",
      "epoch: 8 | 33088 / 114272 | training loss: 0.00017201049195136875\n",
      "epoch: 8 | 33120 / 114272 | training loss: 0.0020290480460971594\n",
      "epoch: 8 | 33152 / 114272 | training loss: 0.0001311245432589203\n",
      "epoch: 8 | 33184 / 114272 | training loss: 0.0002200811286456883\n",
      "epoch: 8 | 33216 / 114272 | training loss: 0.00013561014202423394\n",
      "epoch: 8 | 33248 / 114272 | training loss: 0.0002240482863271609\n",
      "epoch: 8 | 33280 / 114272 | training loss: 0.00018670423014555126\n",
      "epoch: 8 | 33312 / 114272 | training loss: 0.2919260561466217\n",
      "epoch: 8 | 33344 / 114272 | training loss: 0.00032470308360643685\n",
      "epoch: 8 | 33376 / 114272 | training loss: 0.000260461907600984\n",
      "epoch: 8 | 33408 / 114272 | training loss: 0.00030474591767415404\n",
      "epoch: 8 | 33440 / 114272 | training loss: 0.00018958546570502222\n",
      "epoch: 8 | 33472 / 114272 | training loss: 0.05069638043642044\n",
      "epoch: 8 | 33504 / 114272 | training loss: 0.0002529571938794106\n",
      "epoch: 8 | 33536 / 114272 | training loss: 0.0002644877531565726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 33568 / 114272 | training loss: 0.0003120028704870492\n",
      "epoch: 8 | 33600 / 114272 | training loss: 0.0003776602097786963\n",
      "epoch: 8 | 33632 / 114272 | training loss: 0.0023814484011381865\n",
      "epoch: 8 | 33664 / 114272 | training loss: 0.18599465489387512\n",
      "epoch: 8 | 33696 / 114272 | training loss: 0.13458967208862305\n",
      "epoch: 8 | 33728 / 114272 | training loss: 0.0002066776214633137\n",
      "epoch: 8 | 33760 / 114272 | training loss: 0.017924567684531212\n",
      "epoch: 8 | 33792 / 114272 | training loss: 0.00017886208661366254\n",
      "epoch: 8 | 33824 / 114272 | training loss: 0.0003155296144541353\n",
      "epoch: 8 | 33856 / 114272 | training loss: 0.0003510282840579748\n",
      "epoch: 8 | 33888 / 114272 | training loss: 0.0002435104106552899\n",
      "epoch: 8 | 33920 / 114272 | training loss: 0.0002396726922597736\n",
      "epoch: 8 | 33952 / 114272 | training loss: 0.0002238512970507145\n",
      "epoch: 8 | 33984 / 114272 | training loss: 0.0002777395711746067\n",
      "epoch: 8 | 34016 / 114272 | training loss: 0.0008974010706879199\n",
      "epoch: 8 | 34048 / 114272 | training loss: 0.0003682856331579387\n",
      "epoch: 8 | 34080 / 114272 | training loss: 0.0003681457892525941\n",
      "epoch: 8 | 34112 / 114272 | training loss: 0.0002977416734211147\n",
      "epoch: 8 | 34144 / 114272 | training loss: 0.00020280005992390215\n",
      "epoch: 8 | 34176 / 114272 | training loss: 0.00047267289482988417\n",
      "epoch: 8 | 34208 / 114272 | training loss: 0.0009291686001233757\n",
      "epoch: 8 | 34240 / 114272 | training loss: 0.0003760324034374207\n",
      "epoch: 8 | 34272 / 114272 | training loss: 0.0002680343750398606\n",
      "epoch: 8 | 34304 / 114272 | training loss: 0.00019553257152438164\n",
      "epoch: 8 | 34336 / 114272 | training loss: 0.0004008404794149101\n",
      "epoch: 8 | 34368 / 114272 | training loss: 0.0003173631557729095\n",
      "epoch: 8 | 34400 / 114272 | training loss: 0.2807896137237549\n",
      "epoch: 8 | 34432 / 114272 | training loss: 0.00030127266654744744\n",
      "epoch: 8 | 34464 / 114272 | training loss: 0.1763480305671692\n",
      "epoch: 8 | 34496 / 114272 | training loss: 0.0002623956243041903\n",
      "epoch: 8 | 34528 / 114272 | training loss: 0.15057019889354706\n",
      "epoch: 8 | 34560 / 114272 | training loss: 0.0003662913804873824\n",
      "epoch: 8 | 34592 / 114272 | training loss: 0.00018201502098236233\n",
      "epoch: 8 | 34624 / 114272 | training loss: 0.0003188230621162802\n",
      "epoch: 8 | 34656 / 114272 | training loss: 0.00029641573200933635\n",
      "epoch: 8 | 34688 / 114272 | training loss: 0.0003047589852940291\n",
      "epoch: 8 | 34720 / 114272 | training loss: 0.0002220142778242007\n",
      "epoch: 8 | 34752 / 114272 | training loss: 0.0003248617867939174\n",
      "epoch: 8 | 34784 / 114272 | training loss: 0.0008688064990565181\n",
      "epoch: 8 | 34816 / 114272 | training loss: 0.037236981093883514\n",
      "epoch: 8 | 34848 / 114272 | training loss: 0.0004164464771747589\n",
      "epoch: 8 | 34880 / 114272 | training loss: 0.0004909474519081414\n",
      "epoch: 8 | 34912 / 114272 | training loss: 0.00024131214013323188\n",
      "epoch: 8 | 34944 / 114272 | training loss: 0.000348937843227759\n",
      "epoch: 8 | 34976 / 114272 | training loss: 0.0002853567712008953\n",
      "epoch: 8 | 35008 / 114272 | training loss: 0.0004712857189588249\n",
      "epoch: 8 | 35040 / 114272 | training loss: 0.00031374357058666646\n",
      "epoch: 8 | 35072 / 114272 | training loss: 0.0003343618009239435\n",
      "epoch: 8 | 35104 / 114272 | training loss: 0.00044829180114902556\n",
      "epoch: 8 | 35136 / 114272 | training loss: 0.00044341551256366074\n",
      "epoch: 8 | 35168 / 114272 | training loss: 0.000346456712577492\n",
      "epoch: 8 | 35200 / 114272 | training loss: 0.0003613767621573061\n",
      "epoch: 8 | 35232 / 114272 | training loss: 0.00031591043807566166\n",
      "epoch: 8 | 35264 / 114272 | training loss: 0.0003017987182829529\n",
      "epoch: 8 | 35296 / 114272 | training loss: 0.11664193123579025\n",
      "epoch: 8 | 35328 / 114272 | training loss: 0.0007082122028805315\n",
      "epoch: 8 | 35360 / 114272 | training loss: 0.00038712593959644437\n",
      "epoch: 8 | 35392 / 114272 | training loss: 0.0003797831595875323\n",
      "epoch: 8 | 35424 / 114272 | training loss: 0.00031499736360274255\n",
      "epoch: 8 | 35456 / 114272 | training loss: 0.0004513443564064801\n",
      "epoch: 8 | 35488 / 114272 | training loss: 0.0002736978931352496\n",
      "epoch: 8 | 35520 / 114272 | training loss: 0.00027480089920572937\n",
      "epoch: 8 | 35552 / 114272 | training loss: 0.0003458022838458419\n",
      "epoch: 8 | 35584 / 114272 | training loss: 0.00038005862734280527\n",
      "epoch: 8 | 35616 / 114272 | training loss: 0.00027739733923226595\n",
      "epoch: 8 | 35648 / 114272 | training loss: 0.0005767078837379813\n",
      "epoch: 8 | 35680 / 114272 | training loss: 0.36541077494621277\n",
      "epoch: 8 | 35712 / 114272 | training loss: 0.00030181845068000257\n",
      "epoch: 8 | 35744 / 114272 | training loss: 0.13213925063610077\n",
      "epoch: 8 | 35776 / 114272 | training loss: 0.00021646999812219292\n",
      "epoch: 8 | 35808 / 114272 | training loss: 0.0008347922121174634\n",
      "epoch: 8 | 35840 / 114272 | training loss: 0.00020447126007638872\n",
      "epoch: 8 | 35872 / 114272 | training loss: 0.0006174070877023041\n",
      "epoch: 8 | 35904 / 114272 | training loss: 0.0002549928904045373\n",
      "epoch: 8 | 35936 / 114272 | training loss: 0.00041111093014478683\n",
      "epoch: 8 | 35968 / 114272 | training loss: 0.0014130721101537347\n",
      "epoch: 8 | 36000 / 114272 | training loss: 0.0061469897627830505\n",
      "epoch: 8 | 36032 / 114272 | training loss: 0.00032480875961482525\n",
      "epoch: 8 | 36064 / 114272 | training loss: 0.0002328260598005727\n",
      "epoch: 8 | 36096 / 114272 | training loss: 0.00045074589434079826\n",
      "epoch: 8 | 36128 / 114272 | training loss: 0.0004057822225149721\n",
      "epoch: 8 | 36160 / 114272 | training loss: 0.0002871367323677987\n",
      "epoch: 8 | 36192 / 114272 | training loss: 0.00025698586250655353\n",
      "epoch: 8 | 36224 / 114272 | training loss: 0.00026472494937479496\n",
      "epoch: 8 | 36256 / 114272 | training loss: 0.000738716684281826\n",
      "epoch: 8 | 36288 / 114272 | training loss: 0.17654870450496674\n",
      "epoch: 8 | 36320 / 114272 | training loss: 0.00029867247212678194\n",
      "epoch: 8 | 36352 / 114272 | training loss: 0.00031668145675212145\n",
      "epoch: 8 | 36384 / 114272 | training loss: 0.00025872039259411395\n",
      "epoch: 8 | 36416 / 114272 | training loss: 0.00029180358978919685\n",
      "epoch: 8 | 36448 / 114272 | training loss: 0.00014881683455314487\n",
      "epoch: 8 | 36480 / 114272 | training loss: 0.000683153688441962\n",
      "epoch: 8 | 36512 / 114272 | training loss: 0.0005241420585662127\n",
      "epoch: 8 | 36544 / 114272 | training loss: 0.0012503720354288816\n",
      "epoch: 8 | 36576 / 114272 | training loss: 0.00040174147579818964\n",
      "epoch: 8 | 36608 / 114272 | training loss: 0.0010853572748601437\n",
      "epoch: 8 | 36640 / 114272 | training loss: 0.0008546759490855038\n",
      "epoch: 8 | 36672 / 114272 | training loss: 0.000350472197169438\n",
      "epoch: 8 | 36704 / 114272 | training loss: 0.0002692370908334851\n",
      "epoch: 8 | 36736 / 114272 | training loss: 0.01366901583969593\n",
      "epoch: 8 | 36768 / 114272 | training loss: 0.00032662853482179344\n",
      "epoch: 8 | 36800 / 114272 | training loss: 0.0010701346909627318\n",
      "epoch: 8 | 36832 / 114272 | training loss: 0.00031941308407112956\n",
      "epoch: 8 | 36864 / 114272 | training loss: 0.0002682507038116455\n",
      "epoch: 8 | 36896 / 114272 | training loss: 0.0005096651730127633\n",
      "epoch: 8 | 36928 / 114272 | training loss: 0.0004282113222870976\n",
      "epoch: 8 | 36960 / 114272 | training loss: 0.004400467965751886\n",
      "epoch: 8 | 36992 / 114272 | training loss: 0.0002973682130686939\n",
      "epoch: 8 | 37024 / 114272 | training loss: 0.0007042350480332971\n",
      "epoch: 8 | 37056 / 114272 | training loss: 0.0002743311633821577\n",
      "epoch: 8 | 37088 / 114272 | training loss: 0.000873340293765068\n",
      "epoch: 8 | 37120 / 114272 | training loss: 0.001167604117654264\n",
      "epoch: 8 | 37152 / 114272 | training loss: 0.0004653377109207213\n",
      "epoch: 8 | 37184 / 114272 | training loss: 0.00020894056069664657\n",
      "epoch: 8 | 37216 / 114272 | training loss: 0.0004259354609530419\n",
      "epoch: 8 | 37248 / 114272 | training loss: 0.0006863600574433804\n",
      "epoch: 8 | 37280 / 114272 | training loss: 0.0003195469034835696\n",
      "epoch: 8 | 37312 / 114272 | training loss: 0.0007396569708362222\n",
      "epoch: 8 | 37344 / 114272 | training loss: 0.000267765368334949\n",
      "epoch: 8 | 37376 / 114272 | training loss: 0.00026881348458118737\n",
      "epoch: 8 | 37408 / 114272 | training loss: 0.11475842446088791\n",
      "epoch: 8 | 37440 / 114272 | training loss: 0.0002780099166557193\n",
      "epoch: 8 | 37472 / 114272 | training loss: 0.000307641428662464\n",
      "epoch: 8 | 37504 / 114272 | training loss: 0.00024018457042984664\n",
      "epoch: 8 | 37536 / 114272 | training loss: 0.00021118202130310237\n",
      "epoch: 8 | 37568 / 114272 | training loss: 0.0009030774817802012\n",
      "epoch: 8 | 37600 / 114272 | training loss: 0.00037439930019900203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 37632 / 114272 | training loss: 0.00027027231408283114\n",
      "epoch: 8 | 37664 / 114272 | training loss: 0.00027546583442017436\n",
      "epoch: 8 | 37696 / 114272 | training loss: 0.00040755796362645924\n",
      "epoch: 8 | 37728 / 114272 | training loss: 0.0007945543620735407\n",
      "epoch: 8 | 37760 / 114272 | training loss: 0.0006347275339066982\n",
      "epoch: 8 | 37792 / 114272 | training loss: 0.00039255962474271655\n",
      "epoch: 8 | 37824 / 114272 | training loss: 0.00043761098640970886\n",
      "epoch: 8 | 37856 / 114272 | training loss: 0.0003503558982629329\n",
      "epoch: 8 | 37888 / 114272 | training loss: 0.00026039843214675784\n",
      "epoch: 8 | 37920 / 114272 | training loss: 0.00030431064078584313\n",
      "epoch: 8 | 37952 / 114272 | training loss: 0.0009863920276984572\n",
      "epoch: 8 | 37984 / 114272 | training loss: 0.0002766336256172508\n",
      "epoch: 8 | 38016 / 114272 | training loss: 0.0003155735321342945\n",
      "epoch: 8 | 38048 / 114272 | training loss: 0.00027242055512033403\n",
      "epoch: 8 | 38080 / 114272 | training loss: 0.010244947858154774\n",
      "epoch: 8 | 38112 / 114272 | training loss: 0.0024809385649859905\n",
      "epoch: 8 | 38144 / 114272 | training loss: 0.00040294809150509536\n",
      "epoch: 8 | 38176 / 114272 | training loss: 0.00028199763619340956\n",
      "epoch: 8 | 38208 / 114272 | training loss: 0.0007831387920305133\n",
      "epoch: 8 | 38240 / 114272 | training loss: 0.0002753311418928206\n",
      "epoch: 8 | 38272 / 114272 | training loss: 0.0003033160464838147\n",
      "epoch: 8 | 38304 / 114272 | training loss: 0.0006784722791053355\n",
      "epoch: 8 | 38336 / 114272 | training loss: 0.10683323442935944\n",
      "epoch: 8 | 38368 / 114272 | training loss: 0.0001503285311628133\n",
      "epoch: 8 | 38400 / 114272 | training loss: 0.10230102390050888\n",
      "epoch: 8 | 38432 / 114272 | training loss: 0.000254725367994979\n",
      "epoch: 8 | 38464 / 114272 | training loss: 0.00035270373336970806\n",
      "epoch: 8 | 38496 / 114272 | training loss: 0.00022684555733576417\n",
      "epoch: 8 | 38528 / 114272 | training loss: 0.00041415009764023125\n",
      "epoch: 8 | 38560 / 114272 | training loss: 0.0002673171111382544\n",
      "epoch: 8 | 38592 / 114272 | training loss: 0.000286633032374084\n",
      "epoch: 8 | 38624 / 114272 | training loss: 0.00020608749764505774\n",
      "epoch: 8 | 38656 / 114272 | training loss: 0.0004682491417042911\n",
      "epoch: 8 | 38688 / 114272 | training loss: 0.0004674409283325076\n",
      "epoch: 8 | 38720 / 114272 | training loss: 0.00019819106091745198\n",
      "epoch: 8 | 38752 / 114272 | training loss: 0.0002673081180546433\n",
      "epoch: 8 | 38784 / 114272 | training loss: 0.0002554934471845627\n",
      "epoch: 8 | 38816 / 114272 | training loss: 0.00020968099124729633\n",
      "epoch: 8 | 38848 / 114272 | training loss: 0.0002813111641444266\n",
      "epoch: 8 | 38880 / 114272 | training loss: 0.000302086875308305\n",
      "epoch: 8 | 38912 / 114272 | training loss: 0.078732430934906\n",
      "epoch: 8 | 38944 / 114272 | training loss: 0.00027453727670945227\n",
      "epoch: 8 | 38976 / 114272 | training loss: 0.000204956901143305\n",
      "epoch: 8 | 39008 / 114272 | training loss: 0.00038182962452992797\n",
      "epoch: 8 | 39040 / 114272 | training loss: 0.00029613374499604106\n",
      "epoch: 8 | 39072 / 114272 | training loss: 0.1315782368183136\n",
      "epoch: 8 | 39104 / 114272 | training loss: 0.00041932007297873497\n",
      "epoch: 8 | 39136 / 114272 | training loss: 0.00034187480923719704\n",
      "epoch: 8 | 39168 / 114272 | training loss: 0.0003248261637054384\n",
      "epoch: 8 | 39200 / 114272 | training loss: 0.0003668375429697335\n",
      "epoch: 8 | 39232 / 114272 | training loss: 0.0019101682119071484\n",
      "epoch: 8 | 39264 / 114272 | training loss: 0.00022448805975727737\n",
      "epoch: 8 | 39296 / 114272 | training loss: 0.00615357980132103\n",
      "epoch: 8 | 39328 / 114272 | training loss: 0.000941723003052175\n",
      "epoch: 8 | 39360 / 114272 | training loss: 0.00029497663490474224\n",
      "epoch: 8 | 39392 / 114272 | training loss: 0.000941211823374033\n",
      "epoch: 8 | 39424 / 114272 | training loss: 0.00021989441302139312\n",
      "epoch: 8 | 39456 / 114272 | training loss: 0.00028829032089561224\n",
      "epoch: 8 | 39488 / 114272 | training loss: 0.0002722468343563378\n",
      "epoch: 8 | 39520 / 114272 | training loss: 0.00038672226946800947\n",
      "epoch: 8 | 39552 / 114272 | training loss: 0.00030379171948879957\n",
      "epoch: 8 | 39584 / 114272 | training loss: 0.00040508300298824906\n",
      "epoch: 8 | 39616 / 114272 | training loss: 0.0016769798239693046\n",
      "epoch: 8 | 39648 / 114272 | training loss: 0.218709796667099\n",
      "epoch: 8 | 39680 / 114272 | training loss: 0.00020472868345677853\n",
      "epoch: 8 | 39712 / 114272 | training loss: 0.000568336748983711\n",
      "epoch: 8 | 39744 / 114272 | training loss: 0.1292746365070343\n",
      "epoch: 8 | 39776 / 114272 | training loss: 0.00035404751542955637\n",
      "epoch: 8 | 39808 / 114272 | training loss: 0.0017211618833243847\n",
      "epoch: 8 | 39840 / 114272 | training loss: 0.0007939977804198861\n",
      "epoch: 8 | 39872 / 114272 | training loss: 0.0001723099558148533\n",
      "epoch: 8 | 39904 / 114272 | training loss: 0.005027128383517265\n",
      "epoch: 8 | 39936 / 114272 | training loss: 0.00024848440079949796\n",
      "epoch: 8 | 39968 / 114272 | training loss: 0.00027430569753050804\n",
      "epoch: 8 | 40000 / 114272 | training loss: 0.16659463942050934\n",
      "epoch: 8 | 40032 / 114272 | training loss: 0.0002670862013474107\n",
      "epoch: 8 | 40064 / 114272 | training loss: 0.00041756051359698176\n",
      "epoch: 8 | 40096 / 114272 | training loss: 0.0013669604668393731\n",
      "epoch: 8 | 40128 / 114272 | training loss: 0.1923152655363083\n",
      "epoch: 8 | 40160 / 114272 | training loss: 0.0002974078815896064\n",
      "epoch: 8 | 40192 / 114272 | training loss: 0.0003119285393040627\n",
      "epoch: 8 | 40224 / 114272 | training loss: 0.001793589093722403\n",
      "epoch: 8 | 40256 / 114272 | training loss: 0.00019855346181429923\n",
      "epoch: 8 | 40288 / 114272 | training loss: 0.00026744173374027014\n",
      "epoch: 8 | 40320 / 114272 | training loss: 0.0005896291113458574\n",
      "epoch: 8 | 40352 / 114272 | training loss: 0.000395986222429201\n",
      "epoch: 8 | 40384 / 114272 | training loss: 0.000269637064775452\n",
      "epoch: 8 | 40416 / 114272 | training loss: 0.00019180191156920046\n",
      "epoch: 8 | 40448 / 114272 | training loss: 0.0004895489546470344\n",
      "epoch: 8 | 40480 / 114272 | training loss: 0.00036124809412285686\n",
      "epoch: 8 | 40512 / 114272 | training loss: 0.0003675252082757652\n",
      "epoch: 8 | 40544 / 114272 | training loss: 0.00024443012080155313\n",
      "epoch: 8 | 40576 / 114272 | training loss: 0.00026434881146997213\n",
      "epoch: 8 | 40608 / 114272 | training loss: 0.00036056575481779873\n",
      "epoch: 8 | 40640 / 114272 | training loss: 0.2560514211654663\n",
      "epoch: 8 | 40672 / 114272 | training loss: 0.00028038679738529027\n",
      "epoch: 8 | 40704 / 114272 | training loss: 0.0004527404671534896\n",
      "epoch: 8 | 40736 / 114272 | training loss: 0.0003859585849568248\n",
      "epoch: 8 | 40768 / 114272 | training loss: 0.0005488903261721134\n",
      "epoch: 8 | 40800 / 114272 | training loss: 0.0002994589740410447\n",
      "epoch: 8 | 40832 / 114272 | training loss: 0.00043547272798605263\n",
      "epoch: 8 | 40864 / 114272 | training loss: 0.0005261518526822329\n",
      "epoch: 8 | 40896 / 114272 | training loss: 0.00044038609485141933\n",
      "epoch: 8 | 40928 / 114272 | training loss: 0.0002826795098371804\n",
      "epoch: 8 | 40960 / 114272 | training loss: 0.0003174605662934482\n",
      "epoch: 8 | 40992 / 114272 | training loss: 0.00033017114037647843\n",
      "epoch: 8 | 41024 / 114272 | training loss: 0.00033322046510875225\n",
      "epoch: 8 | 41056 / 114272 | training loss: 0.0003623176016844809\n",
      "epoch: 8 | 41088 / 114272 | training loss: 0.0009810480987653136\n",
      "epoch: 8 | 41120 / 114272 | training loss: 0.0003597560280468315\n",
      "epoch: 8 | 41152 / 114272 | training loss: 0.0013160699745640159\n",
      "epoch: 8 | 41184 / 114272 | training loss: 0.0003070636303164065\n",
      "epoch: 8 | 41216 / 114272 | training loss: 0.004939612932503223\n",
      "epoch: 8 | 41248 / 114272 | training loss: 0.0003883449826389551\n",
      "epoch: 8 | 41280 / 114272 | training loss: 0.00044368027010932565\n",
      "epoch: 8 | 41312 / 114272 | training loss: 0.0007246062159538269\n",
      "epoch: 8 | 41344 / 114272 | training loss: 0.0003408322518225759\n",
      "epoch: 8 | 41376 / 114272 | training loss: 0.08046042919158936\n",
      "epoch: 8 | 41408 / 114272 | training loss: 0.00027947634225711226\n",
      "epoch: 8 | 41440 / 114272 | training loss: 0.00039012375054880977\n",
      "epoch: 8 | 41472 / 114272 | training loss: 0.00027320036315359175\n",
      "epoch: 8 | 41504 / 114272 | training loss: 0.0003793228534050286\n",
      "epoch: 8 | 41536 / 114272 | training loss: 0.00039434549398720264\n",
      "epoch: 8 | 41568 / 114272 | training loss: 0.0013490522978827357\n",
      "epoch: 8 | 41600 / 114272 | training loss: 0.00034640904050320387\n",
      "epoch: 8 | 41632 / 114272 | training loss: 0.00021604771609418094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 41664 / 114272 | training loss: 0.00041702674934640527\n",
      "epoch: 8 | 41696 / 114272 | training loss: 0.0005995719693601131\n",
      "epoch: 8 | 41728 / 114272 | training loss: 0.0002659019664861262\n",
      "epoch: 8 | 41760 / 114272 | training loss: 0.04004873335361481\n",
      "epoch: 8 | 41792 / 114272 | training loss: 0.0003103793424088508\n",
      "epoch: 8 | 41824 / 114272 | training loss: 0.00038753185071982443\n",
      "epoch: 8 | 41856 / 114272 | training loss: 0.0002167458296753466\n",
      "epoch: 8 | 41888 / 114272 | training loss: 0.1756410002708435\n",
      "epoch: 8 | 41920 / 114272 | training loss: 0.00047515088226646185\n",
      "epoch: 8 | 41952 / 114272 | training loss: 0.0003576003946363926\n",
      "epoch: 8 | 41984 / 114272 | training loss: 0.00029324114439077675\n",
      "epoch: 8 | 42016 / 114272 | training loss: 0.19545131921768188\n",
      "epoch: 8 | 42048 / 114272 | training loss: 0.0012912345118820667\n",
      "epoch: 8 | 42080 / 114272 | training loss: 0.002137277740985155\n",
      "epoch: 8 | 42112 / 114272 | training loss: 0.00029278863803483546\n",
      "epoch: 8 | 42144 / 114272 | training loss: 0.00047337348223663867\n",
      "epoch: 8 | 42176 / 114272 | training loss: 0.0003059413284063339\n",
      "epoch: 8 | 42208 / 114272 | training loss: 0.00031470260000787675\n",
      "epoch: 8 | 42240 / 114272 | training loss: 0.0003687817952595651\n",
      "epoch: 8 | 42272 / 114272 | training loss: 0.00033516722032800317\n",
      "epoch: 8 | 42304 / 114272 | training loss: 0.0007080135983414948\n",
      "epoch: 8 | 42336 / 114272 | training loss: 0.00020884419791400433\n",
      "epoch: 8 | 42368 / 114272 | training loss: 0.013529714196920395\n",
      "epoch: 8 | 42400 / 114272 | training loss: 0.00025747911422513425\n",
      "epoch: 8 | 42432 / 114272 | training loss: 0.0003240838414058089\n",
      "epoch: 8 | 42464 / 114272 | training loss: 0.21834039688110352\n",
      "epoch: 8 | 42496 / 114272 | training loss: 0.0002815580228343606\n",
      "epoch: 8 | 42528 / 114272 | training loss: 0.000631747068837285\n",
      "epoch: 8 | 42560 / 114272 | training loss: 0.0004954648320563138\n",
      "epoch: 8 | 42592 / 114272 | training loss: 0.0002194330736529082\n",
      "epoch: 8 | 42624 / 114272 | training loss: 0.00030449117184616625\n",
      "epoch: 8 | 42656 / 114272 | training loss: 0.00036161625757813454\n",
      "epoch: 8 | 42688 / 114272 | training loss: 0.0005527919856831431\n",
      "epoch: 8 | 42720 / 114272 | training loss: 0.00040330973570235074\n",
      "epoch: 8 | 42752 / 114272 | training loss: 0.00022656349756289274\n",
      "epoch: 8 | 42784 / 114272 | training loss: 0.0007461484638042748\n",
      "epoch: 8 | 42816 / 114272 | training loss: 0.00053340510930866\n",
      "epoch: 8 | 42848 / 114272 | training loss: 0.0007679704576730728\n",
      "epoch: 8 | 42880 / 114272 | training loss: 0.00029748311499133706\n",
      "epoch: 8 | 42912 / 114272 | training loss: 0.0006524046184495091\n",
      "epoch: 8 | 42944 / 114272 | training loss: 0.0024704181123524904\n",
      "epoch: 8 | 42976 / 114272 | training loss: 0.00032028317218646407\n",
      "epoch: 8 | 43008 / 114272 | training loss: 0.0005669464007951319\n",
      "epoch: 8 | 43040 / 114272 | training loss: 0.00040985457599163055\n",
      "epoch: 8 | 43072 / 114272 | training loss: 0.0003097414446529001\n",
      "epoch: 8 | 43104 / 114272 | training loss: 0.000531383731868118\n",
      "epoch: 8 | 43136 / 114272 | training loss: 0.00036565601476468146\n",
      "epoch: 8 | 43168 / 114272 | training loss: 0.0003626676043495536\n",
      "epoch: 8 | 43200 / 114272 | training loss: 0.05806085467338562\n",
      "epoch: 8 | 43232 / 114272 | training loss: 0.00031497038435190916\n",
      "epoch: 8 | 43264 / 114272 | training loss: 0.0001996086648432538\n",
      "epoch: 8 | 43296 / 114272 | training loss: 0.00020827190019190311\n",
      "epoch: 8 | 43328 / 114272 | training loss: 0.00031680543906986713\n",
      "epoch: 8 | 43360 / 114272 | training loss: 0.00033079113927669823\n",
      "epoch: 8 | 43392 / 114272 | training loss: 0.00040802182047627866\n",
      "epoch: 8 | 43424 / 114272 | training loss: 0.0006666299887001514\n",
      "epoch: 8 | 43456 / 114272 | training loss: 0.0001974628248717636\n",
      "epoch: 8 | 43488 / 114272 | training loss: 0.00023210617655422539\n",
      "epoch: 8 | 43520 / 114272 | training loss: 0.0005717232124879956\n",
      "epoch: 8 | 43552 / 114272 | training loss: 0.0002165029291063547\n",
      "epoch: 8 | 43584 / 114272 | training loss: 0.0003797787649091333\n",
      "epoch: 8 | 43616 / 114272 | training loss: 0.000306654314044863\n",
      "epoch: 8 | 43648 / 114272 | training loss: 0.0002911922347266227\n",
      "epoch: 8 | 43680 / 114272 | training loss: 0.0002930826449301094\n",
      "epoch: 8 | 43712 / 114272 | training loss: 0.00024212815333157778\n",
      "epoch: 8 | 43744 / 114272 | training loss: 0.0005364191019907594\n",
      "epoch: 8 | 43776 / 114272 | training loss: 0.0003240697842556983\n",
      "epoch: 8 | 43808 / 114272 | training loss: 0.00027743997634388506\n",
      "epoch: 8 | 43840 / 114272 | training loss: 0.00037247498403303325\n",
      "epoch: 8 | 43872 / 114272 | training loss: 0.0010470557026565075\n",
      "epoch: 8 | 43904 / 114272 | training loss: 0.0003922482719644904\n",
      "epoch: 8 | 43936 / 114272 | training loss: 0.0006656960467807949\n",
      "epoch: 8 | 43968 / 114272 | training loss: 0.00035475665936246514\n",
      "epoch: 8 | 44000 / 114272 | training loss: 0.000996141228824854\n",
      "epoch: 8 | 44032 / 114272 | training loss: 0.0005001950776204467\n",
      "epoch: 8 | 44064 / 114272 | training loss: 0.00034394548856653273\n",
      "epoch: 8 | 44096 / 114272 | training loss: 0.0003765386645682156\n",
      "epoch: 8 | 44128 / 114272 | training loss: 0.0003055350389331579\n",
      "epoch: 8 | 44160 / 114272 | training loss: 0.0016031204722821712\n",
      "epoch: 8 | 44192 / 114272 | training loss: 0.00035911548184230924\n",
      "epoch: 8 | 44224 / 114272 | training loss: 0.0006112291594035923\n",
      "epoch: 8 | 44256 / 114272 | training loss: 0.00022597797214984894\n",
      "epoch: 8 | 44288 / 114272 | training loss: 0.00034470570972189307\n",
      "epoch: 8 | 44320 / 114272 | training loss: 0.0005990502540953457\n",
      "epoch: 8 | 44352 / 114272 | training loss: 0.000530771620105952\n",
      "epoch: 8 | 44384 / 114272 | training loss: 0.00023586500901728868\n",
      "epoch: 8 | 44416 / 114272 | training loss: 0.00032084458507597446\n",
      "epoch: 8 | 44448 / 114272 | training loss: 0.00035663149901665747\n",
      "epoch: 8 | 44480 / 114272 | training loss: 0.000393399823224172\n",
      "epoch: 8 | 44512 / 114272 | training loss: 0.1905243992805481\n",
      "epoch: 8 | 44544 / 114272 | training loss: 0.0004160971147939563\n",
      "epoch: 8 | 44576 / 114272 | training loss: 0.0004953158204443753\n",
      "epoch: 8 | 44608 / 114272 | training loss: 0.0007995429914444685\n",
      "epoch: 8 | 44640 / 114272 | training loss: 0.0005384666146710515\n",
      "epoch: 8 | 44672 / 114272 | training loss: 0.02905218116939068\n",
      "epoch: 8 | 44704 / 114272 | training loss: 0.0008112945361062884\n",
      "epoch: 8 | 44736 / 114272 | training loss: 0.00033046104363165796\n",
      "epoch: 8 | 44768 / 114272 | training loss: 0.00030316077754832804\n",
      "epoch: 8 | 44800 / 114272 | training loss: 0.00027209706604480743\n",
      "epoch: 8 | 44832 / 114272 | training loss: 0.13220636546611786\n",
      "epoch: 8 | 44864 / 114272 | training loss: 0.00047859695041552186\n",
      "epoch: 8 | 44896 / 114272 | training loss: 0.0002831647580023855\n",
      "epoch: 8 | 44928 / 114272 | training loss: 0.001431541284546256\n",
      "epoch: 8 | 44960 / 114272 | training loss: 0.0007490074494853616\n",
      "epoch: 8 | 44992 / 114272 | training loss: 0.0861562043428421\n",
      "epoch: 8 | 45024 / 114272 | training loss: 0.000554529600776732\n",
      "epoch: 8 | 45056 / 114272 | training loss: 0.0003574826114345342\n",
      "epoch: 8 | 45088 / 114272 | training loss: 0.00038034660974517465\n",
      "epoch: 8 | 45120 / 114272 | training loss: 0.00030961204902268946\n",
      "epoch: 8 | 45152 / 114272 | training loss: 0.00048750342102721334\n",
      "epoch: 8 | 45184 / 114272 | training loss: 0.0003043875622097403\n",
      "epoch: 8 | 45216 / 114272 | training loss: 0.0005039697280153632\n",
      "epoch: 8 | 45248 / 114272 | training loss: 0.21217620372772217\n",
      "epoch: 8 | 45280 / 114272 | training loss: 0.00017003454559016973\n",
      "epoch: 8 | 45312 / 114272 | training loss: 0.0008333719451911747\n",
      "epoch: 8 | 45344 / 114272 | training loss: 0.001247634063474834\n",
      "epoch: 8 | 45376 / 114272 | training loss: 0.00024575681891292334\n",
      "epoch: 8 | 45408 / 114272 | training loss: 0.00020168654737062752\n",
      "epoch: 8 | 45440 / 114272 | training loss: 0.00028884015046060085\n",
      "epoch: 8 | 45472 / 114272 | training loss: 0.0009317121002823114\n",
      "epoch: 8 | 45504 / 114272 | training loss: 0.00043261010432615876\n",
      "epoch: 8 | 45536 / 114272 | training loss: 0.00026457695639692247\n",
      "epoch: 8 | 45568 / 114272 | training loss: 0.0003284730191808194\n",
      "epoch: 8 | 45600 / 114272 | training loss: 0.0004859438631683588\n",
      "epoch: 8 | 45632 / 114272 | training loss: 0.0006219269125722349\n",
      "epoch: 8 | 45664 / 114272 | training loss: 0.0007601065444760025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 45696 / 114272 | training loss: 0.0003766794688999653\n",
      "epoch: 8 | 45728 / 114272 | training loss: 0.000874823541380465\n",
      "epoch: 8 | 45760 / 114272 | training loss: 0.0004251138889230788\n",
      "epoch: 8 | 45792 / 114272 | training loss: 0.0014318455941975117\n",
      "epoch: 8 | 45824 / 114272 | training loss: 0.00023802192299626768\n",
      "epoch: 8 | 45856 / 114272 | training loss: 0.0007964546093717217\n",
      "epoch: 8 | 45888 / 114272 | training loss: 0.0007740053115412593\n",
      "epoch: 8 | 45920 / 114272 | training loss: 0.0015294922050088644\n",
      "epoch: 8 | 45952 / 114272 | training loss: 0.0004026817623525858\n",
      "epoch: 8 | 45984 / 114272 | training loss: 0.0008291044505313039\n",
      "epoch: 8 | 46016 / 114272 | training loss: 0.00045136798871681094\n",
      "epoch: 8 | 46048 / 114272 | training loss: 0.0007219468825496733\n",
      "epoch: 8 | 46080 / 114272 | training loss: 0.0008097547106444836\n",
      "epoch: 8 | 46112 / 114272 | training loss: 0.00030886667082086205\n",
      "epoch: 8 | 46144 / 114272 | training loss: 0.00046295803622342646\n",
      "epoch: 8 | 46176 / 114272 | training loss: 0.001234408700838685\n",
      "epoch: 8 | 46208 / 114272 | training loss: 0.0005781167419627309\n",
      "epoch: 8 | 46240 / 114272 | training loss: 0.00048634823178872466\n",
      "epoch: 8 | 46272 / 114272 | training loss: 0.0004945927648805082\n",
      "epoch: 8 | 46304 / 114272 | training loss: 0.0004115748160984367\n",
      "epoch: 8 | 46336 / 114272 | training loss: 0.0003593264555092901\n",
      "epoch: 8 | 46368 / 114272 | training loss: 0.0021868948824703693\n",
      "epoch: 8 | 46400 / 114272 | training loss: 0.00031605997355654836\n",
      "epoch: 8 | 46432 / 114272 | training loss: 0.000545070564839989\n",
      "epoch: 8 | 46464 / 114272 | training loss: 0.0003176222089678049\n",
      "epoch: 8 | 46496 / 114272 | training loss: 0.0004651615454349667\n",
      "epoch: 8 | 46528 / 114272 | training loss: 0.0005863356636837125\n",
      "epoch: 8 | 46560 / 114272 | training loss: 0.000451423431513831\n",
      "epoch: 8 | 46592 / 114272 | training loss: 0.0004817205772269517\n",
      "epoch: 8 | 46624 / 114272 | training loss: 0.061083100736141205\n",
      "epoch: 8 | 46656 / 114272 | training loss: 0.0007046971586532891\n",
      "epoch: 8 | 46688 / 114272 | training loss: 0.0002926189044956118\n",
      "epoch: 8 | 46720 / 114272 | training loss: 0.00025842204922810197\n",
      "epoch: 8 | 46752 / 114272 | training loss: 0.00027986118220724165\n",
      "epoch: 8 | 46784 / 114272 | training loss: 0.00027801693067885935\n",
      "epoch: 8 | 46816 / 114272 | training loss: 0.0003928887890651822\n",
      "epoch: 8 | 46848 / 114272 | training loss: 0.08587611466646194\n",
      "epoch: 8 | 46880 / 114272 | training loss: 0.00034148170379921794\n",
      "epoch: 8 | 46912 / 114272 | training loss: 0.001072067767381668\n",
      "epoch: 8 | 46944 / 114272 | training loss: 0.0003875120310112834\n",
      "epoch: 8 | 46976 / 114272 | training loss: 0.00055295618949458\n",
      "epoch: 8 | 47008 / 114272 | training loss: 0.00096918671624735\n",
      "epoch: 8 | 47040 / 114272 | training loss: 0.00043132592691108584\n",
      "epoch: 8 | 47072 / 114272 | training loss: 0.00022022622579243034\n",
      "epoch: 8 | 47104 / 114272 | training loss: 0.0003145656664855778\n",
      "epoch: 8 | 47136 / 114272 | training loss: 0.0003026988706551492\n",
      "epoch: 8 | 47168 / 114272 | training loss: 0.1567678302526474\n",
      "epoch: 8 | 47200 / 114272 | training loss: 0.0016867573140189052\n",
      "epoch: 8 | 47232 / 114272 | training loss: 0.00019258954853285104\n",
      "epoch: 8 | 47264 / 114272 | training loss: 0.0004152374458499253\n",
      "epoch: 8 | 47296 / 114272 | training loss: 0.0005473171477206051\n",
      "epoch: 8 | 47328 / 114272 | training loss: 0.0010194978676736355\n",
      "epoch: 8 | 47360 / 114272 | training loss: 0.0003767527232412249\n",
      "epoch: 8 | 47392 / 114272 | training loss: 0.00031416129786521196\n",
      "epoch: 8 | 47424 / 114272 | training loss: 0.00033057405380532146\n",
      "epoch: 8 | 47456 / 114272 | training loss: 0.00037234160117805004\n",
      "epoch: 8 | 47488 / 114272 | training loss: 0.0010747949127107859\n",
      "epoch: 8 | 47520 / 114272 | training loss: 0.0002977915573865175\n",
      "epoch: 8 | 47552 / 114272 | training loss: 0.00045492188655771315\n",
      "epoch: 8 | 47584 / 114272 | training loss: 0.0007972478051669896\n",
      "epoch: 8 | 47616 / 114272 | training loss: 0.07197330892086029\n",
      "epoch: 8 | 47648 / 114272 | training loss: 0.00020865541591774672\n",
      "epoch: 8 | 47680 / 114272 | training loss: 0.00034811923978850245\n",
      "epoch: 8 | 47712 / 114272 | training loss: 0.0012357598170638084\n",
      "epoch: 8 | 47744 / 114272 | training loss: 0.0009772356133908033\n",
      "epoch: 8 | 47776 / 114272 | training loss: 0.0009242216474376619\n",
      "epoch: 8 | 47808 / 114272 | training loss: 0.00026127227465622127\n",
      "epoch: 8 | 47840 / 114272 | training loss: 0.19882552325725555\n",
      "epoch: 8 | 47872 / 114272 | training loss: 0.0007912763976491988\n",
      "epoch: 8 | 47904 / 114272 | training loss: 0.000741913914680481\n",
      "epoch: 8 | 47936 / 114272 | training loss: 0.0004400817269925028\n",
      "epoch: 8 | 47968 / 114272 | training loss: 0.0010006939992308617\n",
      "epoch: 8 | 48000 / 114272 | training loss: 0.00023853873426560313\n",
      "epoch: 8 | 48032 / 114272 | training loss: 0.0011442333925515413\n",
      "epoch: 8 | 48064 / 114272 | training loss: 0.00047611381160095334\n",
      "epoch: 8 | 48096 / 114272 | training loss: 0.0003071050450671464\n",
      "epoch: 8 | 48128 / 114272 | training loss: 0.0008377924095839262\n",
      "epoch: 8 | 48160 / 114272 | training loss: 0.0002963826118502766\n",
      "epoch: 8 | 48192 / 114272 | training loss: 0.0003710903984028846\n",
      "epoch: 8 | 48224 / 114272 | training loss: 0.0028736512176692486\n",
      "epoch: 8 | 48256 / 114272 | training loss: 0.0010461306665092707\n",
      "epoch: 8 | 48288 / 114272 | training loss: 0.14292411506175995\n",
      "epoch: 8 | 48320 / 114272 | training loss: 0.00033819733653217554\n",
      "epoch: 8 | 48352 / 114272 | training loss: 0.0002598791616037488\n",
      "epoch: 8 | 48384 / 114272 | training loss: 0.004641043953597546\n",
      "epoch: 8 | 48416 / 114272 | training loss: 0.0003473378892522305\n",
      "epoch: 8 | 48448 / 114272 | training loss: 0.00036884797737002373\n",
      "epoch: 8 | 48480 / 114272 | training loss: 0.0005513684009201825\n",
      "epoch: 8 | 48512 / 114272 | training loss: 0.0005500754341483116\n",
      "epoch: 8 | 48544 / 114272 | training loss: 0.000344077154295519\n",
      "epoch: 8 | 48576 / 114272 | training loss: 0.1320233941078186\n",
      "epoch: 8 | 48608 / 114272 | training loss: 0.0005362778319977224\n",
      "epoch: 8 | 48640 / 114272 | training loss: 0.0002469893079251051\n",
      "epoch: 8 | 48672 / 114272 | training loss: 0.0015077247517183423\n",
      "epoch: 8 | 48704 / 114272 | training loss: 0.00043079658644273877\n",
      "epoch: 8 | 48736 / 114272 | training loss: 0.00036140461452305317\n",
      "epoch: 8 | 48768 / 114272 | training loss: 0.12491685152053833\n",
      "epoch: 8 | 48800 / 114272 | training loss: 0.0011161884758621454\n",
      "epoch: 8 | 48832 / 114272 | training loss: 0.00028036374715156853\n",
      "epoch: 8 | 48864 / 114272 | training loss: 0.00035000997013412416\n",
      "epoch: 8 | 48896 / 114272 | training loss: 0.00027337786741554737\n",
      "epoch: 8 | 48928 / 114272 | training loss: 0.00028689976898021996\n",
      "epoch: 8 | 48960 / 114272 | training loss: 0.00029787415405735373\n",
      "epoch: 8 | 48992 / 114272 | training loss: 0.0003677766362670809\n",
      "epoch: 8 | 49024 / 114272 | training loss: 0.0003603764344006777\n",
      "epoch: 8 | 49056 / 114272 | training loss: 0.0004927256377413869\n",
      "epoch: 8 | 49088 / 114272 | training loss: 0.0003231037699151784\n",
      "epoch: 8 | 49120 / 114272 | training loss: 0.000351076916558668\n",
      "epoch: 8 | 49152 / 114272 | training loss: 0.00020730600226670504\n",
      "epoch: 8 | 49184 / 114272 | training loss: 0.00044894535676576197\n",
      "epoch: 8 | 49216 / 114272 | training loss: 0.0003643150848802179\n",
      "epoch: 8 | 49248 / 114272 | training loss: 0.0006076768040657043\n",
      "epoch: 8 | 49280 / 114272 | training loss: 0.0006017608102411032\n",
      "epoch: 8 | 49312 / 114272 | training loss: 0.0003092972037848085\n",
      "epoch: 8 | 49344 / 114272 | training loss: 0.00045209366362541914\n",
      "epoch: 8 | 49376 / 114272 | training loss: 0.0004722665180452168\n",
      "epoch: 8 | 49408 / 114272 | training loss: 0.00036109969369135797\n",
      "epoch: 8 | 49440 / 114272 | training loss: 0.000271218188572675\n",
      "epoch: 8 | 49472 / 114272 | training loss: 0.00034194847103208303\n",
      "epoch: 8 | 49504 / 114272 | training loss: 0.00033528232597745955\n",
      "epoch: 8 | 49536 / 114272 | training loss: 0.0006000901921652257\n",
      "epoch: 8 | 49568 / 114272 | training loss: 0.17505037784576416\n",
      "epoch: 8 | 49600 / 114272 | training loss: 0.0038825450465083122\n",
      "epoch: 8 | 49632 / 114272 | training loss: 0.0003743389679584652\n",
      "epoch: 8 | 49664 / 114272 | training loss: 0.00036032620118930936\n",
      "epoch: 8 | 49696 / 114272 | training loss: 0.00022337579866871238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 49728 / 114272 | training loss: 0.0004984531551599503\n",
      "epoch: 8 | 49760 / 114272 | training loss: 0.0005729692056775093\n",
      "epoch: 8 | 49792 / 114272 | training loss: 0.00035559735260903835\n",
      "epoch: 8 | 49824 / 114272 | training loss: 0.00047909043496474624\n",
      "epoch: 8 | 49856 / 114272 | training loss: 0.00039006880251690745\n",
      "epoch: 8 | 49888 / 114272 | training loss: 0.00042308648698963225\n",
      "epoch: 8 | 49920 / 114272 | training loss: 0.0004863112117163837\n",
      "epoch: 8 | 49952 / 114272 | training loss: 0.00033757236087694764\n",
      "epoch: 8 | 49984 / 114272 | training loss: 0.00041086229612119496\n",
      "epoch: 8 | 50016 / 114272 | training loss: 0.00028177033527754247\n",
      "epoch: 8 | 50048 / 114272 | training loss: 0.003021258395165205\n",
      "epoch: 8 | 50080 / 114272 | training loss: 0.0004160495300311595\n",
      "epoch: 8 | 50112 / 114272 | training loss: 0.00023894257901702076\n",
      "epoch: 8 | 50144 / 114272 | training loss: 0.0004207609163131565\n",
      "epoch: 8 | 50176 / 114272 | training loss: 0.0023085451684892178\n",
      "epoch: 8 | 50208 / 114272 | training loss: 0.0005672317929565907\n",
      "epoch: 8 | 50240 / 114272 | training loss: 0.00033795792842283845\n",
      "epoch: 8 | 50272 / 114272 | training loss: 0.00029517573420889676\n",
      "epoch: 8 | 50304 / 114272 | training loss: 0.0007976314518600702\n",
      "epoch: 8 | 50336 / 114272 | training loss: 0.00038055088953115046\n",
      "epoch: 8 | 50368 / 114272 | training loss: 0.00020821113139390945\n",
      "epoch: 8 | 50400 / 114272 | training loss: 0.00025896658189594746\n",
      "epoch: 8 | 50432 / 114272 | training loss: 0.00031911092810332775\n",
      "epoch: 8 | 50464 / 114272 | training loss: 0.00029680924490094185\n",
      "epoch: 8 | 50496 / 114272 | training loss: 0.00030125348712317646\n",
      "epoch: 8 | 50528 / 114272 | training loss: 0.0003202045918442309\n",
      "epoch: 8 | 50560 / 114272 | training loss: 0.00032742999610491097\n",
      "epoch: 8 | 50592 / 114272 | training loss: 0.00031000759918242693\n",
      "epoch: 8 | 50624 / 114272 | training loss: 0.0002820523804984987\n",
      "epoch: 8 | 50656 / 114272 | training loss: 0.00043334229849278927\n",
      "epoch: 8 | 50688 / 114272 | training loss: 0.0005110332858748734\n",
      "epoch: 8 | 50720 / 114272 | training loss: 0.0004625565488822758\n",
      "epoch: 8 | 50752 / 114272 | training loss: 0.00019884828361682594\n",
      "epoch: 8 | 50784 / 114272 | training loss: 0.0003144571674056351\n",
      "epoch: 8 | 50816 / 114272 | training loss: 0.0004056875768583268\n",
      "epoch: 8 | 50848 / 114272 | training loss: 0.03617237135767937\n",
      "epoch: 8 | 50880 / 114272 | training loss: 0.0003492593241389841\n",
      "epoch: 8 | 50912 / 114272 | training loss: 0.0003693134931381792\n",
      "epoch: 8 | 50944 / 114272 | training loss: 0.28475451469421387\n",
      "epoch: 8 | 50976 / 114272 | training loss: 0.0002773744345176965\n",
      "epoch: 8 | 51008 / 114272 | training loss: 0.0007293466478586197\n",
      "epoch: 8 | 51040 / 114272 | training loss: 0.00030445109587162733\n",
      "epoch: 8 | 51072 / 114272 | training loss: 0.0006322023109532893\n",
      "epoch: 8 | 51104 / 114272 | training loss: 0.0004938703496009111\n",
      "epoch: 8 | 51136 / 114272 | training loss: 0.0012136275181546807\n",
      "epoch: 8 | 51168 / 114272 | training loss: 0.0003303751000203192\n",
      "epoch: 8 | 51200 / 114272 | training loss: 0.000328813650412485\n",
      "epoch: 8 | 51232 / 114272 | training loss: 0.0004215315275359899\n",
      "epoch: 8 | 51264 / 114272 | training loss: 0.0005521976854652166\n",
      "epoch: 8 | 51296 / 114272 | training loss: 0.0008660164894536138\n",
      "epoch: 8 | 51328 / 114272 | training loss: 0.0005825925036333501\n",
      "epoch: 8 | 51360 / 114272 | training loss: 0.000637667893897742\n",
      "epoch: 8 | 51392 / 114272 | training loss: 0.00030879562837071717\n",
      "epoch: 8 | 51424 / 114272 | training loss: 0.00032025593100115657\n",
      "epoch: 8 | 51456 / 114272 | training loss: 0.00022359425202012062\n",
      "epoch: 8 | 51488 / 114272 | training loss: 0.00034095588489435613\n",
      "epoch: 8 | 51520 / 114272 | training loss: 0.00040599703788757324\n",
      "epoch: 8 | 51552 / 114272 | training loss: 0.0003991642442997545\n",
      "epoch: 8 | 51584 / 114272 | training loss: 0.00026295354473404586\n",
      "epoch: 8 | 51616 / 114272 | training loss: 0.0005201411549933255\n",
      "epoch: 8 | 51648 / 114272 | training loss: 0.2367369383573532\n",
      "epoch: 8 | 51680 / 114272 | training loss: 0.00030458488618023694\n",
      "epoch: 8 | 51712 / 114272 | training loss: 0.00071236293297261\n",
      "epoch: 8 | 51744 / 114272 | training loss: 0.0009355500806123018\n",
      "epoch: 8 | 51776 / 114272 | training loss: 0.00038008386036381125\n",
      "epoch: 8 | 51808 / 114272 | training loss: 0.0009186078677885234\n",
      "epoch: 8 | 51840 / 114272 | training loss: 0.00042616104474291205\n",
      "epoch: 8 | 51872 / 114272 | training loss: 0.0005524501320905983\n",
      "epoch: 8 | 51904 / 114272 | training loss: 0.0004530719015747309\n",
      "epoch: 8 | 51936 / 114272 | training loss: 0.00038849739939905703\n",
      "epoch: 8 | 51968 / 114272 | training loss: 0.000432511733379215\n",
      "epoch: 8 | 52000 / 114272 | training loss: 0.00048014079220592976\n",
      "epoch: 8 | 52032 / 114272 | training loss: 0.00035454126191325486\n",
      "epoch: 8 | 52064 / 114272 | training loss: 0.00038858718471601605\n",
      "epoch: 8 | 52096 / 114272 | training loss: 0.000551980163436383\n",
      "epoch: 8 | 52128 / 114272 | training loss: 0.0005126916221342981\n",
      "epoch: 8 | 52160 / 114272 | training loss: 0.00047693122178316116\n",
      "epoch: 8 | 52192 / 114272 | training loss: 0.00027926810435019433\n",
      "epoch: 8 | 52224 / 114272 | training loss: 0.0003659121284727007\n",
      "epoch: 8 | 52256 / 114272 | training loss: 0.0005156600964255631\n",
      "epoch: 8 | 52288 / 114272 | training loss: 0.03928950056433678\n",
      "epoch: 8 | 52320 / 114272 | training loss: 0.0005050124018453062\n",
      "epoch: 8 | 52352 / 114272 | training loss: 0.0005270512774586678\n",
      "epoch: 8 | 52384 / 114272 | training loss: 0.0007710835780017078\n",
      "epoch: 8 | 52416 / 114272 | training loss: 0.15315032005310059\n",
      "epoch: 8 | 52448 / 114272 | training loss: 0.001952211488969624\n",
      "epoch: 8 | 52480 / 114272 | training loss: 0.0021159914322197437\n",
      "epoch: 8 | 52512 / 114272 | training loss: 0.0004637328675016761\n",
      "epoch: 8 | 52544 / 114272 | training loss: 0.0008243585471063852\n",
      "epoch: 8 | 52576 / 114272 | training loss: 0.00031264679273590446\n",
      "epoch: 8 | 52608 / 114272 | training loss: 0.0007788205402903259\n",
      "epoch: 8 | 52640 / 114272 | training loss: 0.0005588190979324281\n",
      "epoch: 8 | 52672 / 114272 | training loss: 0.000759705959353596\n",
      "epoch: 8 | 52704 / 114272 | training loss: 0.0006442893063649535\n",
      "epoch: 8 | 52736 / 114272 | training loss: 0.0008875519270077348\n",
      "epoch: 8 | 52768 / 114272 | training loss: 0.0004723966121673584\n",
      "epoch: 8 | 52800 / 114272 | training loss: 0.000557248480618\n",
      "epoch: 8 | 52832 / 114272 | training loss: 0.0004494143940974027\n",
      "epoch: 8 | 52864 / 114272 | training loss: 0.0007563859689980745\n",
      "epoch: 8 | 52896 / 114272 | training loss: 0.24232721328735352\n",
      "epoch: 8 | 52928 / 114272 | training loss: 0.0003904790210071951\n",
      "epoch: 8 | 52960 / 114272 | training loss: 0.00037173074088059366\n",
      "epoch: 8 | 52992 / 114272 | training loss: 0.0008972667856141925\n",
      "epoch: 8 | 53024 / 114272 | training loss: 0.07353056222200394\n",
      "epoch: 8 | 53056 / 114272 | training loss: 0.000414411595556885\n",
      "epoch: 8 | 53088 / 114272 | training loss: 0.0007579928496852517\n",
      "epoch: 8 | 53120 / 114272 | training loss: 0.0009055485716089606\n",
      "epoch: 8 | 53152 / 114272 | training loss: 0.0008264817297458649\n",
      "epoch: 8 | 53184 / 114272 | training loss: 0.0006896615959703922\n",
      "epoch: 8 | 53216 / 114272 | training loss: 0.0007200371474027634\n",
      "epoch: 8 | 53248 / 114272 | training loss: 0.0005399486981332302\n",
      "epoch: 8 | 53280 / 114272 | training loss: 0.000668529886752367\n",
      "epoch: 8 | 53312 / 114272 | training loss: 0.0009856641991063952\n",
      "epoch: 8 | 53344 / 114272 | training loss: 0.0007894857553765178\n",
      "epoch: 8 | 53376 / 114272 | training loss: 0.0009299726807512343\n",
      "epoch: 8 | 53408 / 114272 | training loss: 0.0007756389095447958\n",
      "epoch: 8 | 53440 / 114272 | training loss: 0.0010911304270848632\n",
      "epoch: 8 | 53472 / 114272 | training loss: 0.0008253763662651181\n",
      "epoch: 8 | 53504 / 114272 | training loss: 0.000731395382899791\n",
      "epoch: 8 | 53536 / 114272 | training loss: 0.0005447982694022357\n",
      "epoch: 8 | 53568 / 114272 | training loss: 0.0007002187194302678\n",
      "epoch: 8 | 53600 / 114272 | training loss: 0.0006630142452195287\n",
      "epoch: 8 | 53632 / 114272 | training loss: 0.0009159361943602562\n",
      "epoch: 8 | 53664 / 114272 | training loss: 0.07956123352050781\n",
      "epoch: 8 | 53696 / 114272 | training loss: 0.0010414691641926765\n",
      "epoch: 8 | 53728 / 114272 | training loss: 0.0010480372002348304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 53760 / 114272 | training loss: 0.0012322416296228766\n",
      "epoch: 8 | 53792 / 114272 | training loss: 0.0008459301316179335\n",
      "epoch: 8 | 53824 / 114272 | training loss: 0.0009904270991683006\n",
      "epoch: 8 | 53856 / 114272 | training loss: 0.0005355876637622714\n",
      "epoch: 8 | 53888 / 114272 | training loss: 0.004226836375892162\n",
      "epoch: 8 | 53920 / 114272 | training loss: 0.017899101600050926\n",
      "epoch: 8 | 53952 / 114272 | training loss: 0.0023683926556259394\n",
      "epoch: 8 | 53984 / 114272 | training loss: 0.0017587001202628016\n",
      "epoch: 8 | 54016 / 114272 | training loss: 0.0005747412797063589\n",
      "epoch: 8 | 54048 / 114272 | training loss: 0.000664532883092761\n",
      "epoch: 8 | 54080 / 114272 | training loss: 0.0006400030688382685\n",
      "epoch: 8 | 54112 / 114272 | training loss: 0.0009214156889356673\n",
      "epoch: 8 | 54144 / 114272 | training loss: 0.0005796864861622453\n",
      "epoch: 8 | 54176 / 114272 | training loss: 0.0006302982801571488\n",
      "epoch: 8 | 54208 / 114272 | training loss: 0.11582233756780624\n",
      "epoch: 8 | 54240 / 114272 | training loss: 0.00037566700484603643\n",
      "epoch: 8 | 54272 / 114272 | training loss: 0.000692305329721421\n",
      "epoch: 8 | 54304 / 114272 | training loss: 0.0006294699851423502\n",
      "epoch: 8 | 54336 / 114272 | training loss: 0.0028934774454683065\n",
      "epoch: 8 | 54368 / 114272 | training loss: 0.0007065700483508408\n",
      "epoch: 8 | 54400 / 114272 | training loss: 0.0006354106590151787\n",
      "epoch: 8 | 54432 / 114272 | training loss: 0.0005025188438594341\n",
      "epoch: 8 | 54464 / 114272 | training loss: 0.0004697834374383092\n",
      "epoch: 8 | 54496 / 114272 | training loss: 0.00031968712573871017\n",
      "epoch: 8 | 54528 / 114272 | training loss: 0.000491162936668843\n",
      "epoch: 8 | 54560 / 114272 | training loss: 0.0018802756676450372\n",
      "epoch: 8 | 54592 / 114272 | training loss: 0.0005485169240273535\n",
      "epoch: 8 | 54624 / 114272 | training loss: 0.0003903942124452442\n",
      "epoch: 8 | 54656 / 114272 | training loss: 0.0006426147883757949\n",
      "epoch: 8 | 54688 / 114272 | training loss: 0.0003677725326269865\n",
      "epoch: 8 | 54720 / 114272 | training loss: 0.10292376577854156\n",
      "epoch: 8 | 54752 / 114272 | training loss: 0.00047052782610990107\n",
      "epoch: 8 | 54784 / 114272 | training loss: 0.0003977235173806548\n",
      "epoch: 8 | 54816 / 114272 | training loss: 0.000636599725112319\n",
      "epoch: 8 | 54848 / 114272 | training loss: 0.0005372828454710543\n",
      "epoch: 8 | 54880 / 114272 | training loss: 0.0006275895284488797\n",
      "epoch: 8 | 54912 / 114272 | training loss: 0.0005560021381825209\n",
      "epoch: 8 | 54944 / 114272 | training loss: 0.0004322808817960322\n",
      "epoch: 8 | 54976 / 114272 | training loss: 0.00042934203520417213\n",
      "epoch: 8 | 55008 / 114272 | training loss: 0.0005224651540629566\n",
      "epoch: 8 | 55040 / 114272 | training loss: 0.0008654264966025949\n",
      "epoch: 8 | 55072 / 114272 | training loss: 0.0004796752182301134\n",
      "epoch: 8 | 55104 / 114272 | training loss: 0.0005682947230525315\n",
      "epoch: 8 | 55136 / 114272 | training loss: 0.0006090020760893822\n",
      "epoch: 8 | 55168 / 114272 | training loss: 0.001020912197418511\n",
      "epoch: 8 | 55200 / 114272 | training loss: 0.0004984948318451643\n",
      "epoch: 8 | 55232 / 114272 | training loss: 0.0005758720799349248\n",
      "epoch: 8 | 55264 / 114272 | training loss: 0.0005070585175417364\n",
      "epoch: 8 | 55296 / 114272 | training loss: 0.0016046243254095316\n",
      "epoch: 8 | 55328 / 114272 | training loss: 0.0008057348895817995\n",
      "epoch: 8 | 55360 / 114272 | training loss: 0.0006664366228505969\n",
      "epoch: 8 | 55392 / 114272 | training loss: 0.00033340961090289056\n",
      "epoch: 8 | 55424 / 114272 | training loss: 0.0004386708897072822\n",
      "epoch: 8 | 55456 / 114272 | training loss: 0.0003786048910114914\n",
      "epoch: 8 | 55488 / 114272 | training loss: 0.0005636217538267374\n",
      "epoch: 8 | 55520 / 114272 | training loss: 0.0003005561011377722\n",
      "epoch: 8 | 55552 / 114272 | training loss: 0.00046897202264517546\n",
      "epoch: 8 | 55584 / 114272 | training loss: 0.0004144728882238269\n",
      "epoch: 8 | 55616 / 114272 | training loss: 0.00044987600995227695\n",
      "epoch: 8 | 55648 / 114272 | training loss: 0.00033547068596817553\n",
      "epoch: 8 | 55680 / 114272 | training loss: 0.0002963627630379051\n",
      "epoch: 8 | 55712 / 114272 | training loss: 0.0008539549889974296\n",
      "epoch: 8 | 55744 / 114272 | training loss: 0.0003883980098180473\n",
      "epoch: 8 | 55776 / 114272 | training loss: 0.0003330560284666717\n",
      "epoch: 8 | 55808 / 114272 | training loss: 0.0005607536877505481\n",
      "epoch: 8 | 55840 / 114272 | training loss: 0.0004029762640129775\n",
      "epoch: 8 | 55872 / 114272 | training loss: 0.0003985599905718118\n",
      "epoch: 8 | 55904 / 114272 | training loss: 0.00046426031622104347\n",
      "epoch: 8 | 55936 / 114272 | training loss: 0.0006358383106999099\n",
      "epoch: 8 | 55968 / 114272 | training loss: 0.00042199090239591897\n",
      "epoch: 8 | 56000 / 114272 | training loss: 0.00024283476523123682\n",
      "epoch: 8 | 56032 / 114272 | training loss: 0.0029871154110878706\n",
      "epoch: 8 | 56064 / 114272 | training loss: 0.0005694758729077876\n",
      "epoch: 8 | 56096 / 114272 | training loss: 0.00040935768629424274\n",
      "epoch: 8 | 56128 / 114272 | training loss: 0.00048719820915721357\n",
      "epoch: 8 | 56160 / 114272 | training loss: 0.0004239656263962388\n",
      "epoch: 8 | 56192 / 114272 | training loss: 0.0004179575480520725\n",
      "epoch: 8 | 56224 / 114272 | training loss: 0.00046963038039393723\n",
      "epoch: 8 | 56256 / 114272 | training loss: 0.0004839786561205983\n",
      "epoch: 8 | 56288 / 114272 | training loss: 0.0003743856796063483\n",
      "epoch: 8 | 56320 / 114272 | training loss: 0.0003724941052496433\n",
      "epoch: 8 | 56352 / 114272 | training loss: 0.5160096883773804\n",
      "epoch: 8 | 56384 / 114272 | training loss: 0.0010607161093503237\n",
      "epoch: 8 | 56416 / 114272 | training loss: 0.0004923283704556525\n",
      "epoch: 8 | 56448 / 114272 | training loss: 0.0005223539192229509\n",
      "epoch: 8 | 56480 / 114272 | training loss: 0.00032169363112188876\n",
      "epoch: 8 | 56512 / 114272 | training loss: 0.00043739526881836355\n",
      "epoch: 8 | 56544 / 114272 | training loss: 0.00042836283682845533\n",
      "epoch: 8 | 56576 / 114272 | training loss: 0.016648918390274048\n",
      "epoch: 8 | 56608 / 114272 | training loss: 0.0003768964670598507\n",
      "epoch: 8 | 56640 / 114272 | training loss: 0.0005380997899919748\n",
      "epoch: 8 | 56672 / 114272 | training loss: 0.08704233169555664\n",
      "epoch: 8 | 56704 / 114272 | training loss: 0.0002352628216613084\n",
      "epoch: 8 | 56736 / 114272 | training loss: 0.0003196706238668412\n",
      "epoch: 8 | 56768 / 114272 | training loss: 0.00043097027810290456\n",
      "epoch: 8 | 56800 / 114272 | training loss: 0.00039647522498853505\n",
      "epoch: 8 | 56832 / 114272 | training loss: 0.0003816273820120841\n",
      "epoch: 8 | 56864 / 114272 | training loss: 0.00039708716212771833\n",
      "epoch: 8 | 56896 / 114272 | training loss: 0.0004484757373575121\n",
      "epoch: 8 | 56928 / 114272 | training loss: 0.00038545692223124206\n",
      "epoch: 8 | 56960 / 114272 | training loss: 0.00046674429904669523\n",
      "epoch: 8 | 56992 / 114272 | training loss: 0.0003900135343428701\n",
      "epoch: 8 | 57024 / 114272 | training loss: 0.0004350261588115245\n",
      "epoch: 8 | 57056 / 114272 | training loss: 0.00031989076524041593\n",
      "epoch: 8 | 57088 / 114272 | training loss: 0.0005447798175737262\n",
      "epoch: 8 | 57120 / 114272 | training loss: 0.000505412055645138\n",
      "epoch: 8 | 57152 / 114272 | training loss: 0.0004436679882928729\n",
      "epoch: 8 | 57184 / 114272 | training loss: 0.0004857676976826042\n",
      "epoch: 8 | 57216 / 114272 | training loss: 0.0005908076418563724\n",
      "epoch: 8 | 57248 / 114272 | training loss: 0.00028706592274829745\n",
      "epoch: 8 | 57280 / 114272 | training loss: 0.0017355564050376415\n",
      "epoch: 8 | 57312 / 114272 | training loss: 0.0003807460889220238\n",
      "epoch: 8 | 57344 / 114272 | training loss: 0.00048058133688755333\n",
      "epoch: 8 | 57376 / 114272 | training loss: 0.0003968204546254128\n",
      "epoch: 8 | 57408 / 114272 | training loss: 0.0005152219673618674\n",
      "epoch: 8 | 57440 / 114272 | training loss: 0.0004654538061004132\n",
      "epoch: 8 | 57472 / 114272 | training loss: 0.0004880196647718549\n",
      "epoch: 8 | 57504 / 114272 | training loss: 0.000402131729060784\n",
      "epoch: 8 | 57536 / 114272 | training loss: 0.0002963276638183743\n",
      "epoch: 8 | 57568 / 114272 | training loss: 0.002913043834269047\n",
      "epoch: 8 | 57600 / 114272 | training loss: 0.0003424826427362859\n",
      "epoch: 8 | 57632 / 114272 | training loss: 0.0003509617818053812\n",
      "epoch: 8 | 57664 / 114272 | training loss: 0.0007073984015733004\n",
      "epoch: 8 | 57696 / 114272 | training loss: 0.00034919934114441276\n",
      "epoch: 8 | 57728 / 114272 | training loss: 0.0008902049157768488\n",
      "epoch: 8 | 57760 / 114272 | training loss: 0.0005401662201620638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 57792 / 114272 | training loss: 0.2079067975282669\n",
      "epoch: 8 | 57824 / 114272 | training loss: 0.000990098575130105\n",
      "epoch: 8 | 57856 / 114272 | training loss: 0.0004661727580241859\n",
      "epoch: 8 | 57888 / 114272 | training loss: 0.0003802923019975424\n",
      "epoch: 8 | 57920 / 114272 | training loss: 0.0004120075609534979\n",
      "epoch: 8 | 57952 / 114272 | training loss: 0.0003306593280285597\n",
      "epoch: 8 | 57984 / 114272 | training loss: 0.008570751175284386\n",
      "epoch: 8 | 58016 / 114272 | training loss: 0.0013938077026978135\n",
      "epoch: 8 | 58048 / 114272 | training loss: 0.00038294034311547875\n",
      "epoch: 8 | 58080 / 114272 | training loss: 0.00035764725180342793\n",
      "epoch: 8 | 58112 / 114272 | training loss: 0.0003151714918203652\n",
      "epoch: 8 | 58144 / 114272 | training loss: 0.00030669322586618364\n",
      "epoch: 8 | 58176 / 114272 | training loss: 0.00041310282540507615\n",
      "epoch: 8 | 58208 / 114272 | training loss: 0.000552475918084383\n",
      "epoch: 8 | 58240 / 114272 | training loss: 0.0004640769911929965\n",
      "epoch: 8 | 58272 / 114272 | training loss: 0.000465194956632331\n",
      "epoch: 8 | 58304 / 114272 | training loss: 0.00033226312370970845\n",
      "epoch: 8 | 58336 / 114272 | training loss: 0.0004474405141081661\n",
      "epoch: 8 | 58368 / 114272 | training loss: 0.07041528075933456\n",
      "epoch: 8 | 58400 / 114272 | training loss: 0.0005187048809602857\n",
      "epoch: 8 | 58432 / 114272 | training loss: 0.00047359333257190883\n",
      "epoch: 8 | 58464 / 114272 | training loss: 0.27089470624923706\n",
      "epoch: 8 | 58496 / 114272 | training loss: 0.000388411310268566\n",
      "epoch: 8 | 58528 / 114272 | training loss: 0.09331845492124557\n",
      "epoch: 8 | 58560 / 114272 | training loss: 0.0005028039449825883\n",
      "epoch: 8 | 58592 / 114272 | training loss: 0.0004346752248238772\n",
      "epoch: 8 | 58624 / 114272 | training loss: 0.0005442523397505283\n",
      "epoch: 8 | 58656 / 114272 | training loss: 0.0004786347853951156\n",
      "epoch: 8 | 58688 / 114272 | training loss: 0.05130882188677788\n",
      "epoch: 8 | 58720 / 114272 | training loss: 0.0003977040178142488\n",
      "epoch: 8 | 58752 / 114272 | training loss: 0.00030779835651628673\n",
      "epoch: 8 | 58784 / 114272 | training loss: 0.0016001478070393205\n",
      "epoch: 8 | 58816 / 114272 | training loss: 0.0006302320398390293\n",
      "epoch: 8 | 58848 / 114272 | training loss: 0.000562224886380136\n",
      "epoch: 8 | 58880 / 114272 | training loss: 0.0003501962055452168\n",
      "epoch: 8 | 58912 / 114272 | training loss: 0.011736640706658363\n",
      "epoch: 8 | 58944 / 114272 | training loss: 0.00023397471522912383\n",
      "epoch: 8 | 58976 / 114272 | training loss: 0.000408538879128173\n",
      "epoch: 8 | 59008 / 114272 | training loss: 0.00044946675188839436\n",
      "epoch: 8 | 59040 / 114272 | training loss: 0.00022255786461755633\n",
      "epoch: 8 | 59072 / 114272 | training loss: 0.0006762451375834644\n",
      "epoch: 8 | 59104 / 114272 | training loss: 0.0003819545963779092\n",
      "epoch: 8 | 59136 / 114272 | training loss: 0.01709170825779438\n",
      "epoch: 8 | 59168 / 114272 | training loss: 0.004861005116254091\n",
      "epoch: 8 | 59200 / 114272 | training loss: 0.061133116483688354\n",
      "epoch: 8 | 59232 / 114272 | training loss: 0.00041373822023160756\n",
      "epoch: 8 | 59264 / 114272 | training loss: 0.00046051249955780804\n",
      "epoch: 8 | 59296 / 114272 | training loss: 0.00044682365842163563\n",
      "epoch: 8 | 59328 / 114272 | training loss: 0.00034514861181378365\n",
      "epoch: 8 | 59360 / 114272 | training loss: 0.0005534619558602571\n",
      "epoch: 8 | 59392 / 114272 | training loss: 0.00033337020431645215\n",
      "epoch: 8 | 59424 / 114272 | training loss: 0.00035614316584542394\n",
      "epoch: 8 | 59456 / 114272 | training loss: 0.0003794285876210779\n",
      "epoch: 8 | 59488 / 114272 | training loss: 0.0005773775046691298\n",
      "epoch: 8 | 59520 / 114272 | training loss: 0.00046446939813904464\n",
      "epoch: 8 | 59552 / 114272 | training loss: 0.00030295722535811365\n",
      "epoch: 8 | 59584 / 114272 | training loss: 0.0006501935422420502\n",
      "epoch: 8 | 59616 / 114272 | training loss: 0.0005474392091855407\n",
      "epoch: 8 | 59648 / 114272 | training loss: 0.00044559352681972086\n",
      "epoch: 8 | 59680 / 114272 | training loss: 0.00032653476228006184\n",
      "epoch: 8 | 59712 / 114272 | training loss: 0.0004107432614546269\n",
      "epoch: 8 | 59744 / 114272 | training loss: 0.000309382303385064\n",
      "epoch: 8 | 59776 / 114272 | training loss: 0.00046530377585440874\n",
      "epoch: 8 | 59808 / 114272 | training loss: 0.0003873220703098923\n",
      "epoch: 8 | 59840 / 114272 | training loss: 0.0002621406747493893\n",
      "epoch: 8 | 59872 / 114272 | training loss: 0.0003447519848123193\n",
      "epoch: 8 | 59904 / 114272 | training loss: 0.00037094371509738266\n",
      "epoch: 8 | 59936 / 114272 | training loss: 0.00041778662125580013\n",
      "epoch: 8 | 59968 / 114272 | training loss: 0.0005671312101185322\n",
      "epoch: 8 | 60000 / 114272 | training loss: 0.00027532142121344805\n",
      "epoch: 8 | 60032 / 114272 | training loss: 0.00038139335811138153\n",
      "epoch: 8 | 60064 / 114272 | training loss: 0.0002392716851318255\n",
      "epoch: 8 | 60096 / 114272 | training loss: 0.00036585572524927557\n",
      "epoch: 8 | 60128 / 114272 | training loss: 0.00043978539179079235\n",
      "epoch: 8 | 60160 / 114272 | training loss: 0.00022528143017552793\n",
      "epoch: 8 | 60192 / 114272 | training loss: 0.0003778395475819707\n",
      "epoch: 8 | 60224 / 114272 | training loss: 0.04483326897025108\n",
      "epoch: 8 | 60256 / 114272 | training loss: 0.0004500810173340142\n",
      "epoch: 8 | 60288 / 114272 | training loss: 0.00032405403908342123\n",
      "epoch: 8 | 60320 / 114272 | training loss: 0.0003801383136305958\n",
      "epoch: 8 | 60352 / 114272 | training loss: 0.0006599189946427941\n",
      "epoch: 8 | 60384 / 114272 | training loss: 0.0005383997340686619\n",
      "epoch: 8 | 60416 / 114272 | training loss: 0.0006479601142928004\n",
      "epoch: 8 | 60448 / 114272 | training loss: 0.00022100326896179467\n",
      "epoch: 8 | 60480 / 114272 | training loss: 0.0004112999886274338\n",
      "epoch: 8 | 60512 / 114272 | training loss: 0.011039567179977894\n",
      "epoch: 8 | 60544 / 114272 | training loss: 0.0004615895450115204\n",
      "epoch: 8 | 60576 / 114272 | training loss: 0.00034599180798977613\n",
      "epoch: 8 | 60608 / 114272 | training loss: 0.06305959075689316\n",
      "epoch: 8 | 60640 / 114272 | training loss: 0.00045161868911236525\n",
      "epoch: 8 | 60672 / 114272 | training loss: 0.00024011207278817892\n",
      "epoch: 8 | 60704 / 114272 | training loss: 0.0005389143479987979\n",
      "epoch: 8 | 60736 / 114272 | training loss: 0.00020772700372617692\n",
      "epoch: 8 | 60768 / 114272 | training loss: 0.0004152520268689841\n",
      "epoch: 8 | 60800 / 114272 | training loss: 0.00039419124368578196\n",
      "epoch: 8 | 60832 / 114272 | training loss: 0.0007114002946764231\n",
      "epoch: 8 | 60864 / 114272 | training loss: 0.00035358063178136945\n",
      "epoch: 8 | 60896 / 114272 | training loss: 0.00028008411754854023\n",
      "epoch: 8 | 60928 / 114272 | training loss: 0.344692200422287\n",
      "epoch: 8 | 60960 / 114272 | training loss: 0.0004610012110788375\n",
      "epoch: 8 | 60992 / 114272 | training loss: 0.003176025813445449\n",
      "epoch: 8 | 61024 / 114272 | training loss: 0.00044601011904887855\n",
      "epoch: 8 | 61056 / 114272 | training loss: 0.0003525198553688824\n",
      "epoch: 8 | 61088 / 114272 | training loss: 0.0004816149012185633\n",
      "epoch: 8 | 61120 / 114272 | training loss: 0.0002143481542589143\n",
      "epoch: 8 | 61152 / 114272 | training loss: 0.013375567272305489\n",
      "epoch: 8 | 61184 / 114272 | training loss: 0.0003523039922583848\n",
      "epoch: 8 | 61216 / 114272 | training loss: 0.00038522310205735266\n",
      "epoch: 8 | 61248 / 114272 | training loss: 0.0006329773459583521\n",
      "epoch: 8 | 61280 / 114272 | training loss: 0.0003587458049878478\n",
      "epoch: 8 | 61312 / 114272 | training loss: 0.0002893068885896355\n",
      "epoch: 8 | 61344 / 114272 | training loss: 0.00037330505438148975\n",
      "epoch: 8 | 61376 / 114272 | training loss: 0.007924241945147514\n",
      "epoch: 8 | 61408 / 114272 | training loss: 0.00032668147468939424\n",
      "epoch: 8 | 61440 / 114272 | training loss: 0.00364711694419384\n",
      "epoch: 8 | 61472 / 114272 | training loss: 0.0003579314216040075\n",
      "epoch: 8 | 61504 / 114272 | training loss: 0.00047165033174678683\n",
      "epoch: 8 | 61536 / 114272 | training loss: 0.00026103388518095016\n",
      "epoch: 8 | 61568 / 114272 | training loss: 0.00029519174131564796\n",
      "epoch: 8 | 61600 / 114272 | training loss: 0.0002622762112878263\n",
      "epoch: 8 | 61632 / 114272 | training loss: 0.00042887634481303394\n",
      "epoch: 8 | 61664 / 114272 | training loss: 0.0006350908079184592\n",
      "epoch: 8 | 61696 / 114272 | training loss: 0.0003673400788102299\n",
      "epoch: 8 | 61728 / 114272 | training loss: 0.0002308260736754164\n",
      "epoch: 8 | 61760 / 114272 | training loss: 0.11561834812164307\n",
      "epoch: 8 | 61792 / 114272 | training loss: 0.0004004996735602617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 61824 / 114272 | training loss: 0.017158152535557747\n",
      "epoch: 8 | 61856 / 114272 | training loss: 0.0005820858641527593\n",
      "epoch: 8 | 61888 / 114272 | training loss: 0.000345191772794351\n",
      "epoch: 8 | 61920 / 114272 | training loss: 0.00026398382033221424\n",
      "epoch: 8 | 61952 / 114272 | training loss: 0.00036421441473066807\n",
      "epoch: 8 | 61984 / 114272 | training loss: 0.0003786891757044941\n",
      "epoch: 8 | 62016 / 114272 | training loss: 0.0002714393485803157\n",
      "epoch: 8 | 62048 / 114272 | training loss: 0.00029351175180636346\n",
      "epoch: 8 | 62080 / 114272 | training loss: 0.0003002135781571269\n",
      "epoch: 8 | 62112 / 114272 | training loss: 0.00030008290195837617\n",
      "epoch: 8 | 62144 / 114272 | training loss: 0.12785308063030243\n",
      "epoch: 8 | 62176 / 114272 | training loss: 0.00038468875573016703\n",
      "epoch: 8 | 62208 / 114272 | training loss: 0.000282631313893944\n",
      "epoch: 8 | 62240 / 114272 | training loss: 0.0005416360218077898\n",
      "epoch: 8 | 62272 / 114272 | training loss: 0.0003140040789730847\n",
      "epoch: 8 | 62304 / 114272 | training loss: 0.000817719497717917\n",
      "epoch: 8 | 62336 / 114272 | training loss: 0.00034387686173431575\n",
      "epoch: 8 | 62368 / 114272 | training loss: 0.0005779587081633508\n",
      "epoch: 8 | 62400 / 114272 | training loss: 0.0004179499810561538\n",
      "epoch: 8 | 62432 / 114272 | training loss: 0.00036770320730283856\n",
      "epoch: 8 | 62464 / 114272 | training loss: 0.0003034123801626265\n",
      "epoch: 8 | 62496 / 114272 | training loss: 0.0005295756855048239\n",
      "epoch: 8 | 62528 / 114272 | training loss: 0.00046190962893888354\n",
      "epoch: 8 | 62560 / 114272 | training loss: 0.00046672183088958263\n",
      "epoch: 8 | 62592 / 114272 | training loss: 0.0003699927474372089\n",
      "epoch: 8 | 62624 / 114272 | training loss: 0.0004930990398861468\n",
      "epoch: 8 | 62656 / 114272 | training loss: 0.0005716737359762192\n",
      "epoch: 8 | 62688 / 114272 | training loss: 0.0006339518004097044\n",
      "epoch: 8 | 62720 / 114272 | training loss: 0.16910435259342194\n",
      "epoch: 8 | 62752 / 114272 | training loss: 0.0004240246198605746\n",
      "epoch: 8 | 62784 / 114272 | training loss: 0.00035019172355532646\n",
      "epoch: 8 | 62816 / 114272 | training loss: 0.00032510687015019357\n",
      "epoch: 8 | 62848 / 114272 | training loss: 0.0003662615781649947\n",
      "epoch: 8 | 62880 / 114272 | training loss: 0.0011975874658674002\n",
      "epoch: 8 | 62912 / 114272 | training loss: 0.00041782023617997766\n",
      "epoch: 8 | 62944 / 114272 | training loss: 0.00044243581942282617\n",
      "epoch: 8 | 62976 / 114272 | training loss: 0.000295936013571918\n",
      "epoch: 8 | 63008 / 114272 | training loss: 0.0003953727427870035\n",
      "epoch: 8 | 63040 / 114272 | training loss: 0.00030888334731571376\n",
      "epoch: 8 | 63072 / 114272 | training loss: 0.0003956164582632482\n",
      "epoch: 8 | 63104 / 114272 | training loss: 0.0007954280008561909\n",
      "epoch: 8 | 63136 / 114272 | training loss: 0.0006779952091164887\n",
      "epoch: 8 | 63168 / 114272 | training loss: 0.0012016858672723174\n",
      "epoch: 8 | 63200 / 114272 | training loss: 0.0016355443513020873\n",
      "epoch: 8 | 63232 / 114272 | training loss: 0.000370684516383335\n",
      "epoch: 8 | 63264 / 114272 | training loss: 0.0011163174640387297\n",
      "epoch: 8 | 63296 / 114272 | training loss: 0.0003129589313175529\n",
      "epoch: 8 | 63328 / 114272 | training loss: 0.0003199947823304683\n",
      "epoch: 8 | 63360 / 114272 | training loss: 0.0004648346221074462\n",
      "epoch: 8 | 63392 / 114272 | training loss: 0.00036484605516307056\n",
      "epoch: 8 | 63424 / 114272 | training loss: 0.00024063486489467323\n",
      "epoch: 8 | 63456 / 114272 | training loss: 0.001861115568317473\n",
      "epoch: 8 | 63488 / 114272 | training loss: 0.00036390768946148455\n",
      "epoch: 8 | 63520 / 114272 | training loss: 0.00023712181427981704\n",
      "epoch: 8 | 63552 / 114272 | training loss: 0.00028980025672353804\n",
      "epoch: 8 | 63584 / 114272 | training loss: 0.0002833410690072924\n",
      "epoch: 8 | 63616 / 114272 | training loss: 0.0005539754056371748\n",
      "epoch: 8 | 63648 / 114272 | training loss: 0.0009617602918297052\n",
      "epoch: 8 | 63680 / 114272 | training loss: 0.000378901808289811\n",
      "epoch: 8 | 63712 / 114272 | training loss: 0.0003039546136278659\n",
      "epoch: 8 | 63744 / 114272 | training loss: 0.00020490304450504482\n",
      "epoch: 8 | 63776 / 114272 | training loss: 0.00035972640034742653\n",
      "epoch: 8 | 63808 / 114272 | training loss: 0.22715291380882263\n",
      "epoch: 8 | 63840 / 114272 | training loss: 0.0003655253385659307\n",
      "epoch: 8 | 63872 / 114272 | training loss: 0.004256016109138727\n",
      "epoch: 8 | 63904 / 114272 | training loss: 0.00028006985667161644\n",
      "epoch: 8 | 63936 / 114272 | training loss: 0.00029419182101264596\n",
      "epoch: 8 | 63968 / 114272 | training loss: 0.00026416496257297695\n",
      "epoch: 8 | 64000 / 114272 | training loss: 0.00040181033546105027\n",
      "epoch: 8 | 64032 / 114272 | training loss: 0.0003754018689505756\n",
      "epoch: 8 | 64064 / 114272 | training loss: 0.0003502345352899283\n",
      "epoch: 8 | 64096 / 114272 | training loss: 0.0005233977572061121\n",
      "epoch: 8 | 64128 / 114272 | training loss: 0.0003446114424150437\n",
      "epoch: 8 | 64160 / 114272 | training loss: 0.0006928045768290758\n",
      "epoch: 8 | 64192 / 114272 | training loss: 0.00029894066392444074\n",
      "epoch: 8 | 64224 / 114272 | training loss: 0.0003366369637660682\n",
      "epoch: 8 | 64256 / 114272 | training loss: 0.0004095688054803759\n",
      "epoch: 8 | 64288 / 114272 | training loss: 0.0002987199113704264\n",
      "epoch: 8 | 64320 / 114272 | training loss: 0.0002680628385860473\n",
      "epoch: 8 | 64352 / 114272 | training loss: 0.011023545637726784\n",
      "epoch: 8 | 64384 / 114272 | training loss: 0.00033198174787685275\n",
      "epoch: 8 | 64416 / 114272 | training loss: 0.00037818384589627385\n",
      "epoch: 8 | 64448 / 114272 | training loss: 0.0003768126480281353\n",
      "epoch: 8 | 64480 / 114272 | training loss: 0.0003271325840614736\n",
      "epoch: 8 | 64512 / 114272 | training loss: 0.0002828918513841927\n",
      "epoch: 8 | 64544 / 114272 | training loss: 0.3028216063976288\n",
      "epoch: 8 | 64576 / 114272 | training loss: 0.0004092452290933579\n",
      "epoch: 8 | 64608 / 114272 | training loss: 0.0003368319012224674\n",
      "epoch: 8 | 64640 / 114272 | training loss: 0.00038673958624713123\n",
      "epoch: 8 | 64672 / 114272 | training loss: 0.0007002351339906454\n",
      "epoch: 8 | 64704 / 114272 | training loss: 0.0004218325484544039\n",
      "epoch: 8 | 64736 / 114272 | training loss: 0.0003061406023334712\n",
      "epoch: 8 | 64768 / 114272 | training loss: 0.0006542814080603421\n",
      "epoch: 8 | 64800 / 114272 | training loss: 0.00019367021741345525\n",
      "epoch: 8 | 64832 / 114272 | training loss: 0.00032288720831274986\n",
      "epoch: 8 | 64864 / 114272 | training loss: 0.0003392664948478341\n",
      "epoch: 8 | 64896 / 114272 | training loss: 0.0003243527899030596\n",
      "epoch: 8 | 64928 / 114272 | training loss: 0.0002982674050144851\n",
      "epoch: 8 | 64960 / 114272 | training loss: 0.0003609406412579119\n",
      "epoch: 8 | 64992 / 114272 | training loss: 0.0005690305843017995\n",
      "epoch: 8 | 65024 / 114272 | training loss: 0.0004316766280680895\n",
      "epoch: 8 | 65056 / 114272 | training loss: 0.0003400506975594908\n",
      "epoch: 8 | 65088 / 114272 | training loss: 0.0003451530064921826\n",
      "epoch: 8 | 65120 / 114272 | training loss: 0.0003912370011676103\n",
      "epoch: 8 | 65152 / 114272 | training loss: 0.0319921150803566\n",
      "epoch: 8 | 65184 / 114272 | training loss: 0.09848149120807648\n",
      "epoch: 8 | 65216 / 114272 | training loss: 0.0003797353128902614\n",
      "epoch: 8 | 65248 / 114272 | training loss: 0.0002938179241027683\n",
      "epoch: 8 | 65280 / 114272 | training loss: 0.00027707990375347435\n",
      "epoch: 8 | 65312 / 114272 | training loss: 0.00025233998894691467\n",
      "epoch: 8 | 65344 / 114272 | training loss: 0.00023169719497673213\n",
      "epoch: 8 | 65376 / 114272 | training loss: 0.0003395116073079407\n",
      "epoch: 8 | 65408 / 114272 | training loss: 0.0005974631058052182\n",
      "epoch: 8 | 65440 / 114272 | training loss: 0.00028833781834691763\n",
      "epoch: 8 | 65472 / 114272 | training loss: 0.0002093837538268417\n",
      "epoch: 8 | 65504 / 114272 | training loss: 0.004542839713394642\n",
      "epoch: 8 | 65536 / 114272 | training loss: 0.00013810051314067096\n",
      "epoch: 8 | 65568 / 114272 | training loss: 0.0006841173162683845\n",
      "epoch: 8 | 65600 / 114272 | training loss: 0.002721550175920129\n",
      "epoch: 8 | 65632 / 114272 | training loss: 0.00029598662513308227\n",
      "epoch: 8 | 65664 / 114272 | training loss: 0.00022132243611849844\n",
      "epoch: 8 | 65696 / 114272 | training loss: 0.07875894010066986\n",
      "epoch: 8 | 65728 / 114272 | training loss: 0.0002790941798593849\n",
      "epoch: 8 | 65760 / 114272 | training loss: 0.00044200572301633656\n",
      "epoch: 8 | 65792 / 114272 | training loss: 0.00042583883623592556\n",
      "epoch: 8 | 65824 / 114272 | training loss: 0.00028728708275593817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 65856 / 114272 | training loss: 0.0005261863116174936\n",
      "epoch: 8 | 65888 / 114272 | training loss: 0.0002734229201450944\n",
      "epoch: 8 | 65920 / 114272 | training loss: 0.00029593100771307945\n",
      "epoch: 8 | 65952 / 114272 | training loss: 0.0004687805485446006\n",
      "epoch: 8 | 65984 / 114272 | training loss: 0.04456879198551178\n",
      "epoch: 8 | 66016 / 114272 | training loss: 0.0004123435355722904\n",
      "epoch: 8 | 66048 / 114272 | training loss: 0.0002821660600602627\n",
      "epoch: 8 | 66080 / 114272 | training loss: 0.00024758809013292193\n",
      "epoch: 8 | 66112 / 114272 | training loss: 0.00029979151440784335\n",
      "epoch: 8 | 66144 / 114272 | training loss: 0.0001939512585522607\n",
      "epoch: 8 | 66176 / 114272 | training loss: 0.00018766104767564684\n",
      "epoch: 8 | 66208 / 114272 | training loss: 0.00033484393497928977\n",
      "epoch: 8 | 66240 / 114272 | training loss: 0.00037866082857362926\n",
      "epoch: 8 | 66272 / 114272 | training loss: 0.0004612038319464773\n",
      "epoch: 8 | 66304 / 114272 | training loss: 0.0004242520371917635\n",
      "epoch: 8 | 66336 / 114272 | training loss: 0.0003907880454789847\n",
      "epoch: 8 | 66368 / 114272 | training loss: 0.00021662181825377047\n",
      "epoch: 8 | 66400 / 114272 | training loss: 0.0005110686179250479\n",
      "epoch: 8 | 66432 / 114272 | training loss: 0.00030483133741654456\n",
      "epoch: 8 | 66464 / 114272 | training loss: 0.001110631157644093\n",
      "epoch: 8 | 66496 / 114272 | training loss: 0.00033249444095417857\n",
      "epoch: 8 | 66528 / 114272 | training loss: 0.00034710357431322336\n",
      "epoch: 8 | 66560 / 114272 | training loss: 0.05106307938694954\n",
      "epoch: 8 | 66592 / 114272 | training loss: 0.0014616301050409675\n",
      "epoch: 8 | 66624 / 114272 | training loss: 0.03042743168771267\n",
      "epoch: 8 | 66656 / 114272 | training loss: 0.0003319530515000224\n",
      "epoch: 8 | 66688 / 114272 | training loss: 0.00028331889188848436\n",
      "epoch: 8 | 66720 / 114272 | training loss: 0.016152871772646904\n",
      "epoch: 8 | 66752 / 114272 | training loss: 0.00029718954465352\n",
      "epoch: 8 | 66784 / 114272 | training loss: 0.00031973965815268457\n",
      "epoch: 8 | 66816 / 114272 | training loss: 0.00020762161875609308\n",
      "epoch: 8 | 66848 / 114272 | training loss: 0.00029251715750433505\n",
      "epoch: 8 | 66880 / 114272 | training loss: 0.0004670257621910423\n",
      "epoch: 8 | 66912 / 114272 | training loss: 0.0003527709050104022\n",
      "epoch: 8 | 66944 / 114272 | training loss: 0.0003285351558588445\n",
      "epoch: 8 | 66976 / 114272 | training loss: 0.0002784554963000119\n",
      "epoch: 8 | 67008 / 114272 | training loss: 0.0003562229103408754\n",
      "epoch: 8 | 67040 / 114272 | training loss: 0.000274809543043375\n",
      "epoch: 8 | 67072 / 114272 | training loss: 0.00025326237664557993\n",
      "epoch: 8 | 67104 / 114272 | training loss: 0.0003877241397276521\n",
      "epoch: 8 | 67136 / 114272 | training loss: 0.0003344294091220945\n",
      "epoch: 8 | 67168 / 114272 | training loss: 0.0004277766274753958\n",
      "epoch: 8 | 67200 / 114272 | training loss: 0.00028910429682582617\n",
      "epoch: 8 | 67232 / 114272 | training loss: 0.0002203397743869573\n",
      "epoch: 8 | 67264 / 114272 | training loss: 0.00022798083955422044\n",
      "epoch: 8 | 67296 / 114272 | training loss: 0.030257482081651688\n",
      "epoch: 8 | 67328 / 114272 | training loss: 0.00015163705393206328\n",
      "epoch: 8 | 67360 / 114272 | training loss: 0.0002451726468279958\n",
      "epoch: 8 | 67392 / 114272 | training loss: 0.002197914058342576\n",
      "epoch: 8 | 67424 / 114272 | training loss: 0.0002621315361466259\n",
      "epoch: 8 | 67456 / 114272 | training loss: 0.00023883156245574355\n",
      "epoch: 8 | 67488 / 114272 | training loss: 0.00023361700004898012\n",
      "epoch: 8 | 67520 / 114272 | training loss: 0.0002969666093122214\n",
      "epoch: 8 | 67552 / 114272 | training loss: 0.0002039425598923117\n",
      "epoch: 8 | 67584 / 114272 | training loss: 0.0006570349796675146\n",
      "epoch: 8 | 67616 / 114272 | training loss: 0.00024144478084053844\n",
      "epoch: 8 | 67648 / 114272 | training loss: 0.18583151698112488\n",
      "epoch: 8 | 67680 / 114272 | training loss: 0.0002632237446960062\n",
      "epoch: 8 | 67712 / 114272 | training loss: 0.00022222755069378763\n",
      "epoch: 8 | 67744 / 114272 | training loss: 0.00028314057271927595\n",
      "epoch: 8 | 67776 / 114272 | training loss: 0.00036269647534936666\n",
      "epoch: 8 | 67808 / 114272 | training loss: 0.00024644017685204744\n",
      "epoch: 8 | 67840 / 114272 | training loss: 0.00047549791634082794\n",
      "epoch: 8 | 67872 / 114272 | training loss: 0.08751817792654037\n",
      "epoch: 8 | 67904 / 114272 | training loss: 0.0002321786159882322\n",
      "epoch: 8 | 67936 / 114272 | training loss: 0.0008101140265353024\n",
      "epoch: 8 | 67968 / 114272 | training loss: 0.00025277872919104993\n",
      "epoch: 8 | 68000 / 114272 | training loss: 0.00023721097386442125\n",
      "epoch: 8 | 68032 / 114272 | training loss: 0.0004789065569639206\n",
      "epoch: 8 | 68064 / 114272 | training loss: 0.00029650505166500807\n",
      "epoch: 8 | 68096 / 114272 | training loss: 0.00028279918478801847\n",
      "epoch: 8 | 68128 / 114272 | training loss: 0.13605275750160217\n",
      "epoch: 8 | 68160 / 114272 | training loss: 0.0003852415829896927\n",
      "epoch: 8 | 68192 / 114272 | training loss: 0.00023582929861731827\n",
      "epoch: 8 | 68224 / 114272 | training loss: 0.000718924100510776\n",
      "epoch: 8 | 68256 / 114272 | training loss: 0.0002644743653945625\n",
      "epoch: 8 | 68288 / 114272 | training loss: 0.0004219741385895759\n",
      "epoch: 8 | 68320 / 114272 | training loss: 0.0006544464849866927\n",
      "epoch: 8 | 68352 / 114272 | training loss: 0.0003131601260975003\n",
      "epoch: 8 | 68384 / 114272 | training loss: 0.00043822190491482615\n",
      "epoch: 8 | 68416 / 114272 | training loss: 0.004473305307328701\n",
      "epoch: 8 | 68448 / 114272 | training loss: 0.00022457186423707753\n",
      "epoch: 8 | 68480 / 114272 | training loss: 0.0003505921922624111\n",
      "epoch: 8 | 68512 / 114272 | training loss: 0.0003087735385634005\n",
      "epoch: 8 | 68544 / 114272 | training loss: 0.001380119239911437\n",
      "epoch: 8 | 68576 / 114272 | training loss: 0.00042846103315241635\n",
      "epoch: 8 | 68608 / 114272 | training loss: 0.00024060218129307032\n",
      "epoch: 8 | 68640 / 114272 | training loss: 0.0004273692611604929\n",
      "epoch: 8 | 68672 / 114272 | training loss: 0.08114483207464218\n",
      "epoch: 8 | 68704 / 114272 | training loss: 0.00027814778150059283\n",
      "epoch: 8 | 68736 / 114272 | training loss: 0.018290478736162186\n",
      "epoch: 8 | 68768 / 114272 | training loss: 0.0003804367152042687\n",
      "epoch: 8 | 68800 / 114272 | training loss: 0.00039821615791879594\n",
      "epoch: 8 | 68832 / 114272 | training loss: 0.0007049381965771317\n",
      "epoch: 8 | 68864 / 114272 | training loss: 0.00028200825909152627\n",
      "epoch: 8 | 68896 / 114272 | training loss: 0.0003024052712135017\n",
      "epoch: 8 | 68928 / 114272 | training loss: 0.00029681395972147584\n",
      "epoch: 8 | 68960 / 114272 | training loss: 0.0005251247202977538\n",
      "epoch: 8 | 68992 / 114272 | training loss: 0.00033919091220013797\n",
      "epoch: 8 | 69024 / 114272 | training loss: 0.00020318847964517772\n",
      "epoch: 8 | 69056 / 114272 | training loss: 0.1297512799501419\n",
      "epoch: 8 | 69088 / 114272 | training loss: 0.00017312173440586776\n",
      "epoch: 8 | 69120 / 114272 | training loss: 0.10999932885169983\n",
      "epoch: 8 | 69152 / 114272 | training loss: 0.00031221023527905345\n",
      "epoch: 8 | 69184 / 114272 | training loss: 0.00032207401818595827\n",
      "epoch: 8 | 69216 / 114272 | training loss: 0.0005464257556013763\n",
      "epoch: 8 | 69248 / 114272 | training loss: 0.0017206139164045453\n",
      "epoch: 8 | 69280 / 114272 | training loss: 0.00019905943190678954\n",
      "epoch: 8 | 69312 / 114272 | training loss: 0.0003407698532100767\n",
      "epoch: 8 | 69344 / 114272 | training loss: 0.00027495541144162416\n",
      "epoch: 8 | 69376 / 114272 | training loss: 0.0003298396768514067\n",
      "epoch: 8 | 69408 / 114272 | training loss: 0.00028062559431418777\n",
      "epoch: 8 | 69440 / 114272 | training loss: 0.000278362917015329\n",
      "epoch: 8 | 69472 / 114272 | training loss: 0.00031896508880890906\n",
      "epoch: 8 | 69504 / 114272 | training loss: 0.00034105367376469076\n",
      "epoch: 8 | 69536 / 114272 | training loss: 0.00031122349901124835\n",
      "epoch: 8 | 69568 / 114272 | training loss: 0.0003049153892789036\n",
      "epoch: 8 | 69600 / 114272 | training loss: 0.0005819599027745426\n",
      "epoch: 8 | 69632 / 114272 | training loss: 0.00022275715309660882\n",
      "epoch: 8 | 69664 / 114272 | training loss: 0.00014576534158550203\n",
      "epoch: 8 | 69696 / 114272 | training loss: 0.00024263632076326758\n",
      "epoch: 8 | 69728 / 114272 | training loss: 0.04137187823653221\n",
      "epoch: 8 | 69760 / 114272 | training loss: 0.00018738638027571142\n",
      "epoch: 8 | 69792 / 114272 | training loss: 0.00032328173983842134\n",
      "epoch: 8 | 69824 / 114272 | training loss: 0.0002554455422796309\n",
      "epoch: 8 | 69856 / 114272 | training loss: 0.00027766430866904557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 69888 / 114272 | training loss: 0.00028484477661550045\n",
      "epoch: 8 | 69920 / 114272 | training loss: 0.00022220266691874713\n",
      "epoch: 8 | 69952 / 114272 | training loss: 0.000180790142621845\n",
      "epoch: 8 | 69984 / 114272 | training loss: 0.0005288203246891499\n",
      "epoch: 8 | 70016 / 114272 | training loss: 0.00022537403856404126\n",
      "epoch: 8 | 70048 / 114272 | training loss: 0.0003715544880833477\n",
      "epoch: 8 | 70080 / 114272 | training loss: 0.0002533583901822567\n",
      "epoch: 8 | 70112 / 114272 | training loss: 0.0003424798196647316\n",
      "epoch: 8 | 70144 / 114272 | training loss: 0.0003507602959871292\n",
      "epoch: 8 | 70176 / 114272 | training loss: 0.0012654879828915\n",
      "epoch: 8 | 70208 / 114272 | training loss: 0.00021874369122087955\n",
      "epoch: 8 | 70240 / 114272 | training loss: 0.0002941157144960016\n",
      "epoch: 8 | 70272 / 114272 | training loss: 0.000362548918928951\n",
      "epoch: 8 | 70304 / 114272 | training loss: 0.0002621009771246463\n",
      "epoch: 8 | 70336 / 114272 | training loss: 0.0008597467676736414\n",
      "epoch: 8 | 70368 / 114272 | training loss: 0.0002459157258272171\n",
      "epoch: 8 | 70400 / 114272 | training loss: 0.00021471557556651533\n",
      "epoch: 8 | 70432 / 114272 | training loss: 0.0002142214507330209\n",
      "epoch: 8 | 70464 / 114272 | training loss: 0.0002500067639630288\n",
      "epoch: 8 | 70496 / 114272 | training loss: 0.0004916502512060106\n",
      "epoch: 8 | 70528 / 114272 | training loss: 0.00022548175184056163\n",
      "epoch: 8 | 70560 / 114272 | training loss: 0.00028817501151934266\n",
      "epoch: 8 | 70592 / 114272 | training loss: 0.000247560121351853\n",
      "epoch: 8 | 70624 / 114272 | training loss: 0.0004423482168931514\n",
      "epoch: 8 | 70656 / 114272 | training loss: 0.0002281353808939457\n",
      "epoch: 8 | 70688 / 114272 | training loss: 0.00039177716826088727\n",
      "epoch: 8 | 70720 / 114272 | training loss: 0.00026674551190808415\n",
      "epoch: 8 | 70752 / 114272 | training loss: 0.0003994137514382601\n",
      "epoch: 8 | 70784 / 114272 | training loss: 0.0002402682730462402\n",
      "epoch: 8 | 70816 / 114272 | training loss: 0.00022362111485563219\n",
      "epoch: 8 | 70848 / 114272 | training loss: 0.00033591175451874733\n",
      "epoch: 8 | 70880 / 114272 | training loss: 0.20011530816555023\n",
      "epoch: 8 | 70912 / 114272 | training loss: 0.1927146017551422\n",
      "epoch: 8 | 70944 / 114272 | training loss: 0.000524248112924397\n",
      "epoch: 8 | 70976 / 114272 | training loss: 0.00017947267042472959\n",
      "epoch: 8 | 71008 / 114272 | training loss: 0.0001821070909500122\n",
      "epoch: 8 | 71040 / 114272 | training loss: 0.00038994383066892624\n",
      "epoch: 8 | 71072 / 114272 | training loss: 0.00029688762151636183\n",
      "epoch: 8 | 71104 / 114272 | training loss: 0.0002825957490131259\n",
      "epoch: 8 | 71136 / 114272 | training loss: 0.00018298470240551978\n",
      "epoch: 8 | 71168 / 114272 | training loss: 0.0014130728086456656\n",
      "epoch: 8 | 71200 / 114272 | training loss: 0.21859417855739594\n",
      "epoch: 8 | 71232 / 114272 | training loss: 0.00037805535248480737\n",
      "epoch: 8 | 71264 / 114272 | training loss: 0.00033253623405471444\n",
      "epoch: 8 | 71296 / 114272 | training loss: 0.0002209662925451994\n",
      "epoch: 8 | 71328 / 114272 | training loss: 0.00020744830544572324\n",
      "epoch: 8 | 71360 / 114272 | training loss: 0.00028101750649511814\n",
      "epoch: 8 | 71392 / 114272 | training loss: 0.0003859687130898237\n",
      "epoch: 8 | 71424 / 114272 | training loss: 0.0011037916410714388\n",
      "epoch: 8 | 71456 / 114272 | training loss: 0.0005871413741260767\n",
      "epoch: 8 | 71488 / 114272 | training loss: 0.0002753129811026156\n",
      "epoch: 8 | 71520 / 114272 | training loss: 0.00037421108572743833\n",
      "epoch: 8 | 71552 / 114272 | training loss: 0.00026504939887672663\n",
      "epoch: 8 | 71584 / 114272 | training loss: 0.00026457424974069\n",
      "epoch: 8 | 71616 / 114272 | training loss: 0.2380126565694809\n",
      "epoch: 8 | 71648 / 114272 | training loss: 0.00033000987605191767\n",
      "epoch: 8 | 71680 / 114272 | training loss: 0.00024267756089102477\n",
      "epoch: 8 | 71712 / 114272 | training loss: 0.00041021237848326564\n",
      "epoch: 8 | 71744 / 114272 | training loss: 0.022939717397093773\n",
      "epoch: 8 | 71776 / 114272 | training loss: 0.00036854814970865846\n",
      "epoch: 8 | 71808 / 114272 | training loss: 0.00037510308902710676\n",
      "epoch: 8 | 71840 / 114272 | training loss: 0.0004593839403241873\n",
      "epoch: 8 | 71872 / 114272 | training loss: 0.0003683064423967153\n",
      "epoch: 8 | 71904 / 114272 | training loss: 0.0002509365731384605\n",
      "epoch: 8 | 71936 / 114272 | training loss: 0.0002767077530734241\n",
      "epoch: 8 | 71968 / 114272 | training loss: 0.0005576765979640186\n",
      "epoch: 8 | 72000 / 114272 | training loss: 0.0005185998161323369\n",
      "epoch: 8 | 72032 / 114272 | training loss: 0.0003823789593297988\n",
      "epoch: 8 | 72064 / 114272 | training loss: 0.05650177597999573\n",
      "epoch: 8 | 72096 / 114272 | training loss: 0.00020034154294990003\n",
      "epoch: 8 | 72128 / 114272 | training loss: 0.00023689695808570832\n",
      "epoch: 8 | 72160 / 114272 | training loss: 0.0010549958096817136\n",
      "epoch: 8 | 72192 / 114272 | training loss: 0.0003157402388751507\n",
      "epoch: 8 | 72224 / 114272 | training loss: 0.00038500240771099925\n",
      "epoch: 8 | 72256 / 114272 | training loss: 0.0003001375589519739\n",
      "epoch: 8 | 72288 / 114272 | training loss: 0.00026163883740082383\n",
      "epoch: 8 | 72320 / 114272 | training loss: 0.000396802177419886\n",
      "epoch: 8 | 72352 / 114272 | training loss: 0.00035398127511143684\n",
      "epoch: 8 | 72384 / 114272 | training loss: 0.0005514243384823203\n",
      "epoch: 8 | 72416 / 114272 | training loss: 0.0002709479595068842\n",
      "epoch: 8 | 72448 / 114272 | training loss: 0.00028337296680547297\n",
      "epoch: 8 | 72480 / 114272 | training loss: 0.00036280302447266877\n",
      "epoch: 8 | 72512 / 114272 | training loss: 0.00035066221607849\n",
      "epoch: 8 | 72544 / 114272 | training loss: 0.005750175099819899\n",
      "epoch: 8 | 72576 / 114272 | training loss: 0.0002464782737661153\n",
      "epoch: 8 | 72608 / 114272 | training loss: 0.0010988815920427442\n",
      "epoch: 8 | 72640 / 114272 | training loss: 0.0002219255839008838\n",
      "epoch: 8 | 72672 / 114272 | training loss: 0.0003704755799844861\n",
      "epoch: 8 | 72704 / 114272 | training loss: 0.00021951539383735508\n",
      "epoch: 8 | 72736 / 114272 | training loss: 0.0003060055314563215\n",
      "epoch: 8 | 72768 / 114272 | training loss: 0.0002499893889762461\n",
      "epoch: 8 | 72800 / 114272 | training loss: 0.00018054191605187953\n",
      "epoch: 8 | 72832 / 114272 | training loss: 0.0003183636872563511\n",
      "epoch: 8 | 72864 / 114272 | training loss: 0.000321078667184338\n",
      "epoch: 8 | 72896 / 114272 | training loss: 0.0006489781080745161\n",
      "epoch: 8 | 72928 / 114272 | training loss: 0.0007330570952035487\n",
      "epoch: 8 | 72960 / 114272 | training loss: 0.014081896282732487\n",
      "epoch: 8 | 72992 / 114272 | training loss: 0.0003732819459401071\n",
      "epoch: 8 | 73024 / 114272 | training loss: 0.00025922851637005806\n",
      "epoch: 8 | 73056 / 114272 | training loss: 0.1987365484237671\n",
      "epoch: 8 | 73088 / 114272 | training loss: 0.00017031973402481526\n",
      "epoch: 8 | 73120 / 114272 | training loss: 0.00025477740564383566\n",
      "epoch: 8 | 73152 / 114272 | training loss: 0.00030194755527190864\n",
      "epoch: 8 | 73184 / 114272 | training loss: 0.00045618604053743184\n",
      "epoch: 8 | 73216 / 114272 | training loss: 0.00024817336816340685\n",
      "epoch: 8 | 73248 / 114272 | training loss: 0.0002533779479563236\n",
      "epoch: 8 | 73280 / 114272 | training loss: 0.00029263991746120155\n",
      "epoch: 8 | 73312 / 114272 | training loss: 0.00023923287517391145\n",
      "epoch: 8 | 73344 / 114272 | training loss: 0.0003533742274157703\n",
      "epoch: 8 | 73376 / 114272 | training loss: 0.0003292426699772477\n",
      "epoch: 8 | 73408 / 114272 | training loss: 0.0005074644577689469\n",
      "epoch: 8 | 73440 / 114272 | training loss: 0.0004553223552647978\n",
      "epoch: 8 | 73472 / 114272 | training loss: 0.0002665486535988748\n",
      "epoch: 8 | 73504 / 114272 | training loss: 0.00035583553835749626\n",
      "epoch: 8 | 73536 / 114272 | training loss: 0.0003240955120418221\n",
      "epoch: 8 | 73568 / 114272 | training loss: 0.00025303184520453215\n",
      "epoch: 8 | 73600 / 114272 | training loss: 0.0011698800371959805\n",
      "epoch: 8 | 73632 / 114272 | training loss: 0.0002920970437116921\n",
      "epoch: 8 | 73664 / 114272 | training loss: 0.0001535276387585327\n",
      "epoch: 8 | 73696 / 114272 | training loss: 0.0001795074640540406\n",
      "epoch: 8 | 73728 / 114272 | training loss: 0.0013376324204728007\n",
      "epoch: 8 | 73760 / 114272 | training loss: 0.0002571191289462149\n",
      "epoch: 8 | 73792 / 114272 | training loss: 0.0003041495510842651\n",
      "epoch: 8 | 73824 / 114272 | training loss: 0.00030885625164955854\n",
      "epoch: 8 | 73856 / 114272 | training loss: 0.0002844184054993093\n",
      "epoch: 8 | 73888 / 114272 | training loss: 0.00022585439728572965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 73920 / 114272 | training loss: 0.00022074051958043128\n",
      "epoch: 8 | 73952 / 114272 | training loss: 0.0002453167107887566\n",
      "epoch: 8 | 73984 / 114272 | training loss: 0.00015560215979348868\n",
      "epoch: 8 | 74016 / 114272 | training loss: 0.00018921060836873949\n",
      "epoch: 8 | 74048 / 114272 | training loss: 0.00020082668925169855\n",
      "epoch: 8 | 74080 / 114272 | training loss: 0.0003973559651058167\n",
      "epoch: 8 | 74112 / 114272 | training loss: 0.00026493804762139916\n",
      "epoch: 8 | 74144 / 114272 | training loss: 0.11595135927200317\n",
      "epoch: 8 | 74176 / 114272 | training loss: 0.00020393672457430512\n",
      "epoch: 8 | 74208 / 114272 | training loss: 0.00030032815993763506\n",
      "epoch: 8 | 74240 / 114272 | training loss: 0.00034040395985357463\n",
      "epoch: 8 | 74272 / 114272 | training loss: 0.00020147494797129184\n",
      "epoch: 8 | 74304 / 114272 | training loss: 0.00030244505614973605\n",
      "epoch: 8 | 74336 / 114272 | training loss: 0.00014603229647036642\n",
      "epoch: 8 | 74368 / 114272 | training loss: 0.00035050100996159017\n",
      "epoch: 8 | 74400 / 114272 | training loss: 0.00020838949421886355\n",
      "epoch: 8 | 74432 / 114272 | training loss: 0.00020463773398660123\n",
      "epoch: 8 | 74464 / 114272 | training loss: 0.12156983464956284\n",
      "epoch: 8 | 74496 / 114272 | training loss: 0.00029538871604017913\n",
      "epoch: 8 | 74528 / 114272 | training loss: 0.00019660027464851737\n",
      "epoch: 8 | 74560 / 114272 | training loss: 0.00026344237267039716\n",
      "epoch: 8 | 74592 / 114272 | training loss: 0.00022838303993921727\n",
      "epoch: 8 | 74624 / 114272 | training loss: 0.00020485381537582725\n",
      "epoch: 8 | 74656 / 114272 | training loss: 0.0003409174387343228\n",
      "epoch: 8 | 74688 / 114272 | training loss: 0.0002642300969455391\n",
      "epoch: 8 | 74720 / 114272 | training loss: 0.000187802710570395\n",
      "epoch: 8 | 74752 / 114272 | training loss: 0.010732652619481087\n",
      "epoch: 8 | 74784 / 114272 | training loss: 0.0002742272336035967\n",
      "epoch: 8 | 74816 / 114272 | training loss: 0.00023313287238124758\n",
      "epoch: 8 | 74848 / 114272 | training loss: 0.00014173187082633376\n",
      "epoch: 8 | 74880 / 114272 | training loss: 0.0002900637628044933\n",
      "epoch: 8 | 74912 / 114272 | training loss: 0.0002711103588808328\n",
      "epoch: 8 | 74944 / 114272 | training loss: 0.09697434306144714\n",
      "epoch: 8 | 74976 / 114272 | training loss: 0.00034415663685649633\n",
      "epoch: 8 | 75008 / 114272 | training loss: 0.0004044669331051409\n",
      "epoch: 8 | 75040 / 114272 | training loss: 0.0002741459757089615\n",
      "epoch: 8 | 75072 / 114272 | training loss: 0.00028317084070295095\n",
      "epoch: 8 | 75104 / 114272 | training loss: 0.024748150259256363\n",
      "epoch: 8 | 75136 / 114272 | training loss: 0.0001751444215187803\n",
      "epoch: 8 | 75168 / 114272 | training loss: 0.0003608246915973723\n",
      "epoch: 8 | 75200 / 114272 | training loss: 0.03681366890668869\n",
      "epoch: 8 | 75232 / 114272 | training loss: 0.016689864918589592\n",
      "epoch: 8 | 75264 / 114272 | training loss: 0.00041739732841961086\n",
      "epoch: 8 | 75296 / 114272 | training loss: 0.1469176858663559\n",
      "epoch: 8 | 75328 / 114272 | training loss: 0.0003521385951898992\n",
      "epoch: 8 | 75360 / 114272 | training loss: 0.00016962025256361812\n",
      "epoch: 8 | 75392 / 114272 | training loss: 0.001308468054048717\n",
      "epoch: 8 | 75424 / 114272 | training loss: 0.0003088254889007658\n",
      "epoch: 8 | 75456 / 114272 | training loss: 0.0004041848296765238\n",
      "epoch: 8 | 75488 / 114272 | training loss: 0.0003407272160984576\n",
      "epoch: 8 | 75520 / 114272 | training loss: 0.0002711871056817472\n",
      "epoch: 8 | 75552 / 114272 | training loss: 0.00019302677537780255\n",
      "epoch: 8 | 75584 / 114272 | training loss: 0.00025952444411814213\n",
      "epoch: 8 | 75616 / 114272 | training loss: 0.0002722852514125407\n",
      "epoch: 8 | 75648 / 114272 | training loss: 0.00027169578243047\n",
      "epoch: 8 | 75680 / 114272 | training loss: 0.0003236421325709671\n",
      "epoch: 8 | 75712 / 114272 | training loss: 0.0002373834140598774\n",
      "epoch: 8 | 75744 / 114272 | training loss: 0.00019882978813257068\n",
      "epoch: 8 | 75776 / 114272 | training loss: 0.0005754932062700391\n",
      "epoch: 8 | 75808 / 114272 | training loss: 0.00030760338995605707\n",
      "epoch: 8 | 75840 / 114272 | training loss: 0.0005062012351118028\n",
      "epoch: 8 | 75872 / 114272 | training loss: 0.0002850824093911797\n",
      "epoch: 8 | 75904 / 114272 | training loss: 0.0013483816292136908\n",
      "epoch: 8 | 75936 / 114272 | training loss: 0.0003109601093456149\n",
      "epoch: 8 | 75968 / 114272 | training loss: 0.0004003492940682918\n",
      "epoch: 8 | 76000 / 114272 | training loss: 0.00029428693233057857\n",
      "epoch: 8 | 76032 / 114272 | training loss: 0.0007657555979676545\n",
      "epoch: 8 | 76064 / 114272 | training loss: 0.00022093190636951476\n",
      "epoch: 8 | 76096 / 114272 | training loss: 0.00033665046794340014\n",
      "epoch: 8 | 76128 / 114272 | training loss: 0.0003084709169343114\n",
      "epoch: 8 | 76160 / 114272 | training loss: 0.00026139989495277405\n",
      "epoch: 8 | 76192 / 114272 | training loss: 0.0002195577835664153\n",
      "epoch: 8 | 76224 / 114272 | training loss: 0.0002824775001499802\n",
      "epoch: 8 | 76256 / 114272 | training loss: 0.0002374488685745746\n",
      "epoch: 8 | 76288 / 114272 | training loss: 0.00020595622481778264\n",
      "epoch: 8 | 76320 / 114272 | training loss: 0.00032153353095054626\n",
      "epoch: 8 | 76352 / 114272 | training loss: 0.0003743452543858439\n",
      "epoch: 8 | 76384 / 114272 | training loss: 0.0002953929943032563\n",
      "epoch: 8 | 76416 / 114272 | training loss: 0.00019810751837212592\n",
      "epoch: 8 | 76448 / 114272 | training loss: 0.00030567290377803147\n",
      "epoch: 8 | 76480 / 114272 | training loss: 0.0002330116112716496\n",
      "epoch: 8 | 76512 / 114272 | training loss: 0.00023308054369408637\n",
      "epoch: 8 | 76544 / 114272 | training loss: 0.00015789337339811027\n",
      "epoch: 8 | 76576 / 114272 | training loss: 0.00021311777527444065\n",
      "epoch: 8 | 76608 / 114272 | training loss: 0.0002258623280795291\n",
      "epoch: 8 | 76640 / 114272 | training loss: 0.0003177648177370429\n",
      "epoch: 8 | 76672 / 114272 | training loss: 0.0003777198726311326\n",
      "epoch: 8 | 76704 / 114272 | training loss: 0.038608748465776443\n",
      "epoch: 8 | 76736 / 114272 | training loss: 0.00015718708164058626\n",
      "epoch: 8 | 76768 / 114272 | training loss: 0.00015306365094147623\n",
      "epoch: 8 | 76800 / 114272 | training loss: 0.00017063853738363832\n",
      "epoch: 8 | 76832 / 114272 | training loss: 0.0005718411412090063\n",
      "epoch: 8 | 76864 / 114272 | training loss: 0.19230926036834717\n",
      "epoch: 8 | 76896 / 114272 | training loss: 0.00033491075737401843\n",
      "epoch: 8 | 76928 / 114272 | training loss: 0.03583994507789612\n",
      "epoch: 8 | 76960 / 114272 | training loss: 0.000220094108954072\n",
      "epoch: 8 | 76992 / 114272 | training loss: 0.00017743388889357448\n",
      "epoch: 8 | 77024 / 114272 | training loss: 0.00029330491088330746\n",
      "epoch: 8 | 77056 / 114272 | training loss: 0.004934877157211304\n",
      "epoch: 8 | 77088 / 114272 | training loss: 0.0002099702978739515\n",
      "epoch: 8 | 77120 / 114272 | training loss: 0.0002229980455012992\n",
      "epoch: 8 | 77152 / 114272 | training loss: 0.0003410315257497132\n",
      "epoch: 8 | 77184 / 114272 | training loss: 0.00028051561093889177\n",
      "epoch: 8 | 77216 / 114272 | training loss: 0.00014743153587915003\n",
      "epoch: 8 | 77248 / 114272 | training loss: 0.00023268649238161743\n",
      "epoch: 8 | 77280 / 114272 | training loss: 0.00034613037132658064\n",
      "epoch: 8 | 77312 / 114272 | training loss: 0.00028393513639457524\n",
      "epoch: 8 | 77344 / 114272 | training loss: 0.0004139862139709294\n",
      "epoch: 8 | 77376 / 114272 | training loss: 0.0002520339621696621\n",
      "epoch: 8 | 77408 / 114272 | training loss: 0.00022211519535630941\n",
      "epoch: 8 | 77440 / 114272 | training loss: 0.00017827189003583044\n",
      "epoch: 8 | 77472 / 114272 | training loss: 0.00030064088059589267\n",
      "epoch: 8 | 77504 / 114272 | training loss: 0.0002277990715811029\n",
      "epoch: 8 | 77536 / 114272 | training loss: 0.00016223157581407577\n",
      "epoch: 8 | 77568 / 114272 | training loss: 0.00023476508795283735\n",
      "epoch: 8 | 77600 / 114272 | training loss: 0.18827497959136963\n",
      "epoch: 8 | 77632 / 114272 | training loss: 0.0005100754788145423\n",
      "epoch: 8 | 77664 / 114272 | training loss: 0.0006226004916243255\n",
      "epoch: 8 | 77696 / 114272 | training loss: 0.00014482530241366476\n",
      "epoch: 8 | 77728 / 114272 | training loss: 0.00015625446394551545\n",
      "epoch: 8 | 77760 / 114272 | training loss: 0.00044236902613192797\n",
      "epoch: 8 | 77792 / 114272 | training loss: 0.019953416660428047\n",
      "epoch: 8 | 77824 / 114272 | training loss: 0.00029977905796840787\n",
      "epoch: 8 | 77856 / 114272 | training loss: 0.00032464685500599444\n",
      "epoch: 8 | 77888 / 114272 | training loss: 0.000251909950748086\n",
      "epoch: 8 | 77920 / 114272 | training loss: 0.0007552402676083148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 77952 / 114272 | training loss: 0.0001690642675384879\n",
      "epoch: 8 | 77984 / 114272 | training loss: 0.0003032631066162139\n",
      "epoch: 8 | 78016 / 114272 | training loss: 0.00016739805869292468\n",
      "epoch: 8 | 78048 / 114272 | training loss: 0.00037045322824269533\n",
      "epoch: 8 | 78080 / 114272 | training loss: 0.00028170752921141684\n",
      "epoch: 8 | 78112 / 114272 | training loss: 0.00023432655143551528\n",
      "epoch: 8 | 78144 / 114272 | training loss: 0.0002523045986890793\n",
      "epoch: 8 | 78176 / 114272 | training loss: 0.0001951824378920719\n",
      "epoch: 8 | 78208 / 114272 | training loss: 0.00044057652121409774\n",
      "epoch: 8 | 78240 / 114272 | training loss: 0.00037957075983285904\n",
      "epoch: 8 | 78272 / 114272 | training loss: 0.00019361800514161587\n",
      "epoch: 8 | 78304 / 114272 | training loss: 0.00031846726778894663\n",
      "epoch: 8 | 78336 / 114272 | training loss: 0.0003125207149423659\n",
      "epoch: 8 | 78368 / 114272 | training loss: 0.00019499890913721174\n",
      "epoch: 8 | 78400 / 114272 | training loss: 0.0005116448155604303\n",
      "epoch: 8 | 78432 / 114272 | training loss: 0.00026716431602835655\n",
      "epoch: 8 | 78464 / 114272 | training loss: 0.0002354474418098107\n",
      "epoch: 8 | 78496 / 114272 | training loss: 0.00021767677390016615\n",
      "epoch: 8 | 78528 / 114272 | training loss: 0.19262686371803284\n",
      "epoch: 8 | 78560 / 114272 | training loss: 0.00027639715699478984\n",
      "epoch: 8 | 78592 / 114272 | training loss: 0.0003204546810593456\n",
      "epoch: 8 | 78624 / 114272 | training loss: 0.0002787351841107011\n",
      "epoch: 8 | 78656 / 114272 | training loss: 0.0007101634400896728\n",
      "epoch: 8 | 78688 / 114272 | training loss: 0.00017319298058282584\n",
      "epoch: 8 | 78720 / 114272 | training loss: 0.00028777768602594733\n",
      "epoch: 8 | 78752 / 114272 | training loss: 0.00033247200190089643\n",
      "epoch: 8 | 78784 / 114272 | training loss: 0.0004271403595339507\n",
      "epoch: 8 | 78816 / 114272 | training loss: 0.19326943159103394\n",
      "epoch: 8 | 78848 / 114272 | training loss: 0.003087489167228341\n",
      "epoch: 8 | 78880 / 114272 | training loss: 0.0001718809362500906\n",
      "epoch: 8 | 78912 / 114272 | training loss: 0.0002820085792336613\n",
      "epoch: 8 | 78944 / 114272 | training loss: 0.00034131636493839324\n",
      "epoch: 8 | 78976 / 114272 | training loss: 0.010897679254412651\n",
      "epoch: 8 | 79008 / 114272 | training loss: 0.0001996262581087649\n",
      "epoch: 8 | 79040 / 114272 | training loss: 0.00047468632692471147\n",
      "epoch: 8 | 79072 / 114272 | training loss: 0.0029553354252129793\n",
      "epoch: 8 | 79104 / 114272 | training loss: 0.0027705999091267586\n",
      "epoch: 8 | 79136 / 114272 | training loss: 0.00016937119653448462\n",
      "epoch: 8 | 79168 / 114272 | training loss: 0.0001805360079742968\n",
      "epoch: 8 | 79200 / 114272 | training loss: 0.00043128628749400377\n",
      "epoch: 8 | 79232 / 114272 | training loss: 0.0012598909670487046\n",
      "epoch: 8 | 79264 / 114272 | training loss: 0.0002598230494186282\n",
      "epoch: 8 | 79296 / 114272 | training loss: 0.0005889083840884268\n",
      "epoch: 8 | 79328 / 114272 | training loss: 0.003244715975597501\n",
      "epoch: 8 | 79360 / 114272 | training loss: 0.04026983305811882\n",
      "epoch: 8 | 79392 / 114272 | training loss: 0.00017315163859166205\n",
      "epoch: 8 | 79424 / 114272 | training loss: 0.00033605232601985335\n",
      "epoch: 8 | 79456 / 114272 | training loss: 0.0004282535519450903\n",
      "epoch: 8 | 79488 / 114272 | training loss: 0.09170811623334885\n",
      "epoch: 8 | 79520 / 114272 | training loss: 0.12775422632694244\n",
      "epoch: 8 | 79552 / 114272 | training loss: 0.0004932401934638619\n",
      "epoch: 8 | 79584 / 114272 | training loss: 0.0002478690876159817\n",
      "epoch: 8 | 79616 / 114272 | training loss: 0.00027523201424628496\n",
      "epoch: 8 | 79648 / 114272 | training loss: 0.00035476250923238695\n",
      "epoch: 8 | 79680 / 114272 | training loss: 0.00026872241869568825\n",
      "epoch: 8 | 79712 / 114272 | training loss: 0.0002470679464749992\n",
      "epoch: 8 | 79744 / 114272 | training loss: 0.00024226853565778583\n",
      "epoch: 8 | 79776 / 114272 | training loss: 0.0002502544957678765\n",
      "epoch: 8 | 79808 / 114272 | training loss: 0.00026293049450032413\n",
      "epoch: 8 | 79840 / 114272 | training loss: 0.0004794342094101012\n",
      "epoch: 8 | 79872 / 114272 | training loss: 0.0004918185877613723\n",
      "epoch: 8 | 79904 / 114272 | training loss: 0.0002918013487942517\n",
      "epoch: 8 | 79936 / 114272 | training loss: 0.00026689187507145107\n",
      "epoch: 8 | 79968 / 114272 | training loss: 0.00023997659445740283\n",
      "epoch: 8 | 80000 / 114272 | training loss: 0.0003014122776221484\n",
      "epoch: 8 | 80032 / 114272 | training loss: 0.00037494327989406884\n",
      "epoch: 8 | 80064 / 114272 | training loss: 0.0002459272800479084\n",
      "epoch: 8 | 80096 / 114272 | training loss: 0.0002708008687477559\n",
      "epoch: 8 | 80128 / 114272 | training loss: 0.0005888186860829592\n",
      "epoch: 8 | 80160 / 114272 | training loss: 0.0002340677019674331\n",
      "epoch: 8 | 80192 / 114272 | training loss: 0.00026868286659009755\n",
      "epoch: 8 | 80224 / 114272 | training loss: 0.00024997093714773655\n",
      "epoch: 8 | 80256 / 114272 | training loss: 0.002532504266127944\n",
      "epoch: 8 | 80288 / 114272 | training loss: 0.0002565132162999362\n",
      "epoch: 8 | 80320 / 114272 | training loss: 0.0029416028410196304\n",
      "epoch: 8 | 80352 / 114272 | training loss: 0.0003050709201488644\n",
      "epoch: 8 | 80384 / 114272 | training loss: 0.0001916459295898676\n",
      "epoch: 8 | 80416 / 114272 | training loss: 0.0002139303833246231\n",
      "epoch: 8 | 80448 / 114272 | training loss: 0.0003373783838469535\n",
      "epoch: 8 | 80480 / 114272 | training loss: 0.000483105395687744\n",
      "epoch: 8 | 80512 / 114272 | training loss: 0.0014282643096521497\n",
      "epoch: 8 | 80544 / 114272 | training loss: 0.20678314566612244\n",
      "epoch: 8 | 80576 / 114272 | training loss: 0.0003429700154811144\n",
      "epoch: 8 | 80608 / 114272 | training loss: 0.026466960087418556\n",
      "epoch: 8 | 80640 / 114272 | training loss: 0.00025296161766164005\n",
      "epoch: 8 | 80672 / 114272 | training loss: 0.00016546926053706557\n",
      "epoch: 8 | 80704 / 114272 | training loss: 0.0003632748848758638\n",
      "epoch: 8 | 80736 / 114272 | training loss: 0.0003901547461282462\n",
      "epoch: 8 | 80768 / 114272 | training loss: 0.0012006654869765043\n",
      "epoch: 8 | 80800 / 114272 | training loss: 0.0002935734228231013\n",
      "epoch: 8 | 80832 / 114272 | training loss: 0.00021707717678509653\n",
      "epoch: 8 | 80864 / 114272 | training loss: 0.00018049063510261476\n",
      "epoch: 8 | 80896 / 114272 | training loss: 0.000261751061771065\n",
      "epoch: 8 | 80928 / 114272 | training loss: 0.0002198906004196033\n",
      "epoch: 8 | 80960 / 114272 | training loss: 0.015125025063753128\n",
      "epoch: 8 | 80992 / 114272 | training loss: 0.0001773095573298633\n",
      "epoch: 8 | 81024 / 114272 | training loss: 0.00021169152751099318\n",
      "epoch: 8 | 81056 / 114272 | training loss: 0.00029260909650474787\n",
      "epoch: 8 | 81088 / 114272 | training loss: 0.00043780842679552734\n",
      "epoch: 8 | 81120 / 114272 | training loss: 0.00043599933269433677\n",
      "epoch: 8 | 81152 / 114272 | training loss: 0.0002093172079185024\n",
      "epoch: 8 | 81184 / 114272 | training loss: 0.00038216423126868904\n",
      "epoch: 8 | 81216 / 114272 | training loss: 0.0003787163004744798\n",
      "epoch: 8 | 81248 / 114272 | training loss: 0.0002520469715818763\n",
      "epoch: 8 | 81280 / 114272 | training loss: 0.004580438137054443\n",
      "epoch: 8 | 81312 / 114272 | training loss: 0.0003405683964956552\n",
      "epoch: 8 | 81344 / 114272 | training loss: 0.0006182374781928957\n",
      "epoch: 8 | 81376 / 114272 | training loss: 0.00014352313883136958\n",
      "epoch: 8 | 81408 / 114272 | training loss: 0.00021066615590825677\n",
      "epoch: 8 | 81440 / 114272 | training loss: 0.000260597764281556\n",
      "epoch: 8 | 81472 / 114272 | training loss: 0.00030517802224494517\n",
      "epoch: 8 | 81504 / 114272 | training loss: 0.00029041548259556293\n",
      "epoch: 8 | 81536 / 114272 | training loss: 0.004014115780591965\n",
      "epoch: 8 | 81568 / 114272 | training loss: 0.0002687293745111674\n",
      "epoch: 8 | 81600 / 114272 | training loss: 0.00029115512734279037\n",
      "epoch: 8 | 81632 / 114272 | training loss: 0.00026675750268623233\n",
      "epoch: 8 | 81664 / 114272 | training loss: 0.0002579863357823342\n",
      "epoch: 8 | 81696 / 114272 | training loss: 0.00036683527287095785\n",
      "epoch: 8 | 81728 / 114272 | training loss: 0.00025409439695067704\n",
      "epoch: 8 | 81760 / 114272 | training loss: 0.00029330968391150236\n",
      "epoch: 8 | 81792 / 114272 | training loss: 0.00020831216534134\n",
      "epoch: 8 | 81824 / 114272 | training loss: 0.0002683764323592186\n",
      "epoch: 8 | 81856 / 114272 | training loss: 0.0002415539202047512\n",
      "epoch: 8 | 81888 / 114272 | training loss: 0.0002582768793217838\n",
      "epoch: 8 | 81920 / 114272 | training loss: 0.00017530166951473802\n",
      "epoch: 8 | 81952 / 114272 | training loss: 0.00028637307696044445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 81984 / 114272 | training loss: 0.00021351837494876236\n",
      "epoch: 8 | 82016 / 114272 | training loss: 0.00028572851442731917\n",
      "epoch: 8 | 82048 / 114272 | training loss: 0.00034592999145388603\n",
      "epoch: 8 | 82080 / 114272 | training loss: 0.00071466009831056\n",
      "epoch: 8 | 82112 / 114272 | training loss: 0.00038432961446233094\n",
      "epoch: 8 | 82144 / 114272 | training loss: 0.00014714355347678065\n",
      "epoch: 8 | 82176 / 114272 | training loss: 0.00042830078746192157\n",
      "epoch: 8 | 82208 / 114272 | training loss: 0.00017239364387933165\n",
      "epoch: 8 | 82240 / 114272 | training loss: 0.21389040350914001\n",
      "epoch: 8 | 82272 / 114272 | training loss: 0.00027669750852510333\n",
      "epoch: 8 | 82304 / 114272 | training loss: 0.0004365741624496877\n",
      "epoch: 8 | 82336 / 114272 | training loss: 0.0003672964812722057\n",
      "epoch: 8 | 82368 / 114272 | training loss: 0.0002429306332487613\n",
      "epoch: 8 | 82400 / 114272 | training loss: 0.0002305368398083374\n",
      "epoch: 8 | 82432 / 114272 | training loss: 0.00031186387059278786\n",
      "epoch: 8 | 82464 / 114272 | training loss: 0.00021212110004853457\n",
      "epoch: 8 | 82496 / 114272 | training loss: 0.00029258738504722714\n",
      "epoch: 8 | 82528 / 114272 | training loss: 0.00073208351386711\n",
      "epoch: 8 | 82560 / 114272 | training loss: 0.00015579783939756453\n",
      "epoch: 8 | 82592 / 114272 | training loss: 0.0002667349181137979\n",
      "epoch: 8 | 82624 / 114272 | training loss: 0.0002230737154604867\n",
      "epoch: 8 | 82656 / 114272 | training loss: 0.000297840335406363\n",
      "epoch: 8 | 82688 / 114272 | training loss: 0.00018593083950690925\n",
      "epoch: 8 | 82720 / 114272 | training loss: 0.0003450270742177963\n",
      "epoch: 8 | 82752 / 114272 | training loss: 0.000298571860184893\n",
      "epoch: 8 | 82784 / 114272 | training loss: 0.00023166810569819063\n",
      "epoch: 8 | 82816 / 114272 | training loss: 0.0002669486857485026\n",
      "epoch: 8 | 82848 / 114272 | training loss: 0.000241357454797253\n",
      "epoch: 8 | 82880 / 114272 | training loss: 0.00017895006749313325\n",
      "epoch: 8 | 82912 / 114272 | training loss: 0.0003413193335290998\n",
      "epoch: 8 | 82944 / 114272 | training loss: 0.00018872995860874653\n",
      "epoch: 8 | 82976 / 114272 | training loss: 0.0002792641462292522\n",
      "epoch: 8 | 83008 / 114272 | training loss: 0.0001809981040423736\n",
      "epoch: 8 | 83040 / 114272 | training loss: 0.0006750094471499324\n",
      "epoch: 8 | 83072 / 114272 | training loss: 0.07508527487516403\n",
      "epoch: 8 | 83104 / 114272 | training loss: 0.00028576437034644186\n",
      "epoch: 8 | 83136 / 114272 | training loss: 0.0002547247859183699\n",
      "epoch: 8 | 83168 / 114272 | training loss: 0.00021938288409728557\n",
      "epoch: 8 | 83200 / 114272 | training loss: 0.00023243814939633012\n",
      "epoch: 8 | 83232 / 114272 | training loss: 0.003876946633681655\n",
      "epoch: 8 | 83264 / 114272 | training loss: 0.001200811704620719\n",
      "epoch: 8 | 83296 / 114272 | training loss: 0.0027974392287433147\n",
      "epoch: 8 | 83328 / 114272 | training loss: 0.0010725397150963545\n",
      "epoch: 8 | 83360 / 114272 | training loss: 0.0002837853680830449\n",
      "epoch: 8 | 83392 / 114272 | training loss: 0.0003305665741208941\n",
      "epoch: 8 | 83424 / 114272 | training loss: 0.0006606635288335383\n",
      "epoch: 8 | 83456 / 114272 | training loss: 0.0003800727427005768\n",
      "epoch: 8 | 83488 / 114272 | training loss: 0.00018544142949394882\n",
      "epoch: 8 | 83520 / 114272 | training loss: 0.00019781757146120071\n",
      "epoch: 8 | 83552 / 114272 | training loss: 0.00018132214609067887\n",
      "epoch: 8 | 83584 / 114272 | training loss: 0.0002037077210843563\n",
      "epoch: 8 | 83616 / 114272 | training loss: 0.00024019884585868567\n",
      "epoch: 8 | 83648 / 114272 | training loss: 0.00022815197007730603\n",
      "epoch: 8 | 83680 / 114272 | training loss: 0.005603928584605455\n",
      "epoch: 8 | 83712 / 114272 | training loss: 0.12250739336013794\n",
      "epoch: 8 | 83744 / 114272 | training loss: 0.0004023378132842481\n",
      "epoch: 8 | 83776 / 114272 | training loss: 0.0002336223842576146\n",
      "epoch: 8 | 83808 / 114272 | training loss: 0.0002642167964950204\n",
      "epoch: 8 | 83840 / 114272 | training loss: 0.00023755678557790816\n",
      "epoch: 8 | 83872 / 114272 | training loss: 0.0022492343559861183\n",
      "epoch: 8 | 83904 / 114272 | training loss: 0.00023486136342398822\n",
      "epoch: 8 | 83936 / 114272 | training loss: 0.00020509384921751916\n",
      "epoch: 8 | 83968 / 114272 | training loss: 0.00035542482510209084\n",
      "epoch: 8 | 84000 / 114272 | training loss: 0.0002283569483552128\n",
      "epoch: 8 | 84032 / 114272 | training loss: 0.08154919743537903\n",
      "epoch: 8 | 84064 / 114272 | training loss: 0.00019030331168323755\n",
      "epoch: 8 | 84096 / 114272 | training loss: 0.00014205684419721365\n",
      "epoch: 8 | 84128 / 114272 | training loss: 0.00023388059344142675\n",
      "epoch: 8 | 84160 / 114272 | training loss: 0.00034080565092153847\n",
      "epoch: 8 | 84192 / 114272 | training loss: 0.0003053247928619385\n",
      "epoch: 8 | 84224 / 114272 | training loss: 0.0002600562875159085\n",
      "epoch: 8 | 84256 / 114272 | training loss: 0.0002509141049813479\n",
      "epoch: 8 | 84288 / 114272 | training loss: 0.0003375164815224707\n",
      "epoch: 8 | 84320 / 114272 | training loss: 0.0003002793528139591\n",
      "epoch: 8 | 84352 / 114272 | training loss: 0.0004449055704753846\n",
      "epoch: 8 | 84384 / 114272 | training loss: 0.00017680863675195724\n",
      "epoch: 8 | 84416 / 114272 | training loss: 0.00030204272479750216\n",
      "epoch: 8 | 84448 / 114272 | training loss: 0.00027540410519577563\n",
      "epoch: 8 | 84480 / 114272 | training loss: 0.0007169651798903942\n",
      "epoch: 8 | 84512 / 114272 | training loss: 0.0003205278771929443\n",
      "epoch: 8 | 84544 / 114272 | training loss: 0.000577527389395982\n",
      "epoch: 8 | 84576 / 114272 | training loss: 0.00022649216407444328\n",
      "epoch: 8 | 84608 / 114272 | training loss: 0.00023747971863485873\n",
      "epoch: 8 | 84640 / 114272 | training loss: 0.21820038557052612\n",
      "epoch: 8 | 84672 / 114272 | training loss: 0.0001562422839924693\n",
      "epoch: 8 | 84704 / 114272 | training loss: 0.0003548488602973521\n",
      "epoch: 8 | 84736 / 114272 | training loss: 0.0003949180827476084\n",
      "epoch: 8 | 84768 / 114272 | training loss: 0.00019897782476618886\n",
      "epoch: 8 | 84800 / 114272 | training loss: 0.0002546598843764514\n",
      "epoch: 8 | 84832 / 114272 | training loss: 0.00017989326443057507\n",
      "epoch: 8 | 84864 / 114272 | training loss: 0.00022948176774661988\n",
      "epoch: 8 | 84896 / 114272 | training loss: 0.00021716188348364085\n",
      "epoch: 8 | 84928 / 114272 | training loss: 0.0003282580873928964\n",
      "epoch: 8 | 84960 / 114272 | training loss: 0.00036844186251983047\n",
      "epoch: 8 | 84992 / 114272 | training loss: 0.00018436278332956135\n",
      "epoch: 8 | 85024 / 114272 | training loss: 0.000305136782117188\n",
      "epoch: 8 | 85056 / 114272 | training loss: 0.00036291766446083784\n",
      "epoch: 8 | 85088 / 114272 | training loss: 0.000280800653854385\n",
      "epoch: 8 | 85120 / 114272 | training loss: 0.00023578495893161744\n",
      "epoch: 8 | 85152 / 114272 | training loss: 0.039783790707588196\n",
      "epoch: 8 | 85184 / 114272 | training loss: 0.0002596814592834562\n",
      "epoch: 8 | 85216 / 114272 | training loss: 0.00027206301456317306\n",
      "epoch: 8 | 85248 / 114272 | training loss: 0.00026633127708919346\n",
      "epoch: 8 | 85280 / 114272 | training loss: 0.0002081333805108443\n",
      "epoch: 8 | 85312 / 114272 | training loss: 0.00023153505753725767\n",
      "epoch: 8 | 85344 / 114272 | training loss: 0.00023131241323426366\n",
      "epoch: 8 | 85376 / 114272 | training loss: 0.0003568457323126495\n",
      "epoch: 8 | 85408 / 114272 | training loss: 0.0396307036280632\n",
      "epoch: 8 | 85440 / 114272 | training loss: 0.00039368917350657284\n",
      "epoch: 8 | 85472 / 114272 | training loss: 0.011374490335583687\n",
      "epoch: 8 | 85504 / 114272 | training loss: 0.00029975405777804554\n",
      "epoch: 8 | 85536 / 114272 | training loss: 0.0004123164981137961\n",
      "epoch: 8 | 85568 / 114272 | training loss: 0.00018815943622030318\n",
      "epoch: 8 | 85600 / 114272 | training loss: 0.00020593436784110963\n",
      "epoch: 8 | 85632 / 114272 | training loss: 0.00048035004874691367\n",
      "epoch: 8 | 85664 / 114272 | training loss: 0.004947494249790907\n",
      "epoch: 8 | 85696 / 114272 | training loss: 0.00025308490148745477\n",
      "epoch: 8 | 85728 / 114272 | training loss: 0.0001783473853720352\n",
      "epoch: 8 | 85760 / 114272 | training loss: 0.0003286605642642826\n",
      "epoch: 8 | 85792 / 114272 | training loss: 0.00044593692291527987\n",
      "epoch: 8 | 85824 / 114272 | training loss: 0.00028193325852043927\n",
      "epoch: 8 | 85856 / 114272 | training loss: 0.00042565076728351414\n",
      "epoch: 8 | 85888 / 114272 | training loss: 0.0003772568888962269\n",
      "epoch: 8 | 85920 / 114272 | training loss: 0.0005770637071691453\n",
      "epoch: 8 | 85952 / 114272 | training loss: 0.012478083372116089\n",
      "epoch: 8 | 85984 / 114272 | training loss: 0.0005481778061948717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 86016 / 114272 | training loss: 0.00028368676430545747\n",
      "epoch: 8 | 86048 / 114272 | training loss: 0.0003168520925100893\n",
      "epoch: 8 | 86080 / 114272 | training loss: 0.0002998602285515517\n",
      "epoch: 8 | 86112 / 114272 | training loss: 0.0001086854754248634\n",
      "epoch: 8 | 86144 / 114272 | training loss: 0.0002510846534278244\n",
      "epoch: 8 | 86176 / 114272 | training loss: 0.015980122610926628\n",
      "epoch: 8 | 86208 / 114272 | training loss: 0.00020815417519770563\n",
      "epoch: 8 | 86240 / 114272 | training loss: 0.0002613261458463967\n",
      "epoch: 8 | 86272 / 114272 | training loss: 0.00032968982122838497\n",
      "epoch: 8 | 86304 / 114272 | training loss: 0.0003467002243269235\n",
      "epoch: 8 | 86336 / 114272 | training loss: 0.00021668607951141894\n",
      "epoch: 8 | 86368 / 114272 | training loss: 0.0006928543443791568\n",
      "epoch: 8 | 86400 / 114272 | training loss: 0.0003753036435227841\n",
      "epoch: 8 | 86432 / 114272 | training loss: 0.00031627301359549165\n",
      "epoch: 8 | 86464 / 114272 | training loss: 0.0003605799865908921\n",
      "epoch: 8 | 86496 / 114272 | training loss: 0.00015973330300766975\n",
      "epoch: 8 | 86528 / 114272 | training loss: 0.00018856718088500202\n",
      "epoch: 8 | 86560 / 114272 | training loss: 0.00018014354282058775\n",
      "epoch: 8 | 86592 / 114272 | training loss: 0.27807408571243286\n",
      "epoch: 8 | 86624 / 114272 | training loss: 0.0002484730794094503\n",
      "epoch: 8 | 86656 / 114272 | training loss: 0.12839370965957642\n",
      "epoch: 8 | 86688 / 114272 | training loss: 0.0002771633444353938\n",
      "epoch: 8 | 86720 / 114272 | training loss: 0.00035537660005502403\n",
      "epoch: 8 | 86752 / 114272 | training loss: 0.0003193290904164314\n",
      "epoch: 8 | 86784 / 114272 | training loss: 0.00021679725614376366\n",
      "epoch: 8 | 86816 / 114272 | training loss: 0.0002547332551330328\n",
      "epoch: 8 | 86848 / 114272 | training loss: 0.1341678947210312\n",
      "epoch: 8 | 86880 / 114272 | training loss: 0.0006604155641980469\n",
      "epoch: 8 | 86912 / 114272 | training loss: 0.0002277293970109895\n",
      "epoch: 8 | 86944 / 114272 | training loss: 0.00031352025689557195\n",
      "epoch: 8 | 86976 / 114272 | training loss: 0.0010726932669058442\n",
      "epoch: 8 | 87008 / 114272 | training loss: 0.00035615486558526754\n",
      "epoch: 8 | 87040 / 114272 | training loss: 0.06304249167442322\n",
      "epoch: 8 | 87072 / 114272 | training loss: 0.000247818446950987\n",
      "epoch: 8 | 87104 / 114272 | training loss: 0.00032036451739259064\n",
      "epoch: 8 | 87136 / 114272 | training loss: 0.0008510943735018373\n",
      "epoch: 8 | 87168 / 114272 | training loss: 0.14761319756507874\n",
      "epoch: 8 | 87200 / 114272 | training loss: 0.0002056211669696495\n",
      "epoch: 8 | 87232 / 114272 | training loss: 0.0003123096830677241\n",
      "epoch: 8 | 87264 / 114272 | training loss: 0.000531353522092104\n",
      "epoch: 8 | 87296 / 114272 | training loss: 0.00017338982434011996\n",
      "epoch: 8 | 87328 / 114272 | training loss: 0.0002892652410082519\n",
      "epoch: 8 | 87360 / 114272 | training loss: 0.00037302219425328076\n",
      "epoch: 8 | 87392 / 114272 | training loss: 0.00025189208099618554\n",
      "epoch: 8 | 87424 / 114272 | training loss: 0.0003060235467273742\n",
      "epoch: 8 | 87456 / 114272 | training loss: 0.0003862703451886773\n",
      "epoch: 8 | 87488 / 114272 | training loss: 0.0003305391001049429\n",
      "epoch: 8 | 87520 / 114272 | training loss: 0.0004747775092255324\n",
      "epoch: 8 | 87552 / 114272 | training loss: 0.00023934879573062062\n",
      "epoch: 8 | 87584 / 114272 | training loss: 0.00044274484389461577\n",
      "epoch: 8 | 87616 / 114272 | training loss: 0.028543055057525635\n",
      "epoch: 8 | 87648 / 114272 | training loss: 0.0002049485337920487\n",
      "epoch: 8 | 87680 / 114272 | training loss: 0.0017797612817957997\n",
      "epoch: 8 | 87712 / 114272 | training loss: 0.00022537649783771485\n",
      "epoch: 8 | 87744 / 114272 | training loss: 0.00020817665790673345\n",
      "epoch: 8 | 87776 / 114272 | training loss: 0.00018468820780981332\n",
      "epoch: 8 | 87808 / 114272 | training loss: 0.00031676513026468456\n",
      "epoch: 8 | 87840 / 114272 | training loss: 0.00035398753243498504\n",
      "epoch: 8 | 87872 / 114272 | training loss: 0.0016982206143438816\n",
      "epoch: 8 | 87904 / 114272 | training loss: 0.0888635590672493\n",
      "epoch: 8 | 87936 / 114272 | training loss: 0.00044640415580943227\n",
      "epoch: 8 | 87968 / 114272 | training loss: 0.00037707522278651595\n",
      "epoch: 8 | 88000 / 114272 | training loss: 0.00016880154726095498\n",
      "epoch: 8 | 88032 / 114272 | training loss: 0.0002067506720777601\n",
      "epoch: 8 | 88064 / 114272 | training loss: 0.00043497682781890035\n",
      "epoch: 8 | 88096 / 114272 | training loss: 0.0003187232359778136\n",
      "epoch: 8 | 88128 / 114272 | training loss: 0.24187418818473816\n",
      "epoch: 8 | 88160 / 114272 | training loss: 0.00018429516057949513\n",
      "epoch: 8 | 88192 / 114272 | training loss: 0.002003246685490012\n",
      "epoch: 8 | 88224 / 114272 | training loss: 0.000501440663356334\n",
      "epoch: 8 | 88256 / 114272 | training loss: 0.0004668762267101556\n",
      "epoch: 8 | 88288 / 114272 | training loss: 0.0006233275053091347\n",
      "epoch: 8 | 88320 / 114272 | training loss: 0.00026584070292301476\n",
      "epoch: 8 | 88352 / 114272 | training loss: 0.0006813862710259855\n",
      "epoch: 8 | 88384 / 114272 | training loss: 0.00025860482128337026\n",
      "epoch: 8 | 88416 / 114272 | training loss: 0.000277411425486207\n",
      "epoch: 8 | 88448 / 114272 | training loss: 0.0002934352378360927\n",
      "epoch: 8 | 88480 / 114272 | training loss: 0.00015984215133357793\n",
      "epoch: 8 | 88512 / 114272 | training loss: 0.0004072718438692391\n",
      "epoch: 8 | 88544 / 114272 | training loss: 0.00020642414165195078\n",
      "epoch: 8 | 88576 / 114272 | training loss: 0.000455829402199015\n",
      "epoch: 8 | 88608 / 114272 | training loss: 0.00044687220361083746\n",
      "epoch: 8 | 88640 / 114272 | training loss: 0.0006714421324431896\n",
      "epoch: 8 | 88672 / 114272 | training loss: 0.000279902305919677\n",
      "epoch: 8 | 88704 / 114272 | training loss: 0.0003407282929401845\n",
      "epoch: 8 | 88736 / 114272 | training loss: 0.00022179896768648177\n",
      "epoch: 8 | 88768 / 114272 | training loss: 0.00037856350536458194\n",
      "epoch: 8 | 88800 / 114272 | training loss: 0.00027661322383210063\n",
      "epoch: 8 | 88832 / 114272 | training loss: 0.00030451538623310626\n",
      "epoch: 8 | 88864 / 114272 | training loss: 0.0004779741575475782\n",
      "epoch: 8 | 88896 / 114272 | training loss: 0.00031443583429791033\n",
      "epoch: 8 | 88928 / 114272 | training loss: 0.00023430991859640926\n",
      "epoch: 8 | 88960 / 114272 | training loss: 0.0010597689542919397\n",
      "epoch: 8 | 88992 / 114272 | training loss: 0.000215659718378447\n",
      "epoch: 8 | 89024 / 114272 | training loss: 0.0005041834665462375\n",
      "epoch: 8 | 89056 / 114272 | training loss: 0.0002143796591553837\n",
      "epoch: 8 | 89088 / 114272 | training loss: 0.000227051175897941\n",
      "epoch: 8 | 89120 / 114272 | training loss: 0.00015538031584583223\n",
      "epoch: 8 | 89152 / 114272 | training loss: 0.0002366798435105011\n",
      "epoch: 8 | 89184 / 114272 | training loss: 0.00018195011944044381\n",
      "epoch: 8 | 89216 / 114272 | training loss: 0.0002715455775614828\n",
      "epoch: 8 | 89248 / 114272 | training loss: 0.0003718238149303943\n",
      "epoch: 8 | 89280 / 114272 | training loss: 0.0002617117133922875\n",
      "epoch: 8 | 89312 / 114272 | training loss: 0.00022288557374849916\n",
      "epoch: 8 | 89344 / 114272 | training loss: 0.00037466935464181006\n",
      "epoch: 8 | 89376 / 114272 | training loss: 0.146164670586586\n",
      "epoch: 8 | 89408 / 114272 | training loss: 0.00026234102551825345\n",
      "epoch: 8 | 89440 / 114272 | training loss: 0.0005517681711353362\n",
      "epoch: 8 | 89472 / 114272 | training loss: 0.00038793100975453854\n",
      "epoch: 8 | 89504 / 114272 | training loss: 0.027589190751314163\n",
      "epoch: 8 | 89536 / 114272 | training loss: 0.0003063442709390074\n",
      "epoch: 8 | 89568 / 114272 | training loss: 0.00037099537439644337\n",
      "epoch: 8 | 89600 / 114272 | training loss: 0.00161427678540349\n",
      "epoch: 8 | 89632 / 114272 | training loss: 0.00028984510572627187\n",
      "epoch: 8 | 89664 / 114272 | training loss: 0.0002925892185885459\n",
      "epoch: 8 | 89696 / 114272 | training loss: 0.0002976012765429914\n",
      "epoch: 8 | 89728 / 114272 | training loss: 0.00028871564427390695\n",
      "epoch: 8 | 89760 / 114272 | training loss: 0.000681764620821923\n",
      "epoch: 8 | 89792 / 114272 | training loss: 0.00036016208468936384\n",
      "epoch: 8 | 89824 / 114272 | training loss: 0.0002665360807441175\n",
      "epoch: 8 | 89856 / 114272 | training loss: 0.0002656725118868053\n",
      "epoch: 8 | 89888 / 114272 | training loss: 0.0003666610282380134\n",
      "epoch: 8 | 89920 / 114272 | training loss: 0.10398519784212112\n",
      "epoch: 8 | 89952 / 114272 | training loss: 0.00026073388289660215\n",
      "epoch: 8 | 89984 / 114272 | training loss: 0.0003024016332346946\n",
      "epoch: 8 | 90016 / 114272 | training loss: 0.0002264713984914124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 90048 / 114272 | training loss: 0.09217792749404907\n",
      "epoch: 8 | 90080 / 114272 | training loss: 0.00030368450097739697\n",
      "epoch: 8 | 90112 / 114272 | training loss: 0.00019260433327872306\n",
      "epoch: 8 | 90144 / 114272 | training loss: 0.00033325349795632064\n",
      "epoch: 8 | 90176 / 114272 | training loss: 0.0006218106136657298\n",
      "epoch: 8 | 90208 / 114272 | training loss: 0.00023669342044740915\n",
      "epoch: 8 | 90240 / 114272 | training loss: 0.0008120147394947708\n",
      "epoch: 8 | 90272 / 114272 | training loss: 0.0009163730428554118\n",
      "epoch: 8 | 90304 / 114272 | training loss: 0.0004455197195056826\n",
      "epoch: 8 | 90336 / 114272 | training loss: 0.14011548459529877\n",
      "epoch: 8 | 90368 / 114272 | training loss: 0.000529890472535044\n",
      "epoch: 8 | 90400 / 114272 | training loss: 0.0003715874336194247\n",
      "epoch: 8 | 90432 / 114272 | training loss: 0.0002052076015388593\n",
      "epoch: 8 | 90464 / 114272 | training loss: 0.00961268786340952\n",
      "epoch: 8 | 90496 / 114272 | training loss: 0.1085226759314537\n",
      "epoch: 8 | 90528 / 114272 | training loss: 0.00026972536579705775\n",
      "epoch: 8 | 90560 / 114272 | training loss: 0.0005290480330586433\n",
      "epoch: 8 | 90592 / 114272 | training loss: 0.0009079313022084534\n",
      "epoch: 8 | 90624 / 114272 | training loss: 0.11949189752340317\n",
      "epoch: 8 | 90656 / 114272 | training loss: 0.000704237085301429\n",
      "epoch: 8 | 90688 / 114272 | training loss: 0.0002423939440632239\n",
      "epoch: 8 | 90720 / 114272 | training loss: 0.0002443862031213939\n",
      "epoch: 8 | 90752 / 114272 | training loss: 0.0004263755399733782\n",
      "epoch: 8 | 90784 / 114272 | training loss: 0.0006658764323219657\n",
      "epoch: 8 | 90816 / 114272 | training loss: 0.00020299706375226378\n",
      "epoch: 8 | 90848 / 114272 | training loss: 0.00035233443486504257\n",
      "epoch: 8 | 90880 / 114272 | training loss: 0.00024580780882388353\n",
      "epoch: 8 | 90912 / 114272 | training loss: 0.00021566625218838453\n",
      "epoch: 8 | 90944 / 114272 | training loss: 0.00032455765176564455\n",
      "epoch: 8 | 90976 / 114272 | training loss: 0.0009168278775177896\n",
      "epoch: 8 | 91008 / 114272 | training loss: 0.00024272967129945755\n",
      "epoch: 8 | 91040 / 114272 | training loss: 0.0005575981922447681\n",
      "epoch: 8 | 91072 / 114272 | training loss: 0.0005669221864081919\n",
      "epoch: 8 | 91104 / 114272 | training loss: 0.00016455305740237236\n",
      "epoch: 8 | 91136 / 114272 | training loss: 0.0005914317443966866\n",
      "epoch: 8 | 91168 / 114272 | training loss: 0.24365460872650146\n",
      "epoch: 8 | 91200 / 114272 | training loss: 0.00045164217590354383\n",
      "epoch: 8 | 91232 / 114272 | training loss: 0.0007646059384569526\n",
      "epoch: 8 | 91264 / 114272 | training loss: 0.08399537950754166\n",
      "epoch: 8 | 91296 / 114272 | training loss: 0.001554696587845683\n",
      "epoch: 8 | 91328 / 114272 | training loss: 0.0003892294189427048\n",
      "epoch: 8 | 91360 / 114272 | training loss: 0.000376158335711807\n",
      "epoch: 8 | 91392 / 114272 | training loss: 0.0002072027127724141\n",
      "epoch: 8 | 91424 / 114272 | training loss: 0.00035172433126717806\n",
      "epoch: 8 | 91456 / 114272 | training loss: 0.00037692373734898865\n",
      "epoch: 8 | 91488 / 114272 | training loss: 0.22043299674987793\n",
      "epoch: 8 | 91520 / 114272 | training loss: 0.0004218699468765408\n",
      "epoch: 8 | 91552 / 114272 | training loss: 0.00022440227621700615\n",
      "epoch: 8 | 91584 / 114272 | training loss: 0.00036965106846764684\n",
      "epoch: 8 | 91616 / 114272 | training loss: 0.00025145962717942894\n",
      "epoch: 8 | 91648 / 114272 | training loss: 0.00035344576463103294\n",
      "epoch: 8 | 91680 / 114272 | training loss: 0.1620914191007614\n",
      "epoch: 8 | 91712 / 114272 | training loss: 0.0002390235458733514\n",
      "epoch: 8 | 91744 / 114272 | training loss: 0.00045558594865724444\n",
      "epoch: 8 | 91776 / 114272 | training loss: 0.0001561561948619783\n",
      "epoch: 8 | 91808 / 114272 | training loss: 0.0002199179434683174\n",
      "epoch: 8 | 91840 / 114272 | training loss: 0.08025779575109482\n",
      "epoch: 8 | 91872 / 114272 | training loss: 0.00019906801753677428\n",
      "epoch: 8 | 91904 / 114272 | training loss: 0.0003582602948881686\n",
      "epoch: 8 | 91936 / 114272 | training loss: 0.00026365037774667144\n",
      "epoch: 8 | 91968 / 114272 | training loss: 0.00030926783801987767\n",
      "epoch: 8 | 92000 / 114272 | training loss: 0.026547398418188095\n",
      "epoch: 8 | 92032 / 114272 | training loss: 0.3346715271472931\n",
      "epoch: 8 | 92064 / 114272 | training loss: 0.00038736697752028704\n",
      "epoch: 8 | 92096 / 114272 | training loss: 0.00034818111453205347\n",
      "epoch: 8 | 92128 / 114272 | training loss: 0.12410634011030197\n",
      "epoch: 8 | 92160 / 114272 | training loss: 0.0003979747125413269\n",
      "epoch: 8 | 92192 / 114272 | training loss: 0.0003332549531478435\n",
      "epoch: 8 | 92224 / 114272 | training loss: 0.0002798725035972893\n",
      "epoch: 8 | 92256 / 114272 | training loss: 0.00033329942380078137\n",
      "epoch: 8 | 92288 / 114272 | training loss: 0.0007292513619177043\n",
      "epoch: 8 | 92320 / 114272 | training loss: 0.000543384812772274\n",
      "epoch: 8 | 92352 / 114272 | training loss: 0.00046471526729874313\n",
      "epoch: 8 | 92384 / 114272 | training loss: 0.0004234763327986002\n",
      "epoch: 8 | 92416 / 114272 | training loss: 0.0002108891203533858\n",
      "epoch: 8 | 92448 / 114272 | training loss: 0.0014453850453719497\n",
      "epoch: 8 | 92480 / 114272 | training loss: 0.00043405595351941884\n",
      "epoch: 8 | 92512 / 114272 | training loss: 0.0012978707673028111\n",
      "epoch: 8 | 92544 / 114272 | training loss: 0.0005586458137258887\n",
      "epoch: 8 | 92576 / 114272 | training loss: 0.0005124232266098261\n",
      "epoch: 8 | 92608 / 114272 | training loss: 0.00021688219567295164\n",
      "epoch: 8 | 92640 / 114272 | training loss: 0.00029079694650135934\n",
      "epoch: 8 | 92672 / 114272 | training loss: 0.0009186916286125779\n",
      "epoch: 8 | 92704 / 114272 | training loss: 0.21008050441741943\n",
      "epoch: 8 | 92736 / 114272 | training loss: 0.00044561122194863856\n",
      "epoch: 8 | 92768 / 114272 | training loss: 0.00036975255352444947\n",
      "epoch: 8 | 92800 / 114272 | training loss: 0.0003780175466090441\n",
      "epoch: 8 | 92832 / 114272 | training loss: 0.00019470401457510889\n",
      "epoch: 8 | 92864 / 114272 | training loss: 0.00046530342660844326\n",
      "epoch: 8 | 92896 / 114272 | training loss: 0.0005153441452421248\n",
      "epoch: 8 | 92928 / 114272 | training loss: 0.00017160257266368717\n",
      "epoch: 8 | 92960 / 114272 | training loss: 0.0006571930134668946\n",
      "epoch: 8 | 92992 / 114272 | training loss: 0.0003303708217572421\n",
      "epoch: 8 | 93024 / 114272 | training loss: 0.00029940891545265913\n",
      "epoch: 8 | 93056 / 114272 | training loss: 0.2936466634273529\n",
      "epoch: 8 | 93088 / 114272 | training loss: 0.0002725190424825996\n",
      "epoch: 8 | 93120 / 114272 | training loss: 0.0004015763115603477\n",
      "epoch: 8 | 93152 / 114272 | training loss: 0.0013024946674704552\n",
      "epoch: 8 | 93184 / 114272 | training loss: 0.000605274282861501\n",
      "epoch: 8 | 93216 / 114272 | training loss: 0.00019275194790679961\n",
      "epoch: 8 | 93248 / 114272 | training loss: 0.0003548867243807763\n",
      "epoch: 8 | 93280 / 114272 | training loss: 0.000358489400241524\n",
      "epoch: 8 | 93312 / 114272 | training loss: 0.0003596965689212084\n",
      "epoch: 8 | 93344 / 114272 | training loss: 0.000277580285910517\n",
      "epoch: 8 | 93376 / 114272 | training loss: 0.00029192728106863797\n",
      "epoch: 8 | 93408 / 114272 | training loss: 0.0003677353961393237\n",
      "epoch: 8 | 93440 / 114272 | training loss: 0.0007055739406496286\n",
      "epoch: 8 | 93472 / 114272 | training loss: 0.0004393732233438641\n",
      "epoch: 8 | 93504 / 114272 | training loss: 0.010360046289861202\n",
      "epoch: 8 | 93536 / 114272 | training loss: 0.0344037301838398\n",
      "epoch: 8 | 93568 / 114272 | training loss: 0.14603576064109802\n",
      "epoch: 8 | 93600 / 114272 | training loss: 0.0002863040426746011\n",
      "epoch: 8 | 93632 / 114272 | training loss: 0.0017229793593287468\n",
      "epoch: 8 | 93664 / 114272 | training loss: 0.00029921845998615026\n",
      "epoch: 8 | 93696 / 114272 | training loss: 0.0003863766323775053\n",
      "epoch: 8 | 93728 / 114272 | training loss: 0.0003894534893333912\n",
      "epoch: 8 | 93760 / 114272 | training loss: 0.0006553243147209287\n",
      "epoch: 8 | 93792 / 114272 | training loss: 0.0002763367083389312\n",
      "epoch: 8 | 93824 / 114272 | training loss: 0.0002545471943449229\n",
      "epoch: 8 | 93856 / 114272 | training loss: 0.0002859339874703437\n",
      "epoch: 8 | 93888 / 114272 | training loss: 0.10241872817277908\n",
      "epoch: 8 | 93920 / 114272 | training loss: 0.001549231237731874\n",
      "epoch: 8 | 93952 / 114272 | training loss: 0.00034611381124705076\n",
      "epoch: 8 | 93984 / 114272 | training loss: 0.0007148436270654202\n",
      "epoch: 8 | 94016 / 114272 | training loss: 0.0002947978500742465\n",
      "epoch: 8 | 94048 / 114272 | training loss: 0.000273166224360466\n",
      "epoch: 8 | 94080 / 114272 | training loss: 0.0003101593756582588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 94112 / 114272 | training loss: 0.0002704618964344263\n",
      "epoch: 8 | 94144 / 114272 | training loss: 0.008601821959018707\n",
      "epoch: 8 | 94176 / 114272 | training loss: 0.00024861429119482636\n",
      "epoch: 8 | 94208 / 114272 | training loss: 0.0002851986209861934\n",
      "epoch: 8 | 94240 / 114272 | training loss: 0.0003168170223943889\n",
      "epoch: 8 | 94272 / 114272 | training loss: 0.0006771425250917673\n",
      "epoch: 8 | 94304 / 114272 | training loss: 0.0003237883502151817\n",
      "epoch: 8 | 94336 / 114272 | training loss: 0.0014685337664559484\n",
      "epoch: 8 | 94368 / 114272 | training loss: 0.00020522843988146633\n",
      "epoch: 8 | 94400 / 114272 | training loss: 0.0002801027148962021\n",
      "epoch: 8 | 94432 / 114272 | training loss: 0.0005317241884768009\n",
      "epoch: 8 | 94464 / 114272 | training loss: 0.00041264540050178766\n",
      "epoch: 8 | 94496 / 114272 | training loss: 0.0002955932868644595\n",
      "epoch: 8 | 94528 / 114272 | training loss: 0.02403179369866848\n",
      "epoch: 8 | 94560 / 114272 | training loss: 0.00045715627493336797\n",
      "epoch: 8 | 94592 / 114272 | training loss: 0.0003048749058507383\n",
      "epoch: 8 | 94624 / 114272 | training loss: 0.0003324482822790742\n",
      "epoch: 8 | 94656 / 114272 | training loss: 0.000248696276685223\n",
      "epoch: 8 | 94688 / 114272 | training loss: 0.00033568416256457567\n",
      "epoch: 8 | 94720 / 114272 | training loss: 0.0002672161499504\n",
      "epoch: 8 | 94752 / 114272 | training loss: 0.0003736806975211948\n",
      "epoch: 8 | 94784 / 114272 | training loss: 0.0002697855525184423\n",
      "epoch: 8 | 94816 / 114272 | training loss: 0.00013705927995033562\n",
      "epoch: 8 | 94848 / 114272 | training loss: 0.13975754380226135\n",
      "epoch: 8 | 94880 / 114272 | training loss: 0.00023999666154850274\n",
      "epoch: 8 | 94912 / 114272 | training loss: 0.0002256857551401481\n",
      "epoch: 8 | 94944 / 114272 | training loss: 0.00032989514875225723\n",
      "epoch: 8 | 94976 / 114272 | training loss: 0.0003831774229183793\n",
      "epoch: 8 | 95008 / 114272 | training loss: 0.00031065859366208315\n",
      "epoch: 8 | 95040 / 114272 | training loss: 0.0007964226533658803\n",
      "epoch: 8 | 95072 / 114272 | training loss: 0.0005701755871996284\n",
      "epoch: 8 | 95104 / 114272 | training loss: 0.0002086627355311066\n",
      "epoch: 8 | 95136 / 114272 | training loss: 0.0010132745373994112\n",
      "epoch: 8 | 95168 / 114272 | training loss: 0.00040066518704406917\n",
      "epoch: 8 | 95200 / 114272 | training loss: 0.0002867705188691616\n",
      "epoch: 8 | 95232 / 114272 | training loss: 0.02463957481086254\n",
      "epoch: 8 | 95264 / 114272 | training loss: 0.00029281346360221505\n",
      "epoch: 8 | 95296 / 114272 | training loss: 0.00048735656309872866\n",
      "epoch: 8 | 95328 / 114272 | training loss: 0.13674068450927734\n",
      "epoch: 8 | 95360 / 114272 | training loss: 0.0003634339082054794\n",
      "epoch: 8 | 95392 / 114272 | training loss: 0.0008623121539130807\n",
      "epoch: 8 | 95424 / 114272 | training loss: 0.00022599988733418286\n",
      "epoch: 8 | 95456 / 114272 | training loss: 0.00036928805639036\n",
      "epoch: 8 | 95488 / 114272 | training loss: 0.0004636176163330674\n",
      "epoch: 8 | 95520 / 114272 | training loss: 0.000403479061787948\n",
      "epoch: 8 | 95552 / 114272 | training loss: 0.005676402244716883\n",
      "epoch: 8 | 95584 / 114272 | training loss: 0.000301751570077613\n",
      "epoch: 8 | 95616 / 114272 | training loss: 0.00047980708768591285\n",
      "epoch: 8 | 95648 / 114272 | training loss: 0.002801436698064208\n",
      "epoch: 8 | 95680 / 114272 | training loss: 0.00017989272600971162\n",
      "epoch: 8 | 95712 / 114272 | training loss: 0.0005288819666020572\n",
      "epoch: 8 | 95744 / 114272 | training loss: 0.0006063568871468306\n",
      "epoch: 8 | 95776 / 114272 | training loss: 0.001065921736881137\n",
      "epoch: 8 | 95808 / 114272 | training loss: 0.0005173988756723702\n",
      "epoch: 8 | 95840 / 114272 | training loss: 0.00021401551202870905\n",
      "epoch: 8 | 95872 / 114272 | training loss: 0.00021418544929474592\n",
      "epoch: 8 | 95904 / 114272 | training loss: 0.014717177487909794\n",
      "epoch: 8 | 95936 / 114272 | training loss: 0.00014282138727139682\n",
      "epoch: 8 | 95968 / 114272 | training loss: 0.0003909205552190542\n",
      "epoch: 8 | 96000 / 114272 | training loss: 0.0006747124716639519\n",
      "epoch: 8 | 96032 / 114272 | training loss: 0.042771968990564346\n",
      "epoch: 8 | 96064 / 114272 | training loss: 0.00038389116525650024\n",
      "epoch: 8 | 96096 / 114272 | training loss: 0.00025699762045405805\n",
      "epoch: 8 | 96128 / 114272 | training loss: 0.0007885136874392629\n",
      "epoch: 8 | 96160 / 114272 | training loss: 0.0001572290639160201\n",
      "epoch: 8 | 96192 / 114272 | training loss: 0.00032028480200096965\n",
      "epoch: 8 | 96224 / 114272 | training loss: 0.0002627988869789988\n",
      "epoch: 8 | 96256 / 114272 | training loss: 0.000761854462325573\n",
      "epoch: 8 | 96288 / 114272 | training loss: 0.001866235863417387\n",
      "epoch: 8 | 96320 / 114272 | training loss: 0.0003509049129206687\n",
      "epoch: 8 | 96352 / 114272 | training loss: 0.0002852895122487098\n",
      "epoch: 8 | 96384 / 114272 | training loss: 0.0005559438723139465\n",
      "epoch: 8 | 96416 / 114272 | training loss: 0.0007266750326380134\n",
      "epoch: 8 | 96448 / 114272 | training loss: 0.00026424016687087715\n",
      "epoch: 8 | 96480 / 114272 | training loss: 0.0002506777527742088\n",
      "epoch: 8 | 96512 / 114272 | training loss: 0.0004441907803993672\n",
      "epoch: 8 | 96544 / 114272 | training loss: 0.0008232156396843493\n",
      "epoch: 8 | 96576 / 114272 | training loss: 0.0002477585803717375\n",
      "epoch: 8 | 96608 / 114272 | training loss: 0.0002109854540321976\n",
      "epoch: 8 | 96640 / 114272 | training loss: 0.00023962670820765197\n",
      "epoch: 8 | 96672 / 114272 | training loss: 0.00020384993695188314\n",
      "epoch: 8 | 96704 / 114272 | training loss: 0.00038557715015485883\n",
      "epoch: 8 | 96736 / 114272 | training loss: 0.000370703695807606\n",
      "epoch: 8 | 96768 / 114272 | training loss: 0.0002881993423216045\n",
      "epoch: 8 | 96800 / 114272 | training loss: 0.0004641905252356082\n",
      "epoch: 8 | 96832 / 114272 | training loss: 0.0004408961976878345\n",
      "epoch: 8 | 96864 / 114272 | training loss: 0.0002587974595371634\n",
      "epoch: 8 | 96896 / 114272 | training loss: 0.0016641061520203948\n",
      "epoch: 8 | 96928 / 114272 | training loss: 0.00018264431855641305\n",
      "epoch: 8 | 96960 / 114272 | training loss: 0.08765269070863724\n",
      "epoch: 8 | 96992 / 114272 | training loss: 0.0001318614522460848\n",
      "epoch: 8 | 97024 / 114272 | training loss: 0.13866041600704193\n",
      "epoch: 8 | 97056 / 114272 | training loss: 0.00043675684719346464\n",
      "epoch: 8 | 97088 / 114272 | training loss: 0.0012362541165202856\n",
      "epoch: 8 | 97120 / 114272 | training loss: 0.1161237582564354\n",
      "epoch: 8 | 97152 / 114272 | training loss: 0.00022408565564546734\n",
      "epoch: 8 | 97184 / 114272 | training loss: 0.1542859524488449\n",
      "epoch: 8 | 97216 / 114272 | training loss: 0.000325326866004616\n",
      "epoch: 8 | 97248 / 114272 | training loss: 0.00029206936596892774\n",
      "epoch: 8 | 97280 / 114272 | training loss: 0.0004962901584804058\n",
      "epoch: 8 | 97312 / 114272 | training loss: 0.00025982680381275713\n",
      "epoch: 8 | 97344 / 114272 | training loss: 0.0002789897262118757\n",
      "epoch: 8 | 97376 / 114272 | training loss: 0.00021174308494664729\n",
      "epoch: 8 | 97408 / 114272 | training loss: 0.0002982153673656285\n",
      "epoch: 8 | 97440 / 114272 | training loss: 0.0003215958713553846\n",
      "epoch: 8 | 97472 / 114272 | training loss: 0.0003301342367194593\n",
      "epoch: 8 | 97504 / 114272 | training loss: 0.00024037000548560172\n",
      "epoch: 8 | 97536 / 114272 | training loss: 0.0008783286320976913\n",
      "epoch: 8 | 97568 / 114272 | training loss: 0.00021835981169715524\n",
      "epoch: 8 | 97600 / 114272 | training loss: 0.0008957826066762209\n",
      "epoch: 8 | 97632 / 114272 | training loss: 0.00038429038249887526\n",
      "epoch: 8 | 97664 / 114272 | training loss: 0.0004517260240390897\n",
      "epoch: 8 | 97696 / 114272 | training loss: 0.00034195504849776626\n",
      "epoch: 8 | 97728 / 114272 | training loss: 0.0002029642346315086\n",
      "epoch: 8 | 97760 / 114272 | training loss: 0.1840560883283615\n",
      "epoch: 8 | 97792 / 114272 | training loss: 0.0006152710411697626\n",
      "epoch: 8 | 97824 / 114272 | training loss: 0.0026847810950130224\n",
      "epoch: 8 | 97856 / 114272 | training loss: 0.00014250240928959101\n",
      "epoch: 8 | 97888 / 114272 | training loss: 0.0003679727960843593\n",
      "epoch: 8 | 97920 / 114272 | training loss: 0.00045137517736293375\n",
      "epoch: 8 | 97952 / 114272 | training loss: 0.00032939770608209074\n",
      "epoch: 8 | 97984 / 114272 | training loss: 0.00023459797375835478\n",
      "epoch: 8 | 98016 / 114272 | training loss: 0.0001366895594401285\n",
      "epoch: 8 | 98048 / 114272 | training loss: 0.00043239540536887944\n",
      "epoch: 8 | 98080 / 114272 | training loss: 0.00036152300890535116\n",
      "epoch: 8 | 98112 / 114272 | training loss: 0.00023988519387785345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 98144 / 114272 | training loss: 0.0003071202081628144\n",
      "epoch: 8 | 98176 / 114272 | training loss: 0.0002558937994763255\n",
      "epoch: 8 | 98208 / 114272 | training loss: 0.0005233978154137731\n",
      "epoch: 8 | 98240 / 114272 | training loss: 0.00038384017534554005\n",
      "epoch: 8 | 98272 / 114272 | training loss: 0.0004972509923391044\n",
      "epoch: 8 | 98304 / 114272 | training loss: 0.0005592798115685582\n",
      "epoch: 8 | 98336 / 114272 | training loss: 0.15688298642635345\n",
      "epoch: 8 | 98368 / 114272 | training loss: 0.028497083112597466\n",
      "epoch: 8 | 98400 / 114272 | training loss: 0.0002919348480645567\n",
      "epoch: 8 | 98432 / 114272 | training loss: 0.2279834896326065\n",
      "epoch: 8 | 98464 / 114272 | training loss: 0.00246925069950521\n",
      "epoch: 8 | 98496 / 114272 | training loss: 0.0004776143468916416\n",
      "epoch: 8 | 98528 / 114272 | training loss: 0.00030434466316364706\n",
      "epoch: 8 | 98560 / 114272 | training loss: 0.00045243604108691216\n",
      "epoch: 8 | 98592 / 114272 | training loss: 0.058351658284664154\n",
      "epoch: 8 | 98624 / 114272 | training loss: 0.0001969453296624124\n",
      "epoch: 8 | 98656 / 114272 | training loss: 0.00029350450495257974\n",
      "epoch: 8 | 98688 / 114272 | training loss: 0.0002956255921162665\n",
      "epoch: 8 | 98720 / 114272 | training loss: 0.00031927673262543976\n",
      "epoch: 8 | 98752 / 114272 | training loss: 0.0002211666142102331\n",
      "epoch: 8 | 98784 / 114272 | training loss: 0.00025489876861684024\n",
      "epoch: 8 | 98816 / 114272 | training loss: 0.00024314990150742233\n",
      "epoch: 8 | 98848 / 114272 | training loss: 0.00034561040229164064\n",
      "epoch: 8 | 98880 / 114272 | training loss: 0.0004761627351399511\n",
      "epoch: 8 | 98912 / 114272 | training loss: 0.00017360734636895359\n",
      "epoch: 8 | 98944 / 114272 | training loss: 0.00036976701812818646\n",
      "epoch: 8 | 98976 / 114272 | training loss: 0.0002630359085742384\n",
      "epoch: 8 | 99008 / 114272 | training loss: 0.00026426583644934\n",
      "epoch: 8 | 99040 / 114272 | training loss: 0.001204805914312601\n",
      "epoch: 8 | 99072 / 114272 | training loss: 0.00031434412812814116\n",
      "epoch: 8 | 99104 / 114272 | training loss: 0.0003111408732365817\n",
      "epoch: 8 | 99136 / 114272 | training loss: 0.00021825976727996022\n",
      "epoch: 8 | 99168 / 114272 | training loss: 0.0003796334203798324\n",
      "epoch: 8 | 99200 / 114272 | training loss: 0.000166188896400854\n",
      "epoch: 8 | 99232 / 114272 | training loss: 0.001895383233204484\n",
      "epoch: 8 | 99264 / 114272 | training loss: 0.00036273652222007513\n",
      "epoch: 8 | 99296 / 114272 | training loss: 0.1092999279499054\n",
      "epoch: 8 | 99328 / 114272 | training loss: 0.00021975125127937645\n",
      "epoch: 8 | 99360 / 114272 | training loss: 0.0002594972320366651\n",
      "epoch: 8 | 99392 / 114272 | training loss: 0.0006483040633611381\n",
      "epoch: 8 | 99424 / 114272 | training loss: 0.00037950766272842884\n",
      "epoch: 8 | 99456 / 114272 | training loss: 0.0001755753328325227\n",
      "epoch: 8 | 99488 / 114272 | training loss: 0.00030526096816174686\n",
      "epoch: 8 | 99520 / 114272 | training loss: 0.00024921720614656806\n",
      "epoch: 8 | 99552 / 114272 | training loss: 0.00025302631547674537\n",
      "epoch: 8 | 99584 / 114272 | training loss: 0.0014010824961587787\n",
      "epoch: 8 | 99616 / 114272 | training loss: 0.0005156508414074779\n",
      "epoch: 8 | 99648 / 114272 | training loss: 0.0006838174886070192\n",
      "epoch: 8 | 99680 / 114272 | training loss: 0.0004031323769595474\n",
      "epoch: 8 | 99712 / 114272 | training loss: 0.0004214271320961416\n",
      "epoch: 8 | 99744 / 114272 | training loss: 0.00026455946499481797\n",
      "epoch: 8 | 99776 / 114272 | training loss: 0.0002956184616778046\n",
      "epoch: 8 | 99808 / 114272 | training loss: 0.00036444392753764987\n",
      "epoch: 8 | 99840 / 114272 | training loss: 0.0008206787751987576\n",
      "epoch: 8 | 99872 / 114272 | training loss: 0.00171787163708359\n",
      "epoch: 8 | 99904 / 114272 | training loss: 0.00017601269064471126\n",
      "epoch: 8 | 99936 / 114272 | training loss: 0.00027969462098553777\n",
      "epoch: 8 | 99968 / 114272 | training loss: 0.00039364572148770094\n",
      "epoch: 8 | 100000 / 114272 | training loss: 0.00034834357211366296\n",
      "epoch: 8 | 100032 / 114272 | training loss: 0.026687851175665855\n",
      "epoch: 8 | 100064 / 114272 | training loss: 0.0001887658581836149\n",
      "epoch: 8 | 100096 / 114272 | training loss: 0.0004537432105280459\n",
      "epoch: 8 | 100128 / 114272 | training loss: 0.0002721075725276023\n",
      "epoch: 8 | 100160 / 114272 | training loss: 0.00032696977723389864\n",
      "epoch: 8 | 100192 / 114272 | training loss: 0.0006682443199679255\n",
      "epoch: 8 | 100224 / 114272 | training loss: 0.00034497902379371226\n",
      "epoch: 8 | 100256 / 114272 | training loss: 0.0003678418870549649\n",
      "epoch: 8 | 100288 / 114272 | training loss: 0.00026736673316918314\n",
      "epoch: 8 | 100320 / 114272 | training loss: 0.0015866570174694061\n",
      "epoch: 8 | 100352 / 114272 | training loss: 0.0009427379118278623\n",
      "epoch: 8 | 100384 / 114272 | training loss: 0.00026653846725821495\n",
      "epoch: 8 | 100416 / 114272 | training loss: 0.00017232971731573343\n",
      "epoch: 8 | 100448 / 114272 | training loss: 0.11218313872814178\n",
      "epoch: 8 | 100480 / 114272 | training loss: 0.00022234622156247497\n",
      "epoch: 8 | 100512 / 114272 | training loss: 0.002709709107875824\n",
      "epoch: 8 | 100544 / 114272 | training loss: 0.000377725315047428\n",
      "epoch: 8 | 100576 / 114272 | training loss: 0.000339978578267619\n",
      "epoch: 8 | 100608 / 114272 | training loss: 0.00036621661274693906\n",
      "epoch: 8 | 100640 / 114272 | training loss: 0.0017741513438522816\n",
      "epoch: 8 | 100672 / 114272 | training loss: 0.0005919835530221462\n",
      "epoch: 8 | 100704 / 114272 | training loss: 0.0004435786686372012\n",
      "epoch: 8 | 100736 / 114272 | training loss: 0.0015123178018257022\n",
      "epoch: 8 | 100768 / 114272 | training loss: 0.0004111882590223104\n",
      "epoch: 8 | 100800 / 114272 | training loss: 0.0003662997332867235\n",
      "epoch: 8 | 100832 / 114272 | training loss: 0.0001777073193807155\n",
      "epoch: 8 | 100864 / 114272 | training loss: 0.0002978079137392342\n",
      "epoch: 8 | 100896 / 114272 | training loss: 0.0001901857613120228\n",
      "epoch: 8 | 100928 / 114272 | training loss: 0.00032729137456044555\n",
      "epoch: 8 | 100960 / 114272 | training loss: 0.000758968701120466\n",
      "epoch: 8 | 100992 / 114272 | training loss: 0.00022435335267800838\n",
      "epoch: 8 | 101024 / 114272 | training loss: 0.00027533495449461043\n",
      "epoch: 8 | 101056 / 114272 | training loss: 0.0004538654466159642\n",
      "epoch: 8 | 101088 / 114272 | training loss: 0.00038445714744739234\n",
      "epoch: 8 | 101120 / 114272 | training loss: 0.002099582925438881\n",
      "epoch: 8 | 101152 / 114272 | training loss: 0.0003877879062201828\n",
      "epoch: 8 | 101184 / 114272 | training loss: 0.007042066194117069\n",
      "epoch: 8 | 101216 / 114272 | training loss: 0.00028393062530085444\n",
      "epoch: 8 | 101248 / 114272 | training loss: 0.003110304707661271\n",
      "epoch: 8 | 101280 / 114272 | training loss: 0.004497918300330639\n",
      "epoch: 8 | 101312 / 114272 | training loss: 0.0002411097812000662\n",
      "epoch: 8 | 101344 / 114272 | training loss: 0.0007027197279967368\n",
      "epoch: 8 | 101376 / 114272 | training loss: 0.034262675791978836\n",
      "epoch: 8 | 101408 / 114272 | training loss: 0.00044511962914839387\n",
      "epoch: 8 | 101440 / 114272 | training loss: 0.00022589020954910666\n",
      "epoch: 8 | 101472 / 114272 | training loss: 0.019786765798926353\n",
      "epoch: 8 | 101504 / 114272 | training loss: 0.0003332459309604019\n",
      "epoch: 8 | 101536 / 114272 | training loss: 0.00015217535838019103\n",
      "epoch: 8 | 101568 / 114272 | training loss: 0.0005220914026722312\n",
      "epoch: 8 | 101600 / 114272 | training loss: 0.0005486423033289611\n",
      "epoch: 8 | 101632 / 114272 | training loss: 0.014996142126619816\n",
      "epoch: 8 | 101664 / 114272 | training loss: 0.0001786881039151922\n",
      "epoch: 8 | 101696 / 114272 | training loss: 0.00019794995023403317\n",
      "epoch: 8 | 101728 / 114272 | training loss: 0.0045077442191541195\n",
      "epoch: 8 | 101760 / 114272 | training loss: 0.0005059802788309753\n",
      "epoch: 8 | 101792 / 114272 | training loss: 0.0002317101607332006\n",
      "epoch: 8 | 101824 / 114272 | training loss: 0.00025122048100456595\n",
      "epoch: 8 | 101856 / 114272 | training loss: 0.0003309333114884794\n",
      "epoch: 8 | 101888 / 114272 | training loss: 0.0896977111697197\n",
      "epoch: 8 | 101920 / 114272 | training loss: 0.00027790700551122427\n",
      "epoch: 8 | 101952 / 114272 | training loss: 0.00032120521063916385\n",
      "epoch: 8 | 101984 / 114272 | training loss: 0.0018744789995253086\n",
      "epoch: 8 | 102016 / 114272 | training loss: 0.0001963391259778291\n",
      "epoch: 8 | 102048 / 114272 | training loss: 0.0001973119069589302\n",
      "epoch: 8 | 102080 / 114272 | training loss: 0.0006942744366824627\n",
      "epoch: 8 | 102112 / 114272 | training loss: 0.00018245932005811483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 102144 / 114272 | training loss: 0.0002115225506713614\n",
      "epoch: 8 | 102176 / 114272 | training loss: 0.0002627789508551359\n",
      "epoch: 8 | 102208 / 114272 | training loss: 0.0002354509779252112\n",
      "epoch: 8 | 102240 / 114272 | training loss: 0.00024825410218909383\n",
      "epoch: 8 | 102272 / 114272 | training loss: 0.2048654705286026\n",
      "epoch: 8 | 102304 / 114272 | training loss: 0.00021404556173365563\n",
      "epoch: 8 | 102336 / 114272 | training loss: 0.005545414984226227\n",
      "epoch: 8 | 102368 / 114272 | training loss: 0.00027976828278042376\n",
      "epoch: 8 | 102400 / 114272 | training loss: 0.00028037262381985784\n",
      "epoch: 8 | 102432 / 114272 | training loss: 0.00011462029942777008\n",
      "epoch: 8 | 102464 / 114272 | training loss: 0.21263085305690765\n",
      "epoch: 8 | 102496 / 114272 | training loss: 0.0002190745435655117\n",
      "epoch: 8 | 102528 / 114272 | training loss: 0.00047962868120521307\n",
      "epoch: 8 | 102560 / 114272 | training loss: 0.00023564430011902004\n",
      "epoch: 8 | 102592 / 114272 | training loss: 0.0003209883871022612\n",
      "epoch: 8 | 102624 / 114272 | training loss: 0.0003867406048811972\n",
      "epoch: 8 | 102656 / 114272 | training loss: 0.000311677431454882\n",
      "epoch: 8 | 102688 / 114272 | training loss: 0.00046768339234404266\n",
      "epoch: 8 | 102720 / 114272 | training loss: 0.0003306799044366926\n",
      "epoch: 8 | 102752 / 114272 | training loss: 0.00029444368556141853\n",
      "epoch: 8 | 102784 / 114272 | training loss: 0.000308754708385095\n",
      "epoch: 8 | 102816 / 114272 | training loss: 0.00015553557022940367\n",
      "epoch: 8 | 102848 / 114272 | training loss: 0.00027793477056548\n",
      "epoch: 8 | 102880 / 114272 | training loss: 0.0002031098847510293\n",
      "epoch: 8 | 102912 / 114272 | training loss: 0.0005101131391711533\n",
      "epoch: 8 | 102944 / 114272 | training loss: 0.0006251605809666216\n",
      "epoch: 8 | 102976 / 114272 | training loss: 0.0005003631231375039\n",
      "epoch: 8 | 103008 / 114272 | training loss: 0.00040368924965150654\n",
      "epoch: 8 | 103040 / 114272 | training loss: 0.00028278413810767233\n",
      "epoch: 8 | 103072 / 114272 | training loss: 0.0002795895852614194\n",
      "epoch: 8 | 103104 / 114272 | training loss: 0.00023275046260096133\n",
      "epoch: 8 | 103136 / 114272 | training loss: 0.00013506073446478695\n",
      "epoch: 8 | 103168 / 114272 | training loss: 0.0002545567986089736\n",
      "epoch: 8 | 103200 / 114272 | training loss: 0.000492272200062871\n",
      "epoch: 8 | 103232 / 114272 | training loss: 0.00026826164685189724\n",
      "epoch: 8 | 103264 / 114272 | training loss: 0.00020412229059729725\n",
      "epoch: 8 | 103296 / 114272 | training loss: 0.00015002349391579628\n",
      "epoch: 8 | 103328 / 114272 | training loss: 8.25322640594095e-05\n",
      "epoch: 8 | 103360 / 114272 | training loss: 0.00038856978062540293\n",
      "epoch: 8 | 103392 / 114272 | training loss: 0.00031381839653477073\n",
      "epoch: 8 | 103424 / 114272 | training loss: 0.0002304210065631196\n",
      "epoch: 8 | 103456 / 114272 | training loss: 0.00013975234469398856\n",
      "epoch: 8 | 103488 / 114272 | training loss: 0.0007432398269884288\n",
      "epoch: 8 | 103520 / 114272 | training loss: 0.000203310715733096\n",
      "epoch: 8 | 103552 / 114272 | training loss: 0.00026578790857456625\n",
      "epoch: 8 | 103584 / 114272 | training loss: 0.0004030460841022432\n",
      "epoch: 8 | 103616 / 114272 | training loss: 0.00022937495668884367\n",
      "epoch: 8 | 103648 / 114272 | training loss: 0.0001533261820441112\n",
      "epoch: 8 | 103680 / 114272 | training loss: 0.2422182857990265\n",
      "epoch: 8 | 103712 / 114272 | training loss: 0.00030713187879882753\n",
      "epoch: 8 | 103744 / 114272 | training loss: 0.00040975079173222184\n",
      "epoch: 8 | 103776 / 114272 | training loss: 0.0003676108899526298\n",
      "epoch: 8 | 103808 / 114272 | training loss: 0.00016114550817292184\n",
      "epoch: 8 | 103840 / 114272 | training loss: 0.0003229189896956086\n",
      "epoch: 8 | 103872 / 114272 | training loss: 0.0002507865137886256\n",
      "epoch: 8 | 103904 / 114272 | training loss: 0.0004037951584905386\n",
      "epoch: 8 | 103936 / 114272 | training loss: 0.0003477114369161427\n",
      "epoch: 8 | 103968 / 114272 | training loss: 0.00025367093621753156\n",
      "epoch: 8 | 104000 / 114272 | training loss: 0.002053466159850359\n",
      "epoch: 8 | 104032 / 114272 | training loss: 0.0002402468817308545\n",
      "epoch: 8 | 104064 / 114272 | training loss: 0.020850496366620064\n",
      "epoch: 8 | 104096 / 114272 | training loss: 0.00047732488019391894\n",
      "epoch: 8 | 104128 / 114272 | training loss: 0.0003904554760083556\n",
      "epoch: 8 | 104160 / 114272 | training loss: 0.09834115207195282\n",
      "epoch: 8 | 104192 / 114272 | training loss: 0.00045090564526617527\n",
      "epoch: 8 | 104224 / 114272 | training loss: 0.0003761945408768952\n",
      "epoch: 8 | 104256 / 114272 | training loss: 0.0003568681422621012\n",
      "epoch: 8 | 104288 / 114272 | training loss: 0.00033958713174797595\n",
      "epoch: 8 | 104320 / 114272 | training loss: 0.00035408136318437755\n",
      "epoch: 8 | 104352 / 114272 | training loss: 0.002626658184453845\n",
      "epoch: 8 | 104384 / 114272 | training loss: 0.00021251097496133298\n",
      "epoch: 8 | 104416 / 114272 | training loss: 0.0003339123213663697\n",
      "epoch: 8 | 104448 / 114272 | training loss: 0.0002707592793740332\n",
      "epoch: 8 | 104480 / 114272 | training loss: 0.0002492442145012319\n",
      "epoch: 8 | 104512 / 114272 | training loss: 0.0001289759820792824\n",
      "epoch: 8 | 104544 / 114272 | training loss: 0.00025108054978773\n",
      "epoch: 8 | 104576 / 114272 | training loss: 0.00034960429184138775\n",
      "epoch: 8 | 104608 / 114272 | training loss: 0.00028029203531332314\n",
      "epoch: 8 | 104640 / 114272 | training loss: 0.000270449323579669\n",
      "epoch: 8 | 104672 / 114272 | training loss: 0.0003570436383597553\n",
      "epoch: 8 | 104704 / 114272 | training loss: 0.0006050296360626817\n",
      "epoch: 8 | 104736 / 114272 | training loss: 0.016311371698975563\n",
      "epoch: 8 | 104768 / 114272 | training loss: 0.0003842543810606003\n",
      "epoch: 8 | 104800 / 114272 | training loss: 0.0011613888200372458\n",
      "epoch: 8 | 104832 / 114272 | training loss: 0.25740182399749756\n",
      "epoch: 8 | 104864 / 114272 | training loss: 0.00025073555298149586\n",
      "epoch: 8 | 104896 / 114272 | training loss: 0.00023580178094562143\n",
      "epoch: 8 | 104928 / 114272 | training loss: 0.000301184831187129\n",
      "epoch: 8 | 104960 / 114272 | training loss: 0.0007944152457639575\n",
      "epoch: 8 | 104992 / 114272 | training loss: 0.000810256868135184\n",
      "epoch: 8 | 105024 / 114272 | training loss: 0.0003177641483489424\n",
      "epoch: 8 | 105056 / 114272 | training loss: 0.0003276571515016258\n",
      "epoch: 8 | 105088 / 114272 | training loss: 0.0003832499496638775\n",
      "epoch: 8 | 105120 / 114272 | training loss: 0.0002486497687641531\n",
      "epoch: 8 | 105152 / 114272 | training loss: 0.0005453205085359514\n",
      "epoch: 8 | 105184 / 114272 | training loss: 0.0002930159098468721\n",
      "epoch: 8 | 105216 / 114272 | training loss: 0.00033757949131540954\n",
      "epoch: 8 | 105248 / 114272 | training loss: 0.00015175569569692016\n",
      "epoch: 8 | 105280 / 114272 | training loss: 0.00043767745955847204\n",
      "epoch: 8 | 105312 / 114272 | training loss: 0.0005585193284787238\n",
      "epoch: 8 | 105344 / 114272 | training loss: 0.0004942206433042884\n",
      "epoch: 8 | 105376 / 114272 | training loss: 0.0002897085214499384\n",
      "epoch: 8 | 105408 / 114272 | training loss: 0.00012296510976739228\n",
      "epoch: 8 | 105440 / 114272 | training loss: 0.0002622634929139167\n",
      "epoch: 8 | 105472 / 114272 | training loss: 0.00035023537930101156\n",
      "epoch: 8 | 105504 / 114272 | training loss: 0.1341383010149002\n",
      "epoch: 8 | 105536 / 114272 | training loss: 0.000416713475715369\n",
      "epoch: 8 | 105568 / 114272 | training loss: 0.00018030908540822566\n",
      "epoch: 8 | 105600 / 114272 | training loss: 0.00028073255089111626\n",
      "epoch: 8 | 105632 / 114272 | training loss: 0.000272043893346563\n",
      "epoch: 8 | 105664 / 114272 | training loss: 0.00044279341818764806\n",
      "epoch: 8 | 105696 / 114272 | training loss: 0.0005676915752701461\n",
      "epoch: 8 | 105728 / 114272 | training loss: 0.0002977133262902498\n",
      "epoch: 8 | 105760 / 114272 | training loss: 0.0010029044933617115\n",
      "epoch: 8 | 105792 / 114272 | training loss: 0.0005784432287327945\n",
      "epoch: 8 | 105824 / 114272 | training loss: 0.00024740234948694706\n",
      "epoch: 8 | 105856 / 114272 | training loss: 0.00033894061925821006\n",
      "epoch: 8 | 105888 / 114272 | training loss: 0.0007087757112458348\n",
      "epoch: 8 | 105920 / 114272 | training loss: 0.0003890030493494123\n",
      "epoch: 8 | 105952 / 114272 | training loss: 0.00027760874945670366\n",
      "epoch: 8 | 105984 / 114272 | training loss: 0.0002457444788888097\n",
      "epoch: 8 | 106016 / 114272 | training loss: 0.0002649448870215565\n",
      "epoch: 8 | 106048 / 114272 | training loss: 0.00025725524756126106\n",
      "epoch: 8 | 106080 / 114272 | training loss: 0.00031622315873391926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 106112 / 114272 | training loss: 0.00017456046771258116\n",
      "epoch: 8 | 106144 / 114272 | training loss: 0.0002906612935476005\n",
      "epoch: 8 | 106176 / 114272 | training loss: 0.00020034774206578732\n",
      "epoch: 8 | 106208 / 114272 | training loss: 0.000665322644636035\n",
      "epoch: 8 | 106240 / 114272 | training loss: 0.00019134377362206578\n",
      "epoch: 8 | 106272 / 114272 | training loss: 0.2779703438282013\n",
      "epoch: 8 | 106304 / 114272 | training loss: 0.0002568917698226869\n",
      "epoch: 8 | 106336 / 114272 | training loss: 0.00031222650432027876\n",
      "epoch: 8 | 106368 / 114272 | training loss: 0.00030463695293292403\n",
      "epoch: 8 | 106400 / 114272 | training loss: 0.21803584694862366\n",
      "epoch: 8 | 106432 / 114272 | training loss: 0.0004457032773643732\n",
      "epoch: 8 | 106464 / 114272 | training loss: 0.0007134937332011759\n",
      "epoch: 8 | 106496 / 114272 | training loss: 0.00019597704522311687\n",
      "epoch: 8 | 106528 / 114272 | training loss: 0.00045776102342642844\n",
      "epoch: 8 | 106560 / 114272 | training loss: 0.0002991453802678734\n",
      "epoch: 8 | 106592 / 114272 | training loss: 0.00038039236096665263\n",
      "epoch: 8 | 106624 / 114272 | training loss: 0.0002022048574872315\n",
      "epoch: 8 | 106656 / 114272 | training loss: 0.0003690982412081212\n",
      "epoch: 8 | 106688 / 114272 | training loss: 0.0003309562162030488\n",
      "epoch: 8 | 106720 / 114272 | training loss: 0.0014023479307070374\n",
      "epoch: 8 | 106752 / 114272 | training loss: 0.0002452398184686899\n",
      "epoch: 8 | 106784 / 114272 | training loss: 0.08583736419677734\n",
      "epoch: 8 | 106816 / 114272 | training loss: 0.0003746167931240052\n",
      "epoch: 8 | 106848 / 114272 | training loss: 0.0003971651312895119\n",
      "epoch: 8 | 106880 / 114272 | training loss: 0.00026651794905774295\n",
      "epoch: 8 | 106912 / 114272 | training loss: 0.0012236566981300712\n",
      "epoch: 8 | 106944 / 114272 | training loss: 0.004319765605032444\n",
      "epoch: 8 | 106976 / 114272 | training loss: 0.0004605486465152353\n",
      "epoch: 8 | 107008 / 114272 | training loss: 0.0005657237488776445\n",
      "epoch: 8 | 107040 / 114272 | training loss: 0.0008427282446064055\n",
      "epoch: 8 | 107072 / 114272 | training loss: 0.0002324938541278243\n",
      "epoch: 8 | 107104 / 114272 | training loss: 0.00023456229246221483\n",
      "epoch: 8 | 107136 / 114272 | training loss: 0.0005511266645044088\n",
      "epoch: 8 | 107168 / 114272 | training loss: 0.026520006358623505\n",
      "epoch: 8 | 107200 / 114272 | training loss: 0.0005357712507247925\n",
      "epoch: 8 | 107232 / 114272 | training loss: 0.000242448368226178\n",
      "epoch: 8 | 107264 / 114272 | training loss: 0.0005563821760006249\n",
      "epoch: 8 | 107296 / 114272 | training loss: 0.00026810308918356895\n",
      "epoch: 8 | 107328 / 114272 | training loss: 0.0005858080112375319\n",
      "epoch: 8 | 107360 / 114272 | training loss: 0.00040838486165739596\n",
      "epoch: 8 | 107392 / 114272 | training loss: 0.00035217104596085846\n",
      "epoch: 8 | 107424 / 114272 | training loss: 0.00023220089497044683\n",
      "epoch: 8 | 107456 / 114272 | training loss: 0.0003798730904236436\n",
      "epoch: 8 | 107488 / 114272 | training loss: 0.00024426449090242386\n",
      "epoch: 8 | 107520 / 114272 | training loss: 0.001187212415970862\n",
      "epoch: 8 | 107552 / 114272 | training loss: 0.0241188146173954\n",
      "epoch: 8 | 107584 / 114272 | training loss: 0.00028058458701707423\n",
      "epoch: 8 | 107616 / 114272 | training loss: 0.00025175209157168865\n",
      "epoch: 8 | 107648 / 114272 | training loss: 0.0020595979876816273\n",
      "epoch: 8 | 107680 / 114272 | training loss: 0.0007443017093464732\n",
      "epoch: 8 | 107712 / 114272 | training loss: 0.0001569773885421455\n",
      "epoch: 8 | 107744 / 114272 | training loss: 0.00030758994398638606\n",
      "epoch: 8 | 107776 / 114272 | training loss: 0.00033175208955071867\n",
      "epoch: 8 | 107808 / 114272 | training loss: 0.002188575454056263\n",
      "epoch: 8 | 107840 / 114272 | training loss: 0.0005941343843005598\n",
      "epoch: 8 | 107872 / 114272 | training loss: 0.00031656285864301026\n",
      "epoch: 8 | 107904 / 114272 | training loss: 0.00031332054641097784\n",
      "epoch: 8 | 107936 / 114272 | training loss: 0.003744732355698943\n",
      "epoch: 8 | 107968 / 114272 | training loss: 0.00027290076832287014\n",
      "epoch: 8 | 108000 / 114272 | training loss: 0.00023476705246139318\n",
      "epoch: 8 | 108032 / 114272 | training loss: 0.0004992705653421581\n",
      "epoch: 8 | 108064 / 114272 | training loss: 0.0003500407619867474\n",
      "epoch: 8 | 108096 / 114272 | training loss: 0.00021048294729553163\n",
      "epoch: 8 | 108128 / 114272 | training loss: 0.0003034177061636001\n",
      "epoch: 8 | 108160 / 114272 | training loss: 0.026828842237591743\n",
      "epoch: 8 | 108192 / 114272 | training loss: 0.00038984848652035\n",
      "epoch: 8 | 108224 / 114272 | training loss: 0.00027617858722805977\n",
      "epoch: 8 | 108256 / 114272 | training loss: 0.00031080571352504194\n",
      "epoch: 8 | 108288 / 114272 | training loss: 0.0002658790908753872\n",
      "epoch: 8 | 108320 / 114272 | training loss: 0.0010512677254155278\n",
      "epoch: 8 | 108352 / 114272 | training loss: 0.0003452766686677933\n",
      "epoch: 8 | 108384 / 114272 | training loss: 0.0002830475277733058\n",
      "epoch: 8 | 108416 / 114272 | training loss: 0.0008035257924348116\n",
      "epoch: 8 | 108448 / 114272 | training loss: 0.0003892608219757676\n",
      "epoch: 8 | 108480 / 114272 | training loss: 0.00038518293877132237\n",
      "epoch: 8 | 108512 / 114272 | training loss: 0.0003090000245720148\n",
      "epoch: 8 | 108544 / 114272 | training loss: 0.0002658109297044575\n",
      "epoch: 8 | 108576 / 114272 | training loss: 0.0003361216513440013\n",
      "epoch: 8 | 108608 / 114272 | training loss: 0.00036075065145269036\n",
      "epoch: 8 | 108640 / 114272 | training loss: 0.00041593602509237826\n",
      "epoch: 8 | 108672 / 114272 | training loss: 0.000503921473864466\n",
      "epoch: 8 | 108704 / 114272 | training loss: 0.0003455202095210552\n",
      "epoch: 8 | 108736 / 114272 | training loss: 0.0012158637400716543\n",
      "epoch: 8 | 108768 / 114272 | training loss: 0.00022565700055565685\n",
      "epoch: 8 | 108800 / 114272 | training loss: 0.014243784360587597\n",
      "epoch: 8 | 108832 / 114272 | training loss: 0.0002722200006246567\n",
      "epoch: 8 | 108864 / 114272 | training loss: 0.0002975862589664757\n",
      "epoch: 8 | 108896 / 114272 | training loss: 0.0003003064193762839\n",
      "epoch: 8 | 108928 / 114272 | training loss: 0.0004335756821092218\n",
      "epoch: 8 | 108960 / 114272 | training loss: 0.0002383570245001465\n",
      "epoch: 8 | 108992 / 114272 | training loss: 0.00026448696735315025\n",
      "epoch: 8 | 109024 / 114272 | training loss: 0.00025697716046124697\n",
      "epoch: 8 | 109056 / 114272 | training loss: 0.00036963369348086417\n",
      "epoch: 8 | 109088 / 114272 | training loss: 0.0002876999496947974\n",
      "epoch: 8 | 109120 / 114272 | training loss: 0.00026585886371321976\n",
      "epoch: 8 | 109152 / 114272 | training loss: 0.0002712294226512313\n",
      "epoch: 8 | 109184 / 114272 | training loss: 0.00031416252022609115\n",
      "epoch: 8 | 109216 / 114272 | training loss: 0.00027773380861617625\n",
      "epoch: 8 | 109248 / 114272 | training loss: 0.0003293149347882718\n",
      "epoch: 8 | 109280 / 114272 | training loss: 0.00028757931431755424\n",
      "epoch: 8 | 109312 / 114272 | training loss: 0.0002671975234989077\n",
      "epoch: 8 | 109344 / 114272 | training loss: 0.0002610360097605735\n",
      "epoch: 8 | 109376 / 114272 | training loss: 0.0003268329019192606\n",
      "epoch: 8 | 109408 / 114272 | training loss: 0.0003899284347426146\n",
      "epoch: 8 | 109440 / 114272 | training loss: 0.0004366509383544326\n",
      "epoch: 8 | 109472 / 114272 | training loss: 0.000304124754620716\n",
      "epoch: 8 | 109504 / 114272 | training loss: 0.00039215589640662074\n",
      "epoch: 8 | 109536 / 114272 | training loss: 0.00039834424387663603\n",
      "epoch: 8 | 109568 / 114272 | training loss: 0.0003607707330957055\n",
      "epoch: 8 | 109600 / 114272 | training loss: 0.0002473762724548578\n",
      "epoch: 8 | 109632 / 114272 | training loss: 0.00030671729473397136\n",
      "epoch: 8 | 109664 / 114272 | training loss: 0.0003099655732512474\n",
      "epoch: 8 | 109696 / 114272 | training loss: 0.00029599646222777665\n",
      "epoch: 8 | 109728 / 114272 | training loss: 0.00026248677750118077\n",
      "epoch: 8 | 109760 / 114272 | training loss: 0.0003325322759337723\n",
      "epoch: 8 | 109792 / 114272 | training loss: 0.0003083230694755912\n",
      "epoch: 8 | 109824 / 114272 | training loss: 0.0002542440197430551\n",
      "epoch: 8 | 109856 / 114272 | training loss: 0.00017781919450499117\n",
      "epoch: 8 | 109888 / 114272 | training loss: 0.00021242332877591252\n",
      "epoch: 8 | 109920 / 114272 | training loss: 0.0002227211371064186\n",
      "epoch: 8 | 109952 / 114272 | training loss: 0.00031404581386595964\n",
      "epoch: 8 | 109984 / 114272 | training loss: 0.0003812412032857537\n",
      "epoch: 8 | 110016 / 114272 | training loss: 0.00044852413702756166\n",
      "epoch: 8 | 110048 / 114272 | training loss: 0.00021198572358116508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 110080 / 114272 | training loss: 0.00030316441552713513\n",
      "epoch: 8 | 110112 / 114272 | training loss: 0.00023981441336218268\n",
      "epoch: 8 | 110144 / 114272 | training loss: 0.0003647215198725462\n",
      "epoch: 8 | 110176 / 114272 | training loss: 0.00019645242718979716\n",
      "epoch: 8 | 110208 / 114272 | training loss: 0.0003120146575383842\n",
      "epoch: 8 | 110240 / 114272 | training loss: 0.00025414308765903115\n",
      "epoch: 8 | 110272 / 114272 | training loss: 0.0002659664605744183\n",
      "epoch: 8 | 110304 / 114272 | training loss: 0.00015822137356735766\n",
      "epoch: 8 | 110336 / 114272 | training loss: 0.0006406332249753177\n",
      "epoch: 8 | 110368 / 114272 | training loss: 0.07246692478656769\n",
      "epoch: 8 | 110400 / 114272 | training loss: 0.0003768577298615128\n",
      "epoch: 8 | 110432 / 114272 | training loss: 0.0002796540502458811\n",
      "epoch: 8 | 110464 / 114272 | training loss: 0.00029894555336795747\n",
      "epoch: 8 | 110496 / 114272 | training loss: 0.00048326398245990276\n",
      "epoch: 8 | 110528 / 114272 | training loss: 0.00021419419499579817\n",
      "epoch: 8 | 110560 / 114272 | training loss: 0.00039024424040690064\n",
      "epoch: 8 | 110592 / 114272 | training loss: 0.00026542096748016775\n",
      "epoch: 8 | 110624 / 114272 | training loss: 0.00020039065566379577\n",
      "epoch: 8 | 110656 / 114272 | training loss: 0.00027726308326236904\n",
      "epoch: 8 | 110688 / 114272 | training loss: 0.00019135483307763934\n",
      "epoch: 8 | 110720 / 114272 | training loss: 0.0002758293121587485\n",
      "epoch: 8 | 110752 / 114272 | training loss: 0.000276009930530563\n",
      "epoch: 8 | 110784 / 114272 | training loss: 0.0002371010195929557\n",
      "epoch: 8 | 110816 / 114272 | training loss: 0.00044919931679032743\n",
      "epoch: 8 | 110848 / 114272 | training loss: 0.00029220327269285917\n",
      "epoch: 8 | 110880 / 114272 | training loss: 0.00021695451869163662\n",
      "epoch: 8 | 110912 / 114272 | training loss: 0.0002107113687088713\n",
      "epoch: 8 | 110944 / 114272 | training loss: 0.0003347865422256291\n",
      "epoch: 8 | 110976 / 114272 | training loss: 0.00017540782573632896\n",
      "epoch: 8 | 111008 / 114272 | training loss: 0.0007603041012771428\n",
      "epoch: 8 | 111040 / 114272 | training loss: 0.00014368230768013746\n",
      "epoch: 8 | 111072 / 114272 | training loss: 0.00021678484336007386\n",
      "epoch: 8 | 111104 / 114272 | training loss: 0.00026913153124041855\n",
      "epoch: 8 | 111136 / 114272 | training loss: 0.00021732521418016404\n",
      "epoch: 8 | 111168 / 114272 | training loss: 0.00039231718983501196\n",
      "epoch: 8 | 111200 / 114272 | training loss: 0.11165652424097061\n",
      "epoch: 8 | 111232 / 114272 | training loss: 0.0005450970493257046\n",
      "epoch: 8 | 111264 / 114272 | training loss: 0.00030699375201947987\n",
      "epoch: 8 | 111296 / 114272 | training loss: 0.00029101662221364677\n",
      "epoch: 8 | 111328 / 114272 | training loss: 0.0003404350718483329\n",
      "epoch: 8 | 111360 / 114272 | training loss: 0.00038514327025040984\n",
      "epoch: 8 | 111392 / 114272 | training loss: 0.00028133971500210464\n",
      "epoch: 8 | 111424 / 114272 | training loss: 0.00017986755119636655\n",
      "epoch: 8 | 111456 / 114272 | training loss: 0.00016409756790380925\n",
      "epoch: 8 | 111488 / 114272 | training loss: 0.0003551481058821082\n",
      "epoch: 8 | 111520 / 114272 | training loss: 0.0003072247200179845\n",
      "epoch: 8 | 111552 / 114272 | training loss: 0.0011882225517183542\n",
      "epoch: 8 | 111584 / 114272 | training loss: 0.001003023236989975\n",
      "epoch: 8 | 111616 / 114272 | training loss: 0.0015577164012938738\n",
      "epoch: 8 | 111648 / 114272 | training loss: 0.0003155764134135097\n",
      "epoch: 8 | 111680 / 114272 | training loss: 0.000255139748333022\n",
      "epoch: 8 | 111712 / 114272 | training loss: 0.0002683085040189326\n",
      "epoch: 8 | 111744 / 114272 | training loss: 0.00023710662208031863\n",
      "epoch: 8 | 111776 / 114272 | training loss: 0.0003145365626551211\n",
      "epoch: 8 | 111808 / 114272 | training loss: 0.0002968413755297661\n",
      "epoch: 8 | 111840 / 114272 | training loss: 0.0007164926500990987\n",
      "epoch: 8 | 111872 / 114272 | training loss: 0.0003466878551989794\n",
      "epoch: 8 | 111904 / 114272 | training loss: 0.2274346947669983\n",
      "epoch: 8 | 111936 / 114272 | training loss: 0.0005924708675593138\n",
      "epoch: 8 | 111968 / 114272 | training loss: 0.11208970844745636\n",
      "epoch: 8 | 112000 / 114272 | training loss: 0.0003037009737454355\n",
      "epoch: 8 | 112032 / 114272 | training loss: 0.00029122232808731496\n",
      "epoch: 8 | 112064 / 114272 | training loss: 0.00041978489025495946\n",
      "epoch: 8 | 112096 / 114272 | training loss: 0.0003332470660097897\n",
      "epoch: 8 | 112128 / 114272 | training loss: 0.0002854890772141516\n",
      "epoch: 8 | 112160 / 114272 | training loss: 0.0005017248913645744\n",
      "epoch: 8 | 112192 / 114272 | training loss: 0.0006118113524280488\n",
      "epoch: 8 | 112224 / 114272 | training loss: 0.00018272352463100106\n",
      "epoch: 8 | 112256 / 114272 | training loss: 0.000493115745484829\n",
      "epoch: 8 | 112288 / 114272 | training loss: 0.0003430110518820584\n",
      "epoch: 8 | 112320 / 114272 | training loss: 0.00021502353774849325\n",
      "epoch: 8 | 112352 / 114272 | training loss: 0.0002654099080245942\n",
      "epoch: 8 | 112384 / 114272 | training loss: 0.194070503115654\n",
      "epoch: 8 | 112416 / 114272 | training loss: 0.00018757327052298933\n",
      "epoch: 8 | 112448 / 114272 | training loss: 0.00024401125847361982\n",
      "epoch: 8 | 112480 / 114272 | training loss: 0.0002833981125149876\n",
      "epoch: 8 | 112512 / 114272 | training loss: 0.0002448633313179016\n",
      "epoch: 8 | 112544 / 114272 | training loss: 0.0003539241151884198\n",
      "epoch: 8 | 112576 / 114272 | training loss: 0.0005808663554489613\n",
      "epoch: 8 | 112608 / 114272 | training loss: 0.12685753405094147\n",
      "epoch: 8 | 112640 / 114272 | training loss: 0.00025403709150850773\n",
      "epoch: 8 | 112672 / 114272 | training loss: 0.0003120928886346519\n",
      "epoch: 8 | 112704 / 114272 | training loss: 0.0009246042463928461\n",
      "epoch: 8 | 112736 / 114272 | training loss: 0.0002760710194706917\n",
      "epoch: 8 | 112768 / 114272 | training loss: 0.006432985886931419\n",
      "epoch: 8 | 112800 / 114272 | training loss: 0.00019599507504608482\n",
      "epoch: 8 | 112832 / 114272 | training loss: 0.00023491380852647126\n",
      "epoch: 8 | 112864 / 114272 | training loss: 0.0002866667346097529\n",
      "epoch: 8 | 112896 / 114272 | training loss: 0.0001898610935313627\n",
      "epoch: 8 | 112928 / 114272 | training loss: 0.001438613049685955\n",
      "epoch: 8 | 112960 / 114272 | training loss: 0.00024245541135314852\n",
      "epoch: 8 | 112992 / 114272 | training loss: 0.0004325409245211631\n",
      "epoch: 8 | 113024 / 114272 | training loss: 0.00032635527895763516\n",
      "epoch: 8 | 113056 / 114272 | training loss: 0.0002860091917682439\n",
      "epoch: 8 | 113088 / 114272 | training loss: 0.00020755588775500655\n",
      "epoch: 8 | 113120 / 114272 | training loss: 0.0004033069999422878\n",
      "epoch: 8 | 113152 / 114272 | training loss: 0.00029884203104302287\n",
      "epoch: 8 | 113184 / 114272 | training loss: 0.00018558489682618529\n",
      "epoch: 8 | 113216 / 114272 | training loss: 0.2434300184249878\n",
      "epoch: 8 | 113248 / 114272 | training loss: 0.00026850789436139166\n",
      "epoch: 8 | 113280 / 114272 | training loss: 0.00020280020544305444\n",
      "epoch: 8 | 113312 / 114272 | training loss: 0.0003919395967386663\n",
      "epoch: 8 | 113344 / 114272 | training loss: 0.00042924282024614513\n",
      "epoch: 8 | 113376 / 114272 | training loss: 0.00031275214860215783\n",
      "epoch: 8 | 113408 / 114272 | training loss: 0.0006023048772476614\n",
      "epoch: 8 | 113440 / 114272 | training loss: 0.00020142467110417783\n",
      "epoch: 8 | 113472 / 114272 | training loss: 0.0008403441752307117\n",
      "epoch: 8 | 113504 / 114272 | training loss: 0.31740719079971313\n",
      "epoch: 8 | 113536 / 114272 | training loss: 0.0011580748250707984\n",
      "epoch: 8 | 113568 / 114272 | training loss: 0.0003225534164812416\n",
      "epoch: 8 | 113600 / 114272 | training loss: 0.0005719041218981147\n",
      "epoch: 8 | 113632 / 114272 | training loss: 0.00027676852187141776\n",
      "epoch: 8 | 113664 / 114272 | training loss: 0.0002918807731475681\n",
      "epoch: 8 | 113696 / 114272 | training loss: 0.00013946426042821258\n",
      "epoch: 8 | 113728 / 114272 | training loss: 0.0018698214553296566\n",
      "epoch: 8 | 113760 / 114272 | training loss: 0.0003927415527869016\n",
      "epoch: 8 | 113792 / 114272 | training loss: 0.00048182683531194925\n",
      "epoch: 8 | 113824 / 114272 | training loss: 0.00028536838362924755\n",
      "epoch: 8 | 113856 / 114272 | training loss: 0.00046390804345719516\n",
      "epoch: 8 | 113888 / 114272 | training loss: 0.00036306760739535093\n",
      "epoch: 8 | 113920 / 114272 | training loss: 0.0003750388859771192\n",
      "epoch: 8 | 113952 / 114272 | training loss: 0.21196377277374268\n",
      "epoch: 8 | 113984 / 114272 | training loss: 0.0005531403585337102\n",
      "epoch: 8 | 114016 / 114272 | training loss: 0.00042246116208843887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | 114048 / 114272 | training loss: 0.00037814752431586385\n",
      "epoch: 8 | 114080 / 114272 | training loss: 0.0006591270794160664\n",
      "epoch: 8 | 114112 / 114272 | training loss: 0.09036445617675781\n",
      "epoch: 8 | 114144 / 114272 | training loss: 0.14571622014045715\n",
      "epoch: 8 | 114176 / 114272 | training loss: 0.0003034096152987331\n",
      "epoch: 8 | 114208 / 114272 | training loss: 0.0003016256378032267\n",
      "epoch: 8 | 114240 / 114272 | training loss: 0.0002819498477037996\n",
      "Training epoch 8 done! Average loss: 0.009926330651832342. Accuracy: 0.997654718566228\n",
      "Validation epoch 8 done! Average loss: 0.2656016353710429. Accurage: 0.9570050335570469\n",
      "Epoch 10 / 10\n",
      "------------------------------------------------------------------\n",
      "epoch: 9 | 0 / 114272 | training loss: 0.00029395113233476877\n",
      "epoch: 9 | 32 / 114272 | training loss: 0.00029753419221378863\n",
      "epoch: 9 | 64 / 114272 | training loss: 0.00031902140472084284\n",
      "epoch: 9 | 96 / 114272 | training loss: 0.21129673719406128\n",
      "epoch: 9 | 128 / 114272 | training loss: 0.00034233645419590175\n",
      "epoch: 9 | 160 / 114272 | training loss: 0.00032355522853322327\n",
      "epoch: 9 | 192 / 114272 | training loss: 0.00021236184693407267\n",
      "epoch: 9 | 224 / 114272 | training loss: 0.00034750724444165826\n",
      "epoch: 9 | 256 / 114272 | training loss: 0.00042563292663544416\n",
      "epoch: 9 | 288 / 114272 | training loss: 0.00019749400962609798\n",
      "epoch: 9 | 320 / 114272 | training loss: 0.000384925544494763\n",
      "epoch: 9 | 352 / 114272 | training loss: 0.11984027177095413\n",
      "epoch: 9 | 384 / 114272 | training loss: 0.00042296911124140024\n",
      "epoch: 9 | 416 / 114272 | training loss: 0.0002696214651223272\n",
      "epoch: 9 | 448 / 114272 | training loss: 0.00034009944647550583\n",
      "epoch: 9 | 480 / 114272 | training loss: 0.00024797447258606553\n",
      "epoch: 9 | 512 / 114272 | training loss: 0.00027734244940802455\n",
      "epoch: 9 | 544 / 114272 | training loss: 0.0004099679063074291\n",
      "epoch: 9 | 576 / 114272 | training loss: 0.00032287996145896614\n",
      "epoch: 9 | 608 / 114272 | training loss: 0.0004834033316001296\n",
      "epoch: 9 | 640 / 114272 | training loss: 0.0003881599986925721\n",
      "epoch: 9 | 672 / 114272 | training loss: 0.0003445704060140997\n",
      "epoch: 9 | 704 / 114272 | training loss: 0.00043644505785778165\n",
      "epoch: 9 | 736 / 114272 | training loss: 0.0002629933587741107\n",
      "epoch: 9 | 768 / 114272 | training loss: 0.00041231722570955753\n",
      "epoch: 9 | 800 / 114272 | training loss: 0.00028883054619655013\n",
      "epoch: 9 | 832 / 114272 | training loss: 0.0005092165083624423\n",
      "epoch: 9 | 864 / 114272 | training loss: 0.0006445690523833036\n",
      "epoch: 9 | 896 / 114272 | training loss: 0.0004281195579096675\n",
      "epoch: 9 | 928 / 114272 | training loss: 0.0003414827224332839\n",
      "epoch: 9 | 960 / 114272 | training loss: 0.0021813702769577503\n",
      "epoch: 9 | 992 / 114272 | training loss: 0.0006182329379953444\n",
      "epoch: 9 | 1024 / 114272 | training loss: 0.00025934926816262305\n",
      "epoch: 9 | 1056 / 114272 | training loss: 0.0007358597358688712\n",
      "epoch: 9 | 1088 / 114272 | training loss: 0.0003182851360179484\n",
      "epoch: 9 | 1120 / 114272 | training loss: 0.00032457636552862823\n",
      "epoch: 9 | 1152 / 114272 | training loss: 0.00040231976890936494\n",
      "epoch: 9 | 1184 / 114272 | training loss: 0.0005478979437611997\n",
      "epoch: 9 | 1216 / 114272 | training loss: 0.0004476169415283948\n",
      "epoch: 9 | 1248 / 114272 | training loss: 0.0005164688336662948\n",
      "epoch: 9 | 1280 / 114272 | training loss: 0.00035155919613316655\n",
      "epoch: 9 | 1312 / 114272 | training loss: 0.0009937383001670241\n",
      "epoch: 9 | 1344 / 114272 | training loss: 0.0012409636983647943\n",
      "epoch: 9 | 1376 / 114272 | training loss: 0.00039895548252388835\n",
      "epoch: 9 | 1408 / 114272 | training loss: 0.0003638758498709649\n",
      "epoch: 9 | 1440 / 114272 | training loss: 0.0020720569882541895\n",
      "epoch: 9 | 1472 / 114272 | training loss: 0.0003937212168239057\n",
      "epoch: 9 | 1504 / 114272 | training loss: 0.00042531665530987084\n",
      "epoch: 9 | 1536 / 114272 | training loss: 0.0003787573368754238\n",
      "epoch: 9 | 1568 / 114272 | training loss: 0.00047774086124263704\n",
      "epoch: 9 | 1600 / 114272 | training loss: 0.0003312751359771937\n",
      "epoch: 9 | 1632 / 114272 | training loss: 0.00040466379141435027\n",
      "epoch: 9 | 1664 / 114272 | training loss: 0.0002835314371623099\n",
      "epoch: 9 | 1696 / 114272 | training loss: 0.0005477548111230135\n",
      "epoch: 9 | 1728 / 114272 | training loss: 0.0005325365345925093\n",
      "epoch: 9 | 1760 / 114272 | training loss: 0.00043608894338831306\n",
      "epoch: 9 | 1792 / 114272 | training loss: 0.00028044392820447683\n",
      "epoch: 9 | 1824 / 114272 | training loss: 0.00020438131468836218\n",
      "epoch: 9 | 1856 / 114272 | training loss: 0.012199187651276588\n",
      "epoch: 9 | 1888 / 114272 | training loss: 0.0004397751181386411\n",
      "epoch: 9 | 1920 / 114272 | training loss: 0.00028583602397702634\n",
      "epoch: 9 | 1952 / 114272 | training loss: 0.00027897453401237726\n",
      "epoch: 9 | 1984 / 114272 | training loss: 0.0003291473549325019\n",
      "epoch: 9 | 2016 / 114272 | training loss: 0.0007184976129792631\n",
      "epoch: 9 | 2048 / 114272 | training loss: 0.00027098532882519066\n",
      "epoch: 9 | 2080 / 114272 | training loss: 0.0003805871238000691\n",
      "epoch: 9 | 2112 / 114272 | training loss: 0.00039864666177891195\n",
      "epoch: 9 | 2144 / 114272 | training loss: 0.0003248473221901804\n",
      "epoch: 9 | 2176 / 114272 | training loss: 0.0004556100757326931\n",
      "epoch: 9 | 2208 / 114272 | training loss: 0.0003915285342372954\n",
      "epoch: 9 | 2240 / 114272 | training loss: 0.0010204710997641087\n",
      "epoch: 9 | 2272 / 114272 | training loss: 0.0004638522514142096\n",
      "epoch: 9 | 2304 / 114272 | training loss: 0.0004401487240102142\n",
      "epoch: 9 | 2336 / 114272 | training loss: 0.0003712701436597854\n",
      "epoch: 9 | 2368 / 114272 | training loss: 0.00019000346946995705\n",
      "epoch: 9 | 2400 / 114272 | training loss: 0.0002502769057173282\n",
      "epoch: 9 | 2432 / 114272 | training loss: 0.0006014302489347756\n",
      "epoch: 9 | 2464 / 114272 | training loss: 0.0003267938445787877\n",
      "epoch: 9 | 2496 / 114272 | training loss: 0.0003136507293675095\n",
      "epoch: 9 | 2528 / 114272 | training loss: 0.0003774990909732878\n",
      "epoch: 9 | 2560 / 114272 | training loss: 0.18236884474754333\n",
      "epoch: 9 | 2592 / 114272 | training loss: 0.000315526791382581\n",
      "epoch: 9 | 2624 / 114272 | training loss: 0.00042126982589252293\n",
      "epoch: 9 | 2656 / 114272 | training loss: 0.00034602952655404806\n",
      "epoch: 9 | 2688 / 114272 | training loss: 0.0002809186698868871\n",
      "epoch: 9 | 2720 / 114272 | training loss: 0.0007549978909082711\n",
      "epoch: 9 | 2752 / 114272 | training loss: 0.0004555977357085794\n",
      "epoch: 9 | 2784 / 114272 | training loss: 0.0021391173359006643\n",
      "epoch: 9 | 2816 / 114272 | training loss: 0.0004127710417378694\n",
      "epoch: 9 | 2848 / 114272 | training loss: 0.0014476869255304337\n",
      "epoch: 9 | 2880 / 114272 | training loss: 0.0004302770830690861\n",
      "epoch: 9 | 2912 / 114272 | training loss: 0.21138831973075867\n",
      "epoch: 9 | 2944 / 114272 | training loss: 0.0004942006780765951\n",
      "epoch: 9 | 2976 / 114272 | training loss: 0.0010122618405148387\n",
      "epoch: 9 | 3008 / 114272 | training loss: 0.0019178926013410091\n",
      "epoch: 9 | 3040 / 114272 | training loss: 0.0002175291592720896\n",
      "epoch: 9 | 3072 / 114272 | training loss: 0.00023244478506967425\n",
      "epoch: 9 | 3104 / 114272 | training loss: 0.0008864485425874591\n",
      "epoch: 9 | 3136 / 114272 | training loss: 0.0003169236006215215\n",
      "epoch: 9 | 3168 / 114272 | training loss: 0.0003582569188438356\n",
      "epoch: 9 | 3200 / 114272 | training loss: 0.0002551382640376687\n",
      "epoch: 9 | 3232 / 114272 | training loss: 0.000457444490166381\n",
      "epoch: 9 | 3264 / 114272 | training loss: 0.0010513192974030972\n",
      "epoch: 9 | 3296 / 114272 | training loss: 0.00023474621411878616\n",
      "epoch: 9 | 3328 / 114272 | training loss: 0.00024849551846273243\n",
      "epoch: 9 | 3360 / 114272 | training loss: 0.0008813824388198555\n",
      "epoch: 9 | 3392 / 114272 | training loss: 0.000472692510811612\n",
      "epoch: 9 | 3424 / 114272 | training loss: 0.0003108269884251058\n",
      "epoch: 9 | 3456 / 114272 | training loss: 0.00018800896941684186\n",
      "epoch: 9 | 3488 / 114272 | training loss: 0.0005794429453089833\n",
      "epoch: 9 | 3520 / 114272 | training loss: 0.1618470996618271\n",
      "epoch: 9 | 3552 / 114272 | training loss: 0.0003355188819114119\n",
      "epoch: 9 | 3584 / 114272 | training loss: 0.00027002778369933367\n",
      "epoch: 9 | 3616 / 114272 | training loss: 0.0005704046925529838\n",
      "epoch: 9 | 3648 / 114272 | training loss: 0.0004162704572081566\n",
      "epoch: 9 | 3680 / 114272 | training loss: 0.00037413270911201835\n",
      "epoch: 9 | 3712 / 114272 | training loss: 0.0003547722299117595\n",
      "epoch: 9 | 3744 / 114272 | training loss: 0.0005731201963499188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 3776 / 114272 | training loss: 0.000524416973348707\n",
      "epoch: 9 | 3808 / 114272 | training loss: 0.00040024128975346684\n",
      "epoch: 9 | 3840 / 114272 | training loss: 0.0005183795583434403\n",
      "epoch: 9 | 3872 / 114272 | training loss: 0.026341179385781288\n",
      "epoch: 9 | 3904 / 114272 | training loss: 0.0003578155592549592\n",
      "epoch: 9 | 3936 / 114272 | training loss: 0.0006945126224309206\n",
      "epoch: 9 | 3968 / 114272 | training loss: 0.0001941812370205298\n",
      "epoch: 9 | 4000 / 114272 | training loss: 0.0003018403658643365\n",
      "epoch: 9 | 4032 / 114272 | training loss: 0.004412926267832518\n",
      "epoch: 9 | 4064 / 114272 | training loss: 0.00013149864389561117\n",
      "epoch: 9 | 4096 / 114272 | training loss: 0.0004802620969712734\n",
      "epoch: 9 | 4128 / 114272 | training loss: 0.0004632671771105379\n",
      "epoch: 9 | 4160 / 114272 | training loss: 0.0002744845696724951\n",
      "epoch: 9 | 4192 / 114272 | training loss: 0.0002087214816128835\n",
      "epoch: 9 | 4224 / 114272 | training loss: 0.0003997531603090465\n",
      "epoch: 9 | 4256 / 114272 | training loss: 0.00035908640711568296\n",
      "epoch: 9 | 4288 / 114272 | training loss: 0.11341828852891922\n",
      "epoch: 9 | 4320 / 114272 | training loss: 0.0004981922684237361\n",
      "epoch: 9 | 4352 / 114272 | training loss: 0.00032426617690362036\n",
      "epoch: 9 | 4384 / 114272 | training loss: 0.0006039454019628465\n",
      "epoch: 9 | 4416 / 114272 | training loss: 0.0003048719372600317\n",
      "epoch: 9 | 4448 / 114272 | training loss: 0.00029013832681812346\n",
      "epoch: 9 | 4480 / 114272 | training loss: 0.0005768232513219118\n",
      "epoch: 9 | 4512 / 114272 | training loss: 0.00039771394222043455\n",
      "epoch: 9 | 4544 / 114272 | training loss: 0.0005217907018959522\n",
      "epoch: 9 | 4576 / 114272 | training loss: 0.0005148356431163847\n",
      "epoch: 9 | 4608 / 114272 | training loss: 0.00042613191180862486\n",
      "epoch: 9 | 4640 / 114272 | training loss: 0.0003537939046509564\n",
      "epoch: 9 | 4672 / 114272 | training loss: 0.00029754132265225053\n",
      "epoch: 9 | 4704 / 114272 | training loss: 0.0006373758660629392\n",
      "epoch: 9 | 4736 / 114272 | training loss: 0.00031587909325025976\n",
      "epoch: 9 | 4768 / 114272 | training loss: 0.0005371296429075301\n",
      "epoch: 9 | 4800 / 114272 | training loss: 0.00039429357275366783\n",
      "epoch: 9 | 4832 / 114272 | training loss: 0.0003415710525587201\n",
      "epoch: 9 | 4864 / 114272 | training loss: 0.00020197655248921365\n",
      "epoch: 9 | 4896 / 114272 | training loss: 0.0004513739258982241\n",
      "epoch: 9 | 4928 / 114272 | training loss: 0.0005552542861551046\n",
      "epoch: 9 | 4960 / 114272 | training loss: 0.0004047635884489864\n",
      "epoch: 9 | 4992 / 114272 | training loss: 0.00033150980016216636\n",
      "epoch: 9 | 5024 / 114272 | training loss: 0.00039914168883115053\n",
      "epoch: 9 | 5056 / 114272 | training loss: 0.00035799399483948946\n",
      "epoch: 9 | 5088 / 114272 | training loss: 0.00043353933142498136\n",
      "epoch: 9 | 5120 / 114272 | training loss: 0.10477738827466965\n",
      "epoch: 9 | 5152 / 114272 | training loss: 0.0002688456152100116\n",
      "epoch: 9 | 5184 / 114272 | training loss: 0.0015621709171682596\n",
      "epoch: 9 | 5216 / 114272 | training loss: 0.0003665168769657612\n",
      "epoch: 9 | 5248 / 114272 | training loss: 0.00041707628406584263\n",
      "epoch: 9 | 5280 / 114272 | training loss: 0.00037251022877171636\n",
      "epoch: 9 | 5312 / 114272 | training loss: 0.00032298764563165605\n",
      "epoch: 9 | 5344 / 114272 | training loss: 0.000427611346822232\n",
      "epoch: 9 | 5376 / 114272 | training loss: 0.0015624213265255094\n",
      "epoch: 9 | 5408 / 114272 | training loss: 0.0007290816283784807\n",
      "epoch: 9 | 5440 / 114272 | training loss: 0.002058145822957158\n",
      "epoch: 9 | 5472 / 114272 | training loss: 0.00032691340311430395\n",
      "epoch: 9 | 5504 / 114272 | training loss: 0.00031601442606188357\n",
      "epoch: 9 | 5536 / 114272 | training loss: 0.0003606927930377424\n",
      "epoch: 9 | 5568 / 114272 | training loss: 0.00034115821472369134\n",
      "epoch: 9 | 5600 / 114272 | training loss: 0.00017963562277145684\n",
      "epoch: 9 | 5632 / 114272 | training loss: 0.00024521336308680475\n",
      "epoch: 9 | 5664 / 114272 | training loss: 0.00021462734730448574\n",
      "epoch: 9 | 5696 / 114272 | training loss: 0.00038357320590876043\n",
      "epoch: 9 | 5728 / 114272 | training loss: 0.0004478873743209988\n",
      "epoch: 9 | 5760 / 114272 | training loss: 0.00032797775929793715\n",
      "epoch: 9 | 5792 / 114272 | training loss: 0.0003186253015883267\n",
      "epoch: 9 | 5824 / 114272 | training loss: 0.00023816089378669858\n",
      "epoch: 9 | 5856 / 114272 | training loss: 0.00025938532780855894\n",
      "epoch: 9 | 5888 / 114272 | training loss: 0.12728963792324066\n",
      "epoch: 9 | 5920 / 114272 | training loss: 0.0002704122744034976\n",
      "epoch: 9 | 5952 / 114272 | training loss: 0.0006100930040702224\n",
      "epoch: 9 | 5984 / 114272 | training loss: 0.0002658991143107414\n",
      "epoch: 9 | 6016 / 114272 | training loss: 0.0004008196119684726\n",
      "epoch: 9 | 6048 / 114272 | training loss: 0.00036335954791866243\n",
      "epoch: 9 | 6080 / 114272 | training loss: 0.0014440666418522596\n",
      "epoch: 9 | 6112 / 114272 | training loss: 0.00034190627047792077\n",
      "epoch: 9 | 6144 / 114272 | training loss: 0.00039560027653351426\n",
      "epoch: 9 | 6176 / 114272 | training loss: 0.0012262160889804363\n",
      "epoch: 9 | 6208 / 114272 | training loss: 0.00020762908388860524\n",
      "epoch: 9 | 6240 / 114272 | training loss: 0.0005484733846969903\n",
      "epoch: 9 | 6272 / 114272 | training loss: 0.00016109294665511698\n",
      "epoch: 9 | 6304 / 114272 | training loss: 0.00029561633709818125\n",
      "epoch: 9 | 6336 / 114272 | training loss: 0.00027973370742984116\n",
      "epoch: 9 | 6368 / 114272 | training loss: 0.0002861462999135256\n",
      "epoch: 9 | 6400 / 114272 | training loss: 0.00021505620679818094\n",
      "epoch: 9 | 6432 / 114272 | training loss: 0.0003026168269570917\n",
      "epoch: 9 | 6464 / 114272 | training loss: 0.00016940639761742204\n",
      "epoch: 9 | 6496 / 114272 | training loss: 0.00031095644226297736\n",
      "epoch: 9 | 6528 / 114272 | training loss: 0.0002455587382428348\n",
      "epoch: 9 | 6560 / 114272 | training loss: 0.0002688030363060534\n",
      "epoch: 9 | 6592 / 114272 | training loss: 0.0002834741899278015\n",
      "epoch: 9 | 6624 / 114272 | training loss: 0.02973788045346737\n",
      "epoch: 9 | 6656 / 114272 | training loss: 0.0002546707110013813\n",
      "epoch: 9 | 6688 / 114272 | training loss: 0.0004163401899859309\n",
      "epoch: 9 | 6720 / 114272 | training loss: 0.00023203772434499115\n",
      "epoch: 9 | 6752 / 114272 | training loss: 0.012988799251616001\n",
      "epoch: 9 | 6784 / 114272 | training loss: 0.00037126836832612753\n",
      "epoch: 9 | 6816 / 114272 | training loss: 0.0004923990345560014\n",
      "epoch: 9 | 6848 / 114272 | training loss: 0.0008231757674366236\n",
      "epoch: 9 | 6880 / 114272 | training loss: 0.00039326856494881213\n",
      "epoch: 9 | 6912 / 114272 | training loss: 0.00020497319928836077\n",
      "epoch: 9 | 6944 / 114272 | training loss: 0.0003495888668112457\n",
      "epoch: 9 | 6976 / 114272 | training loss: 0.00028919309261254966\n",
      "epoch: 9 | 7008 / 114272 | training loss: 0.00032989736064337194\n",
      "epoch: 9 | 7040 / 114272 | training loss: 0.0034074110444635153\n",
      "epoch: 9 | 7072 / 114272 | training loss: 0.0003594527661334723\n",
      "epoch: 9 | 7104 / 114272 | training loss: 0.0003512499970383942\n",
      "epoch: 9 | 7136 / 114272 | training loss: 0.000317408237606287\n",
      "epoch: 9 | 7168 / 114272 | training loss: 0.0003133782302029431\n",
      "epoch: 9 | 7200 / 114272 | training loss: 0.0004700927238445729\n",
      "epoch: 9 | 7232 / 114272 | training loss: 0.00035013517481274903\n",
      "epoch: 9 | 7264 / 114272 | training loss: 0.0003435630351305008\n",
      "epoch: 9 | 7296 / 114272 | training loss: 0.007754601072520018\n",
      "epoch: 9 | 7328 / 114272 | training loss: 0.0002785396354738623\n",
      "epoch: 9 | 7360 / 114272 | training loss: 0.0002723282959777862\n",
      "epoch: 9 | 7392 / 114272 | training loss: 0.0004513016319833696\n",
      "epoch: 9 | 7424 / 114272 | training loss: 0.000323893444146961\n",
      "epoch: 9 | 7456 / 114272 | training loss: 0.0003002795856446028\n",
      "epoch: 9 | 7488 / 114272 | training loss: 0.0003140664775855839\n",
      "epoch: 9 | 7520 / 114272 | training loss: 0.09169462323188782\n",
      "epoch: 9 | 7552 / 114272 | training loss: 0.0003201195504516363\n",
      "epoch: 9 | 7584 / 114272 | training loss: 0.0005262843915261328\n",
      "epoch: 9 | 7616 / 114272 | training loss: 0.0004834083956666291\n",
      "epoch: 9 | 7648 / 114272 | training loss: 0.0006629882263951004\n",
      "epoch: 9 | 7680 / 114272 | training loss: 0.00046638300409540534\n",
      "epoch: 9 | 7712 / 114272 | training loss: 0.00035750106326304376\n",
      "epoch: 9 | 7744 / 114272 | training loss: 0.00026079590315930545\n",
      "epoch: 9 | 7776 / 114272 | training loss: 0.00032599037513136864\n",
      "epoch: 9 | 7808 / 114272 | training loss: 0.0002519891131669283\n",
      "epoch: 9 | 7840 / 114272 | training loss: 0.0002258168678963557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 7872 / 114272 | training loss: 0.00026005780091509223\n",
      "epoch: 9 | 7904 / 114272 | training loss: 0.0003364731092005968\n",
      "epoch: 9 | 7936 / 114272 | training loss: 0.0011904415441676974\n",
      "epoch: 9 | 7968 / 114272 | training loss: 0.00023621611762791872\n",
      "epoch: 9 | 8000 / 114272 | training loss: 0.0003734915517270565\n",
      "epoch: 9 | 8032 / 114272 | training loss: 0.00038487790152430534\n",
      "epoch: 9 | 8064 / 114272 | training loss: 0.0002738865150604397\n",
      "epoch: 9 | 8096 / 114272 | training loss: 0.00028947132523171604\n",
      "epoch: 9 | 8128 / 114272 | training loss: 0.00026510789757594466\n",
      "epoch: 9 | 8160 / 114272 | training loss: 0.00016202878032345325\n",
      "epoch: 9 | 8192 / 114272 | training loss: 0.00018911302322521806\n",
      "epoch: 9 | 8224 / 114272 | training loss: 0.0002477489470038563\n",
      "epoch: 9 | 8256 / 114272 | training loss: 0.0004864283255301416\n",
      "epoch: 9 | 8288 / 114272 | training loss: 0.00022089284902904183\n",
      "epoch: 9 | 8320 / 114272 | training loss: 0.0003396343963686377\n",
      "epoch: 9 | 8352 / 114272 | training loss: 0.00021064597240183502\n",
      "epoch: 9 | 8384 / 114272 | training loss: 0.0011319596087560058\n",
      "epoch: 9 | 8416 / 114272 | training loss: 0.00044227309990674257\n",
      "epoch: 9 | 8448 / 114272 | training loss: 0.00024216905876528472\n",
      "epoch: 9 | 8480 / 114272 | training loss: 0.020078960806131363\n",
      "epoch: 9 | 8512 / 114272 | training loss: 0.00021200762421358377\n",
      "epoch: 9 | 8544 / 114272 | training loss: 0.0005344917881302536\n",
      "epoch: 9 | 8576 / 114272 | training loss: 0.00024643726646900177\n",
      "epoch: 9 | 8608 / 114272 | training loss: 0.00024041278811637312\n",
      "epoch: 9 | 8640 / 114272 | training loss: 0.00027307530399411917\n",
      "epoch: 9 | 8672 / 114272 | training loss: 0.0003298324882052839\n",
      "epoch: 9 | 8704 / 114272 | training loss: 0.000354341056663543\n",
      "epoch: 9 | 8736 / 114272 | training loss: 0.0001965714036487043\n",
      "epoch: 9 | 8768 / 114272 | training loss: 0.03465840965509415\n",
      "epoch: 9 | 8800 / 114272 | training loss: 0.00031854744884185493\n",
      "epoch: 9 | 8832 / 114272 | training loss: 0.0003327184240333736\n",
      "epoch: 9 | 8864 / 114272 | training loss: 0.000334371899953112\n",
      "epoch: 9 | 8896 / 114272 | training loss: 0.00044708262430503964\n",
      "epoch: 9 | 8928 / 114272 | training loss: 0.00043740731780417264\n",
      "epoch: 9 | 8960 / 114272 | training loss: 0.0002100772107951343\n",
      "epoch: 9 | 8992 / 114272 | training loss: 0.0003033324028365314\n",
      "epoch: 9 | 9024 / 114272 | training loss: 0.00016503245569765568\n",
      "epoch: 9 | 9056 / 114272 | training loss: 0.000277737999567762\n",
      "epoch: 9 | 9088 / 114272 | training loss: 0.0003143881622236222\n",
      "epoch: 9 | 9120 / 114272 | training loss: 0.00024180753098335117\n",
      "epoch: 9 | 9152 / 114272 | training loss: 0.0003022608580067754\n",
      "epoch: 9 | 9184 / 114272 | training loss: 0.00029899642686359584\n",
      "epoch: 9 | 9216 / 114272 | training loss: 0.0002053711941698566\n",
      "epoch: 9 | 9248 / 114272 | training loss: 0.00019854489073622972\n",
      "epoch: 9 | 9280 / 114272 | training loss: 0.000506540120113641\n",
      "epoch: 9 | 9312 / 114272 | training loss: 0.00017234438564628363\n",
      "epoch: 9 | 9344 / 114272 | training loss: 0.0003415775136090815\n",
      "epoch: 9 | 9376 / 114272 | training loss: 0.0001540343655506149\n",
      "epoch: 9 | 9408 / 114272 | training loss: 0.00039951593498699367\n",
      "epoch: 9 | 9440 / 114272 | training loss: 0.08945300430059433\n",
      "epoch: 9 | 9472 / 114272 | training loss: 0.00041110877646133304\n",
      "epoch: 9 | 9504 / 114272 | training loss: 0.0004432844871189445\n",
      "epoch: 9 | 9536 / 114272 | training loss: 0.0005312550347298384\n",
      "epoch: 9 | 9568 / 114272 | training loss: 0.00014771132555324584\n",
      "epoch: 9 | 9600 / 114272 | training loss: 0.00028353548259474337\n",
      "epoch: 9 | 9632 / 114272 | training loss: 0.000252651225309819\n",
      "epoch: 9 | 9664 / 114272 | training loss: 0.0009341472759842873\n",
      "epoch: 9 | 9696 / 114272 | training loss: 0.0004943379899486899\n",
      "epoch: 9 | 9728 / 114272 | training loss: 0.00017738161841407418\n",
      "epoch: 9 | 9760 / 114272 | training loss: 0.0003265394479967654\n",
      "epoch: 9 | 9792 / 114272 | training loss: 0.0002471837215125561\n",
      "epoch: 9 | 9824 / 114272 | training loss: 0.00015401854761876166\n",
      "epoch: 9 | 9856 / 114272 | training loss: 0.15135826170444489\n",
      "epoch: 9 | 9888 / 114272 | training loss: 0.00031575391767546535\n",
      "epoch: 9 | 9920 / 114272 | training loss: 0.00022483862994704396\n",
      "epoch: 9 | 9952 / 114272 | training loss: 0.00024400325492024422\n",
      "epoch: 9 | 9984 / 114272 | training loss: 0.00044901156798005104\n",
      "epoch: 9 | 10016 / 114272 | training loss: 0.000812491518445313\n",
      "epoch: 9 | 10048 / 114272 | training loss: 0.00029027272830717266\n",
      "epoch: 9 | 10080 / 114272 | training loss: 0.00142213876824826\n",
      "epoch: 9 | 10112 / 114272 | training loss: 0.00026285674539394677\n",
      "epoch: 9 | 10144 / 114272 | training loss: 0.26165756583213806\n",
      "epoch: 9 | 10176 / 114272 | training loss: 0.0002387475542491302\n",
      "epoch: 9 | 10208 / 114272 | training loss: 0.00036111182998865843\n",
      "epoch: 9 | 10240 / 114272 | training loss: 0.00024108840443659574\n",
      "epoch: 9 | 10272 / 114272 | training loss: 0.0005354851600714028\n",
      "epoch: 9 | 10304 / 114272 | training loss: 0.00025022850604727864\n",
      "epoch: 9 | 10336 / 114272 | training loss: 0.000275440834229812\n",
      "epoch: 9 | 10368 / 114272 | training loss: 0.13242794573307037\n",
      "epoch: 9 | 10400 / 114272 | training loss: 0.0003659674257505685\n",
      "epoch: 9 | 10432 / 114272 | training loss: 0.00018272224406246096\n",
      "epoch: 9 | 10464 / 114272 | training loss: 0.00018184042710345238\n",
      "epoch: 9 | 10496 / 114272 | training loss: 0.0007311246008612216\n",
      "epoch: 9 | 10528 / 114272 | training loss: 0.00019238024833612144\n",
      "epoch: 9 | 10560 / 114272 | training loss: 0.00019821184105239809\n",
      "epoch: 9 | 10592 / 114272 | training loss: 0.0009218370541930199\n",
      "epoch: 9 | 10624 / 114272 | training loss: 0.000245000293944031\n",
      "epoch: 9 | 10656 / 114272 | training loss: 0.00033410938340239227\n",
      "epoch: 9 | 10688 / 114272 | training loss: 0.0002788175770547241\n",
      "epoch: 9 | 10720 / 114272 | training loss: 0.20890410244464874\n",
      "epoch: 9 | 10752 / 114272 | training loss: 0.0004126582352910191\n",
      "epoch: 9 | 10784 / 114272 | training loss: 0.0005478068487718701\n",
      "epoch: 9 | 10816 / 114272 | training loss: 0.00031187699642032385\n",
      "epoch: 9 | 10848 / 114272 | training loss: 0.003638621885329485\n",
      "epoch: 9 | 10880 / 114272 | training loss: 0.0005088104517199099\n",
      "epoch: 9 | 10912 / 114272 | training loss: 0.00019932648865506053\n",
      "epoch: 9 | 10944 / 114272 | training loss: 0.0003616226022131741\n",
      "epoch: 9 | 10976 / 114272 | training loss: 0.0003338671231176704\n",
      "epoch: 9 | 11008 / 114272 | training loss: 0.00033368964795954525\n",
      "epoch: 9 | 11040 / 114272 | training loss: 0.0003449501236900687\n",
      "epoch: 9 | 11072 / 114272 | training loss: 0.0002629558730404824\n",
      "epoch: 9 | 11104 / 114272 | training loss: 0.0005755360471084714\n",
      "epoch: 9 | 11136 / 114272 | training loss: 0.00037790604983456433\n",
      "epoch: 9 | 11168 / 114272 | training loss: 0.0007737272535450757\n",
      "epoch: 9 | 11200 / 114272 | training loss: 0.0003042971948161721\n",
      "epoch: 9 | 11232 / 114272 | training loss: 0.0002758432237897068\n",
      "epoch: 9 | 11264 / 114272 | training loss: 0.00034118472831323743\n",
      "epoch: 9 | 11296 / 114272 | training loss: 0.00035103902337141335\n",
      "epoch: 9 | 11328 / 114272 | training loss: 0.0002616267011035234\n",
      "epoch: 9 | 11360 / 114272 | training loss: 0.0004522856615949422\n",
      "epoch: 9 | 11392 / 114272 | training loss: 0.0007053402368910611\n",
      "epoch: 9 | 11424 / 114272 | training loss: 0.000325306347804144\n",
      "epoch: 9 | 11456 / 114272 | training loss: 0.00019676817464642227\n",
      "epoch: 9 | 11488 / 114272 | training loss: 0.0003467602364253253\n",
      "epoch: 9 | 11520 / 114272 | training loss: 0.0004182076081633568\n",
      "epoch: 9 | 11552 / 114272 | training loss: 0.00033957071718759835\n",
      "epoch: 9 | 11584 / 114272 | training loss: 0.00020693869737442583\n",
      "epoch: 9 | 11616 / 114272 | training loss: 0.0002639695012476295\n",
      "epoch: 9 | 11648 / 114272 | training loss: 0.00025525345699861646\n",
      "epoch: 9 | 11680 / 114272 | training loss: 0.00012899567082058638\n",
      "epoch: 9 | 11712 / 114272 | training loss: 0.0002634006377775222\n",
      "epoch: 9 | 11744 / 114272 | training loss: 0.00038900761865079403\n",
      "epoch: 9 | 11776 / 114272 | training loss: 0.00022594344045501202\n",
      "epoch: 9 | 11808 / 114272 | training loss: 0.00020524574210867286\n",
      "epoch: 9 | 11840 / 114272 | training loss: 0.0015693464083597064\n",
      "epoch: 9 | 11872 / 114272 | training loss: 0.00018865193123929203\n",
      "epoch: 9 | 11904 / 114272 | training loss: 0.00037071449332870543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 11936 / 114272 | training loss: 0.0005044415011070669\n",
      "epoch: 9 | 11968 / 114272 | training loss: 0.0005298095638863742\n",
      "epoch: 9 | 12000 / 114272 | training loss: 0.00022718314721714705\n",
      "epoch: 9 | 12032 / 114272 | training loss: 0.027935869991779327\n",
      "epoch: 9 | 12064 / 114272 | training loss: 0.1502295881509781\n",
      "epoch: 9 | 12096 / 114272 | training loss: 0.00031574079184792936\n",
      "epoch: 9 | 12128 / 114272 | training loss: 0.00011051116598537192\n",
      "epoch: 9 | 12160 / 114272 | training loss: 0.00018169420945923775\n",
      "epoch: 9 | 12192 / 114272 | training loss: 0.0002897614031098783\n",
      "epoch: 9 | 12224 / 114272 | training loss: 0.00042535876855254173\n",
      "epoch: 9 | 12256 / 114272 | training loss: 0.000308023183606565\n",
      "epoch: 9 | 12288 / 114272 | training loss: 0.0004123038670513779\n",
      "epoch: 9 | 12320 / 114272 | training loss: 0.00012053614045726135\n",
      "epoch: 9 | 12352 / 114272 | training loss: 0.0003694753395393491\n",
      "epoch: 9 | 12384 / 114272 | training loss: 0.0006224859389476478\n",
      "epoch: 9 | 12416 / 114272 | training loss: 0.0003137088206131011\n",
      "epoch: 9 | 12448 / 114272 | training loss: 0.00026894602342508733\n",
      "epoch: 9 | 12480 / 114272 | training loss: 0.00015367520973086357\n",
      "epoch: 9 | 12512 / 114272 | training loss: 0.00033433467615395784\n",
      "epoch: 9 | 12544 / 114272 | training loss: 0.00025900034233927727\n",
      "epoch: 9 | 12576 / 114272 | training loss: 0.00033277514739893377\n",
      "epoch: 9 | 12608 / 114272 | training loss: 0.0002433994086459279\n",
      "epoch: 9 | 12640 / 114272 | training loss: 0.0015297220088541508\n",
      "epoch: 9 | 12672 / 114272 | training loss: 0.0003866645274683833\n",
      "epoch: 9 | 12704 / 114272 | training loss: 0.00037535623414441943\n",
      "epoch: 9 | 12736 / 114272 | training loss: 0.00015202538634184748\n",
      "epoch: 9 | 12768 / 114272 | training loss: 0.0002656872384250164\n",
      "epoch: 9 | 12800 / 114272 | training loss: 0.00035303819458931684\n",
      "epoch: 9 | 12832 / 114272 | training loss: 0.00012434237578418106\n",
      "epoch: 9 | 12864 / 114272 | training loss: 0.00027191374101676047\n",
      "epoch: 9 | 12896 / 114272 | training loss: 0.00024311256129294634\n",
      "epoch: 9 | 12928 / 114272 | training loss: 0.00019026678637601435\n",
      "epoch: 9 | 12960 / 114272 | training loss: 0.08513762056827545\n",
      "epoch: 9 | 12992 / 114272 | training loss: 0.01855647563934326\n",
      "epoch: 9 | 13024 / 114272 | training loss: 0.00022107762924861163\n",
      "epoch: 9 | 13056 / 114272 | training loss: 0.0009153094724752009\n",
      "epoch: 9 | 13088 / 114272 | training loss: 0.0003293636837042868\n",
      "epoch: 9 | 13120 / 114272 | training loss: 0.0003782072162721306\n",
      "epoch: 9 | 13152 / 114272 | training loss: 0.0002650643000379205\n",
      "epoch: 9 | 13184 / 114272 | training loss: 0.00026605400489643216\n",
      "epoch: 9 | 13216 / 114272 | training loss: 0.0003657325287349522\n",
      "epoch: 9 | 13248 / 114272 | training loss: 0.0005383921088650823\n",
      "epoch: 9 | 13280 / 114272 | training loss: 0.0003173296863678843\n",
      "epoch: 9 | 13312 / 114272 | training loss: 0.10451707988977432\n",
      "epoch: 9 | 13344 / 114272 | training loss: 0.00022701520356349647\n",
      "epoch: 9 | 13376 / 114272 | training loss: 0.00030095255351625383\n",
      "epoch: 9 | 13408 / 114272 | training loss: 0.0003945411881431937\n",
      "epoch: 9 | 13440 / 114272 | training loss: 0.00046131451381370425\n",
      "epoch: 9 | 13472 / 114272 | training loss: 0.00029242184245958924\n",
      "epoch: 9 | 13504 / 114272 | training loss: 0.00024377483350690454\n",
      "epoch: 9 | 13536 / 114272 | training loss: 0.00022569372958969325\n",
      "epoch: 9 | 13568 / 114272 | training loss: 0.00027477616094984114\n",
      "epoch: 9 | 13600 / 114272 | training loss: 0.0005312200519256294\n",
      "epoch: 9 | 13632 / 114272 | training loss: 0.0001948123099282384\n",
      "epoch: 9 | 13664 / 114272 | training loss: 0.0005317866452969611\n",
      "epoch: 9 | 13696 / 114272 | training loss: 0.0002845449198503047\n",
      "epoch: 9 | 13728 / 114272 | training loss: 0.00031401938758790493\n",
      "epoch: 9 | 13760 / 114272 | training loss: 0.0003454902325756848\n",
      "epoch: 9 | 13792 / 114272 | training loss: 0.00023473282635677606\n",
      "epoch: 9 | 13824 / 114272 | training loss: 0.09843611717224121\n",
      "epoch: 9 | 13856 / 114272 | training loss: 0.0003109236422460526\n",
      "epoch: 9 | 13888 / 114272 | training loss: 0.00026982001145370305\n",
      "epoch: 9 | 13920 / 114272 | training loss: 0.00022797175915911794\n",
      "epoch: 9 | 13952 / 114272 | training loss: 0.0003948177327401936\n",
      "epoch: 9 | 13984 / 114272 | training loss: 0.0003374587104190141\n",
      "epoch: 9 | 14016 / 114272 | training loss: 0.20680135488510132\n",
      "epoch: 9 | 14048 / 114272 | training loss: 0.0001936789194587618\n",
      "epoch: 9 | 14080 / 114272 | training loss: 0.0003490396193228662\n",
      "epoch: 9 | 14112 / 114272 | training loss: 0.00014045134594198316\n",
      "epoch: 9 | 14144 / 114272 | training loss: 0.00042039487743750215\n",
      "epoch: 9 | 14176 / 114272 | training loss: 0.00028435923741199076\n",
      "epoch: 9 | 14208 / 114272 | training loss: 0.00020070988102816045\n",
      "epoch: 9 | 14240 / 114272 | training loss: 0.0002791931328829378\n",
      "epoch: 9 | 14272 / 114272 | training loss: 0.0001971056335605681\n",
      "epoch: 9 | 14304 / 114272 | training loss: 0.1184692457318306\n",
      "epoch: 9 | 14336 / 114272 | training loss: 0.00028129288693889976\n",
      "epoch: 9 | 14368 / 114272 | training loss: 0.00018196205201093107\n",
      "epoch: 9 | 14400 / 114272 | training loss: 0.00037473507109098136\n",
      "epoch: 9 | 14432 / 114272 | training loss: 0.00020464511180762202\n",
      "epoch: 9 | 14464 / 114272 | training loss: 0.00021676583855878562\n",
      "epoch: 9 | 14496 / 114272 | training loss: 0.000304871384287253\n",
      "epoch: 9 | 14528 / 114272 | training loss: 0.00031436904100701213\n",
      "epoch: 9 | 14560 / 114272 | training loss: 0.0002931943454314023\n",
      "epoch: 9 | 14592 / 114272 | training loss: 0.0003545873623806983\n",
      "epoch: 9 | 14624 / 114272 | training loss: 0.00011651388194877654\n",
      "epoch: 9 | 14656 / 114272 | training loss: 0.00036102510057389736\n",
      "epoch: 9 | 14688 / 114272 | training loss: 0.00041978672379627824\n",
      "epoch: 9 | 14720 / 114272 | training loss: 0.0002479733666405082\n",
      "epoch: 9 | 14752 / 114272 | training loss: 0.0002202370233135298\n",
      "epoch: 9 | 14784 / 114272 | training loss: 0.00026747205993160605\n",
      "epoch: 9 | 14816 / 114272 | training loss: 0.0003810595953837037\n",
      "epoch: 9 | 14848 / 114272 | training loss: 0.00048019742825999856\n",
      "epoch: 9 | 14880 / 114272 | training loss: 0.0002798573696054518\n",
      "epoch: 9 | 14912 / 114272 | training loss: 0.00027946149930357933\n",
      "epoch: 9 | 14944 / 114272 | training loss: 0.00029690066003240645\n",
      "epoch: 9 | 14976 / 114272 | training loss: 0.0007097796769812703\n",
      "epoch: 9 | 15008 / 114272 | training loss: 0.00033701685606501997\n",
      "epoch: 9 | 15040 / 114272 | training loss: 0.0012686027912423015\n",
      "epoch: 9 | 15072 / 114272 | training loss: 0.0005295703886076808\n",
      "epoch: 9 | 15104 / 114272 | training loss: 0.0004671508795581758\n",
      "epoch: 9 | 15136 / 114272 | training loss: 0.00032781591289676726\n",
      "epoch: 9 | 15168 / 114272 | training loss: 0.0002645977074280381\n",
      "epoch: 9 | 15200 / 114272 | training loss: 0.00035070240846835077\n",
      "epoch: 9 | 15232 / 114272 | training loss: 0.0007956462795846164\n",
      "epoch: 9 | 15264 / 114272 | training loss: 0.0006269228179007769\n",
      "epoch: 9 | 15296 / 114272 | training loss: 0.00044523493852466345\n",
      "epoch: 9 | 15328 / 114272 | training loss: 0.0004182617994956672\n",
      "epoch: 9 | 15360 / 114272 | training loss: 0.0004967193235643208\n",
      "epoch: 9 | 15392 / 114272 | training loss: 0.00023043436522129923\n",
      "epoch: 9 | 15424 / 114272 | training loss: 0.0003945522184949368\n",
      "epoch: 9 | 15456 / 114272 | training loss: 0.0005600852891802788\n",
      "epoch: 9 | 15488 / 114272 | training loss: 0.0002937793324235827\n",
      "epoch: 9 | 15520 / 114272 | training loss: 0.00033638105378486216\n",
      "epoch: 9 | 15552 / 114272 | training loss: 0.00021838878456037492\n",
      "epoch: 9 | 15584 / 114272 | training loss: 0.0001793498231563717\n",
      "epoch: 9 | 15616 / 114272 | training loss: 0.00042989227222278714\n",
      "epoch: 9 | 15648 / 114272 | training loss: 0.00018556174472905695\n",
      "epoch: 9 | 15680 / 114272 | training loss: 0.00016937439795583487\n",
      "epoch: 9 | 15712 / 114272 | training loss: 0.00030727835837751627\n",
      "epoch: 9 | 15744 / 114272 | training loss: 0.0002276353188790381\n",
      "epoch: 9 | 15776 / 114272 | training loss: 0.0002387045678915456\n",
      "epoch: 9 | 15808 / 114272 | training loss: 0.00019159082148689777\n",
      "epoch: 9 | 15840 / 114272 | training loss: 0.0001342907635262236\n",
      "epoch: 9 | 15872 / 114272 | training loss: 0.00018602554337121546\n",
      "epoch: 9 | 15904 / 114272 | training loss: 0.0003458924766164273\n",
      "epoch: 9 | 15936 / 114272 | training loss: 0.0003661983064375818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 15968 / 114272 | training loss: 0.09906086325645447\n",
      "epoch: 9 | 16000 / 114272 | training loss: 0.00034416347625665367\n",
      "epoch: 9 | 16032 / 114272 | training loss: 0.00016337934357579798\n",
      "epoch: 9 | 16064 / 114272 | training loss: 0.02138589508831501\n",
      "epoch: 9 | 16096 / 114272 | training loss: 0.00034192093880847096\n",
      "epoch: 9 | 16128 / 114272 | training loss: 0.00034262161352671683\n",
      "epoch: 9 | 16160 / 114272 | training loss: 0.0007373022381216288\n",
      "epoch: 9 | 16192 / 114272 | training loss: 0.00033292159787379205\n",
      "epoch: 9 | 16224 / 114272 | training loss: 0.00017236979329027236\n",
      "epoch: 9 | 16256 / 114272 | training loss: 0.018696976825594902\n",
      "epoch: 9 | 16288 / 114272 | training loss: 0.00024684134405106306\n",
      "epoch: 9 | 16320 / 114272 | training loss: 0.0003923037729691714\n",
      "epoch: 9 | 16352 / 114272 | training loss: 0.0013930328423157334\n",
      "epoch: 9 | 16384 / 114272 | training loss: 0.016912192106246948\n",
      "epoch: 9 | 16416 / 114272 | training loss: 0.0002969965571537614\n",
      "epoch: 9 | 16448 / 114272 | training loss: 0.00041961795068345964\n",
      "epoch: 9 | 16480 / 114272 | training loss: 0.0005523981526494026\n",
      "epoch: 9 | 16512 / 114272 | training loss: 0.00033204510691575706\n",
      "epoch: 9 | 16544 / 114272 | training loss: 0.0008284912328235805\n",
      "epoch: 9 | 16576 / 114272 | training loss: 0.0002776511828415096\n",
      "epoch: 9 | 16608 / 114272 | training loss: 0.00030472135404124856\n",
      "epoch: 9 | 16640 / 114272 | training loss: 0.00029822115902788937\n",
      "epoch: 9 | 16672 / 114272 | training loss: 0.167737677693367\n",
      "epoch: 9 | 16704 / 114272 | training loss: 0.20081914961338043\n",
      "epoch: 9 | 16736 / 114272 | training loss: 0.00034556654281914234\n",
      "epoch: 9 | 16768 / 114272 | training loss: 0.00046159158227965236\n",
      "epoch: 9 | 16800 / 114272 | training loss: 0.0004143452679272741\n",
      "epoch: 9 | 16832 / 114272 | training loss: 0.00037755456287413836\n",
      "epoch: 9 | 16864 / 114272 | training loss: 0.00019059587793890387\n",
      "epoch: 9 | 16896 / 114272 | training loss: 0.003274645423516631\n",
      "epoch: 9 | 16928 / 114272 | training loss: 0.00016879526083357632\n",
      "epoch: 9 | 16960 / 114272 | training loss: 0.0003413118829485029\n",
      "epoch: 9 | 16992 / 114272 | training loss: 0.00014251745596993715\n",
      "epoch: 9 | 17024 / 114272 | training loss: 0.0003523031191434711\n",
      "epoch: 9 | 17056 / 114272 | training loss: 0.0003651160513982177\n",
      "epoch: 9 | 17088 / 114272 | training loss: 0.0003007970517501235\n",
      "epoch: 9 | 17120 / 114272 | training loss: 0.000257274106843397\n",
      "epoch: 9 | 17152 / 114272 | training loss: 0.0002656207070685923\n",
      "epoch: 9 | 17184 / 114272 | training loss: 0.00037226968561299145\n",
      "epoch: 9 | 17216 / 114272 | training loss: 0.0005777793121524155\n",
      "epoch: 9 | 17248 / 114272 | training loss: 0.00044171095942147076\n",
      "epoch: 9 | 17280 / 114272 | training loss: 0.00018041295697912574\n",
      "epoch: 9 | 17312 / 114272 | training loss: 0.00047077718772925436\n",
      "epoch: 9 | 17344 / 114272 | training loss: 0.00047749545774422586\n",
      "epoch: 9 | 17376 / 114272 | training loss: 0.00026091968175023794\n",
      "epoch: 9 | 17408 / 114272 | training loss: 0.00035787123488262296\n",
      "epoch: 9 | 17440 / 114272 | training loss: 0.00028220913372933865\n",
      "epoch: 9 | 17472 / 114272 | training loss: 0.07151118665933609\n",
      "epoch: 9 | 17504 / 114272 | training loss: 0.00019857077859342098\n",
      "epoch: 9 | 17536 / 114272 | training loss: 0.0005577845731750131\n",
      "epoch: 9 | 17568 / 114272 | training loss: 0.0003945466596633196\n",
      "epoch: 9 | 17600 / 114272 | training loss: 0.0022595119662582874\n",
      "epoch: 9 | 17632 / 114272 | training loss: 0.0005765057285316288\n",
      "epoch: 9 | 17664 / 114272 | training loss: 0.0002956637181341648\n",
      "epoch: 9 | 17696 / 114272 | training loss: 0.0060132946819067\n",
      "epoch: 9 | 17728 / 114272 | training loss: 0.00032323377672582865\n",
      "epoch: 9 | 17760 / 114272 | training loss: 0.0002330927673028782\n",
      "epoch: 9 | 17792 / 114272 | training loss: 0.0008483418496325612\n",
      "epoch: 9 | 17824 / 114272 | training loss: 0.00027935486286878586\n",
      "epoch: 9 | 17856 / 114272 | training loss: 0.00022258091485127807\n",
      "epoch: 9 | 17888 / 114272 | training loss: 0.00029034249018877745\n",
      "epoch: 9 | 17920 / 114272 | training loss: 0.0012029410572722554\n",
      "epoch: 9 | 17952 / 114272 | training loss: 0.00024436163948848844\n",
      "epoch: 9 | 17984 / 114272 | training loss: 0.00035929784644395113\n",
      "epoch: 9 | 18016 / 114272 | training loss: 0.00030346971470862627\n",
      "epoch: 9 | 18048 / 114272 | training loss: 0.0002158114657504484\n",
      "epoch: 9 | 18080 / 114272 | training loss: 0.00034407805651426315\n",
      "epoch: 9 | 18112 / 114272 | training loss: 0.0003795293450821191\n",
      "epoch: 9 | 18144 / 114272 | training loss: 0.0002455227659083903\n",
      "epoch: 9 | 18176 / 114272 | training loss: 0.0003126728115603328\n",
      "epoch: 9 | 18208 / 114272 | training loss: 0.00044416164746508\n",
      "epoch: 9 | 18240 / 114272 | training loss: 0.0031289865728467703\n",
      "epoch: 9 | 18272 / 114272 | training loss: 0.00018268340500071645\n",
      "epoch: 9 | 18304 / 114272 | training loss: 0.0002751516876742244\n",
      "epoch: 9 | 18336 / 114272 | training loss: 0.0002704175713006407\n",
      "epoch: 9 | 18368 / 114272 | training loss: 0.0003219505597371608\n",
      "epoch: 9 | 18400 / 114272 | training loss: 0.00039049863698892295\n",
      "epoch: 9 | 18432 / 114272 | training loss: 0.0001616796653252095\n",
      "epoch: 9 | 18464 / 114272 | training loss: 0.0002824424591381103\n",
      "epoch: 9 | 18496 / 114272 | training loss: 0.0001347811339655891\n",
      "epoch: 9 | 18528 / 114272 | training loss: 0.00042801047675311565\n",
      "epoch: 9 | 18560 / 114272 | training loss: 0.000293225166387856\n",
      "epoch: 9 | 18592 / 114272 | training loss: 0.0003320637915749103\n",
      "epoch: 9 | 18624 / 114272 | training loss: 0.12722846865653992\n",
      "epoch: 9 | 18656 / 114272 | training loss: 0.00030748863355256617\n",
      "epoch: 9 | 18688 / 114272 | training loss: 0.0004751301894430071\n",
      "epoch: 9 | 18720 / 114272 | training loss: 0.00024436702369712293\n",
      "epoch: 9 | 18752 / 114272 | training loss: 0.0004210300394333899\n",
      "epoch: 9 | 18784 / 114272 | training loss: 0.00034927885280922055\n",
      "epoch: 9 | 18816 / 114272 | training loss: 0.0003833112132269889\n",
      "epoch: 9 | 18848 / 114272 | training loss: 0.00026127599994651973\n",
      "epoch: 9 | 18880 / 114272 | training loss: 0.0002532168000470847\n",
      "epoch: 9 | 18912 / 114272 | training loss: 0.000335821823682636\n",
      "epoch: 9 | 18944 / 114272 | training loss: 0.00032364288927055895\n",
      "epoch: 9 | 18976 / 114272 | training loss: 0.00030946600600145757\n",
      "epoch: 9 | 19008 / 114272 | training loss: 0.0002802466042339802\n",
      "epoch: 9 | 19040 / 114272 | training loss: 0.00024090959050226957\n",
      "epoch: 9 | 19072 / 114272 | training loss: 0.00029368081595748663\n",
      "epoch: 9 | 19104 / 114272 | training loss: 0.0003330024774186313\n",
      "epoch: 9 | 19136 / 114272 | training loss: 0.0003855491813737899\n",
      "epoch: 9 | 19168 / 114272 | training loss: 0.000254618440521881\n",
      "epoch: 9 | 19200 / 114272 | training loss: 0.00020630653307307512\n",
      "epoch: 9 | 19232 / 114272 | training loss: 0.0004775506386067718\n",
      "epoch: 9 | 19264 / 114272 | training loss: 0.0001749567163642496\n",
      "epoch: 9 | 19296 / 114272 | training loss: 0.00028163607930764556\n",
      "epoch: 9 | 19328 / 114272 | training loss: 0.00017860365915112197\n",
      "epoch: 9 | 19360 / 114272 | training loss: 0.000500159920193255\n",
      "epoch: 9 | 19392 / 114272 | training loss: 0.0003135403385385871\n",
      "epoch: 9 | 19424 / 114272 | training loss: 0.0003190721618011594\n",
      "epoch: 9 | 19456 / 114272 | training loss: 0.00033713268931023777\n",
      "epoch: 9 | 19488 / 114272 | training loss: 0.00045494575169868767\n",
      "epoch: 9 | 19520 / 114272 | training loss: 0.0002666999353095889\n",
      "epoch: 9 | 19552 / 114272 | training loss: 0.00021787246805615723\n",
      "epoch: 9 | 19584 / 114272 | training loss: 0.00028989979182370007\n",
      "epoch: 9 | 19616 / 114272 | training loss: 0.0002416432835161686\n",
      "epoch: 9 | 19648 / 114272 | training loss: 0.00039351527811959386\n",
      "epoch: 9 | 19680 / 114272 | training loss: 0.02001500315964222\n",
      "epoch: 9 | 19712 / 114272 | training loss: 0.00020150223281234503\n",
      "epoch: 9 | 19744 / 114272 | training loss: 0.0003638986381702125\n",
      "epoch: 9 | 19776 / 114272 | training loss: 0.00031308637699112296\n",
      "epoch: 9 | 19808 / 114272 | training loss: 0.0003477061400189996\n",
      "epoch: 9 | 19840 / 114272 | training loss: 0.00022390253434423357\n",
      "epoch: 9 | 19872 / 114272 | training loss: 0.00019691292254719883\n",
      "epoch: 9 | 19904 / 114272 | training loss: 0.00021429792104754597\n",
      "epoch: 9 | 19936 / 114272 | training loss: 0.0003438499115873128\n",
      "epoch: 9 | 19968 / 114272 | training loss: 0.00026938426890410483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 20000 / 114272 | training loss: 0.0004777918220497668\n",
      "epoch: 9 | 20032 / 114272 | training loss: 0.00024104802287183702\n",
      "epoch: 9 | 20064 / 114272 | training loss: 0.001310899155214429\n",
      "epoch: 9 | 20096 / 114272 | training loss: 0.0004234491498209536\n",
      "epoch: 9 | 20128 / 114272 | training loss: 0.0002744434168562293\n",
      "epoch: 9 | 20160 / 114272 | training loss: 0.00014236947754397988\n",
      "epoch: 9 | 20192 / 114272 | training loss: 0.00012752421025652438\n",
      "epoch: 9 | 20224 / 114272 | training loss: 0.000486787932459265\n",
      "epoch: 9 | 20256 / 114272 | training loss: 0.00029600001289509237\n",
      "epoch: 9 | 20288 / 114272 | training loss: 0.0017114507500082254\n",
      "epoch: 9 | 20320 / 114272 | training loss: 0.00020551873603835702\n",
      "epoch: 9 | 20352 / 114272 | training loss: 0.00022527437249664217\n",
      "epoch: 9 | 20384 / 114272 | training loss: 0.0002733865403570235\n",
      "epoch: 9 | 20416 / 114272 | training loss: 0.00023845444957260042\n",
      "epoch: 9 | 20448 / 114272 | training loss: 0.0002904076245613396\n",
      "epoch: 9 | 20480 / 114272 | training loss: 0.0002490549231879413\n",
      "epoch: 9 | 20512 / 114272 | training loss: 0.00022707972675561905\n",
      "epoch: 9 | 20544 / 114272 | training loss: 0.0002447865845169872\n",
      "epoch: 9 | 20576 / 114272 | training loss: 0.000354733900167048\n",
      "epoch: 9 | 20608 / 114272 | training loss: 0.00038557726657018065\n",
      "epoch: 9 | 20640 / 114272 | training loss: 0.00022138214262668043\n",
      "epoch: 9 | 20672 / 114272 | training loss: 0.0002605276822578162\n",
      "epoch: 9 | 20704 / 114272 | training loss: 0.0007385674980469048\n",
      "epoch: 9 | 20736 / 114272 | training loss: 0.18591231107711792\n",
      "epoch: 9 | 20768 / 114272 | training loss: 0.00044003018410876393\n",
      "epoch: 9 | 20800 / 114272 | training loss: 0.01400461420416832\n",
      "epoch: 9 | 20832 / 114272 | training loss: 0.00011879316298291087\n",
      "epoch: 9 | 20864 / 114272 | training loss: 0.00023105574655346572\n",
      "epoch: 9 | 20896 / 114272 | training loss: 0.0003021398151759058\n",
      "epoch: 9 | 20928 / 114272 | training loss: 0.0002454835921525955\n",
      "epoch: 9 | 20960 / 114272 | training loss: 0.0002517796528991312\n",
      "epoch: 9 | 20992 / 114272 | training loss: 0.0003783406282309443\n",
      "epoch: 9 | 21024 / 114272 | training loss: 0.0004071415460202843\n",
      "epoch: 9 | 21056 / 114272 | training loss: 0.0003023294557351619\n",
      "epoch: 9 | 21088 / 114272 | training loss: 0.0001921982766361907\n",
      "epoch: 9 | 21120 / 114272 | training loss: 0.0002892617485485971\n",
      "epoch: 9 | 21152 / 114272 | training loss: 0.0009233579621650279\n",
      "epoch: 9 | 21184 / 114272 | training loss: 0.00046462647151201963\n",
      "epoch: 9 | 21216 / 114272 | training loss: 0.00021807802841067314\n",
      "epoch: 9 | 21248 / 114272 | training loss: 0.0018653040751814842\n",
      "epoch: 9 | 21280 / 114272 | training loss: 0.0003086942306254059\n",
      "epoch: 9 | 21312 / 114272 | training loss: 0.0003131777048110962\n",
      "epoch: 9 | 21344 / 114272 | training loss: 0.0001339374139206484\n",
      "epoch: 9 | 21376 / 114272 | training loss: 0.00031964541994966567\n",
      "epoch: 9 | 21408 / 114272 | training loss: 0.0004232727806083858\n",
      "epoch: 9 | 21440 / 114272 | training loss: 0.00038222831790335476\n",
      "epoch: 9 | 21472 / 114272 | training loss: 0.00035645317984744906\n",
      "epoch: 9 | 21504 / 114272 | training loss: 0.0005562272272072732\n",
      "epoch: 9 | 21536 / 114272 | training loss: 0.00024388334713876247\n",
      "epoch: 9 | 21568 / 114272 | training loss: 0.00024081359151750803\n",
      "epoch: 9 | 21600 / 114272 | training loss: 0.0002160273870686069\n",
      "epoch: 9 | 21632 / 114272 | training loss: 0.0003425833710934967\n",
      "epoch: 9 | 21664 / 114272 | training loss: 0.0002415314083918929\n",
      "epoch: 9 | 21696 / 114272 | training loss: 0.00024480558931827545\n",
      "epoch: 9 | 21728 / 114272 | training loss: 0.000223631999688223\n",
      "epoch: 9 | 21760 / 114272 | training loss: 0.00031751117785461247\n",
      "epoch: 9 | 21792 / 114272 | training loss: 0.0003989434044342488\n",
      "epoch: 9 | 21824 / 114272 | training loss: 0.0002755319292191416\n",
      "epoch: 9 | 21856 / 114272 | training loss: 0.00018373792408965528\n",
      "epoch: 9 | 21888 / 114272 | training loss: 0.0003691354359034449\n",
      "epoch: 9 | 21920 / 114272 | training loss: 0.00023981455888133496\n",
      "epoch: 9 | 21952 / 114272 | training loss: 7.745526818325743e-05\n",
      "epoch: 9 | 21984 / 114272 | training loss: 0.0004049951967317611\n",
      "epoch: 9 | 22016 / 114272 | training loss: 0.0003036479465663433\n",
      "epoch: 9 | 22048 / 114272 | training loss: 0.0003925390774384141\n",
      "epoch: 9 | 22080 / 114272 | training loss: 0.00029568455647677183\n",
      "epoch: 9 | 22112 / 114272 | training loss: 0.00014224489859770983\n",
      "epoch: 9 | 22144 / 114272 | training loss: 0.009206709451973438\n",
      "epoch: 9 | 22176 / 114272 | training loss: 0.0004185077268630266\n",
      "epoch: 9 | 22208 / 114272 | training loss: 0.00034132483415305614\n",
      "epoch: 9 | 22240 / 114272 | training loss: 0.00026769979740493\n",
      "epoch: 9 | 22272 / 114272 | training loss: 0.00022593022731598467\n",
      "epoch: 9 | 22304 / 114272 | training loss: 0.00014986316091381013\n",
      "epoch: 9 | 22336 / 114272 | training loss: 0.00033361135865561664\n",
      "epoch: 9 | 22368 / 114272 | training loss: 0.0003376026579644531\n",
      "epoch: 9 | 22400 / 114272 | training loss: 0.00033221798366867006\n",
      "epoch: 9 | 22432 / 114272 | training loss: 0.0003713067271746695\n",
      "epoch: 9 | 22464 / 114272 | training loss: 0.00018230396381113678\n",
      "epoch: 9 | 22496 / 114272 | training loss: 0.0002423266851110384\n",
      "epoch: 9 | 22528 / 114272 | training loss: 0.00031193086761049926\n",
      "epoch: 9 | 22560 / 114272 | training loss: 0.000152393156895414\n",
      "epoch: 9 | 22592 / 114272 | training loss: 0.00012800977856386453\n",
      "epoch: 9 | 22624 / 114272 | training loss: 0.00038204697193577886\n",
      "epoch: 9 | 22656 / 114272 | training loss: 0.00015308026922866702\n",
      "epoch: 9 | 22688 / 114272 | training loss: 0.00025458275922574103\n",
      "epoch: 9 | 22720 / 114272 | training loss: 0.00041302168392576277\n",
      "epoch: 9 | 22752 / 114272 | training loss: 0.000695863040164113\n",
      "epoch: 9 | 22784 / 114272 | training loss: 0.0002383704559179023\n",
      "epoch: 9 | 22816 / 114272 | training loss: 0.005356127861887217\n",
      "epoch: 9 | 22848 / 114272 | training loss: 0.0003333959321025759\n",
      "epoch: 9 | 22880 / 114272 | training loss: 0.0003182490181643516\n",
      "epoch: 9 | 22912 / 114272 | training loss: 0.0003400215646252036\n",
      "epoch: 9 | 22944 / 114272 | training loss: 0.2009952813386917\n",
      "epoch: 9 | 22976 / 114272 | training loss: 0.0001641006674617529\n",
      "epoch: 9 | 23008 / 114272 | training loss: 0.0003664703690446913\n",
      "epoch: 9 | 23040 / 114272 | training loss: 0.00021442087017931044\n",
      "epoch: 9 | 23072 / 114272 | training loss: 0.00039240182377398014\n",
      "epoch: 9 | 23104 / 114272 | training loss: 0.00041667057666927576\n",
      "epoch: 9 | 23136 / 114272 | training loss: 0.00022693820938002318\n",
      "epoch: 9 | 23168 / 114272 | training loss: 0.021882686764001846\n",
      "epoch: 9 | 23200 / 114272 | training loss: 0.0002820136724039912\n",
      "epoch: 9 | 23232 / 114272 | training loss: 0.0006343876593746245\n",
      "epoch: 9 | 23264 / 114272 | training loss: 0.20112277567386627\n",
      "epoch: 9 | 23296 / 114272 | training loss: 0.00036464177537709475\n",
      "epoch: 9 | 23328 / 114272 | training loss: 0.00025777833070605993\n",
      "epoch: 9 | 23360 / 114272 | training loss: 0.021205568686127663\n",
      "epoch: 9 | 23392 / 114272 | training loss: 0.012911566533148289\n",
      "epoch: 9 | 23424 / 114272 | training loss: 0.0005740082706324756\n",
      "epoch: 9 | 23456 / 114272 | training loss: 0.0002816469641402364\n",
      "epoch: 9 | 23488 / 114272 | training loss: 0.00028857128927484155\n",
      "epoch: 9 | 23520 / 114272 | training loss: 0.00030032850918360054\n",
      "epoch: 9 | 23552 / 114272 | training loss: 0.00027974005206488073\n",
      "epoch: 9 | 23584 / 114272 | training loss: 0.00034876083373092115\n",
      "epoch: 9 | 23616 / 114272 | training loss: 0.00025901314802467823\n",
      "epoch: 9 | 23648 / 114272 | training loss: 0.00018740378436632454\n",
      "epoch: 9 | 23680 / 114272 | training loss: 0.00023148265609052032\n",
      "epoch: 9 | 23712 / 114272 | training loss: 0.00018048315541818738\n",
      "epoch: 9 | 23744 / 114272 | training loss: 0.0003466670459602028\n",
      "epoch: 9 | 23776 / 114272 | training loss: 0.0003117877058684826\n",
      "epoch: 9 | 23808 / 114272 | training loss: 0.00037198769859969616\n",
      "epoch: 9 | 23840 / 114272 | training loss: 0.0003329419996589422\n",
      "epoch: 9 | 23872 / 114272 | training loss: 0.0002460129908286035\n",
      "epoch: 9 | 23904 / 114272 | training loss: 0.00025130034191533923\n",
      "epoch: 9 | 23936 / 114272 | training loss: 0.000638775120023638\n",
      "epoch: 9 | 23968 / 114272 | training loss: 0.0002836518397089094\n",
      "epoch: 9 | 24000 / 114272 | training loss: 0.00021883762383367866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 24032 / 114272 | training loss: 0.0002405097766313702\n",
      "epoch: 9 | 24064 / 114272 | training loss: 0.000328741007251665\n",
      "epoch: 9 | 24096 / 114272 | training loss: 0.00018001141143031418\n",
      "epoch: 9 | 24128 / 114272 | training loss: 0.00026536211953498423\n",
      "epoch: 9 | 24160 / 114272 | training loss: 0.0002822401002049446\n",
      "epoch: 9 | 24192 / 114272 | training loss: 0.00044293366954661906\n",
      "epoch: 9 | 24224 / 114272 | training loss: 0.0002280149346916005\n",
      "epoch: 9 | 24256 / 114272 | training loss: 0.0004059051861986518\n",
      "epoch: 9 | 24288 / 114272 | training loss: 0.00022220687242224813\n",
      "epoch: 9 | 24320 / 114272 | training loss: 0.00029282845207490027\n",
      "epoch: 9 | 24352 / 114272 | training loss: 0.0004023282090201974\n",
      "epoch: 9 | 24384 / 114272 | training loss: 0.00018358718079980463\n",
      "epoch: 9 | 24416 / 114272 | training loss: 0.0006887749186716974\n",
      "epoch: 9 | 24448 / 114272 | training loss: 0.00039199605816975236\n",
      "epoch: 9 | 24480 / 114272 | training loss: 0.0002337651385460049\n",
      "epoch: 9 | 24512 / 114272 | training loss: 0.00016198380035348237\n",
      "epoch: 9 | 24544 / 114272 | training loss: 0.00023129861801862717\n",
      "epoch: 9 | 24576 / 114272 | training loss: 0.00030726331169717014\n",
      "epoch: 9 | 24608 / 114272 | training loss: 0.00034738166141323745\n",
      "epoch: 9 | 24640 / 114272 | training loss: 0.000134309331770055\n",
      "epoch: 9 | 24672 / 114272 | training loss: 0.0003168473194818944\n",
      "epoch: 9 | 24704 / 114272 | training loss: 0.0003482800384517759\n",
      "epoch: 9 | 24736 / 114272 | training loss: 0.0001912001898745075\n",
      "epoch: 9 | 24768 / 114272 | training loss: 0.0003210931026842445\n",
      "epoch: 9 | 24800 / 114272 | training loss: 0.00036354095209389925\n",
      "epoch: 9 | 24832 / 114272 | training loss: 0.0003167956310790032\n",
      "epoch: 9 | 24864 / 114272 | training loss: 0.00023601160501129925\n",
      "epoch: 9 | 24896 / 114272 | training loss: 0.0001800012687454\n",
      "epoch: 9 | 24928 / 114272 | training loss: 0.0001637611712794751\n",
      "epoch: 9 | 24960 / 114272 | training loss: 0.00025174152688123286\n",
      "epoch: 9 | 24992 / 114272 | training loss: 0.00021834367362316698\n",
      "epoch: 9 | 25024 / 114272 | training loss: 0.0005004380946047604\n",
      "epoch: 9 | 25056 / 114272 | training loss: 0.0002737205068115145\n",
      "epoch: 9 | 25088 / 114272 | training loss: 0.0003145111259073019\n",
      "epoch: 9 | 25120 / 114272 | training loss: 0.00027179389144293964\n",
      "epoch: 9 | 25152 / 114272 | training loss: 0.0002575020189397037\n",
      "epoch: 9 | 25184 / 114272 | training loss: 0.00020381907233968377\n",
      "epoch: 9 | 25216 / 114272 | training loss: 0.00012739254452753812\n",
      "epoch: 9 | 25248 / 114272 | training loss: 0.003570056287571788\n",
      "epoch: 9 | 25280 / 114272 | training loss: 0.004522331524640322\n",
      "epoch: 9 | 25312 / 114272 | training loss: 0.011513707228004932\n",
      "epoch: 9 | 25344 / 114272 | training loss: 0.0004855448205489665\n",
      "epoch: 9 | 25376 / 114272 | training loss: 0.0003672743041533977\n",
      "epoch: 9 | 25408 / 114272 | training loss: 0.00016732847143430263\n",
      "epoch: 9 | 25440 / 114272 | training loss: 0.00037242640974000096\n",
      "epoch: 9 | 25472 / 114272 | training loss: 0.0002091922942781821\n",
      "epoch: 9 | 25504 / 114272 | training loss: 0.00023879235959611833\n",
      "epoch: 9 | 25536 / 114272 | training loss: 9.01900784811005e-05\n",
      "epoch: 9 | 25568 / 114272 | training loss: 0.00032798180473037064\n",
      "epoch: 9 | 25600 / 114272 | training loss: 0.0001565704442327842\n",
      "epoch: 9 | 25632 / 114272 | training loss: 0.1370357871055603\n",
      "epoch: 9 | 25664 / 114272 | training loss: 0.0005305807571858168\n",
      "epoch: 9 | 25696 / 114272 | training loss: 0.0002716309390962124\n",
      "epoch: 9 | 25728 / 114272 | training loss: 0.0003472959215287119\n",
      "epoch: 9 | 25760 / 114272 | training loss: 0.00012995746510569006\n",
      "epoch: 9 | 25792 / 114272 | training loss: 0.00031052972190082073\n",
      "epoch: 9 | 25824 / 114272 | training loss: 0.00021660953643731773\n",
      "epoch: 9 | 25856 / 114272 | training loss: 0.00024485631729476154\n",
      "epoch: 9 | 25888 / 114272 | training loss: 0.000332969386363402\n",
      "epoch: 9 | 25920 / 114272 | training loss: 0.00033409835305064917\n",
      "epoch: 9 | 25952 / 114272 | training loss: 0.00019141177472192794\n",
      "epoch: 9 | 25984 / 114272 | training loss: 0.00041664589662104845\n",
      "epoch: 9 | 26016 / 114272 | training loss: 0.00032413736335001886\n",
      "epoch: 9 | 26048 / 114272 | training loss: 0.00042399854282848537\n",
      "epoch: 9 | 26080 / 114272 | training loss: 0.00029206537874415517\n",
      "epoch: 9 | 26112 / 114272 | training loss: 0.00022055239242035896\n",
      "epoch: 9 | 26144 / 114272 | training loss: 0.00013883905194234103\n",
      "epoch: 9 | 26176 / 114272 | training loss: 0.00014449399895966053\n",
      "epoch: 9 | 26208 / 114272 | training loss: 0.00030416809022426605\n",
      "epoch: 9 | 26240 / 114272 | training loss: 0.0007178997038863599\n",
      "epoch: 9 | 26272 / 114272 | training loss: 0.0004199761897325516\n",
      "epoch: 9 | 26304 / 114272 | training loss: 0.00033173529664054513\n",
      "epoch: 9 | 26336 / 114272 | training loss: 0.0002543802256695926\n",
      "epoch: 9 | 26368 / 114272 | training loss: 0.0001284003519685939\n",
      "epoch: 9 | 26400 / 114272 | training loss: 0.0008969878545030951\n",
      "epoch: 9 | 26432 / 114272 | training loss: 0.00020561509882099926\n",
      "epoch: 9 | 26464 / 114272 | training loss: 0.0002995445393025875\n",
      "epoch: 9 | 26496 / 114272 | training loss: 0.014709608629345894\n",
      "epoch: 9 | 26528 / 114272 | training loss: 0.000238480293774046\n",
      "epoch: 9 | 26560 / 114272 | training loss: 0.0003680033260025084\n",
      "epoch: 9 | 26592 / 114272 | training loss: 0.00015056610573083162\n",
      "epoch: 9 | 26624 / 114272 | training loss: 0.000281234213616699\n",
      "epoch: 9 | 26656 / 114272 | training loss: 0.0004211833293084055\n",
      "epoch: 9 | 26688 / 114272 | training loss: 0.0015959209995344281\n",
      "epoch: 9 | 26720 / 114272 | training loss: 0.0005807262496091425\n",
      "epoch: 9 | 26752 / 114272 | training loss: 0.00015339604578912258\n",
      "epoch: 9 | 26784 / 114272 | training loss: 0.0006356896483339369\n",
      "epoch: 9 | 26816 / 114272 | training loss: 0.00017499306704849005\n",
      "epoch: 9 | 26848 / 114272 | training loss: 0.00028041793848387897\n",
      "epoch: 9 | 26880 / 114272 | training loss: 0.22636598348617554\n",
      "epoch: 9 | 26912 / 114272 | training loss: 0.00045678007882088423\n",
      "epoch: 9 | 26944 / 114272 | training loss: 0.00021081262093503028\n",
      "epoch: 9 | 26976 / 114272 | training loss: 0.0002549367491155863\n",
      "epoch: 9 | 27008 / 114272 | training loss: 0.00034836045233532786\n",
      "epoch: 9 | 27040 / 114272 | training loss: 0.00026375160086899996\n",
      "epoch: 9 | 27072 / 114272 | training loss: 0.0001376591098960489\n",
      "epoch: 9 | 27104 / 114272 | training loss: 0.027346506714820862\n",
      "epoch: 9 | 27136 / 114272 | training loss: 0.000200886424863711\n",
      "epoch: 9 | 27168 / 114272 | training loss: 0.00029685418121516705\n",
      "epoch: 9 | 27200 / 114272 | training loss: 0.00018018843547906727\n",
      "epoch: 9 | 27232 / 114272 | training loss: 0.0004719499556813389\n",
      "epoch: 9 | 27264 / 114272 | training loss: 0.0003764290304388851\n",
      "epoch: 9 | 27296 / 114272 | training loss: 0.00023671241069678217\n",
      "epoch: 9 | 27328 / 114272 | training loss: 0.0002541434660088271\n",
      "epoch: 9 | 27360 / 114272 | training loss: 0.00034005314228124917\n",
      "epoch: 9 | 27392 / 114272 | training loss: 0.0014128050534054637\n",
      "epoch: 9 | 27424 / 114272 | training loss: 0.00021824202849529684\n",
      "epoch: 9 | 27456 / 114272 | training loss: 0.0003903814940713346\n",
      "epoch: 9 | 27488 / 114272 | training loss: 0.0008468356099911034\n",
      "epoch: 9 | 27520 / 114272 | training loss: 0.0002573234378360212\n",
      "epoch: 9 | 27552 / 114272 | training loss: 0.00029898746288381517\n",
      "epoch: 9 | 27584 / 114272 | training loss: 0.0003569873224478215\n",
      "epoch: 9 | 27616 / 114272 | training loss: 0.00031426938949152827\n",
      "epoch: 9 | 27648 / 114272 | training loss: 0.00044229382183402777\n",
      "epoch: 9 | 27680 / 114272 | training loss: 0.00019210134632885456\n",
      "epoch: 9 | 27712 / 114272 | training loss: 0.00017366661631967872\n",
      "epoch: 9 | 27744 / 114272 | training loss: 0.00022404667106457055\n",
      "epoch: 9 | 27776 / 114272 | training loss: 0.00031178476638160646\n",
      "epoch: 9 | 27808 / 114272 | training loss: 0.0001610735198482871\n",
      "epoch: 9 | 27840 / 114272 | training loss: 0.00031798548297956586\n",
      "epoch: 9 | 27872 / 114272 | training loss: 0.0002501955896150321\n",
      "epoch: 9 | 27904 / 114272 | training loss: 0.0002358176134293899\n",
      "epoch: 9 | 27936 / 114272 | training loss: 0.0006232282612472773\n",
      "epoch: 9 | 27968 / 114272 | training loss: 0.0002672306727617979\n",
      "epoch: 9 | 28000 / 114272 | training loss: 0.00031347552430815995\n",
      "epoch: 9 | 28032 / 114272 | training loss: 0.00024243543157353997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 28064 / 114272 | training loss: 0.00016520947974640876\n",
      "epoch: 9 | 28096 / 114272 | training loss: 0.00022669447935186327\n",
      "epoch: 9 | 28128 / 114272 | training loss: 0.00030727701960131526\n",
      "epoch: 9 | 28160 / 114272 | training loss: 0.13293063640594482\n",
      "epoch: 9 | 28192 / 114272 | training loss: 0.00027296191547065973\n",
      "epoch: 9 | 28224 / 114272 | training loss: 0.00020451795717235655\n",
      "epoch: 9 | 28256 / 114272 | training loss: 0.0002754697052296251\n",
      "epoch: 9 | 28288 / 114272 | training loss: 0.00040705548599362373\n",
      "epoch: 9 | 28320 / 114272 | training loss: 0.0003030060324817896\n",
      "epoch: 9 | 28352 / 114272 | training loss: 0.00030815883656032383\n",
      "epoch: 9 | 28384 / 114272 | training loss: 0.00026076502399519086\n",
      "epoch: 9 | 28416 / 114272 | training loss: 0.00039579582517035306\n",
      "epoch: 9 | 28448 / 114272 | training loss: 0.0004187073209322989\n",
      "epoch: 9 | 28480 / 114272 | training loss: 0.00026223258464597166\n",
      "epoch: 9 | 28512 / 114272 | training loss: 0.00039487669710069895\n",
      "epoch: 9 | 28544 / 114272 | training loss: 0.00010929690324701369\n",
      "epoch: 9 | 28576 / 114272 | training loss: 0.0002482177224010229\n",
      "epoch: 9 | 28608 / 114272 | training loss: 0.00031907280208542943\n",
      "epoch: 9 | 28640 / 114272 | training loss: 0.0005884234560653567\n",
      "epoch: 9 | 28672 / 114272 | training loss: 0.031960755586624146\n",
      "epoch: 9 | 28704 / 114272 | training loss: 0.00017081502301152796\n",
      "epoch: 9 | 28736 / 114272 | training loss: 0.0007347387727349997\n",
      "epoch: 9 | 28768 / 114272 | training loss: 0.0002501281851436943\n",
      "epoch: 9 | 28800 / 114272 | training loss: 0.0005313235451467335\n",
      "epoch: 9 | 28832 / 114272 | training loss: 0.00040727289160713553\n",
      "epoch: 9 | 28864 / 114272 | training loss: 0.0017679031006991863\n",
      "epoch: 9 | 28896 / 114272 | training loss: 0.00024180598848033696\n",
      "epoch: 9 | 28928 / 114272 | training loss: 0.00024611520348116755\n",
      "epoch: 9 | 28960 / 114272 | training loss: 0.0002613241667859256\n",
      "epoch: 9 | 28992 / 114272 | training loss: 0.00011259790335316211\n",
      "epoch: 9 | 29024 / 114272 | training loss: 0.00042901362758129835\n",
      "epoch: 9 | 29056 / 114272 | training loss: 0.0001276783732464537\n",
      "epoch: 9 | 29088 / 114272 | training loss: 0.00024410623882431537\n",
      "epoch: 9 | 29120 / 114272 | training loss: 0.00016272491484414786\n",
      "epoch: 9 | 29152 / 114272 | training loss: 0.00014718966849613935\n",
      "epoch: 9 | 29184 / 114272 | training loss: 0.00027979796868748963\n",
      "epoch: 9 | 29216 / 114272 | training loss: 7.298158016055822e-05\n",
      "epoch: 9 | 29248 / 114272 | training loss: 0.0003478940634522587\n",
      "epoch: 9 | 29280 / 114272 | training loss: 0.00019999714277219027\n",
      "epoch: 9 | 29312 / 114272 | training loss: 0.00016261934069916606\n",
      "epoch: 9 | 29344 / 114272 | training loss: 0.0002154871472157538\n",
      "epoch: 9 | 29376 / 114272 | training loss: 0.0002683787024579942\n",
      "epoch: 9 | 29408 / 114272 | training loss: 0.00023787864483892918\n",
      "epoch: 9 | 29440 / 114272 | training loss: 0.00024868501350283623\n",
      "epoch: 9 | 29472 / 114272 | training loss: 0.0004183976852800697\n",
      "epoch: 9 | 29504 / 114272 | training loss: 0.0013721829745918512\n",
      "epoch: 9 | 29536 / 114272 | training loss: 0.000448203063569963\n",
      "epoch: 9 | 29568 / 114272 | training loss: 0.0002598838182166219\n",
      "epoch: 9 | 29600 / 114272 | training loss: 0.00018689670832827687\n",
      "epoch: 9 | 29632 / 114272 | training loss: 0.0003551474947016686\n",
      "epoch: 9 | 29664 / 114272 | training loss: 0.00020383841183502227\n",
      "epoch: 9 | 29696 / 114272 | training loss: 0.00024210989067796618\n",
      "epoch: 9 | 29728 / 114272 | training loss: 0.00978447962552309\n",
      "epoch: 9 | 29760 / 114272 | training loss: 0.0001472561270929873\n",
      "epoch: 9 | 29792 / 114272 | training loss: 0.0001275329414056614\n",
      "epoch: 9 | 29824 / 114272 | training loss: 0.0001361230097245425\n",
      "epoch: 9 | 29856 / 114272 | training loss: 0.00041332721593789756\n",
      "epoch: 9 | 29888 / 114272 | training loss: 0.00020926234719809145\n",
      "epoch: 9 | 29920 / 114272 | training loss: 0.04921378195285797\n",
      "epoch: 9 | 29952 / 114272 | training loss: 0.00021825869043823332\n",
      "epoch: 9 | 29984 / 114272 | training loss: 0.0003864665050059557\n",
      "epoch: 9 | 30016 / 114272 | training loss: 0.0003227254201192409\n",
      "epoch: 9 | 30048 / 114272 | training loss: 0.00016995587793644518\n",
      "epoch: 9 | 30080 / 114272 | training loss: 0.00034061208134517074\n",
      "epoch: 9 | 30112 / 114272 | training loss: 0.0002456473303027451\n",
      "epoch: 9 | 30144 / 114272 | training loss: 0.00018014432862401009\n",
      "epoch: 9 | 30176 / 114272 | training loss: 0.00021097288117744029\n",
      "epoch: 9 | 30208 / 114272 | training loss: 0.0003628381236921996\n",
      "epoch: 9 | 30240 / 114272 | training loss: 0.0003726808645296842\n",
      "epoch: 9 | 30272 / 114272 | training loss: 0.00012864211748819798\n",
      "epoch: 9 | 30304 / 114272 | training loss: 0.00012568298552650958\n",
      "epoch: 9 | 30336 / 114272 | training loss: 0.0003016349801328033\n",
      "epoch: 9 | 30368 / 114272 | training loss: 0.0006436060648411512\n",
      "epoch: 9 | 30400 / 114272 | training loss: 0.00018967116193380207\n",
      "epoch: 9 | 30432 / 114272 | training loss: 0.00017512663907837123\n",
      "epoch: 9 | 30464 / 114272 | training loss: 0.0005521515849977732\n",
      "epoch: 9 | 30496 / 114272 | training loss: 0.00017593226220924407\n",
      "epoch: 9 | 30528 / 114272 | training loss: 0.00021485525940079242\n",
      "epoch: 9 | 30560 / 114272 | training loss: 0.0005063774297013879\n",
      "epoch: 9 | 30592 / 114272 | training loss: 0.00016649108147248626\n",
      "epoch: 9 | 30624 / 114272 | training loss: 0.00027591909747570753\n",
      "epoch: 9 | 30656 / 114272 | training loss: 0.0002447311708237976\n",
      "epoch: 9 | 30688 / 114272 | training loss: 0.0003671012236736715\n",
      "epoch: 9 | 30720 / 114272 | training loss: 0.00023433618480339646\n",
      "epoch: 9 | 30752 / 114272 | training loss: 0.0003725567657966167\n",
      "epoch: 9 | 30784 / 114272 | training loss: 0.0004002360219601542\n",
      "epoch: 9 | 30816 / 114272 | training loss: 0.00017164615564979613\n",
      "epoch: 9 | 30848 / 114272 | training loss: 0.00014727722737006843\n",
      "epoch: 9 | 30880 / 114272 | training loss: 0.0002342838852200657\n",
      "epoch: 9 | 30912 / 114272 | training loss: 0.00015483886818401515\n",
      "epoch: 9 | 30944 / 114272 | training loss: 0.00024964482872746885\n",
      "epoch: 9 | 30976 / 114272 | training loss: 0.0001508425921201706\n",
      "epoch: 9 | 31008 / 114272 | training loss: 0.00023051245079841465\n",
      "epoch: 9 | 31040 / 114272 | training loss: 0.00029887407436035573\n",
      "epoch: 9 | 31072 / 114272 | training loss: 0.0002144215104635805\n",
      "epoch: 9 | 31104 / 114272 | training loss: 0.0002701145422179252\n",
      "epoch: 9 | 31136 / 114272 | training loss: 0.00022176046331878752\n",
      "epoch: 9 | 31168 / 114272 | training loss: 0.21312738955020905\n",
      "epoch: 9 | 31200 / 114272 | training loss: 0.0002649475063662976\n",
      "epoch: 9 | 31232 / 114272 | training loss: 0.0005611727829091251\n",
      "epoch: 9 | 31264 / 114272 | training loss: 0.0003762305132113397\n",
      "epoch: 9 | 31296 / 114272 | training loss: 0.0004346605855971575\n",
      "epoch: 9 | 31328 / 114272 | training loss: 0.0005861352547071874\n",
      "epoch: 9 | 31360 / 114272 | training loss: 0.00020657607819885015\n",
      "epoch: 9 | 31392 / 114272 | training loss: 0.00019457709277048707\n",
      "epoch: 9 | 31424 / 114272 | training loss: 0.006973722949624062\n",
      "epoch: 9 | 31456 / 114272 | training loss: 0.0003758375532925129\n",
      "epoch: 9 | 31488 / 114272 | training loss: 0.0003491535026114434\n",
      "epoch: 9 | 31520 / 114272 | training loss: 0.00030337172211147845\n",
      "epoch: 9 | 31552 / 114272 | training loss: 0.00024853466311469674\n",
      "epoch: 9 | 31584 / 114272 | training loss: 0.00020135630620643497\n",
      "epoch: 9 | 31616 / 114272 | training loss: 0.0003460738225840032\n",
      "epoch: 9 | 31648 / 114272 | training loss: 0.00019692473870236427\n",
      "epoch: 9 | 31680 / 114272 | training loss: 0.0002767675614450127\n",
      "epoch: 9 | 31712 / 114272 | training loss: 0.00032122491393238306\n",
      "epoch: 9 | 31744 / 114272 | training loss: 0.0011833307798951864\n",
      "epoch: 9 | 31776 / 114272 | training loss: 0.00025819535949267447\n",
      "epoch: 9 | 31808 / 114272 | training loss: 0.00020400885841809213\n",
      "epoch: 9 | 31840 / 114272 | training loss: 0.00027554004918783903\n",
      "epoch: 9 | 31872 / 114272 | training loss: 0.00022895133588463068\n",
      "epoch: 9 | 31904 / 114272 | training loss: 0.00019381422316655517\n",
      "epoch: 9 | 31936 / 114272 | training loss: 0.1430846005678177\n",
      "epoch: 9 | 31968 / 114272 | training loss: 0.000216214670217596\n",
      "epoch: 9 | 32000 / 114272 | training loss: 0.00023186093312688172\n",
      "epoch: 9 | 32032 / 114272 | training loss: 0.00020203062740620226\n",
      "epoch: 9 | 32064 / 114272 | training loss: 0.00031905638752505183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 32096 / 114272 | training loss: 0.00010627659503370523\n",
      "epoch: 9 | 32128 / 114272 | training loss: 0.000398265547119081\n",
      "epoch: 9 | 32160 / 114272 | training loss: 0.0001871509593911469\n",
      "epoch: 9 | 32192 / 114272 | training loss: 0.00023154783411882818\n",
      "epoch: 9 | 32224 / 114272 | training loss: 0.0002859950764104724\n",
      "epoch: 9 | 32256 / 114272 | training loss: 0.0002536758838687092\n",
      "epoch: 9 | 32288 / 114272 | training loss: 0.0016615621279925108\n",
      "epoch: 9 | 32320 / 114272 | training loss: 0.00026793734286911786\n",
      "epoch: 9 | 32352 / 114272 | training loss: 0.00025211836327798665\n",
      "epoch: 9 | 32384 / 114272 | training loss: 0.2996397316455841\n",
      "epoch: 9 | 32416 / 114272 | training loss: 0.0004295736725907773\n",
      "epoch: 9 | 32448 / 114272 | training loss: 0.0003174138837493956\n",
      "epoch: 9 | 32480 / 114272 | training loss: 0.0002872466284316033\n",
      "epoch: 9 | 32512 / 114272 | training loss: 0.00018402314162813127\n",
      "epoch: 9 | 32544 / 114272 | training loss: 0.0002522111462894827\n",
      "epoch: 9 | 32576 / 114272 | training loss: 0.00024090553051792085\n",
      "epoch: 9 | 32608 / 114272 | training loss: 0.0003207922854926437\n",
      "epoch: 9 | 32640 / 114272 | training loss: 0.00030348775908350945\n",
      "epoch: 9 | 32672 / 114272 | training loss: 0.00028587025008164346\n",
      "epoch: 9 | 32704 / 114272 | training loss: 0.00012260909716133028\n",
      "epoch: 9 | 32736 / 114272 | training loss: 0.00010596292122500017\n",
      "epoch: 9 | 32768 / 114272 | training loss: 0.00013867516827303916\n",
      "epoch: 9 | 32800 / 114272 | training loss: 0.00017523448332212865\n",
      "epoch: 9 | 32832 / 114272 | training loss: 0.00033158555743284523\n",
      "epoch: 9 | 32864 / 114272 | training loss: 0.00030606015934608877\n",
      "epoch: 9 | 32896 / 114272 | training loss: 0.054584648460149765\n",
      "epoch: 9 | 32928 / 114272 | training loss: 0.0003417418629396707\n",
      "epoch: 9 | 32960 / 114272 | training loss: 0.0003306142461951822\n",
      "epoch: 9 | 32992 / 114272 | training loss: 0.0001616645895410329\n",
      "epoch: 9 | 33024 / 114272 | training loss: 0.00026100571267306805\n",
      "epoch: 9 | 33056 / 114272 | training loss: 0.0002816685300786048\n",
      "epoch: 9 | 33088 / 114272 | training loss: 0.00018740288214758039\n",
      "epoch: 9 | 33120 / 114272 | training loss: 0.00013488502008840442\n",
      "epoch: 9 | 33152 / 114272 | training loss: 0.00023226677149068564\n",
      "epoch: 9 | 33184 / 114272 | training loss: 0.00029359388281591237\n",
      "epoch: 9 | 33216 / 114272 | training loss: 0.015517359599471092\n",
      "epoch: 9 | 33248 / 114272 | training loss: 0.000390066736144945\n",
      "epoch: 9 | 33280 / 114272 | training loss: 0.00022862869082018733\n",
      "epoch: 9 | 33312 / 114272 | training loss: 0.0002578863932285458\n",
      "epoch: 9 | 33344 / 114272 | training loss: 0.00032816335442475975\n",
      "epoch: 9 | 33376 / 114272 | training loss: 0.0006325276917777956\n",
      "epoch: 9 | 33408 / 114272 | training loss: 0.0002661567705217749\n",
      "epoch: 9 | 33440 / 114272 | training loss: 0.00025518430629745126\n",
      "epoch: 9 | 33472 / 114272 | training loss: 0.00020539313845802099\n",
      "epoch: 9 | 33504 / 114272 | training loss: 0.00030997215071693063\n",
      "epoch: 9 | 33536 / 114272 | training loss: 0.13852395117282867\n",
      "epoch: 9 | 33568 / 114272 | training loss: 0.00043427542550489306\n",
      "epoch: 9 | 33600 / 114272 | training loss: 0.00026460920344106853\n",
      "epoch: 9 | 33632 / 114272 | training loss: 0.0007975151529535651\n",
      "epoch: 9 | 33664 / 114272 | training loss: 0.00021226835087873042\n",
      "epoch: 9 | 33696 / 114272 | training loss: 0.00027138760196976364\n",
      "epoch: 9 | 33728 / 114272 | training loss: 0.0003491486713755876\n",
      "epoch: 9 | 33760 / 114272 | training loss: 0.10771264135837555\n",
      "epoch: 9 | 33792 / 114272 | training loss: 0.0003373193903826177\n",
      "epoch: 9 | 33824 / 114272 | training loss: 0.0004887487157247961\n",
      "epoch: 9 | 33856 / 114272 | training loss: 0.0013573125470429659\n",
      "epoch: 9 | 33888 / 114272 | training loss: 0.0003238203644286841\n",
      "epoch: 9 | 33920 / 114272 | training loss: 0.00022770310170017183\n",
      "epoch: 9 | 33952 / 114272 | training loss: 0.034259241074323654\n",
      "epoch: 9 | 33984 / 114272 | training loss: 0.0004231217608321458\n",
      "epoch: 9 | 34016 / 114272 | training loss: 0.000734192319214344\n",
      "epoch: 9 | 34048 / 114272 | training loss: 0.000379206525394693\n",
      "epoch: 9 | 34080 / 114272 | training loss: 0.00021875306265428662\n",
      "epoch: 9 | 34112 / 114272 | training loss: 0.0001653718063607812\n",
      "epoch: 9 | 34144 / 114272 | training loss: 0.0002103546867147088\n",
      "epoch: 9 | 34176 / 114272 | training loss: 0.00019765090837609023\n",
      "epoch: 9 | 34208 / 114272 | training loss: 0.00017446761194150895\n",
      "epoch: 9 | 34240 / 114272 | training loss: 0.000340637139743194\n",
      "epoch: 9 | 34272 / 114272 | training loss: 0.0001597013761056587\n",
      "epoch: 9 | 34304 / 114272 | training loss: 0.0002679714234545827\n",
      "epoch: 9 | 34336 / 114272 | training loss: 0.0004592585319187492\n",
      "epoch: 9 | 34368 / 114272 | training loss: 0.0006829160847701132\n",
      "epoch: 9 | 34400 / 114272 | training loss: 0.0002300116902915761\n",
      "epoch: 9 | 34432 / 114272 | training loss: 0.0006452584639191628\n",
      "epoch: 9 | 34464 / 114272 | training loss: 0.000313595897750929\n",
      "epoch: 9 | 34496 / 114272 | training loss: 0.000309366820147261\n",
      "epoch: 9 | 34528 / 114272 | training loss: 8.579857967561111e-05\n",
      "epoch: 9 | 34560 / 114272 | training loss: 0.0004037241160403937\n",
      "epoch: 9 | 34592 / 114272 | training loss: 0.0002072724892059341\n",
      "epoch: 9 | 34624 / 114272 | training loss: 0.00022297211398836225\n",
      "epoch: 9 | 34656 / 114272 | training loss: 0.0002189705119235441\n",
      "epoch: 9 | 34688 / 114272 | training loss: 0.00029044997063465416\n",
      "epoch: 9 | 34720 / 114272 | training loss: 0.00020698893058579415\n",
      "epoch: 9 | 34752 / 114272 | training loss: 0.0003078835434280336\n",
      "epoch: 9 | 34784 / 114272 | training loss: 0.00031412887619808316\n",
      "epoch: 9 | 34816 / 114272 | training loss: 0.00029778003226965666\n",
      "epoch: 9 | 34848 / 114272 | training loss: 0.0004169335006736219\n",
      "epoch: 9 | 34880 / 114272 | training loss: 0.022181879729032516\n",
      "epoch: 9 | 34912 / 114272 | training loss: 0.0009863873710855842\n",
      "epoch: 9 | 34944 / 114272 | training loss: 0.00028460926841944456\n",
      "epoch: 9 | 34976 / 114272 | training loss: 0.0003280116943642497\n",
      "epoch: 9 | 35008 / 114272 | training loss: 0.0003478733997326344\n",
      "epoch: 9 | 35040 / 114272 | training loss: 0.0002314092853339389\n",
      "epoch: 9 | 35072 / 114272 | training loss: 0.00023378567129839212\n",
      "epoch: 9 | 35104 / 114272 | training loss: 0.00018126843497157097\n",
      "epoch: 9 | 35136 / 114272 | training loss: 0.0001577573420945555\n",
      "epoch: 9 | 35168 / 114272 | training loss: 0.0003917404683306813\n",
      "epoch: 9 | 35200 / 114272 | training loss: 0.0002890881150960922\n",
      "epoch: 9 | 35232 / 114272 | training loss: 0.0004783782933373004\n",
      "epoch: 9 | 35264 / 114272 | training loss: 0.003405219642445445\n",
      "epoch: 9 | 35296 / 114272 | training loss: 0.00032589153852313757\n",
      "epoch: 9 | 35328 / 114272 | training loss: 0.0001971861784113571\n",
      "epoch: 9 | 35360 / 114272 | training loss: 0.00033422643900848925\n",
      "epoch: 9 | 35392 / 114272 | training loss: 0.00032605661544948816\n",
      "epoch: 9 | 35424 / 114272 | training loss: 0.00026441679801791906\n",
      "epoch: 9 | 35456 / 114272 | training loss: 0.058066774159669876\n",
      "epoch: 9 | 35488 / 114272 | training loss: 0.0001859094190876931\n",
      "epoch: 9 | 35520 / 114272 | training loss: 0.0002344450622331351\n",
      "epoch: 9 | 35552 / 114272 | training loss: 0.00257613486610353\n",
      "epoch: 9 | 35584 / 114272 | training loss: 0.0006058939616195858\n",
      "epoch: 9 | 35616 / 114272 | training loss: 0.0002755728201009333\n",
      "epoch: 9 | 35648 / 114272 | training loss: 0.0002475560177117586\n",
      "epoch: 9 | 35680 / 114272 | training loss: 0.00020853230671491474\n",
      "epoch: 9 | 35712 / 114272 | training loss: 0.00024049004423432052\n",
      "epoch: 9 | 35744 / 114272 | training loss: 0.0002976222021970898\n",
      "epoch: 9 | 35776 / 114272 | training loss: 0.0002471389889251441\n",
      "epoch: 9 | 35808 / 114272 | training loss: 0.0002775473694782704\n",
      "epoch: 9 | 35840 / 114272 | training loss: 0.0002704038342926651\n",
      "epoch: 9 | 35872 / 114272 | training loss: 0.00027248714468441904\n",
      "epoch: 9 | 35904 / 114272 | training loss: 0.0009744644048623741\n",
      "epoch: 9 | 35936 / 114272 | training loss: 0.0011081573320552707\n",
      "epoch: 9 | 35968 / 114272 | training loss: 0.0003492971882224083\n",
      "epoch: 9 | 36000 / 114272 | training loss: 0.0002821013913489878\n",
      "epoch: 9 | 36032 / 114272 | training loss: 0.0002386954874964431\n",
      "epoch: 9 | 36064 / 114272 | training loss: 0.0001810977846616879\n",
      "epoch: 9 | 36096 / 114272 | training loss: 0.0002614273107610643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 36128 / 114272 | training loss: 0.0005293769063428044\n",
      "epoch: 9 | 36160 / 114272 | training loss: 0.0001994840131374076\n",
      "epoch: 9 | 36192 / 114272 | training loss: 0.00018455325334798545\n",
      "epoch: 9 | 36224 / 114272 | training loss: 0.00018933331011794508\n",
      "epoch: 9 | 36256 / 114272 | training loss: 0.0001142211549449712\n",
      "epoch: 9 | 36288 / 114272 | training loss: 0.0008499083342030644\n",
      "epoch: 9 | 36320 / 114272 | training loss: 0.00032362970523536205\n",
      "epoch: 9 | 36352 / 114272 | training loss: 0.19344808161258698\n",
      "epoch: 9 | 36384 / 114272 | training loss: 0.00022916092711966485\n",
      "epoch: 9 | 36416 / 114272 | training loss: 0.0007825057837180793\n",
      "epoch: 9 | 36448 / 114272 | training loss: 0.00032169464975595474\n",
      "epoch: 9 | 36480 / 114272 | training loss: 0.0003503181505948305\n",
      "epoch: 9 | 36512 / 114272 | training loss: 0.0003665046824607998\n",
      "epoch: 9 | 36544 / 114272 | training loss: 0.0002154480607714504\n",
      "epoch: 9 | 36576 / 114272 | training loss: 0.00017939243116416037\n",
      "epoch: 9 | 36608 / 114272 | training loss: 0.00037024563062004745\n",
      "epoch: 9 | 36640 / 114272 | training loss: 0.00021086969354655594\n",
      "epoch: 9 | 36672 / 114272 | training loss: 0.0003107026859652251\n",
      "epoch: 9 | 36704 / 114272 | training loss: 0.0007372535183094442\n",
      "epoch: 9 | 36736 / 114272 | training loss: 0.11801613867282867\n",
      "epoch: 9 | 36768 / 114272 | training loss: 0.00028484422364272177\n",
      "epoch: 9 | 36800 / 114272 | training loss: 0.0002448251470923424\n",
      "epoch: 9 | 36832 / 114272 | training loss: 0.0002947727043647319\n",
      "epoch: 9 | 36864 / 114272 | training loss: 0.028875643387436867\n",
      "epoch: 9 | 36896 / 114272 | training loss: 0.00031401775777339935\n",
      "epoch: 9 | 36928 / 114272 | training loss: 0.0003800352569669485\n",
      "epoch: 9 | 36960 / 114272 | training loss: 0.0003277313953731209\n",
      "epoch: 9 | 36992 / 114272 | training loss: 0.00039706187089905143\n",
      "epoch: 9 | 37024 / 114272 | training loss: 0.0002961058635264635\n",
      "epoch: 9 | 37056 / 114272 | training loss: 0.0002351468283450231\n",
      "epoch: 9 | 37088 / 114272 | training loss: 0.00021020424901507795\n",
      "epoch: 9 | 37120 / 114272 | training loss: 0.0010786032071337104\n",
      "epoch: 9 | 37152 / 114272 | training loss: 0.00014787583495490253\n",
      "epoch: 9 | 37184 / 114272 | training loss: 0.0002624974586069584\n",
      "epoch: 9 | 37216 / 114272 | training loss: 0.00020070960454177111\n",
      "epoch: 9 | 37248 / 114272 | training loss: 0.001943137962371111\n",
      "epoch: 9 | 37280 / 114272 | training loss: 0.00018973613623529673\n",
      "epoch: 9 | 37312 / 114272 | training loss: 0.0003201916697435081\n",
      "epoch: 9 | 37344 / 114272 | training loss: 0.006574034690856934\n",
      "epoch: 9 | 37376 / 114272 | training loss: 0.0003416569088585675\n",
      "epoch: 9 | 37408 / 114272 | training loss: 0.0004882848588749766\n",
      "epoch: 9 | 37440 / 114272 | training loss: 0.00010479302727617323\n",
      "epoch: 9 | 37472 / 114272 | training loss: 0.0003105104551650584\n",
      "epoch: 9 | 37504 / 114272 | training loss: 0.00024965355987660587\n",
      "epoch: 9 | 37536 / 114272 | training loss: 0.00012364613940007985\n",
      "epoch: 9 | 37568 / 114272 | training loss: 0.0002714657166507095\n",
      "epoch: 9 | 37600 / 114272 | training loss: 0.00031991518335416913\n",
      "epoch: 9 | 37632 / 114272 | training loss: 0.0003157350292894989\n",
      "epoch: 9 | 37664 / 114272 | training loss: 0.0003410777426324785\n",
      "epoch: 9 | 37696 / 114272 | training loss: 0.0002627178037073463\n",
      "epoch: 9 | 37728 / 114272 | training loss: 0.00034127809340134263\n",
      "epoch: 9 | 37760 / 114272 | training loss: 0.00017279006715398282\n",
      "epoch: 9 | 37792 / 114272 | training loss: 0.0038793091662228107\n",
      "epoch: 9 | 37824 / 114272 | training loss: 0.0003142819623462856\n",
      "epoch: 9 | 37856 / 114272 | training loss: 0.00018493430980015546\n",
      "epoch: 9 | 37888 / 114272 | training loss: 0.0014183578314259648\n",
      "epoch: 9 | 37920 / 114272 | training loss: 0.00030880619306117296\n",
      "epoch: 9 | 37952 / 114272 | training loss: 0.0004029397096019238\n",
      "epoch: 9 | 37984 / 114272 | training loss: 0.048569049686193466\n",
      "epoch: 9 | 38016 / 114272 | training loss: 0.00019142613746225834\n",
      "epoch: 9 | 38048 / 114272 | training loss: 0.0002365919790463522\n",
      "epoch: 9 | 38080 / 114272 | training loss: 0.0002759753551799804\n",
      "epoch: 9 | 38112 / 114272 | training loss: 0.00034839913132600486\n",
      "epoch: 9 | 38144 / 114272 | training loss: 0.0003723067056853324\n",
      "epoch: 9 | 38176 / 114272 | training loss: 0.0009452757076360285\n",
      "epoch: 9 | 38208 / 114272 | training loss: 0.0004094871401321143\n",
      "epoch: 9 | 38240 / 114272 | training loss: 0.00022266570886131376\n",
      "epoch: 9 | 38272 / 114272 | training loss: 0.0004354544507805258\n",
      "epoch: 9 | 38304 / 114272 | training loss: 0.0002977083495352417\n",
      "epoch: 9 | 38336 / 114272 | training loss: 0.0002971034264191985\n",
      "epoch: 9 | 38368 / 114272 | training loss: 0.0005829801666550338\n",
      "epoch: 9 | 38400 / 114272 | training loss: 0.0004299738793633878\n",
      "epoch: 9 | 38432 / 114272 | training loss: 0.00019435568538028747\n",
      "epoch: 9 | 38464 / 114272 | training loss: 0.10307576507329941\n",
      "epoch: 9 | 38496 / 114272 | training loss: 0.0002916331868618727\n",
      "epoch: 9 | 38528 / 114272 | training loss: 0.00022102738148532808\n",
      "epoch: 9 | 38560 / 114272 | training loss: 0.00025359552819281816\n",
      "epoch: 9 | 38592 / 114272 | training loss: 0.00024110919912345707\n",
      "epoch: 9 | 38624 / 114272 | training loss: 0.00043113785795867443\n",
      "epoch: 9 | 38656 / 114272 | training loss: 0.00017582226428203285\n",
      "epoch: 9 | 38688 / 114272 | training loss: 0.00020072096958756447\n",
      "epoch: 9 | 38720 / 114272 | training loss: 0.0004948387504555285\n",
      "epoch: 9 | 38752 / 114272 | training loss: 0.0011634076945483685\n",
      "epoch: 9 | 38784 / 114272 | training loss: 0.00013181657413952053\n",
      "epoch: 9 | 38816 / 114272 | training loss: 0.00022456547594629228\n",
      "epoch: 9 | 38848 / 114272 | training loss: 0.0003071347309742123\n",
      "epoch: 9 | 38880 / 114272 | training loss: 0.00018620099581312388\n",
      "epoch: 9 | 38912 / 114272 | training loss: 0.00018674059538170695\n",
      "epoch: 9 | 38944 / 114272 | training loss: 9.526419307803735e-05\n",
      "epoch: 9 | 38976 / 114272 | training loss: 0.00019504301599226892\n",
      "epoch: 9 | 39008 / 114272 | training loss: 0.000178354763193056\n",
      "epoch: 9 | 39040 / 114272 | training loss: 0.0001827827509259805\n",
      "epoch: 9 | 39072 / 114272 | training loss: 0.0012517657596617937\n",
      "epoch: 9 | 39104 / 114272 | training loss: 0.00017387884145136923\n",
      "epoch: 9 | 39136 / 114272 | training loss: 0.2106698602437973\n",
      "epoch: 9 | 39168 / 114272 | training loss: 0.0004388036613818258\n",
      "epoch: 9 | 39200 / 114272 | training loss: 0.00028122562798671424\n",
      "epoch: 9 | 39232 / 114272 | training loss: 0.00038663981831632555\n",
      "epoch: 9 | 39264 / 114272 | training loss: 0.1099766343832016\n",
      "epoch: 9 | 39296 / 114272 | training loss: 0.0001797820586943999\n",
      "epoch: 9 | 39328 / 114272 | training loss: 0.0002449724415782839\n",
      "epoch: 9 | 39360 / 114272 | training loss: 0.0002436702197883278\n",
      "epoch: 9 | 39392 / 114272 | training loss: 0.0003453486133366823\n",
      "epoch: 9 | 39424 / 114272 | training loss: 0.0002978102129418403\n",
      "epoch: 9 | 39456 / 114272 | training loss: 0.0003392629441805184\n",
      "epoch: 9 | 39488 / 114272 | training loss: 0.00031257831142283976\n",
      "epoch: 9 | 39520 / 114272 | training loss: 0.00015250890282914042\n",
      "epoch: 9 | 39552 / 114272 | training loss: 0.00044489375432021916\n",
      "epoch: 9 | 39584 / 114272 | training loss: 0.00031502594356425107\n",
      "epoch: 9 | 39616 / 114272 | training loss: 0.00020725096692331135\n",
      "epoch: 9 | 39648 / 114272 | training loss: 0.000580912281293422\n",
      "epoch: 9 | 39680 / 114272 | training loss: 0.00027758226497098804\n",
      "epoch: 9 | 39712 / 114272 | training loss: 0.0002512035716790706\n",
      "epoch: 9 | 39744 / 114272 | training loss: 0.00021788731100969017\n",
      "epoch: 9 | 39776 / 114272 | training loss: 0.00016271234198939055\n",
      "epoch: 9 | 39808 / 114272 | training loss: 0.0002418775693513453\n",
      "epoch: 9 | 39840 / 114272 | training loss: 0.0030800083186477423\n",
      "epoch: 9 | 39872 / 114272 | training loss: 0.0009315889910794795\n",
      "epoch: 9 | 39904 / 114272 | training loss: 0.00023256143322214484\n",
      "epoch: 9 | 39936 / 114272 | training loss: 0.0006261391099542379\n",
      "epoch: 9 | 39968 / 114272 | training loss: 0.00022434549464378506\n",
      "epoch: 9 | 40000 / 114272 | training loss: 0.00023414904717355967\n",
      "epoch: 9 | 40032 / 114272 | training loss: 0.00012968051305506378\n",
      "epoch: 9 | 40064 / 114272 | training loss: 0.0004836555744986981\n",
      "epoch: 9 | 40096 / 114272 | training loss: 0.00029152020579203963\n",
      "epoch: 9 | 40128 / 114272 | training loss: 0.0288945734500885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 40160 / 114272 | training loss: 0.0002654659911058843\n",
      "epoch: 9 | 40192 / 114272 | training loss: 0.00020985110313631594\n",
      "epoch: 9 | 40224 / 114272 | training loss: 0.0003823823935817927\n",
      "epoch: 9 | 40256 / 114272 | training loss: 0.0002873153716791421\n",
      "epoch: 9 | 40288 / 114272 | training loss: 0.00022233555500861257\n",
      "epoch: 9 | 40320 / 114272 | training loss: 0.00021937332348898053\n",
      "epoch: 9 | 40352 / 114272 | training loss: 0.0009569381945766509\n",
      "epoch: 9 | 40384 / 114272 | training loss: 0.0003201139043085277\n",
      "epoch: 9 | 40416 / 114272 | training loss: 0.0001918031193781644\n",
      "epoch: 9 | 40448 / 114272 | training loss: 0.202604740858078\n",
      "epoch: 9 | 40480 / 114272 | training loss: 0.00024970932281576097\n",
      "epoch: 9 | 40512 / 114272 | training loss: 0.0005076737725175917\n",
      "epoch: 9 | 40544 / 114272 | training loss: 0.0002397835487499833\n",
      "epoch: 9 | 40576 / 114272 | training loss: 0.004325042013078928\n",
      "epoch: 9 | 40608 / 114272 | training loss: 0.2775653600692749\n",
      "epoch: 9 | 40640 / 114272 | training loss: 0.0001584323908900842\n",
      "epoch: 9 | 40672 / 114272 | training loss: 0.0003070726234000176\n",
      "epoch: 9 | 40704 / 114272 | training loss: 0.0014064291026443243\n",
      "epoch: 9 | 40736 / 114272 | training loss: 0.0001555933413328603\n",
      "epoch: 9 | 40768 / 114272 | training loss: 0.014924312010407448\n",
      "epoch: 9 | 40800 / 114272 | training loss: 0.00031351580400951207\n",
      "epoch: 9 | 40832 / 114272 | training loss: 0.0001573756744619459\n",
      "epoch: 9 | 40864 / 114272 | training loss: 0.0001670961792115122\n",
      "epoch: 9 | 40896 / 114272 | training loss: 0.0002377134223934263\n",
      "epoch: 9 | 40928 / 114272 | training loss: 0.0003520261379890144\n",
      "epoch: 9 | 40960 / 114272 | training loss: 0.00043445220217108727\n",
      "epoch: 9 | 40992 / 114272 | training loss: 0.0002145257603842765\n",
      "epoch: 9 | 41024 / 114272 | training loss: 0.0008367908885702491\n",
      "epoch: 9 | 41056 / 114272 | training loss: 0.0003076608118135482\n",
      "epoch: 9 | 41088 / 114272 | training loss: 0.0001325043267570436\n",
      "epoch: 9 | 41120 / 114272 | training loss: 0.00025180590455420315\n",
      "epoch: 9 | 41152 / 114272 | training loss: 0.0003761994303204119\n",
      "epoch: 9 | 41184 / 114272 | training loss: 0.0009282490937039256\n",
      "epoch: 9 | 41216 / 114272 | training loss: 0.00031417098944075406\n",
      "epoch: 9 | 41248 / 114272 | training loss: 0.0002612057141959667\n",
      "epoch: 9 | 41280 / 114272 | training loss: 0.00027107857749797404\n",
      "epoch: 9 | 41312 / 114272 | training loss: 0.00015081687888596207\n",
      "epoch: 9 | 41344 / 114272 | training loss: 0.0002364773245062679\n",
      "epoch: 9 | 41376 / 114272 | training loss: 0.0003607349353842437\n",
      "epoch: 9 | 41408 / 114272 | training loss: 0.0002029180177487433\n",
      "epoch: 9 | 41440 / 114272 | training loss: 0.0009166921954602003\n",
      "epoch: 9 | 41472 / 114272 | training loss: 0.00036523962626233697\n",
      "epoch: 9 | 41504 / 114272 | training loss: 0.0003465228946879506\n",
      "epoch: 9 | 41536 / 114272 | training loss: 0.00041365306242369115\n",
      "epoch: 9 | 41568 / 114272 | training loss: 0.29716891050338745\n",
      "epoch: 9 | 41600 / 114272 | training loss: 0.00023374581360258162\n",
      "epoch: 9 | 41632 / 114272 | training loss: 0.0005451196921057999\n",
      "epoch: 9 | 41664 / 114272 | training loss: 0.00046407777699641883\n",
      "epoch: 9 | 41696 / 114272 | training loss: 0.0002880867978092283\n",
      "epoch: 9 | 41728 / 114272 | training loss: 0.00016915447486098856\n",
      "epoch: 9 | 41760 / 114272 | training loss: 0.0004387313383631408\n",
      "epoch: 9 | 41792 / 114272 | training loss: 0.00022662256378680468\n",
      "epoch: 9 | 41824 / 114272 | training loss: 0.0029862234368920326\n",
      "epoch: 9 | 41856 / 114272 | training loss: 0.0004869127878919244\n",
      "epoch: 9 | 41888 / 114272 | training loss: 0.00046449314686469734\n",
      "epoch: 9 | 41920 / 114272 | training loss: 0.0002585609909147024\n",
      "epoch: 9 | 41952 / 114272 | training loss: 0.001310307765379548\n",
      "epoch: 9 | 41984 / 114272 | training loss: 0.00022513790463563055\n",
      "epoch: 9 | 42016 / 114272 | training loss: 0.0009164141956716776\n",
      "epoch: 9 | 42048 / 114272 | training loss: 0.0002130990324076265\n",
      "epoch: 9 | 42080 / 114272 | training loss: 0.0002773755695670843\n",
      "epoch: 9 | 42112 / 114272 | training loss: 0.0009992991108447313\n",
      "epoch: 9 | 42144 / 114272 | training loss: 0.00038418470649048686\n",
      "epoch: 9 | 42176 / 114272 | training loss: 0.0002554133243393153\n",
      "epoch: 9 | 42208 / 114272 | training loss: 0.00032649014610797167\n",
      "epoch: 9 | 42240 / 114272 | training loss: 0.00015430206258315593\n",
      "epoch: 9 | 42272 / 114272 | training loss: 0.00033989452640525997\n",
      "epoch: 9 | 42304 / 114272 | training loss: 0.00023212841188069433\n",
      "epoch: 9 | 42336 / 114272 | training loss: 0.0003415636601857841\n",
      "epoch: 9 | 42368 / 114272 | training loss: 0.0002548527263570577\n",
      "epoch: 9 | 42400 / 114272 | training loss: 0.00028586911503225565\n",
      "epoch: 9 | 42432 / 114272 | training loss: 0.0014679731102660298\n",
      "epoch: 9 | 42464 / 114272 | training loss: 0.00021037440455984324\n",
      "epoch: 9 | 42496 / 114272 | training loss: 0.00012050652730977163\n",
      "epoch: 9 | 42528 / 114272 | training loss: 0.0002459030947647989\n",
      "epoch: 9 | 42560 / 114272 | training loss: 0.0011822533560916781\n",
      "epoch: 9 | 42592 / 114272 | training loss: 0.00012023362069157884\n",
      "epoch: 9 | 42624 / 114272 | training loss: 0.00011481911496957764\n",
      "epoch: 9 | 42656 / 114272 | training loss: 0.03041885420680046\n",
      "epoch: 9 | 42688 / 114272 | training loss: 0.00043487746734172106\n",
      "epoch: 9 | 42720 / 114272 | training loss: 0.00022932451975066215\n",
      "epoch: 9 | 42752 / 114272 | training loss: 0.08050999045372009\n",
      "epoch: 9 | 42784 / 114272 | training loss: 0.00046454684343189\n",
      "epoch: 9 | 42816 / 114272 | training loss: 0.043091051280498505\n",
      "epoch: 9 | 42848 / 114272 | training loss: 0.0070047625340521336\n",
      "epoch: 9 | 42880 / 114272 | training loss: 0.00029934983467683196\n",
      "epoch: 9 | 42912 / 114272 | training loss: 0.0002623907057568431\n",
      "epoch: 9 | 42944 / 114272 | training loss: 0.0012727612629532814\n",
      "epoch: 9 | 42976 / 114272 | training loss: 0.00029109884053468704\n",
      "epoch: 9 | 43008 / 114272 | training loss: 0.006053051445633173\n",
      "epoch: 9 | 43040 / 114272 | training loss: 0.00021012595971114933\n",
      "epoch: 9 | 43072 / 114272 | training loss: 0.00022971982252784073\n",
      "epoch: 9 | 43104 / 114272 | training loss: 0.00026576174423098564\n",
      "epoch: 9 | 43136 / 114272 | training loss: 0.0003137820167466998\n",
      "epoch: 9 | 43168 / 114272 | training loss: 0.00018800627731252462\n",
      "epoch: 9 | 43200 / 114272 | training loss: 0.00029630062635987997\n",
      "epoch: 9 | 43232 / 114272 | training loss: 0.00016673268692102283\n",
      "epoch: 9 | 43264 / 114272 | training loss: 0.00031579111237078905\n",
      "epoch: 9 | 43296 / 114272 | training loss: 0.00021480675786733627\n",
      "epoch: 9 | 43328 / 114272 | training loss: 0.0002974902163259685\n",
      "epoch: 9 | 43360 / 114272 | training loss: 0.00029645906761288643\n",
      "epoch: 9 | 43392 / 114272 | training loss: 0.00022844647173769772\n",
      "epoch: 9 | 43424 / 114272 | training loss: 0.02779257856309414\n",
      "epoch: 9 | 43456 / 114272 | training loss: 0.00023389140551444143\n",
      "epoch: 9 | 43488 / 114272 | training loss: 0.00027405255241319537\n",
      "epoch: 9 | 43520 / 114272 | training loss: 0.00017500444664619863\n",
      "epoch: 9 | 43552 / 114272 | training loss: 0.00019855838036164641\n",
      "epoch: 9 | 43584 / 114272 | training loss: 0.00019722931028809398\n",
      "epoch: 9 | 43616 / 114272 | training loss: 0.000181912662810646\n",
      "epoch: 9 | 43648 / 114272 | training loss: 0.000489235739223659\n",
      "epoch: 9 | 43680 / 114272 | training loss: 0.0004259217530488968\n",
      "epoch: 9 | 43712 / 114272 | training loss: 0.0002510558406356722\n",
      "epoch: 9 | 43744 / 114272 | training loss: 0.0002946117310784757\n",
      "epoch: 9 | 43776 / 114272 | training loss: 0.0002884546120185405\n",
      "epoch: 9 | 43808 / 114272 | training loss: 0.0006811442435719073\n",
      "epoch: 9 | 43840 / 114272 | training loss: 0.00028961958014406264\n",
      "epoch: 9 | 43872 / 114272 | training loss: 0.0006185778765939176\n",
      "epoch: 9 | 43904 / 114272 | training loss: 0.00033804410486482084\n",
      "epoch: 9 | 43936 / 114272 | training loss: 0.00028517781174741685\n",
      "epoch: 9 | 43968 / 114272 | training loss: 0.00017588560876902193\n",
      "epoch: 9 | 44000 / 114272 | training loss: 0.0002453239285387099\n",
      "epoch: 9 | 44032 / 114272 | training loss: 0.00033084856113418937\n",
      "epoch: 9 | 44064 / 114272 | training loss: 0.00028123357333242893\n",
      "epoch: 9 | 44096 / 114272 | training loss: 0.0003239682118874043\n",
      "epoch: 9 | 44128 / 114272 | training loss: 0.00030832423362880945\n",
      "epoch: 9 | 44160 / 114272 | training loss: 0.00021575443679466844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 44192 / 114272 | training loss: 0.00039392284816130996\n",
      "epoch: 9 | 44224 / 114272 | training loss: 0.0003427891933824867\n",
      "epoch: 9 | 44256 / 114272 | training loss: 0.00023838324705138803\n",
      "epoch: 9 | 44288 / 114272 | training loss: 0.0002659970195963979\n",
      "epoch: 9 | 44320 / 114272 | training loss: 0.000371484667994082\n",
      "epoch: 9 | 44352 / 114272 | training loss: 0.002313926350325346\n",
      "epoch: 9 | 44384 / 114272 | training loss: 0.00032692140666767955\n",
      "epoch: 9 | 44416 / 114272 | training loss: 0.00028088592807762325\n",
      "epoch: 9 | 44448 / 114272 | training loss: 0.16878272593021393\n",
      "epoch: 9 | 44480 / 114272 | training loss: 0.00017915158241521567\n",
      "epoch: 9 | 44512 / 114272 | training loss: 0.0004066552210133523\n",
      "epoch: 9 | 44544 / 114272 | training loss: 0.00030278784106485546\n",
      "epoch: 9 | 44576 / 114272 | training loss: 0.00021330741583369672\n",
      "epoch: 9 | 44608 / 114272 | training loss: 0.00026388527476228774\n",
      "epoch: 9 | 44640 / 114272 | training loss: 0.08060713112354279\n",
      "epoch: 9 | 44672 / 114272 | training loss: 0.00019612663891166449\n",
      "epoch: 9 | 44704 / 114272 | training loss: 0.00024021782155614346\n",
      "epoch: 9 | 44736 / 114272 | training loss: 0.00030263958615250885\n",
      "epoch: 9 | 44768 / 114272 | training loss: 0.10748129338026047\n",
      "epoch: 9 | 44800 / 114272 | training loss: 0.00031820707954466343\n",
      "epoch: 9 | 44832 / 114272 | training loss: 0.00015775230713188648\n",
      "epoch: 9 | 44864 / 114272 | training loss: 0.00041722049354575574\n",
      "epoch: 9 | 44896 / 114272 | training loss: 0.0003269820299465209\n",
      "epoch: 9 | 44928 / 114272 | training loss: 0.000160106792463921\n",
      "epoch: 9 | 44960 / 114272 | training loss: 0.000599999853875488\n",
      "epoch: 9 | 44992 / 114272 | training loss: 0.0010243754368275404\n",
      "epoch: 9 | 45024 / 114272 | training loss: 0.00021942057355772704\n",
      "epoch: 9 | 45056 / 114272 | training loss: 0.09095868468284607\n",
      "epoch: 9 | 45088 / 114272 | training loss: 0.00024847080931067467\n",
      "epoch: 9 | 45120 / 114272 | training loss: 0.00025696700322441757\n",
      "epoch: 9 | 45152 / 114272 | training loss: 0.00014744632062502205\n",
      "epoch: 9 | 45184 / 114272 | training loss: 0.00019681875710375607\n",
      "epoch: 9 | 45216 / 114272 | training loss: 0.0003492626128718257\n",
      "epoch: 9 | 45248 / 114272 | training loss: 0.00024223257787525654\n",
      "epoch: 9 | 45280 / 114272 | training loss: 0.0010719932615756989\n",
      "epoch: 9 | 45312 / 114272 | training loss: 0.00032968309824354947\n",
      "epoch: 9 | 45344 / 114272 | training loss: 0.000442935386672616\n",
      "epoch: 9 | 45376 / 114272 | training loss: 0.00037988126860000193\n",
      "epoch: 9 | 45408 / 114272 | training loss: 0.00030777230858802795\n",
      "epoch: 9 | 45440 / 114272 | training loss: 0.00033378577791154385\n",
      "epoch: 9 | 45472 / 114272 | training loss: 0.00019174824410583824\n",
      "epoch: 9 | 45504 / 114272 | training loss: 0.0003177494218107313\n",
      "epoch: 9 | 45536 / 114272 | training loss: 0.00038182607386261225\n",
      "epoch: 9 | 45568 / 114272 | training loss: 0.00024324242258444428\n",
      "epoch: 9 | 45600 / 114272 | training loss: 0.00048120992141775787\n",
      "epoch: 9 | 45632 / 114272 | training loss: 0.002458561910316348\n",
      "epoch: 9 | 45664 / 114272 | training loss: 0.00041923238313756883\n",
      "epoch: 9 | 45696 / 114272 | training loss: 0.00016331976803485304\n",
      "epoch: 9 | 45728 / 114272 | training loss: 0.00019214551139157265\n",
      "epoch: 9 | 45760 / 114272 | training loss: 0.1359076052904129\n",
      "epoch: 9 | 45792 / 114272 | training loss: 0.00037196758785285056\n",
      "epoch: 9 | 45824 / 114272 | training loss: 0.00021831387130077928\n",
      "epoch: 9 | 45856 / 114272 | training loss: 0.0002068784087896347\n",
      "epoch: 9 | 45888 / 114272 | training loss: 0.00035174842923879623\n",
      "epoch: 9 | 45920 / 114272 | training loss: 0.2333029955625534\n",
      "epoch: 9 | 45952 / 114272 | training loss: 0.0007487992406822741\n",
      "epoch: 9 | 45984 / 114272 | training loss: 0.0002442310214973986\n",
      "epoch: 9 | 46016 / 114272 | training loss: 0.0002171286614611745\n",
      "epoch: 9 | 46048 / 114272 | training loss: 0.004055810160934925\n",
      "epoch: 9 | 46080 / 114272 | training loss: 0.00025117452605627477\n",
      "epoch: 9 | 46112 / 114272 | training loss: 0.0003587445826269686\n",
      "epoch: 9 | 46144 / 114272 | training loss: 0.0003101021284237504\n",
      "epoch: 9 | 46176 / 114272 | training loss: 0.000192269217222929\n",
      "epoch: 9 | 46208 / 114272 | training loss: 0.0004964210675098002\n",
      "epoch: 9 | 46240 / 114272 | training loss: 0.00013532988668885082\n",
      "epoch: 9 | 46272 / 114272 | training loss: 0.0012402604334056377\n",
      "epoch: 9 | 46304 / 114272 | training loss: 0.00033014657674357295\n",
      "epoch: 9 | 46336 / 114272 | training loss: 0.00014012744941283017\n",
      "epoch: 9 | 46368 / 114272 | training loss: 0.0001762875763233751\n",
      "epoch: 9 | 46400 / 114272 | training loss: 0.001473190262913704\n",
      "epoch: 9 | 46432 / 114272 | training loss: 0.001294890302233398\n",
      "epoch: 9 | 46464 / 114272 | training loss: 0.003654999192804098\n",
      "epoch: 9 | 46496 / 114272 | training loss: 0.0002312104479642585\n",
      "epoch: 9 | 46528 / 114272 | training loss: 0.00045438934466801584\n",
      "epoch: 9 | 46560 / 114272 | training loss: 0.0002573170349933207\n",
      "epoch: 9 | 46592 / 114272 | training loss: 0.00014420687512028962\n",
      "epoch: 9 | 46624 / 114272 | training loss: 0.00027024029986932874\n",
      "epoch: 9 | 46656 / 114272 | training loss: 0.00015205860836431384\n",
      "epoch: 9 | 46688 / 114272 | training loss: 0.00023242075985763222\n",
      "epoch: 9 | 46720 / 114272 | training loss: 0.0002561238070484251\n",
      "epoch: 9 | 46752 / 114272 | training loss: 0.00023862846137490124\n",
      "epoch: 9 | 46784 / 114272 | training loss: 0.0002534716622903943\n",
      "epoch: 9 | 46816 / 114272 | training loss: 0.010026706382632256\n",
      "epoch: 9 | 46848 / 114272 | training loss: 0.0003775815130211413\n",
      "epoch: 9 | 46880 / 114272 | training loss: 0.000288215815089643\n",
      "epoch: 9 | 46912 / 114272 | training loss: 0.00020492625480983406\n",
      "epoch: 9 | 46944 / 114272 | training loss: 0.00014110708434600383\n",
      "epoch: 9 | 46976 / 114272 | training loss: 0.0003904089389834553\n",
      "epoch: 9 | 47008 / 114272 | training loss: 0.0003194386081304401\n",
      "epoch: 9 | 47040 / 114272 | training loss: 0.00019865155627485365\n",
      "epoch: 9 | 47072 / 114272 | training loss: 0.0002353862364543602\n",
      "epoch: 9 | 47104 / 114272 | training loss: 0.00018294199253432453\n",
      "epoch: 9 | 47136 / 114272 | training loss: 0.0004901345819234848\n",
      "epoch: 9 | 47168 / 114272 | training loss: 0.11245229840278625\n",
      "epoch: 9 | 47200 / 114272 | training loss: 0.02491343393921852\n",
      "epoch: 9 | 47232 / 114272 | training loss: 0.00021671193826477975\n",
      "epoch: 9 | 47264 / 114272 | training loss: 0.00019490101840347052\n",
      "epoch: 9 | 47296 / 114272 | training loss: 0.00024732633028179407\n",
      "epoch: 9 | 47328 / 114272 | training loss: 0.00021182543423492461\n",
      "epoch: 9 | 47360 / 114272 | training loss: 0.00015718840586487204\n",
      "epoch: 9 | 47392 / 114272 | training loss: 0.0002685949730221182\n",
      "epoch: 9 | 47424 / 114272 | training loss: 0.0003058503498323262\n",
      "epoch: 9 | 47456 / 114272 | training loss: 0.00019099908240605146\n",
      "epoch: 9 | 47488 / 114272 | training loss: 0.00027014879742637277\n",
      "epoch: 9 | 47520 / 114272 | training loss: 0.00035513067268766463\n",
      "epoch: 9 | 47552 / 114272 | training loss: 0.0003320702526252717\n",
      "epoch: 9 | 47584 / 114272 | training loss: 0.00028982656658627093\n",
      "epoch: 9 | 47616 / 114272 | training loss: 0.0002636648714542389\n",
      "epoch: 9 | 47648 / 114272 | training loss: 0.005489451345056295\n",
      "epoch: 9 | 47680 / 114272 | training loss: 0.00030553064425475895\n",
      "epoch: 9 | 47712 / 114272 | training loss: 0.00019262565183453262\n",
      "epoch: 9 | 47744 / 114272 | training loss: 0.08750953525304794\n",
      "epoch: 9 | 47776 / 114272 | training loss: 0.00016377419524360448\n",
      "epoch: 9 | 47808 / 114272 | training loss: 0.0002975002571474761\n",
      "epoch: 9 | 47840 / 114272 | training loss: 0.00038256909465417266\n",
      "epoch: 9 | 47872 / 114272 | training loss: 0.0008511703927069902\n",
      "epoch: 9 | 47904 / 114272 | training loss: 0.0003918203874491155\n",
      "epoch: 9 | 47936 / 114272 | training loss: 0.00032430572900921106\n",
      "epoch: 9 | 47968 / 114272 | training loss: 0.00041103415424004197\n",
      "epoch: 9 | 48000 / 114272 | training loss: 0.00026450585573911667\n",
      "epoch: 9 | 48032 / 114272 | training loss: 0.00020893516193609685\n",
      "epoch: 9 | 48064 / 114272 | training loss: 0.0003316132933832705\n",
      "epoch: 9 | 48096 / 114272 | training loss: 0.0003270453598815948\n",
      "epoch: 9 | 48128 / 114272 | training loss: 0.000211630598641932\n",
      "epoch: 9 | 48160 / 114272 | training loss: 0.0002489248872734606\n",
      "epoch: 9 | 48192 / 114272 | training loss: 0.00017469818703830242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 48224 / 114272 | training loss: 0.0004466816608328372\n",
      "epoch: 9 | 48256 / 114272 | training loss: 0.00026832197909243405\n",
      "epoch: 9 | 48288 / 114272 | training loss: 0.00032981441472657025\n",
      "epoch: 9 | 48320 / 114272 | training loss: 0.00022295051894616336\n",
      "epoch: 9 | 48352 / 114272 | training loss: 0.0003030271327588707\n",
      "epoch: 9 | 48384 / 114272 | training loss: 0.00014270945393946022\n",
      "epoch: 9 | 48416 / 114272 | training loss: 0.0001893117732834071\n",
      "epoch: 9 | 48448 / 114272 | training loss: 0.0002078885299852118\n",
      "epoch: 9 | 48480 / 114272 | training loss: 0.0004027636896353215\n",
      "epoch: 9 | 48512 / 114272 | training loss: 0.0004084318643435836\n",
      "epoch: 9 | 48544 / 114272 | training loss: 0.0003497373254504055\n",
      "epoch: 9 | 48576 / 114272 | training loss: 0.00039882175042293966\n",
      "epoch: 9 | 48608 / 114272 | training loss: 0.015220064669847488\n",
      "epoch: 9 | 48640 / 114272 | training loss: 0.00019135067122988403\n",
      "epoch: 9 | 48672 / 114272 | training loss: 0.00020311280968599021\n",
      "epoch: 9 | 48704 / 114272 | training loss: 0.00032922899117693305\n",
      "epoch: 9 | 48736 / 114272 | training loss: 0.00043056090362370014\n",
      "epoch: 9 | 48768 / 114272 | training loss: 0.00024529529036954045\n",
      "epoch: 9 | 48800 / 114272 | training loss: 0.0002892318880185485\n",
      "epoch: 9 | 48832 / 114272 | training loss: 0.00023590023920405656\n",
      "epoch: 9 | 48864 / 114272 | training loss: 0.00011013296898454428\n",
      "epoch: 9 | 48896 / 114272 | training loss: 0.000540034263394773\n",
      "epoch: 9 | 48928 / 114272 | training loss: 0.0003009075007867068\n",
      "epoch: 9 | 48960 / 114272 | training loss: 0.00031108493567444384\n",
      "epoch: 9 | 48992 / 114272 | training loss: 0.00017643456521909684\n",
      "epoch: 9 | 49024 / 114272 | training loss: 0.00032343092607334256\n",
      "epoch: 9 | 49056 / 114272 | training loss: 0.12911531329154968\n",
      "epoch: 9 | 49088 / 114272 | training loss: 0.00022082883515395224\n",
      "epoch: 9 | 49120 / 114272 | training loss: 0.00015084468759596348\n",
      "epoch: 9 | 49152 / 114272 | training loss: 0.00027233207947574556\n",
      "epoch: 9 | 49184 / 114272 | training loss: 0.00024146570649463683\n",
      "epoch: 9 | 49216 / 114272 | training loss: 0.0002080500271404162\n",
      "epoch: 9 | 49248 / 114272 | training loss: 0.0002956205571535975\n",
      "epoch: 9 | 49280 / 114272 | training loss: 0.00023486015561502427\n",
      "epoch: 9 | 49312 / 114272 | training loss: 0.0008847352582961321\n",
      "epoch: 9 | 49344 / 114272 | training loss: 0.00022084719967097044\n",
      "epoch: 9 | 49376 / 114272 | training loss: 0.0006126545486040413\n",
      "epoch: 9 | 49408 / 114272 | training loss: 0.0002405320410616696\n",
      "epoch: 9 | 49440 / 114272 | training loss: 0.00032669809297658503\n",
      "epoch: 9 | 49472 / 114272 | training loss: 0.00015604443615302444\n",
      "epoch: 9 | 49504 / 114272 | training loss: 0.0003161178028676659\n",
      "epoch: 9 | 49536 / 114272 | training loss: 0.004219747148454189\n",
      "epoch: 9 | 49568 / 114272 | training loss: 0.00022494616860058159\n",
      "epoch: 9 | 49600 / 114272 | training loss: 0.0002624424232635647\n",
      "epoch: 9 | 49632 / 114272 | training loss: 0.0002546808682382107\n",
      "epoch: 9 | 49664 / 114272 | training loss: 0.0003796997189056128\n",
      "epoch: 9 | 49696 / 114272 | training loss: 0.00033093587262555957\n",
      "epoch: 9 | 49728 / 114272 | training loss: 0.0004029758565593511\n",
      "epoch: 9 | 49760 / 114272 | training loss: 0.0004182941629551351\n",
      "epoch: 9 | 49792 / 114272 | training loss: 0.00026326323859393597\n",
      "epoch: 9 | 49824 / 114272 | training loss: 0.0003450618823990226\n",
      "epoch: 9 | 49856 / 114272 | training loss: 0.0007344884215854108\n",
      "epoch: 9 | 49888 / 114272 | training loss: 0.0003932834370061755\n",
      "epoch: 9 | 49920 / 114272 | training loss: 0.0001760248705977574\n",
      "epoch: 9 | 49952 / 114272 | training loss: 0.0020334897562861443\n",
      "epoch: 9 | 49984 / 114272 | training loss: 0.000332572526531294\n",
      "epoch: 9 | 50016 / 114272 | training loss: 0.000414647365687415\n",
      "epoch: 9 | 50048 / 114272 | training loss: 0.00031062689959071577\n",
      "epoch: 9 | 50080 / 114272 | training loss: 0.00037262000842019916\n",
      "epoch: 9 | 50112 / 114272 | training loss: 0.0001916563487611711\n",
      "epoch: 9 | 50144 / 114272 | training loss: 0.00019398661970626563\n",
      "epoch: 9 | 50176 / 114272 | training loss: 0.00021982996258884668\n",
      "epoch: 9 | 50208 / 114272 | training loss: 0.1987953931093216\n",
      "epoch: 9 | 50240 / 114272 | training loss: 0.00023124710423871875\n",
      "epoch: 9 | 50272 / 114272 | training loss: 0.00015824644651729614\n",
      "epoch: 9 | 50304 / 114272 | training loss: 0.0007241350831463933\n",
      "epoch: 9 | 50336 / 114272 | training loss: 0.00021608230599667877\n",
      "epoch: 9 | 50368 / 114272 | training loss: 0.1101580411195755\n",
      "epoch: 9 | 50400 / 114272 | training loss: 0.00024563862825743854\n",
      "epoch: 9 | 50432 / 114272 | training loss: 0.0004912831936962903\n",
      "epoch: 9 | 50464 / 114272 | training loss: 0.00023778209288138896\n",
      "epoch: 9 | 50496 / 114272 | training loss: 0.0004201139963697642\n",
      "epoch: 9 | 50528 / 114272 | training loss: 0.00036440714029595256\n",
      "epoch: 9 | 50560 / 114272 | training loss: 0.21622738242149353\n",
      "epoch: 9 | 50592 / 114272 | training loss: 0.00041316761053167284\n",
      "epoch: 9 | 50624 / 114272 | training loss: 0.0004317530838306993\n",
      "epoch: 9 | 50656 / 114272 | training loss: 0.0003633231681305915\n",
      "epoch: 9 | 50688 / 114272 | training loss: 0.0004126504936721176\n",
      "epoch: 9 | 50720 / 114272 | training loss: 0.0003980630135629326\n",
      "epoch: 9 | 50752 / 114272 | training loss: 0.0004167561128269881\n",
      "epoch: 9 | 50784 / 114272 | training loss: 0.00017548467440064996\n",
      "epoch: 9 | 50816 / 114272 | training loss: 0.00046723955892957747\n",
      "epoch: 9 | 50848 / 114272 | training loss: 0.00035284634213894606\n",
      "epoch: 9 | 50880 / 114272 | training loss: 0.0002493945066817105\n",
      "epoch: 9 | 50912 / 114272 | training loss: 0.00035877875052392483\n",
      "epoch: 9 | 50944 / 114272 | training loss: 0.0003324952267576009\n",
      "epoch: 9 | 50976 / 114272 | training loss: 0.0002787065168377012\n",
      "epoch: 9 | 51008 / 114272 | training loss: 0.00041818301542662084\n",
      "epoch: 9 | 51040 / 114272 | training loss: 0.00048538504051975906\n",
      "epoch: 9 | 51072 / 114272 | training loss: 0.0001837738964240998\n",
      "epoch: 9 | 51104 / 114272 | training loss: 0.0005707023665308952\n",
      "epoch: 9 | 51136 / 114272 | training loss: 0.19480040669441223\n",
      "epoch: 9 | 51168 / 114272 | training loss: 0.0003507946094032377\n",
      "epoch: 9 | 51200 / 114272 | training loss: 0.0021886900067329407\n",
      "epoch: 9 | 51232 / 114272 | training loss: 0.000263128022197634\n",
      "epoch: 9 | 51264 / 114272 | training loss: 0.0004803469346370548\n",
      "epoch: 9 | 51296 / 114272 | training loss: 0.0005300692282617092\n",
      "epoch: 9 | 51328 / 114272 | training loss: 0.00019928223628085107\n",
      "epoch: 9 | 51360 / 114272 | training loss: 0.00043841858860105276\n",
      "epoch: 9 | 51392 / 114272 | training loss: 0.0001717236009426415\n",
      "epoch: 9 | 51424 / 114272 | training loss: 0.00042876339284703135\n",
      "epoch: 9 | 51456 / 114272 | training loss: 0.00012763809354510158\n",
      "epoch: 9 | 51488 / 114272 | training loss: 0.00023149380285758525\n",
      "epoch: 9 | 51520 / 114272 | training loss: 0.0001232365902978927\n",
      "epoch: 9 | 51552 / 114272 | training loss: 0.00022120351786725223\n",
      "epoch: 9 | 51584 / 114272 | training loss: 0.00035006299731321633\n",
      "epoch: 9 | 51616 / 114272 | training loss: 0.0011819187784567475\n",
      "epoch: 9 | 51648 / 114272 | training loss: 0.00034163077361881733\n",
      "epoch: 9 | 51680 / 114272 | training loss: 0.0002919331018347293\n",
      "epoch: 9 | 51712 / 114272 | training loss: 0.0033976512495428324\n",
      "epoch: 9 | 51744 / 114272 | training loss: 0.0002011741598835215\n",
      "epoch: 9 | 51776 / 114272 | training loss: 0.00022811345115769655\n",
      "epoch: 9 | 51808 / 114272 | training loss: 0.00032190943602472544\n",
      "epoch: 9 | 51840 / 114272 | training loss: 0.00027175003197044134\n",
      "epoch: 9 | 51872 / 114272 | training loss: 0.0004859068721998483\n",
      "epoch: 9 | 51904 / 114272 | training loss: 0.0009211806464008987\n",
      "epoch: 9 | 51936 / 114272 | training loss: 0.00031214390764944255\n",
      "epoch: 9 | 51968 / 114272 | training loss: 0.00022966685355640948\n",
      "epoch: 9 | 52000 / 114272 | training loss: 0.0004300063301343471\n",
      "epoch: 9 | 52032 / 114272 | training loss: 0.0004681431455537677\n",
      "epoch: 9 | 52064 / 114272 | training loss: 0.0002632871037349105\n",
      "epoch: 9 | 52096 / 114272 | training loss: 0.0026256900746375322\n",
      "epoch: 9 | 52128 / 114272 | training loss: 0.0003499201266095042\n",
      "epoch: 9 | 52160 / 114272 | training loss: 0.0002099822013406083\n",
      "epoch: 9 | 52192 / 114272 | training loss: 0.0002927521418314427\n",
      "epoch: 9 | 52224 / 114272 | training loss: 0.0002489810867700726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 52256 / 114272 | training loss: 0.0010579130612313747\n",
      "epoch: 9 | 52288 / 114272 | training loss: 0.0002878346131183207\n",
      "epoch: 9 | 52320 / 114272 | training loss: 0.00036916573299095035\n",
      "epoch: 9 | 52352 / 114272 | training loss: 0.00026556066586636007\n",
      "epoch: 9 | 52384 / 114272 | training loss: 0.00029556077788583934\n",
      "epoch: 9 | 52416 / 114272 | training loss: 0.00031071307603269815\n",
      "epoch: 9 | 52448 / 114272 | training loss: 0.0008151402580551803\n",
      "epoch: 9 | 52480 / 114272 | training loss: 0.015931332483887672\n",
      "epoch: 9 | 52512 / 114272 | training loss: 0.022271353751420975\n",
      "epoch: 9 | 52544 / 114272 | training loss: 0.00029600216657854617\n",
      "epoch: 9 | 52576 / 114272 | training loss: 0.00038479012437164783\n",
      "epoch: 9 | 52608 / 114272 | training loss: 0.0004182200937066227\n",
      "epoch: 9 | 52640 / 114272 | training loss: 0.0003960524045396596\n",
      "epoch: 9 | 52672 / 114272 | training loss: 0.00021134458074811846\n",
      "epoch: 9 | 52704 / 114272 | training loss: 0.00020517047960311174\n",
      "epoch: 9 | 52736 / 114272 | training loss: 0.0002513701329007745\n",
      "epoch: 9 | 52768 / 114272 | training loss: 0.00034864331246353686\n",
      "epoch: 9 | 52800 / 114272 | training loss: 0.00034655691706575453\n",
      "epoch: 9 | 52832 / 114272 | training loss: 0.00035934135667048395\n",
      "epoch: 9 | 52864 / 114272 | training loss: 0.00017827373812906444\n",
      "epoch: 9 | 52896 / 114272 | training loss: 0.00017624378961045295\n",
      "epoch: 9 | 52928 / 114272 | training loss: 0.0004450074629858136\n",
      "epoch: 9 | 52960 / 114272 | training loss: 0.00020315685833338648\n",
      "epoch: 9 | 52992 / 114272 | training loss: 0.00024979497538879514\n",
      "epoch: 9 | 53024 / 114272 | training loss: 0.00029014452593401074\n",
      "epoch: 9 | 53056 / 114272 | training loss: 0.00019542864174582064\n",
      "epoch: 9 | 53088 / 114272 | training loss: 0.00031321277492679656\n",
      "epoch: 9 | 53120 / 114272 | training loss: 0.0002708415559027344\n",
      "epoch: 9 | 53152 / 114272 | training loss: 0.002953790593892336\n",
      "epoch: 9 | 53184 / 114272 | training loss: 0.00015741534298285842\n",
      "epoch: 9 | 53216 / 114272 | training loss: 0.0002981264842674136\n",
      "epoch: 9 | 53248 / 114272 | training loss: 0.013178224675357342\n",
      "epoch: 9 | 53280 / 114272 | training loss: 0.00035047586425207555\n",
      "epoch: 9 | 53312 / 114272 | training loss: 0.0005428045406006277\n",
      "epoch: 9 | 53344 / 114272 | training loss: 0.00044996163342148066\n",
      "epoch: 9 | 53376 / 114272 | training loss: 0.021848950535058975\n",
      "epoch: 9 | 53408 / 114272 | training loss: 0.00037429959047585726\n",
      "epoch: 9 | 53440 / 114272 | training loss: 0.0002565895556472242\n",
      "epoch: 9 | 53472 / 114272 | training loss: 0.0003603053337428719\n",
      "epoch: 9 | 53504 / 114272 | training loss: 0.0003414822567719966\n",
      "epoch: 9 | 53536 / 114272 | training loss: 0.00025487164384685457\n",
      "epoch: 9 | 53568 / 114272 | training loss: 0.017861440777778625\n",
      "epoch: 9 | 53600 / 114272 | training loss: 0.00029472727328538895\n",
      "epoch: 9 | 53632 / 114272 | training loss: 0.014250063337385654\n",
      "epoch: 9 | 53664 / 114272 | training loss: 0.0002254729624837637\n",
      "epoch: 9 | 53696 / 114272 | training loss: 0.00024563699844293296\n",
      "epoch: 9 | 53728 / 114272 | training loss: 0.0004043366643600166\n",
      "epoch: 9 | 53760 / 114272 | training loss: 0.11561379581689835\n",
      "epoch: 9 | 53792 / 114272 | training loss: 0.0004514144966378808\n",
      "epoch: 9 | 53824 / 114272 | training loss: 0.00031000893795862794\n",
      "epoch: 9 | 53856 / 114272 | training loss: 0.0002284995571244508\n",
      "epoch: 9 | 53888 / 114272 | training loss: 0.00021953733812551945\n",
      "epoch: 9 | 53920 / 114272 | training loss: 0.000346519926097244\n",
      "epoch: 9 | 53952 / 114272 | training loss: 0.00010517533519305289\n",
      "epoch: 9 | 53984 / 114272 | training loss: 0.15108652412891388\n",
      "epoch: 9 | 54016 / 114272 | training loss: 0.00028173698228783906\n",
      "epoch: 9 | 54048 / 114272 | training loss: 0.00023033645993564278\n",
      "epoch: 9 | 54080 / 114272 | training loss: 0.000313065480440855\n",
      "epoch: 9 | 54112 / 114272 | training loss: 0.00020085804862901568\n",
      "epoch: 9 | 54144 / 114272 | training loss: 0.00042337982449680567\n",
      "epoch: 9 | 54176 / 114272 | training loss: 0.0003379603731445968\n",
      "epoch: 9 | 54208 / 114272 | training loss: 0.0002899595710914582\n",
      "epoch: 9 | 54240 / 114272 | training loss: 0.000521624693647027\n",
      "epoch: 9 | 54272 / 114272 | training loss: 0.00032927197753451765\n",
      "epoch: 9 | 54304 / 114272 | training loss: 0.0003685951232910156\n",
      "epoch: 9 | 54336 / 114272 | training loss: 0.00020108124590478837\n",
      "epoch: 9 | 54368 / 114272 | training loss: 8.254326530732214e-05\n",
      "epoch: 9 | 54400 / 114272 | training loss: 0.00023157428950071335\n",
      "epoch: 9 | 54432 / 114272 | training loss: 0.00030041535501368344\n",
      "epoch: 9 | 54464 / 114272 | training loss: 0.0004092383896932006\n",
      "epoch: 9 | 54496 / 114272 | training loss: 0.00023281783796846867\n",
      "epoch: 9 | 54528 / 114272 | training loss: 0.000531354860868305\n",
      "epoch: 9 | 54560 / 114272 | training loss: 0.00039499648846685886\n",
      "epoch: 9 | 54592 / 114272 | training loss: 0.0003360198170412332\n",
      "epoch: 9 | 54624 / 114272 | training loss: 0.0002424878184683621\n",
      "epoch: 9 | 54656 / 114272 | training loss: 0.00023494017659686506\n",
      "epoch: 9 | 54688 / 114272 | training loss: 0.0002948899636976421\n",
      "epoch: 9 | 54720 / 114272 | training loss: 0.00039426947478204966\n",
      "epoch: 9 | 54752 / 114272 | training loss: 0.0002910949115175754\n",
      "epoch: 9 | 54784 / 114272 | training loss: 0.00032089833985082805\n",
      "epoch: 9 | 54816 / 114272 | training loss: 0.0007854188443161547\n",
      "epoch: 9 | 54848 / 114272 | training loss: 0.003409293945878744\n",
      "epoch: 9 | 54880 / 114272 | training loss: 0.00021123085753060877\n",
      "epoch: 9 | 54912 / 114272 | training loss: 0.0005935111548751593\n",
      "epoch: 9 | 54944 / 114272 | training loss: 0.0005112444050610065\n",
      "epoch: 9 | 54976 / 114272 | training loss: 0.0005057037342339754\n",
      "epoch: 9 | 55008 / 114272 | training loss: 0.00034969899570569396\n",
      "epoch: 9 | 55040 / 114272 | training loss: 0.10270702093839645\n",
      "epoch: 9 | 55072 / 114272 | training loss: 0.00046133919386193156\n",
      "epoch: 9 | 55104 / 114272 | training loss: 0.000607054156716913\n",
      "epoch: 9 | 55136 / 114272 | training loss: 0.00026703672483563423\n",
      "epoch: 9 | 55168 / 114272 | training loss: 0.0002767542900983244\n",
      "epoch: 9 | 55200 / 114272 | training loss: 0.0002263927599415183\n",
      "epoch: 9 | 55232 / 114272 | training loss: 0.000626064429525286\n",
      "epoch: 9 | 55264 / 114272 | training loss: 0.00019419666205067188\n",
      "epoch: 9 | 55296 / 114272 | training loss: 0.00041599132237024605\n",
      "epoch: 9 | 55328 / 114272 | training loss: 0.00017365031817462295\n",
      "epoch: 9 | 55360 / 114272 | training loss: 0.01469217985868454\n",
      "epoch: 9 | 55392 / 114272 | training loss: 0.00019266321032773703\n",
      "epoch: 9 | 55424 / 114272 | training loss: 0.0006374054355546832\n",
      "epoch: 9 | 55456 / 114272 | training loss: 0.00015741439710836858\n",
      "epoch: 9 | 55488 / 114272 | training loss: 0.00023561087436974049\n",
      "epoch: 9 | 55520 / 114272 | training loss: 0.0010549785802140832\n",
      "epoch: 9 | 55552 / 114272 | training loss: 0.0002969957422465086\n",
      "epoch: 9 | 55584 / 114272 | training loss: 0.00014485797146335244\n",
      "epoch: 9 | 55616 / 114272 | training loss: 0.0002531721256673336\n",
      "epoch: 9 | 55648 / 114272 | training loss: 0.0016966339899227023\n",
      "epoch: 9 | 55680 / 114272 | training loss: 0.00028131529688835144\n",
      "epoch: 9 | 55712 / 114272 | training loss: 0.00011166876356583089\n",
      "epoch: 9 | 55744 / 114272 | training loss: 0.0017181684961542487\n",
      "epoch: 9 | 55776 / 114272 | training loss: 0.001436446444131434\n",
      "epoch: 9 | 55808 / 114272 | training loss: 0.00018548626394476742\n",
      "epoch: 9 | 55840 / 114272 | training loss: 0.00030378421070054173\n",
      "epoch: 9 | 55872 / 114272 | training loss: 0.0005648209480568767\n",
      "epoch: 9 | 55904 / 114272 | training loss: 0.00016745580069255084\n",
      "epoch: 9 | 55936 / 114272 | training loss: 0.0003173984005115926\n",
      "epoch: 9 | 55968 / 114272 | training loss: 0.0003468419599812478\n",
      "epoch: 9 | 56000 / 114272 | training loss: 0.00021860585547983646\n",
      "epoch: 9 | 56032 / 114272 | training loss: 0.08749912679195404\n",
      "epoch: 9 | 56064 / 114272 | training loss: 0.10762704163789749\n",
      "epoch: 9 | 56096 / 114272 | training loss: 0.00023363625223282725\n",
      "epoch: 9 | 56128 / 114272 | training loss: 0.00017197181296069175\n",
      "epoch: 9 | 56160 / 114272 | training loss: 0.0002894513017963618\n",
      "epoch: 9 | 56192 / 114272 | training loss: 0.0002552709775045514\n",
      "epoch: 9 | 56224 / 114272 | training loss: 0.00045181132736615837\n",
      "epoch: 9 | 56256 / 114272 | training loss: 0.00034987746039405465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 56288 / 114272 | training loss: 0.0014696919824928045\n",
      "epoch: 9 | 56320 / 114272 | training loss: 0.0002666925429366529\n",
      "epoch: 9 | 56352 / 114272 | training loss: 0.00024596243747510016\n",
      "epoch: 9 | 56384 / 114272 | training loss: 0.00027382138068787754\n",
      "epoch: 9 | 56416 / 114272 | training loss: 0.00027900372515432537\n",
      "epoch: 9 | 56448 / 114272 | training loss: 0.00021248086704872549\n",
      "epoch: 9 | 56480 / 114272 | training loss: 0.0008679056190885603\n",
      "epoch: 9 | 56512 / 114272 | training loss: 0.0005958761903457344\n",
      "epoch: 9 | 56544 / 114272 | training loss: 0.00010367159120505676\n",
      "epoch: 9 | 56576 / 114272 | training loss: 0.00103306258097291\n",
      "epoch: 9 | 56608 / 114272 | training loss: 0.0003296037029940635\n",
      "epoch: 9 | 56640 / 114272 | training loss: 0.0002361343358643353\n",
      "epoch: 9 | 56672 / 114272 | training loss: 0.00032345738145522773\n",
      "epoch: 9 | 56704 / 114272 | training loss: 0.00036744921817444265\n",
      "epoch: 9 | 56736 / 114272 | training loss: 0.00033048319164663553\n",
      "epoch: 9 | 56768 / 114272 | training loss: 0.0003330355975776911\n",
      "epoch: 9 | 56800 / 114272 | training loss: 0.0035597512032836676\n",
      "epoch: 9 | 56832 / 114272 | training loss: 0.0003944306808989495\n",
      "epoch: 9 | 56864 / 114272 | training loss: 0.00035603303695097566\n",
      "epoch: 9 | 56896 / 114272 | training loss: 0.00044816636363975704\n",
      "epoch: 9 | 56928 / 114272 | training loss: 0.0003419533313717693\n",
      "epoch: 9 | 56960 / 114272 | training loss: 0.00019180066010449082\n",
      "epoch: 9 | 56992 / 114272 | training loss: 0.0003998403262812644\n",
      "epoch: 9 | 57024 / 114272 | training loss: 0.0009213892044499516\n",
      "epoch: 9 | 57056 / 114272 | training loss: 0.0013917878968641162\n",
      "epoch: 9 | 57088 / 114272 | training loss: 0.00023764664365444332\n",
      "epoch: 9 | 57120 / 114272 | training loss: 0.0002967721957247704\n",
      "epoch: 9 | 57152 / 114272 | training loss: 0.061746180057525635\n",
      "epoch: 9 | 57184 / 114272 | training loss: 0.00022616423666477203\n",
      "epoch: 9 | 57216 / 114272 | training loss: 0.00038316266727633774\n",
      "epoch: 9 | 57248 / 114272 | training loss: 0.0005579350981861353\n",
      "epoch: 9 | 57280 / 114272 | training loss: 0.00029409522539936006\n",
      "epoch: 9 | 57312 / 114272 | training loss: 0.00017544701404403895\n",
      "epoch: 9 | 57344 / 114272 | training loss: 0.00035304733319208026\n",
      "epoch: 9 | 57376 / 114272 | training loss: 0.0006533144041895866\n",
      "epoch: 9 | 57408 / 114272 | training loss: 0.000292608659947291\n",
      "epoch: 9 | 57440 / 114272 | training loss: 0.00041559484088793397\n",
      "epoch: 9 | 57472 / 114272 | training loss: 0.00038225401658564806\n",
      "epoch: 9 | 57504 / 114272 | training loss: 0.00017994586960412562\n",
      "epoch: 9 | 57536 / 114272 | training loss: 0.00023819116177037358\n",
      "epoch: 9 | 57568 / 114272 | training loss: 0.0007338756113313138\n",
      "epoch: 9 | 57600 / 114272 | training loss: 0.00013485834642779082\n",
      "epoch: 9 | 57632 / 114272 | training loss: 0.0002575655817054212\n",
      "epoch: 9 | 57664 / 114272 | training loss: 0.0005533526418730617\n",
      "epoch: 9 | 57696 / 114272 | training loss: 0.0005053333006799221\n",
      "epoch: 9 | 57728 / 114272 | training loss: 0.00029076827922835946\n",
      "epoch: 9 | 57760 / 114272 | training loss: 0.0002658625890035182\n",
      "epoch: 9 | 57792 / 114272 | training loss: 0.0002998010895680636\n",
      "epoch: 9 | 57824 / 114272 | training loss: 0.00024566083448007703\n",
      "epoch: 9 | 57856 / 114272 | training loss: 0.0003596205497160554\n",
      "epoch: 9 | 57888 / 114272 | training loss: 0.00028386982739903033\n",
      "epoch: 9 | 57920 / 114272 | training loss: 0.00021292823657859117\n",
      "epoch: 9 | 57952 / 114272 | training loss: 0.0003978843742515892\n",
      "epoch: 9 | 57984 / 114272 | training loss: 0.0002619390725158155\n",
      "epoch: 9 | 58016 / 114272 | training loss: 0.0017355933086946607\n",
      "epoch: 9 | 58048 / 114272 | training loss: 0.0004490528372116387\n",
      "epoch: 9 | 58080 / 114272 | training loss: 0.0006470414227806032\n",
      "epoch: 9 | 58112 / 114272 | training loss: 0.00023707638320047408\n",
      "epoch: 9 | 58144 / 114272 | training loss: 0.00021991855464875698\n",
      "epoch: 9 | 58176 / 114272 | training loss: 0.0004182764678262174\n",
      "epoch: 9 | 58208 / 114272 | training loss: 0.00045470520853996277\n",
      "epoch: 9 | 58240 / 114272 | training loss: 0.00018221407663077116\n",
      "epoch: 9 | 58272 / 114272 | training loss: 0.00028699173708446324\n",
      "epoch: 9 | 58304 / 114272 | training loss: 0.0002533727092668414\n",
      "epoch: 9 | 58336 / 114272 | training loss: 0.0003260808007325977\n",
      "epoch: 9 | 58368 / 114272 | training loss: 0.00029663549503311515\n",
      "epoch: 9 | 58400 / 114272 | training loss: 0.00022903933131601661\n",
      "epoch: 9 | 58432 / 114272 | training loss: 0.00017669754743110389\n",
      "epoch: 9 | 58464 / 114272 | training loss: 0.0003579882613848895\n",
      "epoch: 9 | 58496 / 114272 | training loss: 0.0001679463020991534\n",
      "epoch: 9 | 58528 / 114272 | training loss: 0.0003030240477528423\n",
      "epoch: 9 | 58560 / 114272 | training loss: 0.0003220921498723328\n",
      "epoch: 9 | 58592 / 114272 | training loss: 0.00022519048070535064\n",
      "epoch: 9 | 58624 / 114272 | training loss: 0.00041556713404133916\n",
      "epoch: 9 | 58656 / 114272 | training loss: 0.0004726832848973572\n",
      "epoch: 9 | 58688 / 114272 | training loss: 0.00023730502289254218\n",
      "epoch: 9 | 58720 / 114272 | training loss: 0.00042759280768223107\n",
      "epoch: 9 | 58752 / 114272 | training loss: 0.00021868495969101787\n",
      "epoch: 9 | 58784 / 114272 | training loss: 0.0002500923874322325\n",
      "epoch: 9 | 58816 / 114272 | training loss: 0.0002880936081055552\n",
      "epoch: 9 | 58848 / 114272 | training loss: 0.0003225778054911643\n",
      "epoch: 9 | 58880 / 114272 | training loss: 0.0019627746660262346\n",
      "epoch: 9 | 58912 / 114272 | training loss: 0.00020855723414570093\n",
      "epoch: 9 | 58944 / 114272 | training loss: 0.0007993971230462193\n",
      "epoch: 9 | 58976 / 114272 | training loss: 0.00035595320514403284\n",
      "epoch: 9 | 59008 / 114272 | training loss: 0.01989274099469185\n",
      "epoch: 9 | 59040 / 114272 | training loss: 0.00022933563741389662\n",
      "epoch: 9 | 59072 / 114272 | training loss: 0.0005449063028208911\n",
      "epoch: 9 | 59104 / 114272 | training loss: 0.001975191989913583\n",
      "epoch: 9 | 59136 / 114272 | training loss: 0.0003105184296146035\n",
      "epoch: 9 | 59168 / 114272 | training loss: 0.00039007244049571455\n",
      "epoch: 9 | 59200 / 114272 | training loss: 0.00019810548110399395\n",
      "epoch: 9 | 59232 / 114272 | training loss: 0.0002999876451212913\n",
      "epoch: 9 | 59264 / 114272 | training loss: 0.00024049865896813571\n",
      "epoch: 9 | 59296 / 114272 | training loss: 0.0004622721753548831\n",
      "epoch: 9 | 59328 / 114272 | training loss: 0.0003742392873391509\n",
      "epoch: 9 | 59360 / 114272 | training loss: 0.00021244426898192614\n",
      "epoch: 9 | 59392 / 114272 | training loss: 0.0002958048426080495\n",
      "epoch: 9 | 59424 / 114272 | training loss: 0.00026105449069291353\n",
      "epoch: 9 | 59456 / 114272 | training loss: 0.0003483285545371473\n",
      "epoch: 9 | 59488 / 114272 | training loss: 0.00021481778821907938\n",
      "epoch: 9 | 59520 / 114272 | training loss: 0.0339161679148674\n",
      "epoch: 9 | 59552 / 114272 | training loss: 0.00018122169421985745\n",
      "epoch: 9 | 59584 / 114272 | training loss: 0.03674943745136261\n",
      "epoch: 9 | 59616 / 114272 | training loss: 0.01946309581398964\n",
      "epoch: 9 | 59648 / 114272 | training loss: 0.00042773448512889445\n",
      "epoch: 9 | 59680 / 114272 | training loss: 0.0013448813697323203\n",
      "epoch: 9 | 59712 / 114272 | training loss: 0.0002370231959503144\n",
      "epoch: 9 | 59744 / 114272 | training loss: 0.0005428554723039269\n",
      "epoch: 9 | 59776 / 114272 | training loss: 0.0003925685887224972\n",
      "epoch: 9 | 59808 / 114272 | training loss: 0.0011274170828983188\n",
      "epoch: 9 | 59840 / 114272 | training loss: 0.0002749397826846689\n",
      "epoch: 9 | 59872 / 114272 | training loss: 0.00019413790141697973\n",
      "epoch: 9 | 59904 / 114272 | training loss: 0.0002718951436690986\n",
      "epoch: 9 | 59936 / 114272 | training loss: 0.00032818823819980025\n",
      "epoch: 9 | 59968 / 114272 | training loss: 0.00045087849139235914\n",
      "epoch: 9 | 60000 / 114272 | training loss: 0.00022351613733917475\n",
      "epoch: 9 | 60032 / 114272 | training loss: 0.00023653327662032098\n",
      "epoch: 9 | 60064 / 114272 | training loss: 0.00026576261734589934\n",
      "epoch: 9 | 60096 / 114272 | training loss: 0.00038708289503119886\n",
      "epoch: 9 | 60128 / 114272 | training loss: 0.1778111308813095\n",
      "epoch: 9 | 60160 / 114272 | training loss: 0.00022219776292331517\n",
      "epoch: 9 | 60192 / 114272 | training loss: 0.00016173145559150726\n",
      "epoch: 9 | 60224 / 114272 | training loss: 0.001338804024271667\n",
      "epoch: 9 | 60256 / 114272 | training loss: 0.00035583486896939576\n",
      "epoch: 9 | 60288 / 114272 | training loss: 0.00033786860876716673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 60320 / 114272 | training loss: 0.0003884682082571089\n",
      "epoch: 9 | 60352 / 114272 | training loss: 0.0004958600620739162\n",
      "epoch: 9 | 60384 / 114272 | training loss: 0.00030403584241867065\n",
      "epoch: 9 | 60416 / 114272 | training loss: 0.00023309125390369445\n",
      "epoch: 9 | 60448 / 114272 | training loss: 0.00022302822617348284\n",
      "epoch: 9 | 60480 / 114272 | training loss: 0.0002844804257620126\n",
      "epoch: 9 | 60512 / 114272 | training loss: 0.0008924601715989411\n",
      "epoch: 9 | 60544 / 114272 | training loss: 0.00022157869534566998\n",
      "epoch: 9 | 60576 / 114272 | training loss: 0.00020568622858263552\n",
      "epoch: 9 | 60608 / 114272 | training loss: 0.00021227664547041059\n",
      "epoch: 9 | 60640 / 114272 | training loss: 0.0003070759994443506\n",
      "epoch: 9 | 60672 / 114272 | training loss: 0.00019486621022224426\n",
      "epoch: 9 | 60704 / 114272 | training loss: 0.0002287612878717482\n",
      "epoch: 9 | 60736 / 114272 | training loss: 0.0002325045788893476\n",
      "epoch: 9 | 60768 / 114272 | training loss: 0.0022771458607167006\n",
      "epoch: 9 | 60800 / 114272 | training loss: 0.0001956239138962701\n",
      "epoch: 9 | 60832 / 114272 | training loss: 0.00022222253028303385\n",
      "epoch: 9 | 60864 / 114272 | training loss: 0.00043928236118517816\n",
      "epoch: 9 | 60896 / 114272 | training loss: 0.0006221684161573648\n",
      "epoch: 9 | 60928 / 114272 | training loss: 0.0002819451328832656\n",
      "epoch: 9 | 60960 / 114272 | training loss: 0.00045088486513122916\n",
      "epoch: 9 | 60992 / 114272 | training loss: 0.0002655907883308828\n",
      "epoch: 9 | 61024 / 114272 | training loss: 0.00025077987811528146\n",
      "epoch: 9 | 61056 / 114272 | training loss: 0.00017826908151619136\n",
      "epoch: 9 | 61088 / 114272 | training loss: 0.03608369454741478\n",
      "epoch: 9 | 61120 / 114272 | training loss: 0.00033523314050398767\n",
      "epoch: 9 | 61152 / 114272 | training loss: 0.07940901815891266\n",
      "epoch: 9 | 61184 / 114272 | training loss: 0.00025619124062359333\n",
      "epoch: 9 | 61216 / 114272 | training loss: 0.00027059964486397803\n",
      "epoch: 9 | 61248 / 114272 | training loss: 0.00019352487288415432\n",
      "epoch: 9 | 61280 / 114272 | training loss: 0.000322804698953405\n",
      "epoch: 9 | 61312 / 114272 | training loss: 0.0001533921022200957\n",
      "epoch: 9 | 61344 / 114272 | training loss: 0.00021878861298318952\n",
      "epoch: 9 | 61376 / 114272 | training loss: 0.0004718471900559962\n",
      "epoch: 9 | 61408 / 114272 | training loss: 0.000329711998347193\n",
      "epoch: 9 | 61440 / 114272 | training loss: 0.00029715782147832215\n",
      "epoch: 9 | 61472 / 114272 | training loss: 0.0004750570806208998\n",
      "epoch: 9 | 61504 / 114272 | training loss: 0.0003687059215735644\n",
      "epoch: 9 | 61536 / 114272 | training loss: 0.0003338454116601497\n",
      "epoch: 9 | 61568 / 114272 | training loss: 0.009455077350139618\n",
      "epoch: 9 | 61600 / 114272 | training loss: 0.00035749972448684275\n",
      "epoch: 9 | 61632 / 114272 | training loss: 0.00024087309429887682\n",
      "epoch: 9 | 61664 / 114272 | training loss: 0.00018226029351353645\n",
      "epoch: 9 | 61696 / 114272 | training loss: 0.00020323775243014097\n",
      "epoch: 9 | 61728 / 114272 | training loss: 0.0002973646915052086\n",
      "epoch: 9 | 61760 / 114272 | training loss: 0.029933955520391464\n",
      "epoch: 9 | 61792 / 114272 | training loss: 0.00021750526502728462\n",
      "epoch: 9 | 61824 / 114272 | training loss: 0.00022925711527932435\n",
      "epoch: 9 | 61856 / 114272 | training loss: 0.0003077223082073033\n",
      "epoch: 9 | 61888 / 114272 | training loss: 0.0004078933852724731\n",
      "epoch: 9 | 61920 / 114272 | training loss: 0.0001995233615161851\n",
      "epoch: 9 | 61952 / 114272 | training loss: 0.00030084571335464716\n",
      "epoch: 9 | 61984 / 114272 | training loss: 0.0003640154027380049\n",
      "epoch: 9 | 62016 / 114272 | training loss: 0.00024987527285702527\n",
      "epoch: 9 | 62048 / 114272 | training loss: 0.00040982943028211594\n",
      "epoch: 9 | 62080 / 114272 | training loss: 0.00025361214648000896\n",
      "epoch: 9 | 62112 / 114272 | training loss: 0.0006152560235932469\n",
      "epoch: 9 | 62144 / 114272 | training loss: 0.000187821380677633\n",
      "epoch: 9 | 62176 / 114272 | training loss: 0.019566278904676437\n",
      "epoch: 9 | 62208 / 114272 | training loss: 0.000418025505496189\n",
      "epoch: 9 | 62240 / 114272 | training loss: 0.0001892787404358387\n",
      "epoch: 9 | 62272 / 114272 | training loss: 0.00024092871171887964\n",
      "epoch: 9 | 62304 / 114272 | training loss: 0.00020518878591246903\n",
      "epoch: 9 | 62336 / 114272 | training loss: 0.00015606865053996444\n",
      "epoch: 9 | 62368 / 114272 | training loss: 0.00037234320188872516\n",
      "epoch: 9 | 62400 / 114272 | training loss: 0.00020675671112257987\n",
      "epoch: 9 | 62432 / 114272 | training loss: 0.0003567379026208073\n",
      "epoch: 9 | 62464 / 114272 | training loss: 0.0002028329618042335\n",
      "epoch: 9 | 62496 / 114272 | training loss: 0.0004833546408917755\n",
      "epoch: 9 | 62528 / 114272 | training loss: 0.0002743470249697566\n",
      "epoch: 9 | 62560 / 114272 | training loss: 0.0009935533162206411\n",
      "epoch: 9 | 62592 / 114272 | training loss: 0.00041021310607902706\n",
      "epoch: 9 | 62624 / 114272 | training loss: 0.00031689374009147286\n",
      "epoch: 9 | 62656 / 114272 | training loss: 0.0003099438617937267\n",
      "epoch: 9 | 62688 / 114272 | training loss: 0.00019462077762000263\n",
      "epoch: 9 | 62720 / 114272 | training loss: 0.0004150308377575129\n",
      "epoch: 9 | 62752 / 114272 | training loss: 0.00018492233357392251\n",
      "epoch: 9 | 62784 / 114272 | training loss: 0.00020520962425507605\n",
      "epoch: 9 | 62816 / 114272 | training loss: 0.00048322678776457906\n",
      "epoch: 9 | 62848 / 114272 | training loss: 0.00035339492023922503\n",
      "epoch: 9 | 62880 / 114272 | training loss: 0.0003467970236670226\n",
      "epoch: 9 | 62912 / 114272 | training loss: 0.0003792964562308043\n",
      "epoch: 9 | 62944 / 114272 | training loss: 0.00036580904270522296\n",
      "epoch: 9 | 62976 / 114272 | training loss: 0.00018836723756976426\n",
      "epoch: 9 | 63008 / 114272 | training loss: 0.0002794507017824799\n",
      "epoch: 9 | 63040 / 114272 | training loss: 0.00029225368052721024\n",
      "epoch: 9 | 63072 / 114272 | training loss: 0.0001646791642997414\n",
      "epoch: 9 | 63104 / 114272 | training loss: 0.00031379671418108046\n",
      "epoch: 9 | 63136 / 114272 | training loss: 0.0014135085511952639\n",
      "epoch: 9 | 63168 / 114272 | training loss: 0.0002757747133728117\n",
      "epoch: 9 | 63200 / 114272 | training loss: 0.0007792932447046041\n",
      "epoch: 9 | 63232 / 114272 | training loss: 0.00020704844791907817\n",
      "epoch: 9 | 63264 / 114272 | training loss: 0.00039102372829802334\n",
      "epoch: 9 | 63296 / 114272 | training loss: 0.000135887908982113\n",
      "epoch: 9 | 63328 / 114272 | training loss: 0.0002878503000829369\n",
      "epoch: 9 | 63360 / 114272 | training loss: 0.00021409282635431737\n",
      "epoch: 9 | 63392 / 114272 | training loss: 0.0003678702632896602\n",
      "epoch: 9 | 63424 / 114272 | training loss: 0.15748214721679688\n",
      "epoch: 9 | 63456 / 114272 | training loss: 0.0005492689670063555\n",
      "epoch: 9 | 63488 / 114272 | training loss: 0.0003189757408108562\n",
      "epoch: 9 | 63520 / 114272 | training loss: 0.00032124517019838095\n",
      "epoch: 9 | 63552 / 114272 | training loss: 0.001210625283420086\n",
      "epoch: 9 | 63584 / 114272 | training loss: 0.0003912770771421492\n",
      "epoch: 9 | 63616 / 114272 | training loss: 0.00027863893774338067\n",
      "epoch: 9 | 63648 / 114272 | training loss: 0.00030982212047092617\n",
      "epoch: 9 | 63680 / 114272 | training loss: 0.00023801159113645554\n",
      "epoch: 9 | 63712 / 114272 | training loss: 0.0002928340109065175\n",
      "epoch: 9 | 63744 / 114272 | training loss: 0.00014677112631034106\n",
      "epoch: 9 | 63776 / 114272 | training loss: 0.00024065918114501983\n",
      "epoch: 9 | 63808 / 114272 | training loss: 0.0004931708099320531\n",
      "epoch: 9 | 63840 / 114272 | training loss: 0.00018622032075654715\n",
      "epoch: 9 | 63872 / 114272 | training loss: 0.00026269385125488043\n",
      "epoch: 9 | 63904 / 114272 | training loss: 0.00024394362117163837\n",
      "epoch: 9 | 63936 / 114272 | training loss: 0.0003038330760318786\n",
      "epoch: 9 | 63968 / 114272 | training loss: 0.00044663032167591155\n",
      "epoch: 9 | 64000 / 114272 | training loss: 0.00023599551059305668\n",
      "epoch: 9 | 64032 / 114272 | training loss: 0.00036669039400294423\n",
      "epoch: 9 | 64064 / 114272 | training loss: 0.00027728007989935577\n",
      "epoch: 9 | 64096 / 114272 | training loss: 0.010701301507651806\n",
      "epoch: 9 | 64128 / 114272 | training loss: 0.0005322832730598748\n",
      "epoch: 9 | 64160 / 114272 | training loss: 0.00017758730973582715\n",
      "epoch: 9 | 64192 / 114272 | training loss: 0.000244496448431164\n",
      "epoch: 9 | 64224 / 114272 | training loss: 0.00033917505061253905\n",
      "epoch: 9 | 64256 / 114272 | training loss: 0.00023135371156968176\n",
      "epoch: 9 | 64288 / 114272 | training loss: 0.0008846310083754361\n",
      "epoch: 9 | 64320 / 114272 | training loss: 0.000314250064548105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 64352 / 114272 | training loss: 0.00024889077758416533\n",
      "epoch: 9 | 64384 / 114272 | training loss: 0.0003926456847693771\n",
      "epoch: 9 | 64416 / 114272 | training loss: 0.00017415474576409906\n",
      "epoch: 9 | 64448 / 114272 | training loss: 0.00014653633115813136\n",
      "epoch: 9 | 64480 / 114272 | training loss: 0.0002693275746423751\n",
      "epoch: 9 | 64512 / 114272 | training loss: 0.00027627393137663603\n",
      "epoch: 9 | 64544 / 114272 | training loss: 0.00022023721248842776\n",
      "epoch: 9 | 64576 / 114272 | training loss: 0.0034841138403862715\n",
      "epoch: 9 | 64608 / 114272 | training loss: 0.00018549016385804862\n",
      "epoch: 9 | 64640 / 114272 | training loss: 0.00030461762798950076\n",
      "epoch: 9 | 64672 / 114272 | training loss: 0.0006003315793350339\n",
      "epoch: 9 | 64704 / 114272 | training loss: 0.00025596190243959427\n",
      "epoch: 9 | 64736 / 114272 | training loss: 0.00012903443712275475\n",
      "epoch: 9 | 64768 / 114272 | training loss: 0.00024723741807974875\n",
      "epoch: 9 | 64800 / 114272 | training loss: 0.00023734090791549534\n",
      "epoch: 9 | 64832 / 114272 | training loss: 0.00022467083181254566\n",
      "epoch: 9 | 64864 / 114272 | training loss: 0.00021810614271089435\n",
      "epoch: 9 | 64896 / 114272 | training loss: 0.0010955027537420392\n",
      "epoch: 9 | 64928 / 114272 | training loss: 0.0003463173343334347\n",
      "epoch: 9 | 64960 / 114272 | training loss: 0.00035799280158244073\n",
      "epoch: 9 | 64992 / 114272 | training loss: 0.00024358935479540378\n",
      "epoch: 9 | 65024 / 114272 | training loss: 0.0002922592975664884\n",
      "epoch: 9 | 65056 / 114272 | training loss: 0.013966375961899757\n",
      "epoch: 9 | 65088 / 114272 | training loss: 0.00018011967767961323\n",
      "epoch: 9 | 65120 / 114272 | training loss: 0.00108366331551224\n",
      "epoch: 9 | 65152 / 114272 | training loss: 0.0001728618226479739\n",
      "epoch: 9 | 65184 / 114272 | training loss: 0.0013322988525032997\n",
      "epoch: 9 | 65216 / 114272 | training loss: 0.00030067790066823363\n",
      "epoch: 9 | 65248 / 114272 | training loss: 0.0003397352702450007\n",
      "epoch: 9 | 65280 / 114272 | training loss: 0.00046626373659819365\n",
      "epoch: 9 | 65312 / 114272 | training loss: 0.0002569521893747151\n",
      "epoch: 9 | 65344 / 114272 | training loss: 0.0006211978616192937\n",
      "epoch: 9 | 65376 / 114272 | training loss: 0.00021504296455532312\n",
      "epoch: 9 | 65408 / 114272 | training loss: 0.00025630940217524767\n",
      "epoch: 9 | 65440 / 114272 | training loss: 0.0002825119299814105\n",
      "epoch: 9 | 65472 / 114272 | training loss: 0.0033150389790534973\n",
      "epoch: 9 | 65504 / 114272 | training loss: 0.0003326855367049575\n",
      "epoch: 9 | 65536 / 114272 | training loss: 0.00022377310961019248\n",
      "epoch: 9 | 65568 / 114272 | training loss: 0.004373200237751007\n",
      "epoch: 9 | 65600 / 114272 | training loss: 0.0002079407131532207\n",
      "epoch: 9 | 65632 / 114272 | training loss: 0.28767192363739014\n",
      "epoch: 9 | 65664 / 114272 | training loss: 0.00011508344323374331\n",
      "epoch: 9 | 65696 / 114272 | training loss: 0.0002186173078371212\n",
      "epoch: 9 | 65728 / 114272 | training loss: 0.00024381691764574498\n",
      "epoch: 9 | 65760 / 114272 | training loss: 0.00038071683957241476\n",
      "epoch: 9 | 65792 / 114272 | training loss: 0.0003471560194157064\n",
      "epoch: 9 | 65824 / 114272 | training loss: 0.0038416713941842318\n",
      "epoch: 9 | 65856 / 114272 | training loss: 0.0003204352979082614\n",
      "epoch: 9 | 65888 / 114272 | training loss: 0.0002768394479062408\n",
      "epoch: 9 | 65920 / 114272 | training loss: 0.00038561163819395006\n",
      "epoch: 9 | 65952 / 114272 | training loss: 0.000292267621261999\n",
      "epoch: 9 | 65984 / 114272 | training loss: 0.00029403017833828926\n",
      "epoch: 9 | 66016 / 114272 | training loss: 0.00032918626675382257\n",
      "epoch: 9 | 66048 / 114272 | training loss: 0.0002365967957302928\n",
      "epoch: 9 | 66080 / 114272 | training loss: 0.0003777566598728299\n",
      "epoch: 9 | 66112 / 114272 | training loss: 0.000323845335515216\n",
      "epoch: 9 | 66144 / 114272 | training loss: 0.0003973987477365881\n",
      "epoch: 9 | 66176 / 114272 | training loss: 0.0003039356670342386\n",
      "epoch: 9 | 66208 / 114272 | training loss: 0.00017756895977072418\n",
      "epoch: 9 | 66240 / 114272 | training loss: 0.0002103212318615988\n",
      "epoch: 9 | 66272 / 114272 | training loss: 0.00028460653265938163\n",
      "epoch: 9 | 66304 / 114272 | training loss: 0.0005438937223516405\n",
      "epoch: 9 | 66336 / 114272 | training loss: 0.00033389145391993225\n",
      "epoch: 9 | 66368 / 114272 | training loss: 0.0003243910614401102\n",
      "epoch: 9 | 66400 / 114272 | training loss: 0.00020092035993002355\n",
      "epoch: 9 | 66432 / 114272 | training loss: 0.09278585761785507\n",
      "epoch: 9 | 66464 / 114272 | training loss: 0.006565723102539778\n",
      "epoch: 9 | 66496 / 114272 | training loss: 0.00015936863201204687\n",
      "epoch: 9 | 66528 / 114272 | training loss: 0.00017256969294976443\n",
      "epoch: 9 | 66560 / 114272 | training loss: 0.0003213807940483093\n",
      "epoch: 9 | 66592 / 114272 | training loss: 0.0005913152708671987\n",
      "epoch: 9 | 66624 / 114272 | training loss: 0.00025168791762553155\n",
      "epoch: 9 | 66656 / 114272 | training loss: 0.00014216209820006043\n",
      "epoch: 9 | 66688 / 114272 | training loss: 0.00014231109526008368\n",
      "epoch: 9 | 66720 / 114272 | training loss: 0.0003705657727550715\n",
      "epoch: 9 | 66752 / 114272 | training loss: 0.00013932313595432788\n",
      "epoch: 9 | 66784 / 114272 | training loss: 0.0004517089982982725\n",
      "epoch: 9 | 66816 / 114272 | training loss: 0.0002296024322276935\n",
      "epoch: 9 | 66848 / 114272 | training loss: 0.11546531319618225\n",
      "epoch: 9 | 66880 / 114272 | training loss: 0.00018782679399009794\n",
      "epoch: 9 | 66912 / 114272 | training loss: 0.00018847716273739934\n",
      "epoch: 9 | 66944 / 114272 | training loss: 0.0002626502828206867\n",
      "epoch: 9 | 66976 / 114272 | training loss: 0.0002411574823781848\n",
      "epoch: 9 | 67008 / 114272 | training loss: 0.00010554261098150164\n",
      "epoch: 9 | 67040 / 114272 | training loss: 0.0003765087458305061\n",
      "epoch: 9 | 67072 / 114272 | training loss: 0.00013087328989058733\n",
      "epoch: 9 | 67104 / 114272 | training loss: 0.00034835992846637964\n",
      "epoch: 9 | 67136 / 114272 | training loss: 0.0002941593702416867\n",
      "epoch: 9 | 67168 / 114272 | training loss: 0.00046823130105622113\n",
      "epoch: 9 | 67200 / 114272 | training loss: 0.0002436683716950938\n",
      "epoch: 9 | 67232 / 114272 | training loss: 0.0002563202288001776\n",
      "epoch: 9 | 67264 / 114272 | training loss: 0.00021166086662560701\n",
      "epoch: 9 | 67296 / 114272 | training loss: 0.00036019959952682257\n",
      "epoch: 9 | 67328 / 114272 | training loss: 0.00018092153186444193\n",
      "epoch: 9 | 67360 / 114272 | training loss: 0.0005029654712416232\n",
      "epoch: 9 | 67392 / 114272 | training loss: 0.00020560635311994702\n",
      "epoch: 9 | 67424 / 114272 | training loss: 0.00020770759147126228\n",
      "epoch: 9 | 67456 / 114272 | training loss: 0.00037505163345485926\n",
      "epoch: 9 | 67488 / 114272 | training loss: 9.163557842839509e-05\n",
      "epoch: 9 | 67520 / 114272 | training loss: 0.001687922514975071\n",
      "epoch: 9 | 67552 / 114272 | training loss: 0.0003490585077088326\n",
      "epoch: 9 | 67584 / 114272 | training loss: 0.00035057406057603657\n",
      "epoch: 9 | 67616 / 114272 | training loss: 0.00024696640321053565\n",
      "epoch: 9 | 67648 / 114272 | training loss: 0.00019187497673556209\n",
      "epoch: 9 | 67680 / 114272 | training loss: 0.00030760877416469157\n",
      "epoch: 9 | 67712 / 114272 | training loss: 0.000669388915412128\n",
      "epoch: 9 | 67744 / 114272 | training loss: 0.00012843887088820338\n",
      "epoch: 9 | 67776 / 114272 | training loss: 0.00023595608945470303\n",
      "epoch: 9 | 67808 / 114272 | training loss: 0.00017257191939279437\n",
      "epoch: 9 | 67840 / 114272 | training loss: 0.00022342563897836953\n",
      "epoch: 9 | 67872 / 114272 | training loss: 0.0004072912270203233\n",
      "epoch: 9 | 67904 / 114272 | training loss: 0.00026793303550221026\n",
      "epoch: 9 | 67936 / 114272 | training loss: 0.00015374539361800998\n",
      "epoch: 9 | 67968 / 114272 | training loss: 0.0002758513146545738\n",
      "epoch: 9 | 68000 / 114272 | training loss: 0.005201609805226326\n",
      "epoch: 9 | 68032 / 114272 | training loss: 0.0001620433758944273\n",
      "epoch: 9 | 68064 / 114272 | training loss: 0.0005133191007189453\n",
      "epoch: 9 | 68096 / 114272 | training loss: 0.0003810978669207543\n",
      "epoch: 9 | 68128 / 114272 | training loss: 0.0002980786084663123\n",
      "epoch: 9 | 68160 / 114272 | training loss: 0.0004314663528930396\n",
      "epoch: 9 | 68192 / 114272 | training loss: 0.0005170022486709058\n",
      "epoch: 9 | 68224 / 114272 | training loss: 0.0012696136254817247\n",
      "epoch: 9 | 68256 / 114272 | training loss: 0.00034299478284083307\n",
      "epoch: 9 | 68288 / 114272 | training loss: 0.00021466852922458202\n",
      "epoch: 9 | 68320 / 114272 | training loss: 0.00030002035782672465\n",
      "epoch: 9 | 68352 / 114272 | training loss: 0.00016043518553487957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 68384 / 114272 | training loss: 0.0006483230390585959\n",
      "epoch: 9 | 68416 / 114272 | training loss: 0.00024292225134558976\n",
      "epoch: 9 | 68448 / 114272 | training loss: 0.00023727575899101794\n",
      "epoch: 9 | 68480 / 114272 | training loss: 0.00032460514921694994\n",
      "epoch: 9 | 68512 / 114272 | training loss: 0.0001288008934352547\n",
      "epoch: 9 | 68544 / 114272 | training loss: 0.0002646258508320898\n",
      "epoch: 9 | 68576 / 114272 | training loss: 0.00013492890866473317\n",
      "epoch: 9 | 68608 / 114272 | training loss: 0.02410103566944599\n",
      "epoch: 9 | 68640 / 114272 | training loss: 0.00012547402002383024\n",
      "epoch: 9 | 68672 / 114272 | training loss: 0.0003385788877494633\n",
      "epoch: 9 | 68704 / 114272 | training loss: 0.00024003145517781377\n",
      "epoch: 9 | 68736 / 114272 | training loss: 0.00030917616095393896\n",
      "epoch: 9 | 68768 / 114272 | training loss: 0.00023050204617902637\n",
      "epoch: 9 | 68800 / 114272 | training loss: 0.00015824715956114233\n",
      "epoch: 9 | 68832 / 114272 | training loss: 0.00019037947640754282\n",
      "epoch: 9 | 68864 / 114272 | training loss: 0.0001001647106022574\n",
      "epoch: 9 | 68896 / 114272 | training loss: 0.00019438286835793406\n",
      "epoch: 9 | 68928 / 114272 | training loss: 0.00023113752831704915\n",
      "epoch: 9 | 68960 / 114272 | training loss: 0.0004956271150149405\n",
      "epoch: 9 | 68992 / 114272 | training loss: 0.00019861868349835277\n",
      "epoch: 9 | 69024 / 114272 | training loss: 0.00015102358884178102\n",
      "epoch: 9 | 69056 / 114272 | training loss: 0.10644885897636414\n",
      "epoch: 9 | 69088 / 114272 | training loss: 0.00023228931240737438\n",
      "epoch: 9 | 69120 / 114272 | training loss: 0.00038775725988671184\n",
      "epoch: 9 | 69152 / 114272 | training loss: 0.029328016564249992\n",
      "epoch: 9 | 69184 / 114272 | training loss: 0.0007875090814195573\n",
      "epoch: 9 | 69216 / 114272 | training loss: 0.0002670452231541276\n",
      "epoch: 9 | 69248 / 114272 | training loss: 0.0002348526759305969\n",
      "epoch: 9 | 69280 / 114272 | training loss: 0.00012935402628500015\n",
      "epoch: 9 | 69312 / 114272 | training loss: 0.00017317449965048581\n",
      "epoch: 9 | 69344 / 114272 | training loss: 0.0002451969776302576\n",
      "epoch: 9 | 69376 / 114272 | training loss: 0.00034254335332661867\n",
      "epoch: 9 | 69408 / 114272 | training loss: 0.00021889644267503172\n",
      "epoch: 9 | 69440 / 114272 | training loss: 0.00031532306456938386\n",
      "epoch: 9 | 69472 / 114272 | training loss: 0.000282207562122494\n",
      "epoch: 9 | 69504 / 114272 | training loss: 0.00031573159503750503\n",
      "epoch: 9 | 69536 / 114272 | training loss: 0.0003448119095992297\n",
      "epoch: 9 | 69568 / 114272 | training loss: 0.0002908977912738919\n",
      "epoch: 9 | 69600 / 114272 | training loss: 0.00015158405585680157\n",
      "epoch: 9 | 69632 / 114272 | training loss: 0.0002119987620972097\n",
      "epoch: 9 | 69664 / 114272 | training loss: 0.00011152355727972463\n",
      "epoch: 9 | 69696 / 114272 | training loss: 0.00031200837111100554\n",
      "epoch: 9 | 69728 / 114272 | training loss: 0.00027113809483125806\n",
      "epoch: 9 | 69760 / 114272 | training loss: 0.00024899779236875474\n",
      "epoch: 9 | 69792 / 114272 | training loss: 0.0004202695854473859\n",
      "epoch: 9 | 69824 / 114272 | training loss: 0.0004046818648930639\n",
      "epoch: 9 | 69856 / 114272 | training loss: 0.00016857704031281173\n",
      "epoch: 9 | 69888 / 114272 | training loss: 0.00023996505478862673\n",
      "epoch: 9 | 69920 / 114272 | training loss: 0.0002462162810843438\n",
      "epoch: 9 | 69952 / 114272 | training loss: 0.00036168715450912714\n",
      "epoch: 9 | 69984 / 114272 | training loss: 0.0002626560744829476\n",
      "epoch: 9 | 70016 / 114272 | training loss: 0.00025207712315022945\n",
      "epoch: 9 | 70048 / 114272 | training loss: 0.0005279039032757282\n",
      "epoch: 9 | 70080 / 114272 | training loss: 0.00019000317843165249\n",
      "epoch: 9 | 70112 / 114272 | training loss: 0.0003662579401861876\n",
      "epoch: 9 | 70144 / 114272 | training loss: 0.00013657778617925942\n",
      "epoch: 9 | 70176 / 114272 | training loss: 0.00019323499873280525\n",
      "epoch: 9 | 70208 / 114272 | training loss: 0.0002655034768395126\n",
      "epoch: 9 | 70240 / 114272 | training loss: 0.0003028271603398025\n",
      "epoch: 9 | 70272 / 114272 | training loss: 0.00024071562802419066\n",
      "epoch: 9 | 70304 / 114272 | training loss: 0.00018472495139576495\n",
      "epoch: 9 | 70336 / 114272 | training loss: 0.0002673835842870176\n",
      "epoch: 9 | 70368 / 114272 | training loss: 0.00037161121144890785\n",
      "epoch: 9 | 70400 / 114272 | training loss: 0.00026864276151172817\n",
      "epoch: 9 | 70432 / 114272 | training loss: 0.0003396405081730336\n",
      "epoch: 9 | 70464 / 114272 | training loss: 0.00015714316396042705\n",
      "epoch: 9 | 70496 / 114272 | training loss: 0.00022437534062191844\n",
      "epoch: 9 | 70528 / 114272 | training loss: 0.00018692825688049197\n",
      "epoch: 9 | 70560 / 114272 | training loss: 0.00021713125170208514\n",
      "epoch: 9 | 70592 / 114272 | training loss: 0.00020279185264371336\n",
      "epoch: 9 | 70624 / 114272 | training loss: 0.00022247411834541708\n",
      "epoch: 9 | 70656 / 114272 | training loss: 0.00022119912318885326\n",
      "epoch: 9 | 70688 / 114272 | training loss: 0.00029687126516364515\n",
      "epoch: 9 | 70720 / 114272 | training loss: 0.00023538849200122058\n",
      "epoch: 9 | 70752 / 114272 | training loss: 0.0002208072692155838\n",
      "epoch: 9 | 70784 / 114272 | training loss: 0.0003687294665724039\n",
      "epoch: 9 | 70816 / 114272 | training loss: 0.00035389611730352044\n",
      "epoch: 9 | 70848 / 114272 | training loss: 0.1253243386745453\n",
      "epoch: 9 | 70880 / 114272 | training loss: 0.00021352048497647047\n",
      "epoch: 9 | 70912 / 114272 | training loss: 0.00014013651525601745\n",
      "epoch: 9 | 70944 / 114272 | training loss: 0.00030499836429953575\n",
      "epoch: 9 | 70976 / 114272 | training loss: 0.00030603105551563203\n",
      "epoch: 9 | 71008 / 114272 | training loss: 0.00017430605657864362\n",
      "epoch: 9 | 71040 / 114272 | training loss: 0.0002780842478387058\n",
      "epoch: 9 | 71072 / 114272 | training loss: 0.0002370119036640972\n",
      "epoch: 9 | 71104 / 114272 | training loss: 0.00037019545561634004\n",
      "epoch: 9 | 71136 / 114272 | training loss: 0.00038810799014754593\n",
      "epoch: 9 | 71168 / 114272 | training loss: 0.10063585638999939\n",
      "epoch: 9 | 71200 / 114272 | training loss: 0.00016315755783580244\n",
      "epoch: 9 | 71232 / 114272 | training loss: 0.00023077534569893032\n",
      "epoch: 9 | 71264 / 114272 | training loss: 0.00028305654996074736\n",
      "epoch: 9 | 71296 / 114272 | training loss: 0.0002821916132234037\n",
      "epoch: 9 | 71328 / 114272 | training loss: 0.00016750453505665064\n",
      "epoch: 9 | 71360 / 114272 | training loss: 0.0002260346955154091\n",
      "epoch: 9 | 71392 / 114272 | training loss: 0.1344369649887085\n",
      "epoch: 9 | 71424 / 114272 | training loss: 0.00031794566893950105\n",
      "epoch: 9 | 71456 / 114272 | training loss: 0.0002390440204180777\n",
      "epoch: 9 | 71488 / 114272 | training loss: 0.0031357170082628727\n",
      "epoch: 9 | 71520 / 114272 | training loss: 0.00017728519742377102\n",
      "epoch: 9 | 71552 / 114272 | training loss: 0.00022324197925627232\n",
      "epoch: 9 | 71584 / 114272 | training loss: 0.0002592639357317239\n",
      "epoch: 9 | 71616 / 114272 | training loss: 0.00042429001769050956\n",
      "epoch: 9 | 71648 / 114272 | training loss: 0.00021489555365405977\n",
      "epoch: 9 | 71680 / 114272 | training loss: 0.032052647322416306\n",
      "epoch: 9 | 71712 / 114272 | training loss: 0.0003410599019844085\n",
      "epoch: 9 | 71744 / 114272 | training loss: 0.000407656654715538\n",
      "epoch: 9 | 71776 / 114272 | training loss: 0.0002562107110861689\n",
      "epoch: 9 | 71808 / 114272 | training loss: 0.00019886333029717207\n",
      "epoch: 9 | 71840 / 114272 | training loss: 0.0002481836127117276\n",
      "epoch: 9 | 71872 / 114272 | training loss: 0.00019942462677136064\n",
      "epoch: 9 | 71904 / 114272 | training loss: 0.0003000803990289569\n",
      "epoch: 9 | 71936 / 114272 | training loss: 0.0006724729319103062\n",
      "epoch: 9 | 71968 / 114272 | training loss: 0.00020762535859830678\n",
      "epoch: 9 | 72000 / 114272 | training loss: 0.0008470744942314923\n",
      "epoch: 9 | 72032 / 114272 | training loss: 0.0002040928666247055\n",
      "epoch: 9 | 72064 / 114272 | training loss: 0.00021455992828123271\n",
      "epoch: 9 | 72096 / 114272 | training loss: 0.00022842838370706886\n",
      "epoch: 9 | 72128 / 114272 | training loss: 0.0002636395220179111\n",
      "epoch: 9 | 72160 / 114272 | training loss: 0.0003090155078098178\n",
      "epoch: 9 | 72192 / 114272 | training loss: 0.00019893328135367483\n",
      "epoch: 9 | 72224 / 114272 | training loss: 0.00016644461720716208\n",
      "epoch: 9 | 72256 / 114272 | training loss: 0.00018245035607833415\n",
      "epoch: 9 | 72288 / 114272 | training loss: 0.000178893722477369\n",
      "epoch: 9 | 72320 / 114272 | training loss: 0.0002239750901935622\n",
      "epoch: 9 | 72352 / 114272 | training loss: 0.0003317318914923817\n",
      "epoch: 9 | 72384 / 114272 | training loss: 0.0002712463610805571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 72416 / 114272 | training loss: 0.0002013194462051615\n",
      "epoch: 9 | 72448 / 114272 | training loss: 0.0004895774181932211\n",
      "epoch: 9 | 72480 / 114272 | training loss: 0.0010698173427954316\n",
      "epoch: 9 | 72512 / 114272 | training loss: 0.00024317580391652882\n",
      "epoch: 9 | 72544 / 114272 | training loss: 0.0002010312455240637\n",
      "epoch: 9 | 72576 / 114272 | training loss: 0.00037784871528856456\n",
      "epoch: 9 | 72608 / 114272 | training loss: 0.00020510352624114603\n",
      "epoch: 9 | 72640 / 114272 | training loss: 0.00024808006128296256\n",
      "epoch: 9 | 72672 / 114272 | training loss: 0.0002295427257195115\n",
      "epoch: 9 | 72704 / 114272 | training loss: 0.00018936947162728757\n",
      "epoch: 9 | 72736 / 114272 | training loss: 0.00029278063448145986\n",
      "epoch: 9 | 72768 / 114272 | training loss: 0.03228460252285004\n",
      "epoch: 9 | 72800 / 114272 | training loss: 0.0001953486498678103\n",
      "epoch: 9 | 72832 / 114272 | training loss: 0.00014073126658331603\n",
      "epoch: 9 | 72864 / 114272 | training loss: 0.00017336956807412207\n",
      "epoch: 9 | 72896 / 114272 | training loss: 0.000243868023972027\n",
      "epoch: 9 | 72928 / 114272 | training loss: 0.00022885894577484578\n",
      "epoch: 9 | 72960 / 114272 | training loss: 0.00013576031778939068\n",
      "epoch: 9 | 72992 / 114272 | training loss: 0.026135561987757683\n",
      "epoch: 9 | 73024 / 114272 | training loss: 0.000749301805626601\n",
      "epoch: 9 | 73056 / 114272 | training loss: 0.0006058902363292873\n",
      "epoch: 9 | 73088 / 114272 | training loss: 0.0003843551385216415\n",
      "epoch: 9 | 73120 / 114272 | training loss: 0.0002983251470141113\n",
      "epoch: 9 | 73152 / 114272 | training loss: 0.00018459861166775227\n",
      "epoch: 9 | 73184 / 114272 | training loss: 0.00023251773382071406\n",
      "epoch: 9 | 73216 / 114272 | training loss: 0.0001553954352857545\n",
      "epoch: 9 | 73248 / 114272 | training loss: 0.00210381206125021\n",
      "epoch: 9 | 73280 / 114272 | training loss: 0.0002164805046049878\n",
      "epoch: 9 | 73312 / 114272 | training loss: 0.0002000484528252855\n",
      "epoch: 9 | 73344 / 114272 | training loss: 0.000303123815683648\n",
      "epoch: 9 | 73376 / 114272 | training loss: 0.00019575054466258734\n",
      "epoch: 9 | 73408 / 114272 | training loss: 0.0009904364123940468\n",
      "epoch: 9 | 73440 / 114272 | training loss: 0.00013808002404402941\n",
      "epoch: 9 | 73472 / 114272 | training loss: 0.00012897636042907834\n",
      "epoch: 9 | 73504 / 114272 | training loss: 0.19569921493530273\n",
      "epoch: 9 | 73536 / 114272 | training loss: 0.00028148837736807764\n",
      "epoch: 9 | 73568 / 114272 | training loss: 0.0002963441947940737\n",
      "epoch: 9 | 73600 / 114272 | training loss: 0.00026522629195824265\n",
      "epoch: 9 | 73632 / 114272 | training loss: 0.0002565918839536607\n",
      "epoch: 9 | 73664 / 114272 | training loss: 0.00018331664614379406\n",
      "epoch: 9 | 73696 / 114272 | training loss: 0.0002975599782075733\n",
      "epoch: 9 | 73728 / 114272 | training loss: 0.00028390472289174795\n",
      "epoch: 9 | 73760 / 114272 | training loss: 0.0003521436301525682\n",
      "epoch: 9 | 73792 / 114272 | training loss: 0.00026943805278278887\n",
      "epoch: 9 | 73824 / 114272 | training loss: 0.1283658742904663\n",
      "epoch: 9 | 73856 / 114272 | training loss: 0.00029201889992691576\n",
      "epoch: 9 | 73888 / 114272 | training loss: 0.0002619256265461445\n",
      "epoch: 9 | 73920 / 114272 | training loss: 0.00016916971071623266\n",
      "epoch: 9 | 73952 / 114272 | training loss: 0.018151648342609406\n",
      "epoch: 9 | 73984 / 114272 | training loss: 0.0003375582746230066\n",
      "epoch: 9 | 74016 / 114272 | training loss: 0.25961098074913025\n",
      "epoch: 9 | 74048 / 114272 | training loss: 0.0002887480368372053\n",
      "epoch: 9 | 74080 / 114272 | training loss: 0.00015644248924218118\n",
      "epoch: 9 | 74112 / 114272 | training loss: 0.0002314484736416489\n",
      "epoch: 9 | 74144 / 114272 | training loss: 0.00016200474055949599\n",
      "epoch: 9 | 74176 / 114272 | training loss: 0.000244019043748267\n",
      "epoch: 9 | 74208 / 114272 | training loss: 0.001485639251768589\n",
      "epoch: 9 | 74240 / 114272 | training loss: 0.00022818833531346172\n",
      "epoch: 9 | 74272 / 114272 | training loss: 0.00020952134218532592\n",
      "epoch: 9 | 74304 / 114272 | training loss: 0.0005162278539501131\n",
      "epoch: 9 | 74336 / 114272 | training loss: 0.0002334529854124412\n",
      "epoch: 9 | 74368 / 114272 | training loss: 0.00025057309539988637\n",
      "epoch: 9 | 74400 / 114272 | training loss: 0.0004858295142184943\n",
      "epoch: 9 | 74432 / 114272 | training loss: 0.00028086290694773197\n",
      "epoch: 9 | 74464 / 114272 | training loss: 0.0003151350247208029\n",
      "epoch: 9 | 74496 / 114272 | training loss: 0.0002378257631789893\n",
      "epoch: 9 | 74528 / 114272 | training loss: 0.00018210725102107972\n",
      "epoch: 9 | 74560 / 114272 | training loss: 0.00021943960746284574\n",
      "epoch: 9 | 74592 / 114272 | training loss: 0.0038310426753014326\n",
      "epoch: 9 | 74624 / 114272 | training loss: 0.00027109336224384606\n",
      "epoch: 9 | 74656 / 114272 | training loss: 0.00016254468937404454\n",
      "epoch: 9 | 74688 / 114272 | training loss: 0.00012257217895239592\n",
      "epoch: 9 | 74720 / 114272 | training loss: 0.0003140807384625077\n",
      "epoch: 9 | 74752 / 114272 | training loss: 0.00018477122648619115\n",
      "epoch: 9 | 74784 / 114272 | training loss: 0.00027130809030495584\n",
      "epoch: 9 | 74816 / 114272 | training loss: 0.00014189987268764526\n",
      "epoch: 9 | 74848 / 114272 | training loss: 0.00014944880967959762\n",
      "epoch: 9 | 74880 / 114272 | training loss: 0.0001792611728888005\n",
      "epoch: 9 | 74912 / 114272 | training loss: 0.004884080961346626\n",
      "epoch: 9 | 74944 / 114272 | training loss: 0.0002399613440502435\n",
      "epoch: 9 | 74976 / 114272 | training loss: 0.00012038549175485969\n",
      "epoch: 9 | 75008 / 114272 | training loss: 0.00020074598432984203\n",
      "epoch: 9 | 75040 / 114272 | training loss: 0.00035733490949496627\n",
      "epoch: 9 | 75072 / 114272 | training loss: 0.00030698723276145756\n",
      "epoch: 9 | 75104 / 114272 | training loss: 0.00032488489523530006\n",
      "epoch: 9 | 75136 / 114272 | training loss: 0.0001127017822000198\n",
      "epoch: 9 | 75168 / 114272 | training loss: 0.00015341243124566972\n",
      "epoch: 9 | 75200 / 114272 | training loss: 0.00023476887145079672\n",
      "epoch: 9 | 75232 / 114272 | training loss: 0.02800002321600914\n",
      "epoch: 9 | 75264 / 114272 | training loss: 0.00034964029327966273\n",
      "epoch: 9 | 75296 / 114272 | training loss: 0.0002987080661114305\n",
      "epoch: 9 | 75328 / 114272 | training loss: 0.0003089331730734557\n",
      "epoch: 9 | 75360 / 114272 | training loss: 0.00024114690313581377\n",
      "epoch: 9 | 75392 / 114272 | training loss: 0.00022588178399018943\n",
      "epoch: 9 | 75424 / 114272 | training loss: 0.00029537087539210916\n",
      "epoch: 9 | 75456 / 114272 | training loss: 0.0002450715401209891\n",
      "epoch: 9 | 75488 / 114272 | training loss: 0.00028051872504875064\n",
      "epoch: 9 | 75520 / 114272 | training loss: 0.00031683678389526904\n",
      "epoch: 9 | 75552 / 114272 | training loss: 0.0024040977004915476\n",
      "epoch: 9 | 75584 / 114272 | training loss: 0.0003519066085573286\n",
      "epoch: 9 | 75616 / 114272 | training loss: 0.0001554083573864773\n",
      "epoch: 9 | 75648 / 114272 | training loss: 0.00012812875502277166\n",
      "epoch: 9 | 75680 / 114272 | training loss: 0.00033674747101031244\n",
      "epoch: 9 | 75712 / 114272 | training loss: 0.00018682052905205637\n",
      "epoch: 9 | 75744 / 114272 | training loss: 0.00021042217849753797\n",
      "epoch: 9 | 75776 / 114272 | training loss: 0.00020658064750023186\n",
      "epoch: 9 | 75808 / 114272 | training loss: 0.0003303684061393142\n",
      "epoch: 9 | 75840 / 114272 | training loss: 0.0026086680591106415\n",
      "epoch: 9 | 75872 / 114272 | training loss: 0.00013591680908575654\n",
      "epoch: 9 | 75904 / 114272 | training loss: 0.01685730740427971\n",
      "epoch: 9 | 75936 / 114272 | training loss: 0.0004370986425783485\n",
      "epoch: 9 | 75968 / 114272 | training loss: 0.00022104181698523462\n",
      "epoch: 9 | 76000 / 114272 | training loss: 0.0002217126893810928\n",
      "epoch: 9 | 76032 / 114272 | training loss: 0.00017527751333545893\n",
      "epoch: 9 | 76064 / 114272 | training loss: 0.00028719002148136497\n",
      "epoch: 9 | 76096 / 114272 | training loss: 0.0002161940064979717\n",
      "epoch: 9 | 76128 / 114272 | training loss: 0.00025317296967841685\n",
      "epoch: 9 | 76160 / 114272 | training loss: 0.0001997613289859146\n",
      "epoch: 9 | 76192 / 114272 | training loss: 0.00022083432122599334\n",
      "epoch: 9 | 76224 / 114272 | training loss: 0.00016274100926239043\n",
      "epoch: 9 | 76256 / 114272 | training loss: 0.00032251724041998386\n",
      "epoch: 9 | 76288 / 114272 | training loss: 0.0001744766195770353\n",
      "epoch: 9 | 76320 / 114272 | training loss: 0.00020740363106597215\n",
      "epoch: 9 | 76352 / 114272 | training loss: 0.00010381369793321937\n",
      "epoch: 9 | 76384 / 114272 | training loss: 0.00028753685182891786\n",
      "epoch: 9 | 76416 / 114272 | training loss: 0.020479274913668633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 76448 / 114272 | training loss: 0.12439974397420883\n",
      "epoch: 9 | 76480 / 114272 | training loss: 0.00020212469098623842\n",
      "epoch: 9 | 76512 / 114272 | training loss: 0.00019307402544654906\n",
      "epoch: 9 | 76544 / 114272 | training loss: 0.00026038079522550106\n",
      "epoch: 9 | 76576 / 114272 | training loss: 0.0002823638787958771\n",
      "epoch: 9 | 76608 / 114272 | training loss: 0.00030425956356339157\n",
      "epoch: 9 | 76640 / 114272 | training loss: 0.031064966693520546\n",
      "epoch: 9 | 76672 / 114272 | training loss: 0.0002182351890951395\n",
      "epoch: 9 | 76704 / 114272 | training loss: 0.00026950420578941703\n",
      "epoch: 9 | 76736 / 114272 | training loss: 0.0006016237894073129\n",
      "epoch: 9 | 76768 / 114272 | training loss: 0.00021407290478236973\n",
      "epoch: 9 | 76800 / 114272 | training loss: 0.11202292889356613\n",
      "epoch: 9 | 76832 / 114272 | training loss: 0.00023725340724922717\n",
      "epoch: 9 | 76864 / 114272 | training loss: 0.0001717879786156118\n",
      "epoch: 9 | 76896 / 114272 | training loss: 0.0001744328619679436\n",
      "epoch: 9 | 76928 / 114272 | training loss: 0.0003226754779461771\n",
      "epoch: 9 | 76960 / 114272 | training loss: 0.08379160612821579\n",
      "epoch: 9 | 76992 / 114272 | training loss: 0.0005752405268140137\n",
      "epoch: 9 | 77024 / 114272 | training loss: 0.0003118176246061921\n",
      "epoch: 9 | 77056 / 114272 | training loss: 0.00020987773314118385\n",
      "epoch: 9 | 77088 / 114272 | training loss: 0.000335278658894822\n",
      "epoch: 9 | 77120 / 114272 | training loss: 0.0004108176799491048\n",
      "epoch: 9 | 77152 / 114272 | training loss: 0.0002357846824452281\n",
      "epoch: 9 | 77184 / 114272 | training loss: 0.00020748462702613324\n",
      "epoch: 9 | 77216 / 114272 | training loss: 0.0002607565838843584\n",
      "epoch: 9 | 77248 / 114272 | training loss: 0.00020414756727404892\n",
      "epoch: 9 | 77280 / 114272 | training loss: 0.0002699028409551829\n",
      "epoch: 9 | 77312 / 114272 | training loss: 0.0002382048696745187\n",
      "epoch: 9 | 77344 / 114272 | training loss: 0.00019126471306663007\n",
      "epoch: 9 | 77376 / 114272 | training loss: 0.0001095501720556058\n",
      "epoch: 9 | 77408 / 114272 | training loss: 0.00023174549278337508\n",
      "epoch: 9 | 77440 / 114272 | training loss: 0.00022394294501282275\n",
      "epoch: 9 | 77472 / 114272 | training loss: 0.00024255318567156792\n",
      "epoch: 9 | 77504 / 114272 | training loss: 0.00010496180766494945\n",
      "epoch: 9 | 77536 / 114272 | training loss: 0.0003589715051930398\n",
      "epoch: 9 | 77568 / 114272 | training loss: 0.0006635489990003407\n",
      "epoch: 9 | 77600 / 114272 | training loss: 0.0001552419998915866\n",
      "epoch: 9 | 77632 / 114272 | training loss: 0.00018196426390204579\n",
      "epoch: 9 | 77664 / 114272 | training loss: 0.0002007373986998573\n",
      "epoch: 9 | 77696 / 114272 | training loss: 0.00029566773446276784\n",
      "epoch: 9 | 77728 / 114272 | training loss: 0.00013887323439121246\n",
      "epoch: 9 | 77760 / 114272 | training loss: 0.0002761504438240081\n",
      "epoch: 9 | 77792 / 114272 | training loss: 0.00013129465514793992\n",
      "epoch: 9 | 77824 / 114272 | training loss: 0.0005133445374667645\n",
      "epoch: 9 | 77856 / 114272 | training loss: 0.00027164301718585193\n",
      "epoch: 9 | 77888 / 114272 | training loss: 0.00022656426881439984\n",
      "epoch: 9 | 77920 / 114272 | training loss: 0.00023990160843823105\n",
      "epoch: 9 | 77952 / 114272 | training loss: 0.00036392302718013525\n",
      "epoch: 9 | 77984 / 114272 | training loss: 0.00023758836323395371\n",
      "epoch: 9 | 78016 / 114272 | training loss: 0.0001873695000540465\n",
      "epoch: 9 | 78048 / 114272 | training loss: 0.00018629174155648798\n",
      "epoch: 9 | 78080 / 114272 | training loss: 0.00021653386647813022\n",
      "epoch: 9 | 78112 / 114272 | training loss: 0.21163520216941833\n",
      "epoch: 9 | 78144 / 114272 | training loss: 0.00024296304036397487\n",
      "epoch: 9 | 78176 / 114272 | training loss: 0.00028388609644025564\n",
      "epoch: 9 | 78208 / 114272 | training loss: 0.00036435178481042385\n",
      "epoch: 9 | 78240 / 114272 | training loss: 0.0005603659083135426\n",
      "epoch: 9 | 78272 / 114272 | training loss: 0.00030941935256123543\n",
      "epoch: 9 | 78304 / 114272 | training loss: 0.0003207493864465505\n",
      "epoch: 9 | 78336 / 114272 | training loss: 0.0002670891990419477\n",
      "epoch: 9 | 78368 / 114272 | training loss: 0.0002890054602175951\n",
      "epoch: 9 | 78400 / 114272 | training loss: 0.0002139556163456291\n",
      "epoch: 9 | 78432 / 114272 | training loss: 0.0002280648477608338\n",
      "epoch: 9 | 78464 / 114272 | training loss: 0.00023394657182507217\n",
      "epoch: 9 | 78496 / 114272 | training loss: 0.00022859648743178695\n",
      "epoch: 9 | 78528 / 114272 | training loss: 0.00031179009238258004\n",
      "epoch: 9 | 78560 / 114272 | training loss: 0.00023071907344274223\n",
      "epoch: 9 | 78592 / 114272 | training loss: 0.0001720415020827204\n",
      "epoch: 9 | 78624 / 114272 | training loss: 0.0002539530396461487\n",
      "epoch: 9 | 78656 / 114272 | training loss: 0.001348892692476511\n",
      "epoch: 9 | 78688 / 114272 | training loss: 0.014725266955792904\n",
      "epoch: 9 | 78720 / 114272 | training loss: 0.11712778359651566\n",
      "epoch: 9 | 78752 / 114272 | training loss: 0.00028146899421699345\n",
      "epoch: 9 | 78784 / 114272 | training loss: 0.00010904271039180458\n",
      "epoch: 9 | 78816 / 114272 | training loss: 0.00018145667854696512\n",
      "epoch: 9 | 78848 / 114272 | training loss: 0.00017411925364285707\n",
      "epoch: 9 | 78880 / 114272 | training loss: 0.0003294966008979827\n",
      "epoch: 9 | 78912 / 114272 | training loss: 0.0002789426071103662\n",
      "epoch: 9 | 78944 / 114272 | training loss: 0.00020900144590996206\n",
      "epoch: 9 | 78976 / 114272 | training loss: 0.0002986268373206258\n",
      "epoch: 9 | 79008 / 114272 | training loss: 0.00023294484708458185\n",
      "epoch: 9 | 79040 / 114272 | training loss: 0.00023111132031772286\n",
      "epoch: 9 | 79072 / 114272 | training loss: 0.0002893161727115512\n",
      "epoch: 9 | 79104 / 114272 | training loss: 0.02486131340265274\n",
      "epoch: 9 | 79136 / 114272 | training loss: 0.00033600034657865763\n",
      "epoch: 9 | 79168 / 114272 | training loss: 0.0002010791067732498\n",
      "epoch: 9 | 79200 / 114272 | training loss: 9.560392936691642e-05\n",
      "epoch: 9 | 79232 / 114272 | training loss: 0.00030235573649406433\n",
      "epoch: 9 | 79264 / 114272 | training loss: 0.003276999806985259\n",
      "epoch: 9 | 79296 / 114272 | training loss: 0.0005762772052548826\n",
      "epoch: 9 | 79328 / 114272 | training loss: 0.00023682181199546903\n",
      "epoch: 9 | 79360 / 114272 | training loss: 0.0001038410555338487\n",
      "epoch: 9 | 79392 / 114272 | training loss: 0.0002982529404107481\n",
      "epoch: 9 | 79424 / 114272 | training loss: 0.00013744192256126553\n",
      "epoch: 9 | 79456 / 114272 | training loss: 0.0002899659739341587\n",
      "epoch: 9 | 79488 / 114272 | training loss: 0.0002485361183062196\n",
      "epoch: 9 | 79520 / 114272 | training loss: 0.0007946285768412054\n",
      "epoch: 9 | 79552 / 114272 | training loss: 0.00023262087779585272\n",
      "epoch: 9 | 79584 / 114272 | training loss: 0.0002100843848893419\n",
      "epoch: 9 | 79616 / 114272 | training loss: 0.00035387964453548193\n",
      "epoch: 9 | 79648 / 114272 | training loss: 0.0007409140234813094\n",
      "epoch: 9 | 79680 / 114272 | training loss: 0.00016897231398615986\n",
      "epoch: 9 | 79712 / 114272 | training loss: 0.00028269703034311533\n",
      "epoch: 9 | 79744 / 114272 | training loss: 0.0007476511527784169\n",
      "epoch: 9 | 79776 / 114272 | training loss: 0.0003425179165787995\n",
      "epoch: 9 | 79808 / 114272 | training loss: 0.0002151507360395044\n",
      "epoch: 9 | 79840 / 114272 | training loss: 0.0002170918305637315\n",
      "epoch: 9 | 79872 / 114272 | training loss: 0.00021104489860590547\n",
      "epoch: 9 | 79904 / 114272 | training loss: 0.00019787186465691775\n",
      "epoch: 9 | 79936 / 114272 | training loss: 0.00016874786524567753\n",
      "epoch: 9 | 79968 / 114272 | training loss: 0.00021791734616272151\n",
      "epoch: 9 | 80000 / 114272 | training loss: 0.0003032330423593521\n",
      "epoch: 9 | 80032 / 114272 | training loss: 0.0002653613919392228\n",
      "epoch: 9 | 80064 / 114272 | training loss: 0.00014075306535232812\n",
      "epoch: 9 | 80096 / 114272 | training loss: 0.00020587902690749615\n",
      "epoch: 9 | 80128 / 114272 | training loss: 0.008962797932326794\n",
      "epoch: 9 | 80160 / 114272 | training loss: 0.0003424721071496606\n",
      "epoch: 9 | 80192 / 114272 | training loss: 0.0001490595896029845\n",
      "epoch: 9 | 80224 / 114272 | training loss: 0.0002883327833842486\n",
      "epoch: 9 | 80256 / 114272 | training loss: 0.0001036631510942243\n",
      "epoch: 9 | 80288 / 114272 | training loss: 0.0002592071541585028\n",
      "epoch: 9 | 80320 / 114272 | training loss: 0.00032885573455132544\n",
      "epoch: 9 | 80352 / 114272 | training loss: 0.00022499900660477579\n",
      "epoch: 9 | 80384 / 114272 | training loss: 0.00031833077082410455\n",
      "epoch: 9 | 80416 / 114272 | training loss: 0.0002804227115120739\n",
      "epoch: 9 | 80448 / 114272 | training loss: 0.0002595800324343145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 80480 / 114272 | training loss: 0.00028516293969005346\n",
      "epoch: 9 | 80512 / 114272 | training loss: 0.00016525524551980197\n",
      "epoch: 9 | 80544 / 114272 | training loss: 0.00021458706760313362\n",
      "epoch: 9 | 80576 / 114272 | training loss: 0.00017845610273070633\n",
      "epoch: 9 | 80608 / 114272 | training loss: 0.0001797740114852786\n",
      "epoch: 9 | 80640 / 114272 | training loss: 0.00023695421987213194\n",
      "epoch: 9 | 80672 / 114272 | training loss: 0.00021185606601648033\n",
      "epoch: 9 | 80704 / 114272 | training loss: 0.013019888661801815\n",
      "epoch: 9 | 80736 / 114272 | training loss: 0.00022204591368790716\n",
      "epoch: 9 | 80768 / 114272 | training loss: 0.00021814402134623379\n",
      "epoch: 9 | 80800 / 114272 | training loss: 0.00022016964794602245\n",
      "epoch: 9 | 80832 / 114272 | training loss: 0.0002617643913254142\n",
      "epoch: 9 | 80864 / 114272 | training loss: 0.00024929051869548857\n",
      "epoch: 9 | 80896 / 114272 | training loss: 0.000966022489592433\n",
      "epoch: 9 | 80928 / 114272 | training loss: 8.740085468161851e-05\n",
      "epoch: 9 | 80960 / 114272 | training loss: 0.00025732809444889426\n",
      "epoch: 9 | 80992 / 114272 | training loss: 0.00032228996860794723\n",
      "epoch: 9 | 81024 / 114272 | training loss: 0.0002174613910028711\n",
      "epoch: 9 | 81056 / 114272 | training loss: 0.00033335891203023493\n",
      "epoch: 9 | 81088 / 114272 | training loss: 0.0037790911737829447\n",
      "epoch: 9 | 81120 / 114272 | training loss: 0.0003778345708269626\n",
      "epoch: 9 | 81152 / 114272 | training loss: 0.0001532282622065395\n",
      "epoch: 9 | 81184 / 114272 | training loss: 0.00016696764214430004\n",
      "epoch: 9 | 81216 / 114272 | training loss: 0.0004014151345472783\n",
      "epoch: 9 | 81248 / 114272 | training loss: 0.0004097873461432755\n",
      "epoch: 9 | 81280 / 114272 | training loss: 0.00034105320810340345\n",
      "epoch: 9 | 81312 / 114272 | training loss: 0.00019135633192490786\n",
      "epoch: 9 | 81344 / 114272 | training loss: 0.00023848561977501959\n",
      "epoch: 9 | 81376 / 114272 | training loss: 0.00013473673607222736\n",
      "epoch: 9 | 81408 / 114272 | training loss: 0.0001927081757457927\n",
      "epoch: 9 | 81440 / 114272 | training loss: 0.0002770901483017951\n",
      "epoch: 9 | 81472 / 114272 | training loss: 0.0007085433462634683\n",
      "epoch: 9 | 81504 / 114272 | training loss: 0.00018896879919338971\n",
      "epoch: 9 | 81536 / 114272 | training loss: 0.20321762561798096\n",
      "epoch: 9 | 81568 / 114272 | training loss: 0.00018766787252388895\n",
      "epoch: 9 | 81600 / 114272 | training loss: 0.0026187968906015158\n",
      "epoch: 9 | 81632 / 114272 | training loss: 0.0002857362269423902\n",
      "epoch: 9 | 81664 / 114272 | training loss: 0.00028787757037207484\n",
      "epoch: 9 | 81696 / 114272 | training loss: 0.00020473207405302674\n",
      "epoch: 9 | 81728 / 114272 | training loss: 0.00041709994548000395\n",
      "epoch: 9 | 81760 / 114272 | training loss: 0.00012104584311600775\n",
      "epoch: 9 | 81792 / 114272 | training loss: 0.0002442692348267883\n",
      "epoch: 9 | 81824 / 114272 | training loss: 0.0001981878885999322\n",
      "epoch: 9 | 81856 / 114272 | training loss: 0.0002538711705710739\n",
      "epoch: 9 | 81888 / 114272 | training loss: 0.00019588535360526294\n",
      "epoch: 9 | 81920 / 114272 | training loss: 0.0002618920407257974\n",
      "epoch: 9 | 81952 / 114272 | training loss: 0.000165842633577995\n",
      "epoch: 9 | 81984 / 114272 | training loss: 0.00026810355484485626\n",
      "epoch: 9 | 82016 / 114272 | training loss: 0.00017120884149335325\n",
      "epoch: 9 | 82048 / 114272 | training loss: 0.030487190932035446\n",
      "epoch: 9 | 82080 / 114272 | training loss: 0.000262409943388775\n",
      "epoch: 9 | 82112 / 114272 | training loss: 0.0018141648033633828\n",
      "epoch: 9 | 82144 / 114272 | training loss: 0.00013761308218818158\n",
      "epoch: 9 | 82176 / 114272 | training loss: 0.00028987761470489204\n",
      "epoch: 9 | 82208 / 114272 | training loss: 0.0006298052030615509\n",
      "epoch: 9 | 82240 / 114272 | training loss: 0.00030151140526868403\n",
      "epoch: 9 | 82272 / 114272 | training loss: 0.0002998514974024147\n",
      "epoch: 9 | 82304 / 114272 | training loss: 0.00019090603746008128\n",
      "epoch: 9 | 82336 / 114272 | training loss: 0.0004283357411623001\n",
      "epoch: 9 | 82368 / 114272 | training loss: 0.0002972046786453575\n",
      "epoch: 9 | 82400 / 114272 | training loss: 0.00011914334027096629\n",
      "epoch: 9 | 82432 / 114272 | training loss: 0.0002385773550486192\n",
      "epoch: 9 | 82464 / 114272 | training loss: 0.00019035830337088555\n",
      "epoch: 9 | 82496 / 114272 | training loss: 0.00016769238573033363\n",
      "epoch: 9 | 82528 / 114272 | training loss: 0.00015262444503605366\n",
      "epoch: 9 | 82560 / 114272 | training loss: 0.00036068877670913935\n",
      "epoch: 9 | 82592 / 114272 | training loss: 0.0002632603282108903\n",
      "epoch: 9 | 82624 / 114272 | training loss: 0.00018945930059999228\n",
      "epoch: 9 | 82656 / 114272 | training loss: 0.00023694618721492589\n",
      "epoch: 9 | 82688 / 114272 | training loss: 0.00011280155013082549\n",
      "epoch: 9 | 82720 / 114272 | training loss: 0.024560391902923584\n",
      "epoch: 9 | 82752 / 114272 | training loss: 0.0006800733972340822\n",
      "epoch: 9 | 82784 / 114272 | training loss: 0.022964393720030785\n",
      "epoch: 9 | 82816 / 114272 | training loss: 0.00023012462770566344\n",
      "epoch: 9 | 82848 / 114272 | training loss: 0.00018278296920470893\n",
      "epoch: 9 | 82880 / 114272 | training loss: 0.0003425458853598684\n",
      "epoch: 9 | 82912 / 114272 | training loss: 0.020130975171923637\n",
      "epoch: 9 | 82944 / 114272 | training loss: 0.0003152156132273376\n",
      "epoch: 9 | 82976 / 114272 | training loss: 0.00032317786826752126\n",
      "epoch: 9 | 83008 / 114272 | training loss: 0.00013720881543122232\n",
      "epoch: 9 | 83040 / 114272 | training loss: 0.00027045857859775424\n",
      "epoch: 9 | 83072 / 114272 | training loss: 0.0001800798054318875\n",
      "epoch: 9 | 83104 / 114272 | training loss: 0.0003993067075498402\n",
      "epoch: 9 | 83136 / 114272 | training loss: 0.15481258928775787\n",
      "epoch: 9 | 83168 / 114272 | training loss: 0.00026140626869164407\n",
      "epoch: 9 | 83200 / 114272 | training loss: 0.000296038284432143\n",
      "epoch: 9 | 83232 / 114272 | training loss: 0.0003607698599807918\n",
      "epoch: 9 | 83264 / 114272 | training loss: 0.00010860132169909775\n",
      "epoch: 9 | 83296 / 114272 | training loss: 0.00022832927061244845\n",
      "epoch: 9 | 83328 / 114272 | training loss: 0.0001928079145727679\n",
      "epoch: 9 | 83360 / 114272 | training loss: 0.00020517056691460311\n",
      "epoch: 9 | 83392 / 114272 | training loss: 0.0002627117792144418\n",
      "epoch: 9 | 83424 / 114272 | training loss: 0.18431536853313446\n",
      "epoch: 9 | 83456 / 114272 | training loss: 0.00018256322073284537\n",
      "epoch: 9 | 83488 / 114272 | training loss: 0.00018545730563346297\n",
      "epoch: 9 | 83520 / 114272 | training loss: 0.07638668268918991\n",
      "epoch: 9 | 83552 / 114272 | training loss: 0.0001600869873072952\n",
      "epoch: 9 | 83584 / 114272 | training loss: 0.00026854738825932145\n",
      "epoch: 9 | 83616 / 114272 | training loss: 0.00025853642728179693\n",
      "epoch: 9 | 83648 / 114272 | training loss: 0.00024920006399042904\n",
      "epoch: 9 | 83680 / 114272 | training loss: 0.00029075981001369655\n",
      "epoch: 9 | 83712 / 114272 | training loss: 0.0003919042646884918\n",
      "epoch: 9 | 83744 / 114272 | training loss: 0.04253656417131424\n",
      "epoch: 9 | 83776 / 114272 | training loss: 0.00030463020084425807\n",
      "epoch: 9 | 83808 / 114272 | training loss: 0.0001737138954922557\n",
      "epoch: 9 | 83840 / 114272 | training loss: 0.00035578690585680306\n",
      "epoch: 9 | 83872 / 114272 | training loss: 0.00016298105765599757\n",
      "epoch: 9 | 83904 / 114272 | training loss: 0.00014951730554457754\n",
      "epoch: 9 | 83936 / 114272 | training loss: 0.020811233669519424\n",
      "epoch: 9 | 83968 / 114272 | training loss: 0.00020197845879010856\n",
      "epoch: 9 | 84000 / 114272 | training loss: 0.27346494793891907\n",
      "epoch: 9 | 84032 / 114272 | training loss: 0.0011314571602270007\n",
      "epoch: 9 | 84064 / 114272 | training loss: 0.00035911070881411433\n",
      "epoch: 9 | 84096 / 114272 | training loss: 0.0001568287261761725\n",
      "epoch: 9 | 84128 / 114272 | training loss: 0.0002323390363017097\n",
      "epoch: 9 | 84160 / 114272 | training loss: 0.00011004385305568576\n",
      "epoch: 9 | 84192 / 114272 | training loss: 0.0008715636795386672\n",
      "epoch: 9 | 84224 / 114272 | training loss: 0.0005887658917345107\n",
      "epoch: 9 | 84256 / 114272 | training loss: 0.0002444509300403297\n",
      "epoch: 9 | 84288 / 114272 | training loss: 0.0005396544001996517\n",
      "epoch: 9 | 84320 / 114272 | training loss: 0.00023817410692572594\n",
      "epoch: 9 | 84352 / 114272 | training loss: 0.00030251487623900175\n",
      "epoch: 9 | 84384 / 114272 | training loss: 0.12923851609230042\n",
      "epoch: 9 | 84416 / 114272 | training loss: 0.0001025144592858851\n",
      "epoch: 9 | 84448 / 114272 | training loss: 0.0004032428259961307\n",
      "epoch: 9 | 84480 / 114272 | training loss: 0.0009463954484090209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 84512 / 114272 | training loss: 0.000342876446666196\n",
      "epoch: 9 | 84544 / 114272 | training loss: 0.00017554571968503296\n",
      "epoch: 9 | 84576 / 114272 | training loss: 0.00018209201516583562\n",
      "epoch: 9 | 84608 / 114272 | training loss: 0.12655682861804962\n",
      "epoch: 9 | 84640 / 114272 | training loss: 0.0003972645499743521\n",
      "epoch: 9 | 84672 / 114272 | training loss: 0.00021868168551009148\n",
      "epoch: 9 | 84704 / 114272 | training loss: 0.00022968467965256423\n",
      "epoch: 9 | 84736 / 114272 | training loss: 0.0003124030481558293\n",
      "epoch: 9 | 84768 / 114272 | training loss: 0.000260235887253657\n",
      "epoch: 9 | 84800 / 114272 | training loss: 0.00017357822798658162\n",
      "epoch: 9 | 84832 / 114272 | training loss: 0.0001990539749385789\n",
      "epoch: 9 | 84864 / 114272 | training loss: 0.00020676024723798037\n",
      "epoch: 9 | 84896 / 114272 | training loss: 0.0006262052338570356\n",
      "epoch: 9 | 84928 / 114272 | training loss: 0.00021247100085020065\n",
      "epoch: 9 | 84960 / 114272 | training loss: 0.00013391503307502717\n",
      "epoch: 9 | 84992 / 114272 | training loss: 0.0005350823630578816\n",
      "epoch: 9 | 85024 / 114272 | training loss: 0.0001105189003283158\n",
      "epoch: 9 | 85056 / 114272 | training loss: 0.07600808143615723\n",
      "epoch: 9 | 85088 / 114272 | training loss: 0.14102448523044586\n",
      "epoch: 9 | 85120 / 114272 | training loss: 0.00020565693557728082\n",
      "epoch: 9 | 85152 / 114272 | training loss: 0.00025502004427835345\n",
      "epoch: 9 | 85184 / 114272 | training loss: 0.0003616257745306939\n",
      "epoch: 9 | 85216 / 114272 | training loss: 0.020539091899991035\n",
      "epoch: 9 | 85248 / 114272 | training loss: 0.00024123354523908347\n",
      "epoch: 9 | 85280 / 114272 | training loss: 0.00018356107466388494\n",
      "epoch: 9 | 85312 / 114272 | training loss: 0.00019017992599401623\n",
      "epoch: 9 | 85344 / 114272 | training loss: 0.00016100707580335438\n",
      "epoch: 9 | 85376 / 114272 | training loss: 0.0002717692987062037\n",
      "epoch: 9 | 85408 / 114272 | training loss: 0.00023572539794258773\n",
      "epoch: 9 | 85440 / 114272 | training loss: 0.00018341070972383022\n",
      "epoch: 9 | 85472 / 114272 | training loss: 0.0003756430232897401\n",
      "epoch: 9 | 85504 / 114272 | training loss: 0.0002632196119520813\n",
      "epoch: 9 | 85536 / 114272 | training loss: 0.0002118151605827734\n",
      "epoch: 9 | 85568 / 114272 | training loss: 0.0005353206070140004\n",
      "epoch: 9 | 85600 / 114272 | training loss: 0.00019921897910535336\n",
      "epoch: 9 | 85632 / 114272 | training loss: 0.0002572490484453738\n",
      "epoch: 9 | 85664 / 114272 | training loss: 0.0001312008243985474\n",
      "epoch: 9 | 85696 / 114272 | training loss: 0.0001116871353588067\n",
      "epoch: 9 | 85728 / 114272 | training loss: 0.00028807949274778366\n",
      "epoch: 9 | 85760 / 114272 | training loss: 0.0002905299188569188\n",
      "epoch: 9 | 85792 / 114272 | training loss: 0.00021321022359188646\n",
      "epoch: 9 | 85824 / 114272 | training loss: 0.00016694741498213261\n",
      "epoch: 9 | 85856 / 114272 | training loss: 0.00019421054457779974\n",
      "epoch: 9 | 85888 / 114272 | training loss: 0.00037718479870818555\n",
      "epoch: 9 | 85920 / 114272 | training loss: 0.01953127048909664\n",
      "epoch: 9 | 85952 / 114272 | training loss: 0.0002201919414801523\n",
      "epoch: 9 | 85984 / 114272 | training loss: 0.00018663457012735307\n",
      "epoch: 9 | 86016 / 114272 | training loss: 0.00017993964138440788\n",
      "epoch: 9 | 86048 / 114272 | training loss: 0.00020479150407481939\n",
      "epoch: 9 | 86080 / 114272 | training loss: 0.00012789314496330917\n",
      "epoch: 9 | 86112 / 114272 | training loss: 0.0023822225630283356\n",
      "epoch: 9 | 86144 / 114272 | training loss: 0.0001498249766882509\n",
      "epoch: 9 | 86176 / 114272 | training loss: 0.00027123704785481095\n",
      "epoch: 9 | 86208 / 114272 | training loss: 0.00014521075354423374\n",
      "epoch: 9 | 86240 / 114272 | training loss: 0.00020269840024411678\n",
      "epoch: 9 | 86272 / 114272 | training loss: 0.000720067007932812\n",
      "epoch: 9 | 86304 / 114272 | training loss: 0.00034800093271769583\n",
      "epoch: 9 | 86336 / 114272 | training loss: 0.0018457584083080292\n",
      "epoch: 9 | 86368 / 114272 | training loss: 0.000405075988965109\n",
      "epoch: 9 | 86400 / 114272 | training loss: 0.00034952317946590483\n",
      "epoch: 9 | 86432 / 114272 | training loss: 0.00016441417392343283\n",
      "epoch: 9 | 86464 / 114272 | training loss: 0.0004610999603755772\n",
      "epoch: 9 | 86496 / 114272 | training loss: 0.0002412972244201228\n",
      "epoch: 9 | 86528 / 114272 | training loss: 0.0003390080528333783\n",
      "epoch: 9 | 86560 / 114272 | training loss: 0.00023860677902121097\n",
      "epoch: 9 | 86592 / 114272 | training loss: 0.00014096997620072216\n",
      "epoch: 9 | 86624 / 114272 | training loss: 0.0002514135558158159\n",
      "epoch: 9 | 86656 / 114272 | training loss: 0.00029205408645793796\n",
      "epoch: 9 | 86688 / 114272 | training loss: 0.0024529427755624056\n",
      "epoch: 9 | 86720 / 114272 | training loss: 0.00025074457516893744\n",
      "epoch: 9 | 86752 / 114272 | training loss: 0.0004049501148983836\n",
      "epoch: 9 | 86784 / 114272 | training loss: 0.0004357962461654097\n",
      "epoch: 9 | 86816 / 114272 | training loss: 0.0001086209449567832\n",
      "epoch: 9 | 86848 / 114272 | training loss: 0.00020764155487995595\n",
      "epoch: 9 | 86880 / 114272 | training loss: 0.00020558817777782679\n",
      "epoch: 9 | 86912 / 114272 | training loss: 0.0003401933063287288\n",
      "epoch: 9 | 86944 / 114272 | training loss: 0.00014104250294622034\n",
      "epoch: 9 | 86976 / 114272 | training loss: 0.00021785864373669028\n",
      "epoch: 9 | 87008 / 114272 | training loss: 0.0005691645201295614\n",
      "epoch: 9 | 87040 / 114272 | training loss: 0.17299245297908783\n",
      "epoch: 9 | 87072 / 114272 | training loss: 0.0002767761761788279\n",
      "epoch: 9 | 87104 / 114272 | training loss: 0.0001954609324457124\n",
      "epoch: 9 | 87136 / 114272 | training loss: 0.00032558562816120684\n",
      "epoch: 9 | 87168 / 114272 | training loss: 0.000341988867148757\n",
      "epoch: 9 | 87200 / 114272 | training loss: 0.0008731610723771155\n",
      "epoch: 9 | 87232 / 114272 | training loss: 0.0004259334527887404\n",
      "epoch: 9 | 87264 / 114272 | training loss: 0.00013372482499107718\n",
      "epoch: 9 | 87296 / 114272 | training loss: 0.00023524615971837193\n",
      "epoch: 9 | 87328 / 114272 | training loss: 0.00012954453995916992\n",
      "epoch: 9 | 87360 / 114272 | training loss: 0.00024604902137070894\n",
      "epoch: 9 | 87392 / 114272 | training loss: 0.0012151215923950076\n",
      "epoch: 9 | 87424 / 114272 | training loss: 0.00027234735898673534\n",
      "epoch: 9 | 87456 / 114272 | training loss: 0.0003263505641371012\n",
      "epoch: 9 | 87488 / 114272 | training loss: 0.00027119324658997357\n",
      "epoch: 9 | 87520 / 114272 | training loss: 0.00019667956803459674\n",
      "epoch: 9 | 87552 / 114272 | training loss: 0.00020279714954085648\n",
      "epoch: 9 | 87584 / 114272 | training loss: 0.0001750616793287918\n",
      "epoch: 9 | 87616 / 114272 | training loss: 0.00019881865591742098\n",
      "epoch: 9 | 87648 / 114272 | training loss: 0.00011488587188068777\n",
      "epoch: 9 | 87680 / 114272 | training loss: 0.00023435067851096392\n",
      "epoch: 9 | 87712 / 114272 | training loss: 0.00023328208771999925\n",
      "epoch: 9 | 87744 / 114272 | training loss: 0.00017361603386234492\n",
      "epoch: 9 | 87776 / 114272 | training loss: 0.00015552133845631033\n",
      "epoch: 9 | 87808 / 114272 | training loss: 0.00045654462883248925\n",
      "epoch: 9 | 87840 / 114272 | training loss: 0.00019343238091096282\n",
      "epoch: 9 | 87872 / 114272 | training loss: 0.00014295811706688255\n",
      "epoch: 9 | 87904 / 114272 | training loss: 0.00020649883663281798\n",
      "epoch: 9 | 87936 / 114272 | training loss: 0.1733589619398117\n",
      "epoch: 9 | 87968 / 114272 | training loss: 0.00016229449829552323\n",
      "epoch: 9 | 88000 / 114272 | training loss: 0.0001374995190417394\n",
      "epoch: 9 | 88032 / 114272 | training loss: 0.00048058360698632896\n",
      "epoch: 9 | 88064 / 114272 | training loss: 0.00023677207354921848\n",
      "epoch: 9 | 88096 / 114272 | training loss: 0.00023619613784831017\n",
      "epoch: 9 | 88128 / 114272 | training loss: 0.00012612239515874535\n",
      "epoch: 9 | 88160 / 114272 | training loss: 0.0008356207981705666\n",
      "epoch: 9 | 88192 / 114272 | training loss: 0.00016620826500002295\n",
      "epoch: 9 | 88224 / 114272 | training loss: 0.00042969692731276155\n",
      "epoch: 9 | 88256 / 114272 | training loss: 0.00027213103021495044\n",
      "epoch: 9 | 88288 / 114272 | training loss: 0.00021064901375211775\n",
      "epoch: 9 | 88320 / 114272 | training loss: 0.0002706022060010582\n",
      "epoch: 9 | 88352 / 114272 | training loss: 0.0002277613675687462\n",
      "epoch: 9 | 88384 / 114272 | training loss: 0.00021512214152608067\n",
      "epoch: 9 | 88416 / 114272 | training loss: 0.00026099468232132494\n",
      "epoch: 9 | 88448 / 114272 | training loss: 0.00032076818752102554\n",
      "epoch: 9 | 88480 / 114272 | training loss: 0.0002375799958826974\n",
      "epoch: 9 | 88512 / 114272 | training loss: 0.0002637167926877737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 88544 / 114272 | training loss: 0.00016393593978136778\n",
      "epoch: 9 | 88576 / 114272 | training loss: 0.0001708290510578081\n",
      "epoch: 9 | 88608 / 114272 | training loss: 0.00016309005150105804\n",
      "epoch: 9 | 88640 / 114272 | training loss: 0.00023958187375683337\n",
      "epoch: 9 | 88672 / 114272 | training loss: 0.00023362558567896485\n",
      "epoch: 9 | 88704 / 114272 | training loss: 0.00023922779655549675\n",
      "epoch: 9 | 88736 / 114272 | training loss: 0.00020392591250129044\n",
      "epoch: 9 | 88768 / 114272 | training loss: 0.0003746174043044448\n",
      "epoch: 9 | 88800 / 114272 | training loss: 0.00021690892754122615\n",
      "epoch: 9 | 88832 / 114272 | training loss: 0.0002073266514344141\n",
      "epoch: 9 | 88864 / 114272 | training loss: 0.00032755610300228\n",
      "epoch: 9 | 88896 / 114272 | training loss: 0.00015267141861841083\n",
      "epoch: 9 | 88928 / 114272 | training loss: 0.0002589222858659923\n",
      "epoch: 9 | 88960 / 114272 | training loss: 0.00011006776185240597\n",
      "epoch: 9 | 88992 / 114272 | training loss: 0.00024075654800981283\n",
      "epoch: 9 | 89024 / 114272 | training loss: 0.10069973021745682\n",
      "epoch: 9 | 89056 / 114272 | training loss: 0.00015055843687150627\n",
      "epoch: 9 | 89088 / 114272 | training loss: 0.00020591482461895794\n",
      "epoch: 9 | 89120 / 114272 | training loss: 0.0002033109194599092\n",
      "epoch: 9 | 89152 / 114272 | training loss: 0.0002051933843176812\n",
      "epoch: 9 | 89184 / 114272 | training loss: 0.0005288542597554624\n",
      "epoch: 9 | 89216 / 114272 | training loss: 0.0002921735285781324\n",
      "epoch: 9 | 89248 / 114272 | training loss: 8.967629401013255e-05\n",
      "epoch: 9 | 89280 / 114272 | training loss: 0.0003228065324947238\n",
      "epoch: 9 | 89312 / 114272 | training loss: 0.0002671935362741351\n",
      "epoch: 9 | 89344 / 114272 | training loss: 0.0002760974457487464\n",
      "epoch: 9 | 89376 / 114272 | training loss: 0.00022592356253881007\n",
      "epoch: 9 | 89408 / 114272 | training loss: 0.00036106951301917434\n",
      "epoch: 9 | 89440 / 114272 | training loss: 0.0003207520057912916\n",
      "epoch: 9 | 89472 / 114272 | training loss: 0.00017258155276067555\n",
      "epoch: 9 | 89504 / 114272 | training loss: 0.0007586940773762763\n",
      "epoch: 9 | 89536 / 114272 | training loss: 0.00015377883391920477\n",
      "epoch: 9 | 89568 / 114272 | training loss: 0.0016289119375869632\n",
      "epoch: 9 | 89600 / 114272 | training loss: 0.00032048963475972414\n",
      "epoch: 9 | 89632 / 114272 | training loss: 0.0002859026426449418\n",
      "epoch: 9 | 89664 / 114272 | training loss: 0.00013387900253292173\n",
      "epoch: 9 | 89696 / 114272 | training loss: 0.00019029341638088226\n",
      "epoch: 9 | 89728 / 114272 | training loss: 0.00021439435658976436\n",
      "epoch: 9 | 89760 / 114272 | training loss: 0.00018688355339691043\n",
      "epoch: 9 | 89792 / 114272 | training loss: 0.00014271408144850284\n",
      "epoch: 9 | 89824 / 114272 | training loss: 0.00032445392571389675\n",
      "epoch: 9 | 89856 / 114272 | training loss: 0.0002564698806963861\n",
      "epoch: 9 | 89888 / 114272 | training loss: 0.2743678092956543\n",
      "epoch: 9 | 89920 / 114272 | training loss: 0.0001189809845527634\n",
      "epoch: 9 | 89952 / 114272 | training loss: 0.0004185002762824297\n",
      "epoch: 9 | 89984 / 114272 | training loss: 0.00034261419204995036\n",
      "epoch: 9 | 90016 / 114272 | training loss: 0.00014745621592737734\n",
      "epoch: 9 | 90048 / 114272 | training loss: 0.0002269123651785776\n",
      "epoch: 9 | 90080 / 114272 | training loss: 0.001049698330461979\n",
      "epoch: 9 | 90112 / 114272 | training loss: 0.0004014137957710773\n",
      "epoch: 9 | 90144 / 114272 | training loss: 0.00023013679310679436\n",
      "epoch: 9 | 90176 / 114272 | training loss: 0.0003560579498298466\n",
      "epoch: 9 | 90208 / 114272 | training loss: 0.00037462281761690974\n",
      "epoch: 9 | 90240 / 114272 | training loss: 0.0002297373430337757\n",
      "epoch: 9 | 90272 / 114272 | training loss: 0.00014265996287576854\n",
      "epoch: 9 | 90304 / 114272 | training loss: 0.0002629575610626489\n",
      "epoch: 9 | 90336 / 114272 | training loss: 0.00019872306438628584\n",
      "epoch: 9 | 90368 / 114272 | training loss: 0.0018598359310999513\n",
      "epoch: 9 | 90400 / 114272 | training loss: 0.00026719862944446504\n",
      "epoch: 9 | 90432 / 114272 | training loss: 0.00040688496665097773\n",
      "epoch: 9 | 90464 / 114272 | training loss: 0.00033125278423540294\n",
      "epoch: 9 | 90496 / 114272 | training loss: 0.00025518209440633655\n",
      "epoch: 9 | 90528 / 114272 | training loss: 0.002016230020672083\n",
      "epoch: 9 | 90560 / 114272 | training loss: 0.00046657415805384517\n",
      "epoch: 9 | 90592 / 114272 | training loss: 0.00012676588085014373\n",
      "epoch: 9 | 90624 / 114272 | training loss: 0.00020920863607898355\n",
      "epoch: 9 | 90656 / 114272 | training loss: 0.0002100948477163911\n",
      "epoch: 9 | 90688 / 114272 | training loss: 0.0002094391529681161\n",
      "epoch: 9 | 90720 / 114272 | training loss: 0.00022085833188612014\n",
      "epoch: 9 | 90752 / 114272 | training loss: 0.0002338909253012389\n",
      "epoch: 9 | 90784 / 114272 | training loss: 0.00031962781213223934\n",
      "epoch: 9 | 90816 / 114272 | training loss: 0.00027358075021766126\n",
      "epoch: 9 | 90848 / 114272 | training loss: 0.0004152547335252166\n",
      "epoch: 9 | 90880 / 114272 | training loss: 0.00013145063712727278\n",
      "epoch: 9 | 90912 / 114272 | training loss: 0.0005411191377788782\n",
      "epoch: 9 | 90944 / 114272 | training loss: 0.00025381127488799393\n",
      "epoch: 9 | 90976 / 114272 | training loss: 0.0001717088744044304\n",
      "epoch: 9 | 91008 / 114272 | training loss: 0.0006547374650835991\n",
      "epoch: 9 | 91040 / 114272 | training loss: 0.00019506369426380843\n",
      "epoch: 9 | 91072 / 114272 | training loss: 0.00030309378053061664\n",
      "epoch: 9 | 91104 / 114272 | training loss: 0.0002700730401556939\n",
      "epoch: 9 | 91136 / 114272 | training loss: 0.00022434799757320434\n",
      "epoch: 9 | 91168 / 114272 | training loss: 0.000223677750909701\n",
      "epoch: 9 | 91200 / 114272 | training loss: 0.0004202026466373354\n",
      "epoch: 9 | 91232 / 114272 | training loss: 0.000343803025316447\n",
      "epoch: 9 | 91264 / 114272 | training loss: 0.0003571293200366199\n",
      "epoch: 9 | 91296 / 114272 | training loss: 0.0002827114949468523\n",
      "epoch: 9 | 91328 / 114272 | training loss: 0.00030366505961865187\n",
      "epoch: 9 | 91360 / 114272 | training loss: 0.2727455496788025\n",
      "epoch: 9 | 91392 / 114272 | training loss: 0.00014253868721425533\n",
      "epoch: 9 | 91424 / 114272 | training loss: 0.00027833456988446414\n",
      "epoch: 9 | 91456 / 114272 | training loss: 0.00022345305478665978\n",
      "epoch: 9 | 91488 / 114272 | training loss: 0.0003029713698197156\n",
      "epoch: 9 | 91520 / 114272 | training loss: 0.0002139089338015765\n",
      "epoch: 9 | 91552 / 114272 | training loss: 0.00035874664899893105\n",
      "epoch: 9 | 91584 / 114272 | training loss: 0.1237812414765358\n",
      "epoch: 9 | 91616 / 114272 | training loss: 0.0002351852599531412\n",
      "epoch: 9 | 91648 / 114272 | training loss: 0.00025897426530718803\n",
      "epoch: 9 | 91680 / 114272 | training loss: 0.00020556309027597308\n",
      "epoch: 9 | 91712 / 114272 | training loss: 0.014760080724954605\n",
      "epoch: 9 | 91744 / 114272 | training loss: 0.00025430304231122136\n",
      "epoch: 9 | 91776 / 114272 | training loss: 0.0002536481770221144\n",
      "epoch: 9 | 91808 / 114272 | training loss: 0.00023764475190546364\n",
      "epoch: 9 | 91840 / 114272 | training loss: 0.00027483890880830586\n",
      "epoch: 9 | 91872 / 114272 | training loss: 0.00019433200941421092\n",
      "epoch: 9 | 91904 / 114272 | training loss: 0.0005134072271175683\n",
      "epoch: 9 | 91936 / 114272 | training loss: 0.0002009789168369025\n",
      "epoch: 9 | 91968 / 114272 | training loss: 0.00013703946024179459\n",
      "epoch: 9 | 92000 / 114272 | training loss: 0.000259787542745471\n",
      "epoch: 9 | 92032 / 114272 | training loss: 0.00019391633395571262\n",
      "epoch: 9 | 92064 / 114272 | training loss: 0.0005130294011905789\n",
      "epoch: 9 | 92096 / 114272 | training loss: 0.0002940880076494068\n",
      "epoch: 9 | 92128 / 114272 | training loss: 0.00019797442655544728\n",
      "epoch: 9 | 92160 / 114272 | training loss: 0.00037329166661947966\n",
      "epoch: 9 | 92192 / 114272 | training loss: 0.00029866068507544696\n",
      "epoch: 9 | 92224 / 114272 | training loss: 0.0002669589885044843\n",
      "epoch: 9 | 92256 / 114272 | training loss: 0.0001551683380967006\n",
      "epoch: 9 | 92288 / 114272 | training loss: 0.00021676821052096784\n",
      "epoch: 9 | 92320 / 114272 | training loss: 0.0002402926911599934\n",
      "epoch: 9 | 92352 / 114272 | training loss: 0.00026014138711616397\n",
      "epoch: 9 | 92384 / 114272 | training loss: 0.025676386430859566\n",
      "epoch: 9 | 92416 / 114272 | training loss: 0.0012196727329865098\n",
      "epoch: 9 | 92448 / 114272 | training loss: 0.0002931891940534115\n",
      "epoch: 9 | 92480 / 114272 | training loss: 0.0007668215548619628\n",
      "epoch: 9 | 92512 / 114272 | training loss: 0.05535406991839409\n",
      "epoch: 9 | 92544 / 114272 | training loss: 0.00022439134772866964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 92576 / 114272 | training loss: 0.00017471429600846022\n",
      "epoch: 9 | 92608 / 114272 | training loss: 0.0002081601123791188\n",
      "epoch: 9 | 92640 / 114272 | training loss: 0.00028639149968512356\n",
      "epoch: 9 | 92672 / 114272 | training loss: 0.004982784856110811\n",
      "epoch: 9 | 92704 / 114272 | training loss: 0.042977407574653625\n",
      "epoch: 9 | 92736 / 114272 | training loss: 0.00021974355331622064\n",
      "epoch: 9 | 92768 / 114272 | training loss: 0.0003138806205242872\n",
      "epoch: 9 | 92800 / 114272 | training loss: 0.00031315581873059273\n",
      "epoch: 9 | 92832 / 114272 | training loss: 0.02694774605333805\n",
      "epoch: 9 | 92864 / 114272 | training loss: 0.0012155412696301937\n",
      "epoch: 9 | 92896 / 114272 | training loss: 0.00014846204430796206\n",
      "epoch: 9 | 92928 / 114272 | training loss: 0.0002573217498138547\n",
      "epoch: 9 | 92960 / 114272 | training loss: 0.00024393541389144957\n",
      "epoch: 9 | 92992 / 114272 | training loss: 0.0003271473105996847\n",
      "epoch: 9 | 93024 / 114272 | training loss: 0.00018498579447623342\n",
      "epoch: 9 | 93056 / 114272 | training loss: 0.0002563422021921724\n",
      "epoch: 9 | 93088 / 114272 | training loss: 0.00018818021635524929\n",
      "epoch: 9 | 93120 / 114272 | training loss: 0.0004211343766655773\n",
      "epoch: 9 | 93152 / 114272 | training loss: 0.0002176429406972602\n",
      "epoch: 9 | 93184 / 114272 | training loss: 0.00031612193561159074\n",
      "epoch: 9 | 93216 / 114272 | training loss: 0.00017504970310255885\n",
      "epoch: 9 | 93248 / 114272 | training loss: 0.00015416419773828238\n",
      "epoch: 9 | 93280 / 114272 | training loss: 0.0003283584082964808\n",
      "epoch: 9 | 93312 / 114272 | training loss: 0.00021480582654476166\n",
      "epoch: 9 | 93344 / 114272 | training loss: 0.0011091423220932484\n",
      "epoch: 9 | 93376 / 114272 | training loss: 0.00392951862886548\n",
      "epoch: 9 | 93408 / 114272 | training loss: 0.0001867500541266054\n",
      "epoch: 9 | 93440 / 114272 | training loss: 0.0005087258177809417\n",
      "epoch: 9 | 93472 / 114272 | training loss: 0.00017200458387378603\n",
      "epoch: 9 | 93504 / 114272 | training loss: 0.0002321240317542106\n",
      "epoch: 9 | 93536 / 114272 | training loss: 0.0003790834452956915\n",
      "epoch: 9 | 93568 / 114272 | training loss: 0.00024410380865447223\n",
      "epoch: 9 | 93600 / 114272 | training loss: 0.0002637332654558122\n",
      "epoch: 9 | 93632 / 114272 | training loss: 0.0001971094898181036\n",
      "epoch: 9 | 93664 / 114272 | training loss: 0.00019487482495605946\n",
      "epoch: 9 | 93696 / 114272 | training loss: 0.00029726990032941103\n",
      "epoch: 9 | 93728 / 114272 | training loss: 0.0004630510229617357\n",
      "epoch: 9 | 93760 / 114272 | training loss: 0.0002627499052323401\n",
      "epoch: 9 | 93792 / 114272 | training loss: 0.000363250175723806\n",
      "epoch: 9 | 93824 / 114272 | training loss: 0.0003531533875502646\n",
      "epoch: 9 | 93856 / 114272 | training loss: 0.00010453132563270628\n",
      "epoch: 9 | 93888 / 114272 | training loss: 0.00022853072732686996\n",
      "epoch: 9 | 93920 / 114272 | training loss: 0.0015405077720060945\n",
      "epoch: 9 | 93952 / 114272 | training loss: 0.19863922894001007\n",
      "epoch: 9 | 93984 / 114272 | training loss: 0.004912820644676685\n",
      "epoch: 9 | 94016 / 114272 | training loss: 0.00028124917298555374\n",
      "epoch: 9 | 94048 / 114272 | training loss: 0.00010464394290465862\n",
      "epoch: 9 | 94080 / 114272 | training loss: 0.00022408764925785363\n",
      "epoch: 9 | 94112 / 114272 | training loss: 0.000960968085564673\n",
      "epoch: 9 | 94144 / 114272 | training loss: 0.0003923066833522171\n",
      "epoch: 9 | 94176 / 114272 | training loss: 0.0001529610890429467\n",
      "epoch: 9 | 94208 / 114272 | training loss: 0.00023576684179715812\n",
      "epoch: 9 | 94240 / 114272 | training loss: 0.00016194705676753074\n",
      "epoch: 9 | 94272 / 114272 | training loss: 0.00021038517297711223\n",
      "epoch: 9 | 94304 / 114272 | training loss: 0.00019679685647133738\n",
      "epoch: 9 | 94336 / 114272 | training loss: 0.0001466464891564101\n",
      "epoch: 9 | 94368 / 114272 | training loss: 0.00025168797583319247\n",
      "epoch: 9 | 94400 / 114272 | training loss: 0.0003547533415257931\n",
      "epoch: 9 | 94432 / 114272 | training loss: 0.00046089672832749784\n",
      "epoch: 9 | 94464 / 114272 | training loss: 0.14587602019309998\n",
      "epoch: 9 | 94496 / 114272 | training loss: 0.00022824756160844117\n",
      "epoch: 9 | 94528 / 114272 | training loss: 0.00014264017227105796\n",
      "epoch: 9 | 94560 / 114272 | training loss: 0.0002852344769053161\n",
      "epoch: 9 | 94592 / 114272 | training loss: 0.00019155330664943904\n",
      "epoch: 9 | 94624 / 114272 | training loss: 0.0002292892022524029\n",
      "epoch: 9 | 94656 / 114272 | training loss: 0.0002342339139431715\n",
      "epoch: 9 | 94688 / 114272 | training loss: 0.061570025980472565\n",
      "epoch: 9 | 94720 / 114272 | training loss: 0.00014560237468685955\n",
      "epoch: 9 | 94752 / 114272 | training loss: 0.00021283485693857074\n",
      "epoch: 9 | 94784 / 114272 | training loss: 0.0003037437563762069\n",
      "epoch: 9 | 94816 / 114272 | training loss: 0.00036099497810937464\n",
      "epoch: 9 | 94848 / 114272 | training loss: 0.00029033978353254497\n",
      "epoch: 9 | 94880 / 114272 | training loss: 0.0003018016868736595\n",
      "epoch: 9 | 94912 / 114272 | training loss: 0.00010171879694098607\n",
      "epoch: 9 | 94944 / 114272 | training loss: 0.00020680803572759032\n",
      "epoch: 9 | 94976 / 114272 | training loss: 0.0001761580497259274\n",
      "epoch: 9 | 95008 / 114272 | training loss: 0.0002781196963042021\n",
      "epoch: 9 | 95040 / 114272 | training loss: 0.00020139072148595005\n",
      "epoch: 9 | 95072 / 114272 | training loss: 0.00039634076529182494\n",
      "epoch: 9 | 95104 / 114272 | training loss: 0.000415724964113906\n",
      "epoch: 9 | 95136 / 114272 | training loss: 0.0002092889480991289\n",
      "epoch: 9 | 95168 / 114272 | training loss: 0.00017733575077727437\n",
      "epoch: 9 | 95200 / 114272 | training loss: 0.00025656510842964053\n",
      "epoch: 9 | 95232 / 114272 | training loss: 0.6624323725700378\n",
      "epoch: 9 | 95264 / 114272 | training loss: 0.00033778627403080463\n",
      "epoch: 9 | 95296 / 114272 | training loss: 0.0002657368313521147\n",
      "epoch: 9 | 95328 / 114272 | training loss: 0.0003758984094019979\n",
      "epoch: 9 | 95360 / 114272 | training loss: 0.00023657880956307054\n",
      "epoch: 9 | 95392 / 114272 | training loss: 0.00014618152636103332\n",
      "epoch: 9 | 95424 / 114272 | training loss: 0.0015899111749604344\n",
      "epoch: 9 | 95456 / 114272 | training loss: 0.0008745079394429922\n",
      "epoch: 9 | 95488 / 114272 | training loss: 0.00030422478448599577\n",
      "epoch: 9 | 95520 / 114272 | training loss: 0.00021631621348205954\n",
      "epoch: 9 | 95552 / 114272 | training loss: 0.00038951923488639295\n",
      "epoch: 9 | 95584 / 114272 | training loss: 0.0003984669747296721\n",
      "epoch: 9 | 95616 / 114272 | training loss: 0.003708576550707221\n",
      "epoch: 9 | 95648 / 114272 | training loss: 0.0004060366773046553\n",
      "epoch: 9 | 95680 / 114272 | training loss: 0.0001473253796575591\n",
      "epoch: 9 | 95712 / 114272 | training loss: 0.00027663257787935436\n",
      "epoch: 9 | 95744 / 114272 | training loss: 0.0003527411026880145\n",
      "epoch: 9 | 95776 / 114272 | training loss: 0.00026556660304777324\n",
      "epoch: 9 | 95808 / 114272 | training loss: 0.21791556477546692\n",
      "epoch: 9 | 95840 / 114272 | training loss: 0.003702831920236349\n",
      "epoch: 9 | 95872 / 114272 | training loss: 0.00020347103418316692\n",
      "epoch: 9 | 95904 / 114272 | training loss: 0.0023311388213187456\n",
      "epoch: 9 | 95936 / 114272 | training loss: 0.10269812494516373\n",
      "epoch: 9 | 95968 / 114272 | training loss: 0.00018733227625489235\n",
      "epoch: 9 | 96000 / 114272 | training loss: 0.00028172985184937716\n",
      "epoch: 9 | 96032 / 114272 | training loss: 0.1975916475057602\n",
      "epoch: 9 | 96064 / 114272 | training loss: 0.0002459027455188334\n",
      "epoch: 9 | 96096 / 114272 | training loss: 0.00029273671680130064\n",
      "epoch: 9 | 96128 / 114272 | training loss: 0.00020397178013809025\n",
      "epoch: 9 | 96160 / 114272 | training loss: 0.00021077049314044416\n",
      "epoch: 9 | 96192 / 114272 | training loss: 0.00021691311849281192\n",
      "epoch: 9 | 96224 / 114272 | training loss: 0.00022367871133610606\n",
      "epoch: 9 | 96256 / 114272 | training loss: 0.0002801378141157329\n",
      "epoch: 9 | 96288 / 114272 | training loss: 0.0002006103895837441\n",
      "epoch: 9 | 96320 / 114272 | training loss: 0.00024303694954141974\n",
      "epoch: 9 | 96352 / 114272 | training loss: 0.0022497756872326136\n",
      "epoch: 9 | 96384 / 114272 | training loss: 0.00021083139290567487\n",
      "epoch: 9 | 96416 / 114272 | training loss: 0.22154897451400757\n",
      "epoch: 9 | 96448 / 114272 | training loss: 0.00032102203113026917\n",
      "epoch: 9 | 96480 / 114272 | training loss: 0.0003105415671598166\n",
      "epoch: 9 | 96512 / 114272 | training loss: 0.00012730195885524154\n",
      "epoch: 9 | 96544 / 114272 | training loss: 0.00027301799855194986\n",
      "epoch: 9 | 96576 / 114272 | training loss: 0.00019609388255048543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 96608 / 114272 | training loss: 0.00019755672838073224\n",
      "epoch: 9 | 96640 / 114272 | training loss: 0.00035154353827238083\n",
      "epoch: 9 | 96672 / 114272 | training loss: 0.0001498016354162246\n",
      "epoch: 9 | 96704 / 114272 | training loss: 0.0002960132551379502\n",
      "epoch: 9 | 96736 / 114272 | training loss: 0.0001781187456799671\n",
      "epoch: 9 | 96768 / 114272 | training loss: 0.00020791472343262285\n",
      "epoch: 9 | 96800 / 114272 | training loss: 0.00022598670329898596\n",
      "epoch: 9 | 96832 / 114272 | training loss: 0.000167923019034788\n",
      "epoch: 9 | 96864 / 114272 | training loss: 0.00033360670204274356\n",
      "epoch: 9 | 96896 / 114272 | training loss: 0.00016143254470080137\n",
      "epoch: 9 | 96928 / 114272 | training loss: 0.0001794427225831896\n",
      "epoch: 9 | 96960 / 114272 | training loss: 0.00014704169007018209\n",
      "epoch: 9 | 96992 / 114272 | training loss: 0.0001783357438398525\n",
      "epoch: 9 | 97024 / 114272 | training loss: 0.00019014911958947778\n",
      "epoch: 9 | 97056 / 114272 | training loss: 0.0008447971194982529\n",
      "epoch: 9 | 97088 / 114272 | training loss: 0.0003013857640326023\n",
      "epoch: 9 | 97120 / 114272 | training loss: 0.00014268771337810904\n",
      "epoch: 9 | 97152 / 114272 | training loss: 0.00027778587536886334\n",
      "epoch: 9 | 97184 / 114272 | training loss: 0.00021466761245392263\n",
      "epoch: 9 | 97216 / 114272 | training loss: 0.00026216957485303283\n",
      "epoch: 9 | 97248 / 114272 | training loss: 0.00023881385277491063\n",
      "epoch: 9 | 97280 / 114272 | training loss: 0.0677463635802269\n",
      "epoch: 9 | 97312 / 114272 | training loss: 0.00022918963804841042\n",
      "epoch: 9 | 97344 / 114272 | training loss: 0.00021987671789247543\n",
      "epoch: 9 | 97376 / 114272 | training loss: 0.00021127464424353093\n",
      "epoch: 9 | 97408 / 114272 | training loss: 0.00028972121072001755\n",
      "epoch: 9 | 97440 / 114272 | training loss: 0.00032827616087161005\n",
      "epoch: 9 | 97472 / 114272 | training loss: 0.05110088363289833\n",
      "epoch: 9 | 97504 / 114272 | training loss: 0.0005448395968414843\n",
      "epoch: 9 | 97536 / 114272 | training loss: 0.0004439733747858554\n",
      "epoch: 9 | 97568 / 114272 | training loss: 0.00023089231399353594\n",
      "epoch: 9 | 97600 / 114272 | training loss: 0.00041788641829043627\n",
      "epoch: 9 | 97632 / 114272 | training loss: 0.0005860723904334009\n",
      "epoch: 9 | 97664 / 114272 | training loss: 0.00015628797700628638\n",
      "epoch: 9 | 97696 / 114272 | training loss: 0.0003530719841364771\n",
      "epoch: 9 | 97728 / 114272 | training loss: 0.0002549553755670786\n",
      "epoch: 9 | 97760 / 114272 | training loss: 0.00028279260732233524\n",
      "epoch: 9 | 97792 / 114272 | training loss: 0.00035568204475566745\n",
      "epoch: 9 | 97824 / 114272 | training loss: 0.00021107627253513783\n",
      "epoch: 9 | 97856 / 114272 | training loss: 0.0003414322272874415\n",
      "epoch: 9 | 97888 / 114272 | training loss: 0.0003521677863318473\n",
      "epoch: 9 | 97920 / 114272 | training loss: 0.00020191345538478345\n",
      "epoch: 9 | 97952 / 114272 | training loss: 0.0004207799502182752\n",
      "epoch: 9 | 97984 / 114272 | training loss: 0.00016824417980387807\n",
      "epoch: 9 | 98016 / 114272 | training loss: 0.0003471236559562385\n",
      "epoch: 9 | 98048 / 114272 | training loss: 0.0002937319513875991\n",
      "epoch: 9 | 98080 / 114272 | training loss: 0.0002644270134624094\n",
      "epoch: 9 | 98112 / 114272 | training loss: 0.0002547191979829222\n",
      "epoch: 9 | 98144 / 114272 | training loss: 0.0003362587885931134\n",
      "epoch: 9 | 98176 / 114272 | training loss: 0.0002519435074646026\n",
      "epoch: 9 | 98208 / 114272 | training loss: 0.00018652270955499262\n",
      "epoch: 9 | 98240 / 114272 | training loss: 0.0001511777809355408\n",
      "epoch: 9 | 98272 / 114272 | training loss: 0.004195319022983313\n",
      "epoch: 9 | 98304 / 114272 | training loss: 0.00021951590315438807\n",
      "epoch: 9 | 98336 / 114272 | training loss: 0.00015645210805814713\n",
      "epoch: 9 | 98368 / 114272 | training loss: 0.00035551542532630265\n",
      "epoch: 9 | 98400 / 114272 | training loss: 0.00017581514839548618\n",
      "epoch: 9 | 98432 / 114272 | training loss: 0.0014502581907436252\n",
      "epoch: 9 | 98464 / 114272 | training loss: 0.00019233021885156631\n",
      "epoch: 9 | 98496 / 114272 | training loss: 0.00037523702485486865\n",
      "epoch: 9 | 98528 / 114272 | training loss: 0.00034308357862755656\n",
      "epoch: 9 | 98560 / 114272 | training loss: 0.0002870689786504954\n",
      "epoch: 9 | 98592 / 114272 | training loss: 0.00033930971403606236\n",
      "epoch: 9 | 98624 / 114272 | training loss: 0.0006295411731116474\n",
      "epoch: 9 | 98656 / 114272 | training loss: 0.0003004166064783931\n",
      "epoch: 9 | 98688 / 114272 | training loss: 0.00018016381363850087\n",
      "epoch: 9 | 98720 / 114272 | training loss: 0.0002227314398624003\n",
      "epoch: 9 | 98752 / 114272 | training loss: 0.0002758353657554835\n",
      "epoch: 9 | 98784 / 114272 | training loss: 0.00024972358369268477\n",
      "epoch: 9 | 98816 / 114272 | training loss: 0.0003013618115801364\n",
      "epoch: 9 | 98848 / 114272 | training loss: 0.00028883942286483943\n",
      "epoch: 9 | 98880 / 114272 | training loss: 0.019072121009230614\n",
      "epoch: 9 | 98912 / 114272 | training loss: 0.00011537985847098753\n",
      "epoch: 9 | 98944 / 114272 | training loss: 0.0002614585682749748\n",
      "epoch: 9 | 98976 / 114272 | training loss: 0.00020758784376084805\n",
      "epoch: 9 | 99008 / 114272 | training loss: 0.0021661340724676847\n",
      "epoch: 9 | 99040 / 114272 | training loss: 0.0002562559675425291\n",
      "epoch: 9 | 99072 / 114272 | training loss: 0.00020775031589437276\n",
      "epoch: 9 | 99104 / 114272 | training loss: 0.00035883020609617233\n",
      "epoch: 9 | 99136 / 114272 | training loss: 0.0003266790590714663\n",
      "epoch: 9 | 99168 / 114272 | training loss: 0.00027964403852820396\n",
      "epoch: 9 | 99200 / 114272 | training loss: 0.0002839948865585029\n",
      "epoch: 9 | 99232 / 114272 | training loss: 0.0007566002896055579\n",
      "epoch: 9 | 99264 / 114272 | training loss: 0.00030966909253038466\n",
      "epoch: 9 | 99296 / 114272 | training loss: 0.019647644832730293\n",
      "epoch: 9 | 99328 / 114272 | training loss: 0.00046034491970203817\n",
      "epoch: 9 | 99360 / 114272 | training loss: 0.00047630295739509165\n",
      "epoch: 9 | 99392 / 114272 | training loss: 0.0021433811634778976\n",
      "epoch: 9 | 99424 / 114272 | training loss: 0.0001621318224351853\n",
      "epoch: 9 | 99456 / 114272 | training loss: 0.00015956925926730037\n",
      "epoch: 9 | 99488 / 114272 | training loss: 0.0002828989236149937\n",
      "epoch: 9 | 99520 / 114272 | training loss: 0.0003440144646447152\n",
      "epoch: 9 | 99552 / 114272 | training loss: 0.00020895410852972418\n",
      "epoch: 9 | 99584 / 114272 | training loss: 0.00015379753313027322\n",
      "epoch: 9 | 99616 / 114272 | training loss: 0.00038205774035304785\n",
      "epoch: 9 | 99648 / 114272 | training loss: 0.0005184725159779191\n",
      "epoch: 9 | 99680 / 114272 | training loss: 0.00020939567184541374\n",
      "epoch: 9 | 99712 / 114272 | training loss: 0.00033200831967405975\n",
      "epoch: 9 | 99744 / 114272 | training loss: 0.00018977765284944326\n",
      "epoch: 9 | 99776 / 114272 | training loss: 0.0030119295697659254\n",
      "epoch: 9 | 99808 / 114272 | training loss: 0.08123321831226349\n",
      "epoch: 9 | 99840 / 114272 | training loss: 0.000344241940183565\n",
      "epoch: 9 | 99872 / 114272 | training loss: 0.00028880828176625073\n",
      "epoch: 9 | 99904 / 114272 | training loss: 0.0002589622454252094\n",
      "epoch: 9 | 99936 / 114272 | training loss: 0.00012347257870715111\n",
      "epoch: 9 | 99968 / 114272 | training loss: 0.00029644870664924383\n",
      "epoch: 9 | 100000 / 114272 | training loss: 0.0001699017157079652\n",
      "epoch: 9 | 100032 / 114272 | training loss: 0.00025906928931362927\n",
      "epoch: 9 | 100064 / 114272 | training loss: 0.00027127854991704226\n",
      "epoch: 9 | 100096 / 114272 | training loss: 0.00020187800691928715\n",
      "epoch: 9 | 100128 / 114272 | training loss: 0.00014920101966708899\n",
      "epoch: 9 | 100160 / 114272 | training loss: 0.0003918758302461356\n",
      "epoch: 9 | 100192 / 114272 | training loss: 0.0004736661503557116\n",
      "epoch: 9 | 100224 / 114272 | training loss: 0.00012564803182613105\n",
      "epoch: 9 | 100256 / 114272 | training loss: 0.00024541321909055114\n",
      "epoch: 9 | 100288 / 114272 | training loss: 0.0002866734575945884\n",
      "epoch: 9 | 100320 / 114272 | training loss: 0.13403286039829254\n",
      "epoch: 9 | 100352 / 114272 | training loss: 0.0003826383617706597\n",
      "epoch: 9 | 100384 / 114272 | training loss: 0.0003390778147149831\n",
      "epoch: 9 | 100416 / 114272 | training loss: 0.00021563311747740954\n",
      "epoch: 9 | 100448 / 114272 | training loss: 0.00019695419177878648\n",
      "epoch: 9 | 100480 / 114272 | training loss: 0.00037693881313316524\n",
      "epoch: 9 | 100512 / 114272 | training loss: 0.0003386816824786365\n",
      "epoch: 9 | 100544 / 114272 | training loss: 0.0002316357713425532\n",
      "epoch: 9 | 100576 / 114272 | training loss: 0.00019541243091225624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 100608 / 114272 | training loss: 0.00020610571664292365\n",
      "epoch: 9 | 100640 / 114272 | training loss: 0.0002681559126358479\n",
      "epoch: 9 | 100672 / 114272 | training loss: 0.00021682190708816051\n",
      "epoch: 9 | 100704 / 114272 | training loss: 0.00024593810667283833\n",
      "epoch: 9 | 100736 / 114272 | training loss: 0.00034331163624301553\n",
      "epoch: 9 | 100768 / 114272 | training loss: 0.0003329865576233715\n",
      "epoch: 9 | 100800 / 114272 | training loss: 0.00033815353526733816\n",
      "epoch: 9 | 100832 / 114272 | training loss: 0.00017649766232352704\n",
      "epoch: 9 | 100864 / 114272 | training loss: 0.00034067584783770144\n",
      "epoch: 9 | 100896 / 114272 | training loss: 0.0005529872141778469\n",
      "epoch: 9 | 100928 / 114272 | training loss: 0.00029600420384667814\n",
      "epoch: 9 | 100960 / 114272 | training loss: 0.00031420844607055187\n",
      "epoch: 9 | 100992 / 114272 | training loss: 0.04493929445743561\n",
      "epoch: 9 | 101024 / 114272 | training loss: 0.0003735843929462135\n",
      "epoch: 9 | 101056 / 114272 | training loss: 0.00019957723270636052\n",
      "epoch: 9 | 101088 / 114272 | training loss: 0.00021446177561301738\n",
      "epoch: 9 | 101120 / 114272 | training loss: 0.0008725385996513069\n",
      "epoch: 9 | 101152 / 114272 | training loss: 0.0001618604437680915\n",
      "epoch: 9 | 101184 / 114272 | training loss: 0.00012322563270572573\n",
      "epoch: 9 | 101216 / 114272 | training loss: 0.0002755862951744348\n",
      "epoch: 9 | 101248 / 114272 | training loss: 0.0019272572826594114\n",
      "epoch: 9 | 101280 / 114272 | training loss: 0.00027006148593500257\n",
      "epoch: 9 | 101312 / 114272 | training loss: 0.002639987738803029\n",
      "epoch: 9 | 101344 / 114272 | training loss: 0.0003239590732846409\n",
      "epoch: 9 | 101376 / 114272 | training loss: 0.00036312517477199435\n",
      "epoch: 9 | 101408 / 114272 | training loss: 0.00023479598166886717\n",
      "epoch: 9 | 101440 / 114272 | training loss: 0.00038224802119657397\n",
      "epoch: 9 | 101472 / 114272 | training loss: 0.00014814305177424103\n",
      "epoch: 9 | 101504 / 114272 | training loss: 9.313242480857298e-05\n",
      "epoch: 9 | 101536 / 114272 | training loss: 0.00018439575796946883\n",
      "epoch: 9 | 101568 / 114272 | training loss: 0.00034786423202604055\n",
      "epoch: 9 | 101600 / 114272 | training loss: 0.0003360722039360553\n",
      "epoch: 9 | 101632 / 114272 | training loss: 0.00019253815116826445\n",
      "epoch: 9 | 101664 / 114272 | training loss: 0.0003042945172637701\n",
      "epoch: 9 | 101696 / 114272 | training loss: 0.00024414461222477257\n",
      "epoch: 9 | 101728 / 114272 | training loss: 0.00018823804566636682\n",
      "epoch: 9 | 101760 / 114272 | training loss: 0.00034261320251971483\n",
      "epoch: 9 | 101792 / 114272 | training loss: 0.00024128278892021626\n",
      "epoch: 9 | 101824 / 114272 | training loss: 0.23204748332500458\n",
      "epoch: 9 | 101856 / 114272 | training loss: 0.023279769346117973\n",
      "epoch: 9 | 101888 / 114272 | training loss: 0.0002783348027151078\n",
      "epoch: 9 | 101920 / 114272 | training loss: 0.0004056283796671778\n",
      "epoch: 9 | 101952 / 114272 | training loss: 0.00011496595834614709\n",
      "epoch: 9 | 101984 / 114272 | training loss: 0.018548564985394478\n",
      "epoch: 9 | 102016 / 114272 | training loss: 0.00028804270550608635\n",
      "epoch: 9 | 102048 / 114272 | training loss: 0.0005617828574031591\n",
      "epoch: 9 | 102080 / 114272 | training loss: 0.16895584762096405\n",
      "epoch: 9 | 102112 / 114272 | training loss: 0.00033549871295690536\n",
      "epoch: 9 | 102144 / 114272 | training loss: 0.00023094822245184332\n",
      "epoch: 9 | 102176 / 114272 | training loss: 0.0002826759882736951\n",
      "epoch: 9 | 102208 / 114272 | training loss: 0.0002681089681573212\n",
      "epoch: 9 | 102240 / 114272 | training loss: 0.00039984792238101363\n",
      "epoch: 9 | 102272 / 114272 | training loss: 0.00019718549447134137\n",
      "epoch: 9 | 102304 / 114272 | training loss: 0.018521880730986595\n",
      "epoch: 9 | 102336 / 114272 | training loss: 0.0003129517426714301\n",
      "epoch: 9 | 102368 / 114272 | training loss: 0.00013584877888206393\n",
      "epoch: 9 | 102400 / 114272 | training loss: 0.00025966722751036286\n",
      "epoch: 9 | 102432 / 114272 | training loss: 0.00038964557461440563\n",
      "epoch: 9 | 102464 / 114272 | training loss: 0.00016018403402995318\n",
      "epoch: 9 | 102496 / 114272 | training loss: 0.0003006345359608531\n",
      "epoch: 9 | 102528 / 114272 | training loss: 0.00033531259396113455\n",
      "epoch: 9 | 102560 / 114272 | training loss: 0.00025256231310777366\n",
      "epoch: 9 | 102592 / 114272 | training loss: 0.0002577824052423239\n",
      "epoch: 9 | 102624 / 114272 | training loss: 0.00016097720072139055\n",
      "epoch: 9 | 102656 / 114272 | training loss: 0.0003539240569807589\n",
      "epoch: 9 | 102688 / 114272 | training loss: 0.00021335390920285136\n",
      "epoch: 9 | 102720 / 114272 | training loss: 0.0002776091860141605\n",
      "epoch: 9 | 102752 / 114272 | training loss: 0.00019583276298362762\n",
      "epoch: 9 | 102784 / 114272 | training loss: 0.0002939021505881101\n",
      "epoch: 9 | 102816 / 114272 | training loss: 0.0002134869573637843\n",
      "epoch: 9 | 102848 / 114272 | training loss: 0.009024564176797867\n",
      "epoch: 9 | 102880 / 114272 | training loss: 0.00016175585915334523\n",
      "epoch: 9 | 102912 / 114272 | training loss: 0.00022891398111823946\n",
      "epoch: 9 | 102944 / 114272 | training loss: 0.00011799552157754079\n",
      "epoch: 9 | 102976 / 114272 | training loss: 0.00036646760418079793\n",
      "epoch: 9 | 103008 / 114272 | training loss: 0.029467232525348663\n",
      "epoch: 9 | 103040 / 114272 | training loss: 0.00018236305913887918\n",
      "epoch: 9 | 103072 / 114272 | training loss: 0.00019021681509912014\n",
      "epoch: 9 | 103104 / 114272 | training loss: 0.00017978578398469836\n",
      "epoch: 9 | 103136 / 114272 | training loss: 0.000231479003559798\n",
      "epoch: 9 | 103168 / 114272 | training loss: 0.00017652973474469036\n",
      "epoch: 9 | 103200 / 114272 | training loss: 0.00016701928689144552\n",
      "epoch: 9 | 103232 / 114272 | training loss: 0.00022920726041775197\n",
      "epoch: 9 | 103264 / 114272 | training loss: 9.479646541876718e-05\n",
      "epoch: 9 | 103296 / 114272 | training loss: 0.0003386182361282408\n",
      "epoch: 9 | 103328 / 114272 | training loss: 0.00015901957522146404\n",
      "epoch: 9 | 103360 / 114272 | training loss: 0.00016977149061858654\n",
      "epoch: 9 | 103392 / 114272 | training loss: 0.0007984624244272709\n",
      "epoch: 9 | 103424 / 114272 | training loss: 0.0002583843597676605\n",
      "epoch: 9 | 103456 / 114272 | training loss: 0.0002661505131982267\n",
      "epoch: 9 | 103488 / 114272 | training loss: 0.0002989775675814599\n",
      "epoch: 9 | 103520 / 114272 | training loss: 0.0002851109893526882\n",
      "epoch: 9 | 103552 / 114272 | training loss: 0.0002970981877297163\n",
      "epoch: 9 | 103584 / 114272 | training loss: 0.00027602032059803605\n",
      "epoch: 9 | 103616 / 114272 | training loss: 0.00021840675617568195\n",
      "epoch: 9 | 103648 / 114272 | training loss: 0.00013373543333727866\n",
      "epoch: 9 | 103680 / 114272 | training loss: 0.00016753679665271193\n",
      "epoch: 9 | 103712 / 114272 | training loss: 0.0004157346556894481\n",
      "epoch: 9 | 103744 / 114272 | training loss: 0.00028538951301015913\n",
      "epoch: 9 | 103776 / 114272 | training loss: 0.00011627702042460442\n",
      "epoch: 9 | 103808 / 114272 | training loss: 0.00025842664763331413\n",
      "epoch: 9 | 103840 / 114272 | training loss: 0.00030741270165890455\n",
      "epoch: 9 | 103872 / 114272 | training loss: 0.0003569415712263435\n",
      "epoch: 9 | 103904 / 114272 | training loss: 0.0004144723934587091\n",
      "epoch: 9 | 103936 / 114272 | training loss: 0.00011189733049832284\n",
      "epoch: 9 | 103968 / 114272 | training loss: 0.00023342047643382102\n",
      "epoch: 9 | 104000 / 114272 | training loss: 0.00028958270559087396\n",
      "epoch: 9 | 104032 / 114272 | training loss: 0.0034904470667243004\n",
      "epoch: 9 | 104064 / 114272 | training loss: 0.0003858503478113562\n",
      "epoch: 9 | 104096 / 114272 | training loss: 0.0001280560827581212\n",
      "epoch: 9 | 104128 / 114272 | training loss: 0.00022202472609933466\n",
      "epoch: 9 | 104160 / 114272 | training loss: 0.00022110041754785925\n",
      "epoch: 9 | 104192 / 114272 | training loss: 0.00033319482463411987\n",
      "epoch: 9 | 104224 / 114272 | training loss: 0.00020418509666342288\n",
      "epoch: 9 | 104256 / 114272 | training loss: 0.0002532910439185798\n",
      "epoch: 9 | 104288 / 114272 | training loss: 0.00017028639558702707\n",
      "epoch: 9 | 104320 / 114272 | training loss: 0.00024033032241277397\n",
      "epoch: 9 | 104352 / 114272 | training loss: 0.00024296889023389667\n",
      "epoch: 9 | 104384 / 114272 | training loss: 0.00039746618131175637\n",
      "epoch: 9 | 104416 / 114272 | training loss: 0.028983939439058304\n",
      "epoch: 9 | 104448 / 114272 | training loss: 0.00018563929188530892\n",
      "epoch: 9 | 104480 / 114272 | training loss: 0.002672056434676051\n",
      "epoch: 9 | 104512 / 114272 | training loss: 0.00024457176914438605\n",
      "epoch: 9 | 104544 / 114272 | training loss: 0.0003029284125659615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 104576 / 114272 | training loss: 0.0007496562320739031\n",
      "epoch: 9 | 104608 / 114272 | training loss: 0.000271473458269611\n",
      "epoch: 9 | 104640 / 114272 | training loss: 0.0002926049055531621\n",
      "epoch: 9 | 104672 / 114272 | training loss: 0.00017552031204104424\n",
      "epoch: 9 | 104704 / 114272 | training loss: 0.00033057431573979557\n",
      "epoch: 9 | 104736 / 114272 | training loss: 0.00034957504249177873\n",
      "epoch: 9 | 104768 / 114272 | training loss: 0.00016145185509230942\n",
      "epoch: 9 | 104800 / 114272 | training loss: 0.0003427349729463458\n",
      "epoch: 9 | 104832 / 114272 | training loss: 0.000406235340051353\n",
      "epoch: 9 | 104864 / 114272 | training loss: 0.0003647780977189541\n",
      "epoch: 9 | 104896 / 114272 | training loss: 0.0002648359222803265\n",
      "epoch: 9 | 104928 / 114272 | training loss: 0.00048097700346261263\n",
      "epoch: 9 | 104960 / 114272 | training loss: 0.00038756290450692177\n",
      "epoch: 9 | 104992 / 114272 | training loss: 0.00030524691101163626\n",
      "epoch: 9 | 105024 / 114272 | training loss: 0.00037192562012933195\n",
      "epoch: 9 | 105056 / 114272 | training loss: 0.0002815038606058806\n",
      "epoch: 9 | 105088 / 114272 | training loss: 0.0001979434018721804\n",
      "epoch: 9 | 105120 / 114272 | training loss: 0.0003358520334586501\n",
      "epoch: 9 | 105152 / 114272 | training loss: 0.00022901796910446137\n",
      "epoch: 9 | 105184 / 114272 | training loss: 0.00024243915686383843\n",
      "epoch: 9 | 105216 / 114272 | training loss: 0.00031902859336696565\n",
      "epoch: 9 | 105248 / 114272 | training loss: 0.00023021767265163362\n",
      "epoch: 9 | 105280 / 114272 | training loss: 0.00014490667672362179\n",
      "epoch: 9 | 105312 / 114272 | training loss: 0.07637430727481842\n",
      "epoch: 9 | 105344 / 114272 | training loss: 0.0003806004242505878\n",
      "epoch: 9 | 105376 / 114272 | training loss: 0.085957832634449\n",
      "epoch: 9 | 105408 / 114272 | training loss: 0.00019390655506867915\n",
      "epoch: 9 | 105440 / 114272 | training loss: 0.01134746428579092\n",
      "epoch: 9 | 105472 / 114272 | training loss: 0.00025279459077864885\n",
      "epoch: 9 | 105504 / 114272 | training loss: 0.00029057252686470747\n",
      "epoch: 9 | 105536 / 114272 | training loss: 0.00017917074728757143\n",
      "epoch: 9 | 105568 / 114272 | training loss: 0.0003380005364306271\n",
      "epoch: 9 | 105600 / 114272 | training loss: 0.00016034345026127994\n",
      "epoch: 9 | 105632 / 114272 | training loss: 0.0003190208226442337\n",
      "epoch: 9 | 105664 / 114272 | training loss: 0.0012522044125944376\n",
      "epoch: 9 | 105696 / 114272 | training loss: 0.0015394556103274226\n",
      "epoch: 9 | 105728 / 114272 | training loss: 0.00038748409133404493\n",
      "epoch: 9 | 105760 / 114272 | training loss: 0.00026684446493163705\n",
      "epoch: 9 | 105792 / 114272 | training loss: 0.000318467355100438\n",
      "epoch: 9 | 105824 / 114272 | training loss: 0.00021264638053253293\n",
      "epoch: 9 | 105856 / 114272 | training loss: 0.00014682630717288703\n",
      "epoch: 9 | 105888 / 114272 | training loss: 0.0001683813170529902\n",
      "epoch: 9 | 105920 / 114272 | training loss: 0.08870584517717361\n",
      "epoch: 9 | 105952 / 114272 | training loss: 0.20282895863056183\n",
      "epoch: 9 | 105984 / 114272 | training loss: 0.0002328718255739659\n",
      "epoch: 9 | 106016 / 114272 | training loss: 0.0004047396359965205\n",
      "epoch: 9 | 106048 / 114272 | training loss: 0.0002715285518206656\n",
      "epoch: 9 | 106080 / 114272 | training loss: 0.0002003842091653496\n",
      "epoch: 9 | 106112 / 114272 | training loss: 0.00016400236927438527\n",
      "epoch: 9 | 106144 / 114272 | training loss: 0.0025119036436080933\n",
      "epoch: 9 | 106176 / 114272 | training loss: 0.0001482246007071808\n",
      "epoch: 9 | 106208 / 114272 | training loss: 0.0003068102232646197\n",
      "epoch: 9 | 106240 / 114272 | training loss: 0.00042529794154688716\n",
      "epoch: 9 | 106272 / 114272 | training loss: 0.0003381486458238214\n",
      "epoch: 9 | 106304 / 114272 | training loss: 0.0002423982514301315\n",
      "epoch: 9 | 106336 / 114272 | training loss: 0.000151668515172787\n",
      "epoch: 9 | 106368 / 114272 | training loss: 0.00040562954382039607\n",
      "epoch: 9 | 106400 / 114272 | training loss: 0.21390391886234283\n",
      "epoch: 9 | 106432 / 114272 | training loss: 0.0004482655494939536\n",
      "epoch: 9 | 106464 / 114272 | training loss: 0.000282584922388196\n",
      "epoch: 9 | 106496 / 114272 | training loss: 0.01837438903748989\n",
      "epoch: 9 | 106528 / 114272 | training loss: 0.0002047018933808431\n",
      "epoch: 9 | 106560 / 114272 | training loss: 0.00016516655159648508\n",
      "epoch: 9 | 106592 / 114272 | training loss: 0.0002484632714185864\n",
      "epoch: 9 | 106624 / 114272 | training loss: 0.00023524976859334856\n",
      "epoch: 9 | 106656 / 114272 | training loss: 0.00039566788473166525\n",
      "epoch: 9 | 106688 / 114272 | training loss: 0.00030203087953850627\n",
      "epoch: 9 | 106720 / 114272 | training loss: 0.0002275490405736491\n",
      "epoch: 9 | 106752 / 114272 | training loss: 0.00028056648443453014\n",
      "epoch: 9 | 106784 / 114272 | training loss: 0.00024027899780776352\n",
      "epoch: 9 | 106816 / 114272 | training loss: 0.00039109186036512256\n",
      "epoch: 9 | 106848 / 114272 | training loss: 0.00020308331295382231\n",
      "epoch: 9 | 106880 / 114272 | training loss: 0.00016135665646288544\n",
      "epoch: 9 | 106912 / 114272 | training loss: 0.00035721814492717385\n",
      "epoch: 9 | 106944 / 114272 | training loss: 0.00020519914687611163\n",
      "epoch: 9 | 106976 / 114272 | training loss: 0.0002726924722082913\n",
      "epoch: 9 | 107008 / 114272 | training loss: 0.0002434519410599023\n",
      "epoch: 9 | 107040 / 114272 | training loss: 0.0002759240160230547\n",
      "epoch: 9 | 107072 / 114272 | training loss: 0.00017735312576405704\n",
      "epoch: 9 | 107104 / 114272 | training loss: 0.00019813328981399536\n",
      "epoch: 9 | 107136 / 114272 | training loss: 0.00031970624695532024\n",
      "epoch: 9 | 107168 / 114272 | training loss: 0.00028736324748024344\n",
      "epoch: 9 | 107200 / 114272 | training loss: 0.0003081209142692387\n",
      "epoch: 9 | 107232 / 114272 | training loss: 0.00020911815227009356\n",
      "epoch: 9 | 107264 / 114272 | training loss: 0.006376070901751518\n",
      "epoch: 9 | 107296 / 114272 | training loss: 0.00019457026792224497\n",
      "epoch: 9 | 107328 / 114272 | training loss: 0.00030789594165980816\n",
      "epoch: 9 | 107360 / 114272 | training loss: 0.000173664084286429\n",
      "epoch: 9 | 107392 / 114272 | training loss: 0.0003314845962449908\n",
      "epoch: 9 | 107424 / 114272 | training loss: 0.0002549114578869194\n",
      "epoch: 9 | 107456 / 114272 | training loss: 0.003312760964035988\n",
      "epoch: 9 | 107488 / 114272 | training loss: 0.00016855564899742603\n",
      "epoch: 9 | 107520 / 114272 | training loss: 0.00019073233124800026\n",
      "epoch: 9 | 107552 / 114272 | training loss: 0.04348834604024887\n",
      "epoch: 9 | 107584 / 114272 | training loss: 0.0001830725377658382\n",
      "epoch: 9 | 107616 / 114272 | training loss: 0.06724654883146286\n",
      "epoch: 9 | 107648 / 114272 | training loss: 0.00025574571918696165\n",
      "epoch: 9 | 107680 / 114272 | training loss: 0.0002324332162970677\n",
      "epoch: 9 | 107712 / 114272 | training loss: 0.0002338290069019422\n",
      "epoch: 9 | 107744 / 114272 | training loss: 0.0006458313437178731\n",
      "epoch: 9 | 107776 / 114272 | training loss: 0.00019421307661104947\n",
      "epoch: 9 | 107808 / 114272 | training loss: 0.00018065185577142984\n",
      "epoch: 9 | 107840 / 114272 | training loss: 0.0002764759701676667\n",
      "epoch: 9 | 107872 / 114272 | training loss: 0.00029775232542306185\n",
      "epoch: 9 | 107904 / 114272 | training loss: 0.00023814001178834587\n",
      "epoch: 9 | 107936 / 114272 | training loss: 0.0002971575886476785\n",
      "epoch: 9 | 107968 / 114272 | training loss: 0.0002842747780960053\n",
      "epoch: 9 | 108000 / 114272 | training loss: 0.00036534894024953246\n",
      "epoch: 9 | 108032 / 114272 | training loss: 0.00010833807027665898\n",
      "epoch: 9 | 108064 / 114272 | training loss: 0.00028854457195848227\n",
      "epoch: 9 | 108096 / 114272 | training loss: 0.00023339757171925157\n",
      "epoch: 9 | 108128 / 114272 | training loss: 0.00025521632051095366\n",
      "epoch: 9 | 108160 / 114272 | training loss: 9.044789476320148e-05\n",
      "epoch: 9 | 108192 / 114272 | training loss: 0.0009628020925447345\n",
      "epoch: 9 | 108224 / 114272 | training loss: 0.00020865602709818631\n",
      "epoch: 9 | 108256 / 114272 | training loss: 0.0002268994867336005\n",
      "epoch: 9 | 108288 / 114272 | training loss: 0.026884900406003\n",
      "epoch: 9 | 108320 / 114272 | training loss: 0.00015565557987429202\n",
      "epoch: 9 | 108352 / 114272 | training loss: 0.00028740643756464124\n",
      "epoch: 9 | 108384 / 114272 | training loss: 0.0003069917729590088\n",
      "epoch: 9 | 108416 / 114272 | training loss: 0.00016951540601439774\n",
      "epoch: 9 | 108448 / 114272 | training loss: 0.0003240067744627595\n",
      "epoch: 9 | 108480 / 114272 | training loss: 0.00038599505205638707\n",
      "epoch: 9 | 108512 / 114272 | training loss: 0.0002756070753093809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 108544 / 114272 | training loss: 0.0002951134811155498\n",
      "epoch: 9 | 108576 / 114272 | training loss: 0.0010036814492195845\n",
      "epoch: 9 | 108608 / 114272 | training loss: 0.00037739932304248214\n",
      "epoch: 9 | 108640 / 114272 | training loss: 0.00016998614592012018\n",
      "epoch: 9 | 108672 / 114272 | training loss: 0.00023786928795743734\n",
      "epoch: 9 | 108704 / 114272 | training loss: 0.00038594959187321365\n",
      "epoch: 9 | 108736 / 114272 | training loss: 0.000266571732936427\n",
      "epoch: 9 | 108768 / 114272 | training loss: 0.000866134709212929\n",
      "epoch: 9 | 108800 / 114272 | training loss: 0.00012777384836226702\n",
      "epoch: 9 | 108832 / 114272 | training loss: 0.0003114979772362858\n",
      "epoch: 9 | 108864 / 114272 | training loss: 0.00016873623826541007\n",
      "epoch: 9 | 108896 / 114272 | training loss: 0.00014379600179381669\n",
      "epoch: 9 | 108928 / 114272 | training loss: 0.0002700325276236981\n",
      "epoch: 9 | 108960 / 114272 | training loss: 0.0005457107326947153\n",
      "epoch: 9 | 108992 / 114272 | training loss: 0.0002511127386242151\n",
      "epoch: 9 | 109024 / 114272 | training loss: 0.00017422990640625358\n",
      "epoch: 9 | 109056 / 114272 | training loss: 0.00039940528222359717\n",
      "epoch: 9 | 109088 / 114272 | training loss: 0.00021573204139713198\n",
      "epoch: 9 | 109120 / 114272 | training loss: 0.0003396505198907107\n",
      "epoch: 9 | 109152 / 114272 | training loss: 0.0003599101328290999\n",
      "epoch: 9 | 109184 / 114272 | training loss: 0.00028172082966193557\n",
      "epoch: 9 | 109216 / 114272 | training loss: 0.0003333135973662138\n",
      "epoch: 9 | 109248 / 114272 | training loss: 0.00032367761014029384\n",
      "epoch: 9 | 109280 / 114272 | training loss: 0.0002506777527742088\n",
      "epoch: 9 | 109312 / 114272 | training loss: 0.00012174072617199272\n",
      "epoch: 9 | 109344 / 114272 | training loss: 0.0002833337348420173\n",
      "epoch: 9 | 109376 / 114272 | training loss: 0.00027144301566295326\n",
      "epoch: 9 | 109408 / 114272 | training loss: 0.0002659586607478559\n",
      "epoch: 9 | 109440 / 114272 | training loss: 0.0005627673817798495\n",
      "epoch: 9 | 109472 / 114272 | training loss: 0.00018711564189288765\n",
      "epoch: 9 | 109504 / 114272 | training loss: 0.00030240698833949864\n",
      "epoch: 9 | 109536 / 114272 | training loss: 0.0003430463548284024\n",
      "epoch: 9 | 109568 / 114272 | training loss: 0.0002589708601590246\n",
      "epoch: 9 | 109600 / 114272 | training loss: 0.0002833835897035897\n",
      "epoch: 9 | 109632 / 114272 | training loss: 0.00019898121536243707\n",
      "epoch: 9 | 109664 / 114272 | training loss: 0.0003152201825287193\n",
      "epoch: 9 | 109696 / 114272 | training loss: 0.00026142667047679424\n",
      "epoch: 9 | 109728 / 114272 | training loss: 0.00035085107083432376\n",
      "epoch: 9 | 109760 / 114272 | training loss: 0.00021494190150406212\n",
      "epoch: 9 | 109792 / 114272 | training loss: 0.028869763016700745\n",
      "epoch: 9 | 109824 / 114272 | training loss: 0.0003490624949336052\n",
      "epoch: 9 | 109856 / 114272 | training loss: 0.0003664225514512509\n",
      "epoch: 9 | 109888 / 114272 | training loss: 0.0007867218228057027\n",
      "epoch: 9 | 109920 / 114272 | training loss: 0.00018781299877446145\n",
      "epoch: 9 | 109952 / 114272 | training loss: 0.00016065742238424718\n",
      "epoch: 9 | 109984 / 114272 | training loss: 0.00031521139317192137\n",
      "epoch: 9 | 110016 / 114272 | training loss: 0.0001884888333734125\n",
      "epoch: 9 | 110048 / 114272 | training loss: 0.0028978644404560328\n",
      "epoch: 9 | 110080 / 114272 | training loss: 0.00029834581073373556\n",
      "epoch: 9 | 110112 / 114272 | training loss: 0.0009644557721912861\n",
      "epoch: 9 | 110144 / 114272 | training loss: 0.00016793300164863467\n",
      "epoch: 9 | 110176 / 114272 | training loss: 0.0016170035814866424\n",
      "epoch: 9 | 110208 / 114272 | training loss: 0.000454665016150102\n",
      "epoch: 9 | 110240 / 114272 | training loss: 0.0003224222455173731\n",
      "epoch: 9 | 110272 / 114272 | training loss: 0.003389834426343441\n",
      "epoch: 9 | 110304 / 114272 | training loss: 0.00030608632368966937\n",
      "epoch: 9 | 110336 / 114272 | training loss: 0.00030023674480617046\n",
      "epoch: 9 | 110368 / 114272 | training loss: 0.0003128158568870276\n",
      "epoch: 9 | 110400 / 114272 | training loss: 0.008682593703269958\n",
      "epoch: 9 | 110432 / 114272 | training loss: 0.00027879499248228967\n",
      "epoch: 9 | 110464 / 114272 | training loss: 0.0004219046386424452\n",
      "epoch: 9 | 110496 / 114272 | training loss: 0.0001603897544555366\n",
      "epoch: 9 | 110528 / 114272 | training loss: 0.06067987158894539\n",
      "epoch: 9 | 110560 / 114272 | training loss: 0.0002612462849356234\n",
      "epoch: 9 | 110592 / 114272 | training loss: 0.0002231800463050604\n",
      "epoch: 9 | 110624 / 114272 | training loss: 0.00018227948748972267\n",
      "epoch: 9 | 110656 / 114272 | training loss: 0.000421613862272352\n",
      "epoch: 9 | 110688 / 114272 | training loss: 0.0003071741957683116\n",
      "epoch: 9 | 110720 / 114272 | training loss: 0.0001742495660437271\n",
      "epoch: 9 | 110752 / 114272 | training loss: 0.07052524387836456\n",
      "epoch: 9 | 110784 / 114272 | training loss: 0.00026268456713296473\n",
      "epoch: 9 | 110816 / 114272 | training loss: 0.00020867062266916037\n",
      "epoch: 9 | 110848 / 114272 | training loss: 0.00020391395082697272\n",
      "epoch: 9 | 110880 / 114272 | training loss: 0.00021352543262764812\n",
      "epoch: 9 | 110912 / 114272 | training loss: 0.00039020043914206326\n",
      "epoch: 9 | 110944 / 114272 | training loss: 0.00019979866920039058\n",
      "epoch: 9 | 110976 / 114272 | training loss: 0.015396323055028915\n",
      "epoch: 9 | 111008 / 114272 | training loss: 0.0005843293038196862\n",
      "epoch: 9 | 111040 / 114272 | training loss: 0.0022541028447449207\n",
      "epoch: 9 | 111072 / 114272 | training loss: 0.0003400651039555669\n",
      "epoch: 9 | 111104 / 114272 | training loss: 0.00017718831077218056\n",
      "epoch: 9 | 111136 / 114272 | training loss: 0.00021068470960017294\n",
      "epoch: 9 | 111168 / 114272 | training loss: 0.00016088399570435286\n",
      "epoch: 9 | 111200 / 114272 | training loss: 0.0002757507318165153\n",
      "epoch: 9 | 111232 / 114272 | training loss: 0.00019346542831044644\n",
      "epoch: 9 | 111264 / 114272 | training loss: 0.00041649071499705315\n",
      "epoch: 9 | 111296 / 114272 | training loss: 0.0002643394691403955\n",
      "epoch: 9 | 111328 / 114272 | training loss: 0.00031191602465696633\n",
      "epoch: 9 | 111360 / 114272 | training loss: 0.013401213102042675\n",
      "epoch: 9 | 111392 / 114272 | training loss: 0.0002530022757127881\n",
      "epoch: 9 | 111424 / 114272 | training loss: 0.0005377683555707335\n",
      "epoch: 9 | 111456 / 114272 | training loss: 0.00018314739281777292\n",
      "epoch: 9 | 111488 / 114272 | training loss: 0.0012784532736986876\n",
      "epoch: 9 | 111520 / 114272 | training loss: 0.004237301182001829\n",
      "epoch: 9 | 111552 / 114272 | training loss: 0.0002766306570265442\n",
      "epoch: 9 | 111584 / 114272 | training loss: 0.00025509309489279985\n",
      "epoch: 9 | 111616 / 114272 | training loss: 0.00029764650389552116\n",
      "epoch: 9 | 111648 / 114272 | training loss: 0.00031982874497771263\n",
      "epoch: 9 | 111680 / 114272 | training loss: 0.0015613537980243564\n",
      "epoch: 9 | 111712 / 114272 | training loss: 0.000285218731733039\n",
      "epoch: 9 | 111744 / 114272 | training loss: 0.00016090774443000555\n",
      "epoch: 9 | 111776 / 114272 | training loss: 0.00038748449878767133\n",
      "epoch: 9 | 111808 / 114272 | training loss: 0.0002500100526958704\n",
      "epoch: 9 | 111840 / 114272 | training loss: 0.000326575682265684\n",
      "epoch: 9 | 111872 / 114272 | training loss: 0.0002881003019865602\n",
      "epoch: 9 | 111904 / 114272 | training loss: 0.00021285892580635846\n",
      "epoch: 9 | 111936 / 114272 | training loss: 0.0003244745603296906\n",
      "epoch: 9 | 111968 / 114272 | training loss: 0.00037522127968259156\n",
      "epoch: 9 | 112000 / 114272 | training loss: 0.0003121315676253289\n",
      "epoch: 9 | 112032 / 114272 | training loss: 0.00023658153077121824\n",
      "epoch: 9 | 112064 / 114272 | training loss: 0.022326944395899773\n",
      "epoch: 9 | 112096 / 114272 | training loss: 0.000430446641985327\n",
      "epoch: 9 | 112128 / 114272 | training loss: 0.020548390224575996\n",
      "epoch: 9 | 112160 / 114272 | training loss: 0.0003161510394420475\n",
      "epoch: 9 | 112192 / 114272 | training loss: 0.0002194508706452325\n",
      "epoch: 9 | 112224 / 114272 | training loss: 0.0020124453585594893\n",
      "epoch: 9 | 112256 / 114272 | training loss: 0.00021083296451251954\n",
      "epoch: 9 | 112288 / 114272 | training loss: 0.00033077134867198765\n",
      "epoch: 9 | 112320 / 114272 | training loss: 0.0005195060512050986\n",
      "epoch: 9 | 112352 / 114272 | training loss: 0.0002263890055473894\n",
      "epoch: 9 | 112384 / 114272 | training loss: 0.00018224735686089844\n",
      "epoch: 9 | 112416 / 114272 | training loss: 0.00032382438075728714\n",
      "epoch: 9 | 112448 / 114272 | training loss: 0.0002664562198333442\n",
      "epoch: 9 | 112480 / 114272 | training loss: 0.00024401313567068428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | 112512 / 114272 | training loss: 0.00024714067694731057\n",
      "epoch: 9 | 112544 / 114272 | training loss: 0.0002570135402493179\n",
      "epoch: 9 | 112576 / 114272 | training loss: 0.0020368045661598444\n",
      "epoch: 9 | 112608 / 114272 | training loss: 0.000178271671757102\n",
      "epoch: 9 | 112640 / 114272 | training loss: 0.00026189422351308167\n",
      "epoch: 9 | 112672 / 114272 | training loss: 0.023036552593111992\n",
      "epoch: 9 | 112704 / 114272 | training loss: 0.00031128834234550595\n",
      "epoch: 9 | 112736 / 114272 | training loss: 0.00014717457816004753\n",
      "epoch: 9 | 112768 / 114272 | training loss: 0.00027315475745126605\n",
      "epoch: 9 | 112800 / 114272 | training loss: 0.00015325167623814195\n",
      "epoch: 9 | 112832 / 114272 | training loss: 0.00023281338508240879\n",
      "epoch: 9 | 112864 / 114272 | training loss: 0.00015409952902700752\n",
      "epoch: 9 | 112896 / 114272 | training loss: 0.00031181180384010077\n",
      "epoch: 9 | 112928 / 114272 | training loss: 0.0001248548796866089\n",
      "epoch: 9 | 112960 / 114272 | training loss: 0.0001690587232587859\n",
      "epoch: 9 | 112992 / 114272 | training loss: 0.00018074984836857766\n",
      "epoch: 9 | 113024 / 114272 | training loss: 0.0005861417739652097\n",
      "epoch: 9 | 113056 / 114272 | training loss: 0.00027550291270017624\n",
      "epoch: 9 | 113088 / 114272 | training loss: 0.0003063582698814571\n",
      "epoch: 9 | 113120 / 114272 | training loss: 0.0002262547641294077\n",
      "epoch: 9 | 113152 / 114272 | training loss: 0.00038259360007941723\n",
      "epoch: 9 | 113184 / 114272 | training loss: 0.00016868679085746408\n",
      "epoch: 9 | 113216 / 114272 | training loss: 0.0002382313396083191\n",
      "epoch: 9 | 113248 / 114272 | training loss: 0.0002595332043711096\n",
      "epoch: 9 | 113280 / 114272 | training loss: 0.000477660825708881\n",
      "epoch: 9 | 113312 / 114272 | training loss: 0.00029271203675307333\n",
      "epoch: 9 | 113344 / 114272 | training loss: 0.0002543278387747705\n",
      "epoch: 9 | 113376 / 114272 | training loss: 0.0002992482332047075\n",
      "epoch: 9 | 113408 / 114272 | training loss: 0.0003761227708309889\n",
      "epoch: 9 | 113440 / 114272 | training loss: 0.00022266713494900614\n",
      "epoch: 9 | 113472 / 114272 | training loss: 0.0002572708763182163\n",
      "epoch: 9 | 113504 / 114272 | training loss: 0.00024101311282720417\n",
      "epoch: 9 | 113536 / 114272 | training loss: 0.0003442114102654159\n",
      "epoch: 9 | 113568 / 114272 | training loss: 0.0001913342421175912\n",
      "epoch: 9 | 113600 / 114272 | training loss: 0.0001797578443074599\n",
      "epoch: 9 | 113632 / 114272 | training loss: 0.00017287694208789617\n",
      "epoch: 9 | 113664 / 114272 | training loss: 0.0003432607336435467\n",
      "epoch: 9 | 113696 / 114272 | training loss: 0.00037035447894595563\n",
      "epoch: 9 | 113728 / 114272 | training loss: 0.000304328539641574\n",
      "epoch: 9 | 113760 / 114272 | training loss: 0.0002701431221794337\n",
      "epoch: 9 | 113792 / 114272 | training loss: 0.00023676834825892001\n",
      "epoch: 9 | 113824 / 114272 | training loss: 0.00017182165174745023\n",
      "epoch: 9 | 113856 / 114272 | training loss: 0.0009767700685188174\n",
      "epoch: 9 | 113888 / 114272 | training loss: 0.00013845317880623043\n",
      "epoch: 9 | 113920 / 114272 | training loss: 0.00022828098735772073\n",
      "epoch: 9 | 113952 / 114272 | training loss: 0.0003581009805202484\n",
      "epoch: 9 | 113984 / 114272 | training loss: 0.00028322843718342483\n",
      "epoch: 9 | 114016 / 114272 | training loss: 0.00031454587588086724\n",
      "epoch: 9 | 114048 / 114272 | training loss: 0.00014990380441304296\n",
      "epoch: 9 | 114080 / 114272 | training loss: 0.00018687674310058355\n",
      "epoch: 9 | 114112 / 114272 | training loss: 0.0002213737607235089\n",
      "epoch: 9 | 114144 / 114272 | training loss: 0.0002823621325660497\n",
      "epoch: 9 | 114176 / 114272 | training loss: 0.00019908374815713614\n",
      "epoch: 9 | 114208 / 114272 | training loss: 0.0003065256751142442\n",
      "epoch: 9 | 114240 / 114272 | training loss: 0.000204341413336806\n",
      "Training epoch 9 done! Average loss: 0.006149492695674251. Accuracy: 0.9984335620274434\n",
      "Validation epoch 9 done! Average loss: 0.27532024821837864. Accurage: 0.9565855704697986\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1} / {EPOCHS}')\n",
    "    print('-' * 66)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        DEVICE,\n",
    "        scheduler,\n",
    "        BATCH_SIZE,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    print(f'Training epoch {epoch} done! Average loss: {train_loss}. Accuracy: {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_epoch(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_function,\n",
    "        DEVICE,\n",
    "        BATCH_SIZE,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    print(f'Validation epoch {epoch} done! Average loss: {val_loss}. Accurage: {val_acc}')\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), './output/cu_best_model_sentiment_analysis.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d891b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
