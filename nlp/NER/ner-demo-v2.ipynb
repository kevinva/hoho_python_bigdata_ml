{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-14T04:28:09.814779Z","iopub.execute_input":"2023-04-14T04:28:09.815519Z","iopub.status.idle":"2023-04-14T04:28:09.896861Z","shell.execute_reply.started":"2023-04-14T04:28:09.815476Z","shell.execute_reply":"2023-04-14T04:28:09.895726Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/329afternoon/sent2kaggle/readme.md\n/kaggle/input/329afternoon/sent2kaggle/requirements.txt\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data_process.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/train.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/run.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/metrics.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/task_config.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/model.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/cur_best.pkl\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data_loader.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/k.txt\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/loss.png\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/config.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/utils.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/task_train.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/only2test.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/task_builder.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/reptile.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/everytime.pkl\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/same_parameter.pkl\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/case/bad_case.txt\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/experiments/clue/config.json\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/experiments/clue/train.log\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/config.json\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/pytorch_model.bin\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/vocab.txt\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/bert-base-chinese/config.json\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/bert-base-chinese/main.py\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/bert-base-chinese/test.ipynb\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/bert-base-chinese/pytorch_model.bin\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/bert-base-chinese/vocab.txt\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/train.npz\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/test.npz\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/train.json\n/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"### hoho\n\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:09.981239Z","iopub.execute_input":"2023-04-14T04:28:09.981514Z","iopub.status.idle":"2023-04-14T04:28:09.988762Z","shell.execute_reply.started":"2023-04-14T04:28:09.981487Z","shell.execute_reply":"2023-04-14T04:28:09.987645Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"### hoho\nclass config_demo:\n    hoho = 1\n    hoho2 = 2\n    \nconfig_demo.hoho2","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:10.171605Z","iopub.execute_input":"2023-04-14T04:28:10.172222Z","iopub.status.idle":"2023-04-14T04:28:10.178942Z","shell.execute_reply.started":"2023-04-14T04:28:10.172183Z","shell.execute_reply":"2023-04-14T04:28:10.177865Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"RANDOM_STATE = 6666\nBERT_MODEL_NAME = 'bert-base-chinese'\nROBERTA_MODEL_NAME = 'hfl/chinese-roberta-wwm-ext'","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:10.342243Z","iopub.execute_input":"2023-04-14T04:28:10.342771Z","iopub.status.idle":"2023-04-14T04:28:10.347358Z","shell.execute_reply.started":"2023-04-14T04:28:10.342743Z","shell.execute_reply":"2023-04-14T04:28:10.346226Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### config.py\n\nimport os\nimport torch\n\nclass config:\n\n    data_dir = os.getcwd() + '/data/clue/'\n    # train_dir = data_dir + 'train.npz'\n    # test_dir = data_dir + 'test.npz'\n    train_dir = '/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/train.npz'\n    test_dir = '/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/test.npz'\n    files = ['train', 'test']\n    bert_model = BERT_MODEL_NAME #'/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/bert-base-chinese/'\n    roberta_model = ROBERTA_MODEL_NAME #'/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/pretrained_bert_models/chinese_roberta_wwm_large_ext/'\n    model_dir = os.getcwd() + '/experiments/clue/'\n    log_dir = model_dir + 'train.log'\n    case_dir = os.getcwd() + '/case/bad_case.txt'\n\n    # 训练集、验证集划分比例\n    dev_split_size = 0.1\n\n    # 是否加载训练好的NER模型\n    load_before = False\n\n    # 是否对整个BERT进行fine tuning\n    full_fine_tuning = True\n\n    # hyper-parameter\n    learning_rate = 3e-5\n    weight_decay = 0.01\n    clip_grad = 5\n\n    batch_size = 32\n    epoch_num = 50\n    min_epoch_num = 5\n    patience = 0.0002\n    patience_num = 10\n\n    gpu = '1'\n\n    if gpu != '':\n        device = torch.device(f\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    labels = ['address', 'book', 'company', 'game', 'government',\n              'movie', 'name', 'organization', 'position', 'scene']\n\n    label2id = {\n        \"O\": 0,\n        \"B-address\": 1,\n        \"B-book\": 2,\n        \"B-company\": 3,\n        'B-game': 4,\n        'B-government': 5,\n        'B-movie': 6,\n        'B-name': 7,\n        'B-organization': 8,\n        'B-position': 9,\n        'B-scene': 10,\n        \"I-address\": 11,\n        \"I-book\": 12,\n        \"I-company\": 13,\n        'I-game': 14,\n        'I-government': 15,\n        'I-movie': 16,\n        'I-name': 17,\n        'I-organization': 18,\n        'I-position': 19,\n        'I-scene': 20,\n        \"S-address\": 21,\n        \"S-book\": 22,\n        \"S-company\": 23,\n        'S-game': 24,\n        'S-government': 25,\n        'S-movie': 26,\n        'S-name': 27,\n        'S-organization': 28,\n        'S-position': 29,\n        'S-scene': 30\n    }\n\n    id2label = {_id: _label for _label, _id in list(label2id.items())}\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:10.506974Z","iopub.execute_input":"2023-04-14T04:28:10.507264Z","iopub.status.idle":"2023-04-14T04:28:13.073411Z","shell.execute_reply.started":"2023-04-14T04:28:10.507238Z","shell.execute_reply":"2023-04-14T04:28:13.072408Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"### hoho\n\n!mkdir experiments\n!mkdir case\n!mkdir experiments/clue","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:13.078581Z","iopub.execute_input":"2023-04-14T04:28:13.081549Z","iopub.status.idle":"2023-04-14T04:28:16.020740Z","shell.execute_reply.started":"2023-04-14T04:28:13.081509Z","shell.execute_reply":"2023-04-14T04:28:16.019356Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"### data_loader.py\n\nimport torch\nimport numpy as np\nfrom transformers import BertTokenizer\nfrom torch.utils.data import Dataset\n\n\nclass NERDataset(Dataset):\n    def __init__(self, words, labels, config, word_pad_idx = 0, label_pad_idx = -1):\n        self.tokenizer = BertTokenizer.from_pretrained(config.bert_model, do_lower_case=True)\n        self.label2id = config.label2id\n        self.id2label = {_id: _label for _label, _id in list(config.label2id.items())}\n        self.dataset = self.preprocess(words, labels)\n        self.word_pad_idx = word_pad_idx\n        self.label_pad_idx = label_pad_idx\n        self.device = config.device\n\n    def preprocess(self, origin_sentences, origin_labels):\n        \"\"\"\n        Maps tokens and tags to their indices and stores them in the dict data.\n        examples: \n            word:['[CLS]', '浙', '商', '银', '行', '企', '业', '信', '贷', '部']\n            sentence:([101, 3851, 1555, 7213, 6121, 821, 689, 928, 6587, 6956],\n                        array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]))\n            label:[3, 13, 13, 13, 0, 0, 0, 0, 0]\n        \"\"\"\n        data = []\n        sentences = []\n        labels = []\n        for line in origin_sentences:\n            # replace each token by its index\n            # we can not use encode_plus because our sentences are aligned to labels in list type\n            words = []\n            word_lens = []\n            for token in line:\n                words.append(self.tokenizer.tokenize(token))\n                word_lens.append(len(token))     # hoho: w为啥要计算token的长度？难道不是单个字？\n            # 变成单个字的列表，开头加上[CLS]\n            words = ['[CLS]'] + [item for token in words for item in token]\n            token_start_idxs = 1 + np.cumsum([0] + word_lens[:-1])  # 前面加个1，因为第0位位CLS，所以实际开始序号为1\n            sentences.append((self.tokenizer.convert_tokens_to_ids(words), token_start_idxs))\n        for tag in origin_labels:\n            label_id = [self.label2id.get(t) for t in tag]\n            labels.append(label_id)\n        for sentence, label in zip(sentences, labels):\n            data.append((sentence, label))\n        return data\n\n    def __getitem__(self, idx):\n        \"\"\"sample data to get batch\"\"\"\n        word = self.dataset[idx][0]\n        label = self.dataset[idx][1]\n\n        return [word, label]\n\n    def __len__(self):\n        \"\"\"get dataset size\"\"\"\n        return len(self.dataset)\n\n    def collate_fn(self, batch):\n        \"\"\"\n        process batch data, including:\n            1. padding: 将每个batch的data padding到同一长度（batch中最长的data长度）\n            2. aligning: 找到每个sentence sequence里面有label项，文本与label对齐\n            3. tensor：转化为tensor\n        \"\"\"\n        sentences = [x[0] for x in batch]\n        labels = [x[1] for x in batch]\n\n        # batch length\n        batch_len = len(sentences)\n\n        # compute length of longest sentence in batch\n        max_len = max([len(s[0]) for s in sentences])   # hoho: 注意每个sentence的结构，第0位的位真正的sentence\n        max_label_len = 0\n\n        # padding data 初始化\n        batch_data = self.word_pad_idx * np.ones((batch_len, max_len))\n        batch_label_starts = []\n\n        # padding and aligning\n        for j in range(batch_len):\n            cur_len = len(sentences[j][0])\n            batch_data[j][:cur_len] = sentences[j][0]\n            # 找到有标签的数据的index（[CLS]不算）\n            label_start_idx = sentences[j][-1]\n            label_starts = np.zeros(max_len)\n            label_starts[[idx for idx in label_start_idx if idx < max_len]] = 1\n            batch_label_starts.append(label_starts)\n            max_label_len = max(int(sum(label_starts)), max_label_len)\n\n        # padding label\n        batch_labels = self.label_pad_idx * np.ones((batch_len, max_label_len))\n        for j in range(batch_len):\n            cur_tags_len = len(labels[j])\n            batch_labels[j][:cur_tags_len] = labels[j]\n\n        # convert data to torch LongTensors\n        batch_data = torch.tensor(batch_data, dtype=torch.long)\n        batch_label_starts = torch.tensor(batch_label_starts, dtype=torch.long)\n        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n\n        # shift tensors to GPU if available\n        batch_data, batch_label_starts = batch_data.to(self.device), batch_label_starts.to(self.device)\n        batch_labels = batch_labels.to(self.device)\n        return [batch_data, batch_label_starts, batch_labels]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:16.025390Z","iopub.execute_input":"2023-04-14T04:28:16.025700Z","iopub.status.idle":"2023-04-14T04:28:17.572020Z","shell.execute_reply.started":"2023-04-14T04:28:16.025668Z","shell.execute_reply":"2023-04-14T04:28:17.570979Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# hoho\n1 + np.cumsum([0] + [1, 1, 1, 1])","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:17.577081Z","iopub.execute_input":"2023-04-14T04:28:17.577768Z","iopub.status.idle":"2023-04-14T04:28:17.585595Z","shell.execute_reply.started":"2023-04-14T04:28:17.577738Z","shell.execute_reply":"2023-04-14T04:28:17.584634Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([1, 2, 3, 4, 5])"},"metadata":{}}]},{"cell_type":"code","source":"### data_process.py\n\nimport os\nimport json\nimport logging\nimport numpy as np\n\n\nclass Processor:\n    def __init__(self, config):\n        self.data_dir = config.data_dir\n        self.config = config\n\n    def process(self):\n        \"\"\"\n        process train and test data\n        \"\"\"\n        for file_name in self.config.files:\n            self.preprocess(file_name)\n        \n\n    def preprocess(self, mode):\n        \"\"\"\n        params:\n            words：将json文件每一行中的文本分离出来，存储为words列表\n            labels：标记文本对应的标签，存储为labels\n        examples:\n            words示例：['生', '生', '不', '息', 'C', 'S', 'O', 'L']\n            labels示例：['O', 'O', 'O', 'O', 'B-game', 'I-game', 'I-game', 'I-game']\n        \"\"\"\n#         input_dir = self.data_dir + str(mode) + '.json'\n#         output_dir = self.data_dir + str(mode) + '.npz'\n\n        input_dir = '/kaggle/input/329afternoon/sent2kaggle/BERT-LSTM-CRF/data/clue/' + str(mode) + '.json'\n        output_dir = '/kaggle/working/' + str(mode) + '.npz'\n\n        if os.path.exists(output_dir) is True:\n            return\n        word_list = []\n        label_list = []\n        with open(input_dir, 'r', encoding='utf-8') as f:\n            # 先读取到内存中，然后逐行处理\n            for line in f.readlines():\n                # loads()：用于处理内存中的json对象，strip去除可能存在的空格\n                json_line = json.loads(line.strip())\n\n                text = json_line['text']\n                words = list(text)\n                # 如果没有label，则返回None\n                label_entities = json_line.get('label', None)\n                labels = ['O'] * len(words)\n\n                if label_entities is not None:\n                    for key, value in label_entities.items():\n                        for sub_name, sub_index in value.items():\n                            for start_index, end_index in sub_index:\n                                assert ''.join(words[start_index:end_index + 1]) == sub_name\n                                if start_index == end_index:\n                                    labels[start_index] = 'S-' + key\n                                else:\n                                    labels[start_index] = 'B-' + key\n                                    labels[start_index + 1:end_index + 1] = ['I-' + key] * (len(sub_name) - 1)\n                word_list.append(words)\n                label_list.append(labels)\n                # 保存成二进制文件\n            np.savez_compressed(output_dir, words=word_list, labels=label_list)\n            print(\"--------{} data process DONE!--------\".format(mode))","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:17.587412Z","iopub.execute_input":"2023-04-14T04:28:17.588243Z","iopub.status.idle":"2023-04-14T04:28:17.605940Z","shell.execute_reply.started":"2023-04-14T04:28:17.588206Z","shell.execute_reply":"2023-04-14T04:28:17.604951Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"### hoho\nprocessor = Processor(config)\nprocessor.process()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:17.607734Z","iopub.execute_input":"2023-04-14T04:28:17.608102Z","iopub.status.idle":"2023-04-14T04:28:18.535328Z","shell.execute_reply.started":"2023-04-14T04:28:17.608066Z","shell.execute_reply":"2023-04-14T04:28:18.534159Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  val = np.asanyarray(val)\n","output_type":"stream"},{"name":"stdout","text":"--------train data process DONE!--------\n--------test data process DONE!--------\n","output_type":"stream"}]},{"cell_type":"code","source":"# hoho\n# !pip install transformers\n# torch.__version__\n!pip install pytorch-crf -i https://pypi.tuna.tsinghua.edu.cn/simple/","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:18.537105Z","iopub.execute_input":"2023-04-14T04:28:18.537488Z","iopub.status.idle":"2023-04-14T04:28:30.984820Z","shell.execute_reply.started":"2023-04-14T04:28:18.537452Z","shell.execute_reply":"2023-04-14T04:28:30.983579Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\nCollecting pytorch-crf\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers.models.bert.modeling_bert import *\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchcrf import CRF","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:30.989054Z","iopub.execute_input":"2023-04-14T04:28:30.989374Z","iopub.status.idle":"2023-04-14T04:28:32.283073Z","shell.execute_reply.started":"2023-04-14T04:28:30.989343Z","shell.execute_reply":"2023-04-14T04:28:32.281853Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"### model.py\n\nclass BertNER(BertPreTrainedModel):\n    \n    def __init__(self, config):   # 注意：这个config并非自定义的config类\n        super(BertNER, self).__init__(config)\n        \n#         print(f'config: {config}')\n\n        hidden_size = 768  # 这里要跟bert的输入输出size一致\n        \n        self.num_labels = config.num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.bilstm = nn.LSTM(\n            input_size = hidden_size, #config.lstm_embedding_size,    # 这里要跟bert的输出size一致\n            hidden_size = hidden_size // 2, #config.hidden_size // 2,\n            batch_first = True,\n            num_layers = 2,\n            dropout = 0.5, #config.lstm_dropout_prob,\n            bidirectional = True\n        )\n        self.classifier = nn.Linear(hidden_size, config.num_labels)\n        self.crf = CRF(config.num_labels, batch_first = True)\n        \n        self.init_weights()\n        \n    def forward(self, \n                input_data, token_type_ids = None, attention_mask = None, \n                labels = None, position_ids = None, inputs_embeds = None, head_mask = None):\n        input_ids, input_token_starts = input_data\n        outputs = self.bert(input_ids,\n                           attention_mask = attention_mask,\n                           token_type_ids = token_type_ids,\n                           position_ids = position_ids,\n                           head_mask = head_mask,\n                           inputs_embeds = inputs_embeds)\n        sequence_output = outputs[0]\n        \n#         print(f'[hoho], input_ids: {input_ids.shape}')\n#         print(f'[hoho], input_token_starts: {input_token_starts.shape}')\n#         print(f'[hoho], sequence_output: {sequence_output.shape}')\n        \n        origin_sequence_output = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(sequence_output, input_token_starts)]\n        padded_sequence_output = pad_sequence(origin_sequence_output, batch_first = True)\n        padded_sequence_output = self.dropout(padded_sequence_output)\n        lstm_output, _ = self.bilstm(padded_sequence_output)\n        \n#         print(f'[hoho] lstm_output: {lstm_output.shape}')\n        \n        logits = self.classifier(lstm_output)\n        outputs = (logits,)\n        if labels is not None:\n            loss_mask = labels.gt(-1)\n            loss = self.crf(logits, labels, loss_mask) * (-1)\n            \n#             print(f'crf loss： {loss}')\n            \n            outputs = (loss,) + outputs\n            \n        return outputs\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.284949Z","iopub.execute_input":"2023-04-14T04:28:32.285345Z","iopub.status.idle":"2023-04-14T04:28:32.300223Z","shell.execute_reply.started":"2023-04-14T04:28:32.285302Z","shell.execute_reply":"2023-04-14T04:28:32.299164Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### hoho\ntest_tensor = torch.tensor([1, 1, 1, 1, 0, 0])\ntest_tensor.nonzero().squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.304826Z","iopub.execute_input":"2023-04-14T04:28:32.305226Z","iopub.status.idle":"2023-04-14T04:28:32.349951Z","shell.execute_reply.started":"2023-04-14T04:28:32.305197Z","shell.execute_reply":"2023-04-14T04:28:32.349054Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([0, 1, 2, 3])"},"metadata":{}}]},{"cell_type":"code","source":"### metrics.py\n\nimport os\nimport logging\n\ndef get_entities(seq):\n    \"\"\"\n    Gets entities from sequence.\n\n    Args:\n        seq (list): sequence of labels.\n\n    Returns:\n        list: list of (chunk_type, chunk_start, chunk_end).\n\n    Example:\n        seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n        get_entities(seq)\n        [('PER', 0, 1), ('LOC', 3, 3)]\n    \"\"\"\n    \n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + ['O']]   #即拉平数组\n    prev_tag = 'O'\n    prev_type = ''\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + ['O']):\n        tag = chunk[0]\n        type_ = chunk.split('-')[-1]\n        \n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i - 1))\n        \n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n            \n        prev_tag = tag\n        prev_type = type_\n        \n    return chunks\n\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk ended between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_end: boolean.\n    \"\"\"\n    chunk_end = False\n\n    if prev_tag == 'S':\n        chunk_end = True\n    # pred_label中可能出现这种情形\n    if prev_tag == 'B' and tag == 'B':\n        chunk_end = True\n    if prev_tag == 'B' and tag == 'S':\n        chunk_end = True\n    if prev_tag == 'B' and tag == 'O':\n        chunk_end = True\n    if prev_tag == 'I' and tag == 'B':\n        chunk_end = True\n    if prev_tag == 'I' and tag == 'S':\n        chunk_end = True\n    if prev_tag == 'I' and tag == 'O':\n        chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk started between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n        \n    Returns:\n        chunk_start: boolean.\n    \"\"\"\n    chunk_start = False\n\n    if tag == 'B':\n        chunk_start = True\n    if tag == 'S':\n        chunk_start = True\n\n    if prev_tag == 'S' and tag == 'I':\n        chunk_start = True\n    if prev_tag == 'O' and tag == 'I':\n        chunk_start = True\n\n    if tag != 'O' and tag != '.' and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start\n\ndef f1_score(y_true, y_pred, mode = 'dev'):\n    \"\"\"Compute the F1 score.\n\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n\n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        f1_score(y_true, y_pred)\n        0.50\n    \"\"\"\n    \n    true_entities = set(get_entities(y_true))\n    pred_entities = set(get_entities(y_pred))\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n    nb_true = len(true_entities)\n    \n    p = nb_correct / nb_pred if nb_pred > 0 else 0\n    r = nb_correct / nb_true if nb_true > 0 else 0\n    score = 2 * p * r / (p + r) if p + r > 0 else 0\n    \n    if mode == 'dev':\n        return score\n    else:\n        f_score = {}\n        for label in config.labels:\n            true_entities_label = set()\n            pred_entities_label = set()\n            for t in true_entities:\n                if t[0] == label:\n                    true_entities_label.add(t)\n            for p in pred_entities:\n                if p[0] == label:\n                    pred_entities_label.add(p)\n                    \n            nb_correct_label = len(true_entities_label & pred_entities_label)\n            nb_pred_label = len(pred_entities_label)\n            nb_true_label = len(true_entities_label)\n            \n            p_label = nb_correct_label / nb_pred_label if nb_pred_label > 0 else 0\n            r_label = nb_correct_label / nb_true_label if nb_true_label > 0 else 0\n            score_label = 2 * p_label * r_label / (p_label + r_label) if p_label + r_label > 0 else 0\n            f_score[label] = score_label\n        return f_score, score\n    \ndef bad_case(y_true, y_pred, data):\n    if not os.path.exists(config.case_dir):\n        os.system(r\"touch {}\".format(config.case_dir))  # 调用系统命令行来创建文件\n    output = open(config.case_dir, 'w')\n    for idx, (t, p) in enumerate(zip(y_true, y_pred)):\n        if t == p:\n            continue\n        else:\n            output.write(\"bad case \" + str(idx) + \": \\n\")\n            output.write(\"sentence: \" + str(data[idx]) + \"\\n\")\n            output.write(\"golden label: \" + str(t) + \"\\n\")\n            output.write(\"model pred: \" + str(p) + \"\\n\")\n    print(\"--------Bad Cases reserved !--------\")","metadata":{"execution":{"iopub.status.busy":"2023-04-14T06:05:51.145177Z","iopub.execute_input":"2023-04-14T06:05:51.145563Z","iopub.status.idle":"2023-04-14T06:05:51.168210Z","shell.execute_reply.started":"2023-04-14T06:05:51.145528Z","shell.execute_reply":"2023-04-14T06:05:51.166948Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# hoho\n\n# !mkdir case\n\ny_t = [['O', 'O', 'O', 'B-address', 'I-address', 'I-address', 'O'], ['B-name', 'I-name', 'O']]\ny_p = [['O', 'O', 'B-address', 'I-address', 'I-address', 'I-address', 'O'], ['B-name', 'I-name', 'O']]\nsent = [['十', '一', '月', '中', '山', '路', '电'], ['周', '静', '说']]\n# bad_case(y_t, y_p, sent)\nget_entities(y_t)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.375266Z","iopub.execute_input":"2023-04-14T04:28:32.375757Z","iopub.status.idle":"2023-04-14T04:28:32.389047Z","shell.execute_reply.started":"2023-04-14T04:28:32.375719Z","shell.execute_reply":"2023-04-14T04:28:32.387858Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[('address', 3, 5), ('name', 8, 9)]"},"metadata":{}}]},{"cell_type":"code","source":"# train.py\n\nimport torch\nimport logging\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer\n\n\ndef train_epoch(train_loader, model, optimizer, scheduler, epoch):\n    model.train()\n    \n    train_losses = 0\n    for idx, batch_samples in enumerate(tqdm(train_loader)):\n        batch_data, batch_token_starts, batch_labels = batch_samples\n        batch_masks = batch_data.gt(0)\n        \n#         print(f'[hoho], batch_data = {batch_data.shape}')\n#         print(f'[hoho], batch_token_starts = {batch_token_starts.shape}')\n#         print(f'[hoho], batch_labels = {batch_labels.shape}')\n#         print(f'[hoho], batch_masks = {batch_masks.shape}')\n        \n        loss = model((batch_data, batch_token_starts), token_type_ids = None, attention_mask = batch_masks, labels = batch_labels)[0]\n        train_losses += loss.item()\n        \n        model.zero_grad()\n        loss.backward()\n        \n        nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = config.clip_grad)\n        optimizer.step()\n        scheduler.step()\n    train_loss = float(train_losses) / len(train_loader)\n    \n    print(f'Epoch: {epoch}, train loss: {train_loss}')\n    \n    \ndef train(train_loader, dev_loader, model, optimizer, scheduler, model_dir):\n    if model_dir is not None and config.load_before:\n        model = BertNER.from_pretrained(model_dir)\n        model.to(cnofig.device)\n        print(\"--------Load model from {}--------\".format(model_dir))\n    best_val_f1 = 0.0\n    patientce_counter = 0\n    for epoch in range(1, config.epoch_num + 1):\n        train_epoch(train_loader, model, optimizer, scheduler, epoch)\n        val_metrics = evaluate(dev_loader, model, mode = 'dev')\n        val_f1 = val_metrics['f1']\n        print(f\"Epoch: {epoch}, dev loss: {val_metrics['loss']}, f1 score: {val_f1}\")\n        improve_f1 = val_f1 - best_val_f1\n        if improve_f1 > 1e-5:\n            best_val_f1 = val_f1\n            model.save_pretrained(model_dir)\n            if improve_f1 < config.patience:\n                patience_counter += 1\n            else:\n                patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if (patience_counter >= config.patience_num and epoch > config.min_epoch_num) or epoch == config.epoch_num:\n            print('Best val f1: {best_val_f1}')\n            break\n    print('Training Finished!')\n    \n    \ndef evaluate(dev_loader, model, mode='dev'):\n    # set model to evaluation mode\n    model.eval()\n    if mode == 'test':\n        tokenizer = BertTokenizer.from_pretrained(config.bert_model, do_lower_case = True, skip_special_tokens = True)\n    id2label = config.id2label\n    true_tags = []\n    pred_tags = []\n    sent_data = []\n    dev_losses = 0\n\n    with torch.no_grad():\n        for idx, batch_samples in enumerate(dev_loader):\n            batch_data, batch_token_starts, batch_tags = batch_samples\n            if mode == 'test':\n                sent_data.extend([[tokenizer.convert_ids_to_tokens(idx.item()) for idx in indices\n                                   if (idx.item() > 0 and idx.item() != 101)] for indices in batch_data])\n            batch_masks = batch_data.gt(0)  # get padding mask, gt(x): get index greater than x\n            label_masks = batch_tags.gt(-1)  # get padding mask, gt(x): get index greater than x\n            # compute model output and loss\n            loss = model((batch_data, batch_token_starts),\n                         token_type_ids = None, attention_mask = batch_masks, labels = batch_tags)[0]\n            dev_losses += loss.item()\n            # (batch_size, max_len, num_labels)\n            batch_output = model((batch_data, batch_token_starts),\n                                 token_type_ids = None, attention_mask = batch_masks)[0]   # 加label给出loss, 不加label才给出预测\n            # (batch_size, max_len - padding_label_len)\n            batch_output = model.crf.decode(batch_output, mask=label_masks)\n            # (batch_size, max_len)\n            batch_tags = batch_tags.to('cpu').numpy()\n            pred_tags.extend([[id2label.get(idx) for idx in indices] for indices in batch_output])\n            # (batch_size, max_len - padding_label_len)\n            true_tags.extend([[id2label.get(idx) for idx in indices if idx > -1] for indices in batch_tags])\n\n    assert len(pred_tags) == len(true_tags)\n    if mode == 'test':\n        assert len(sent_data) == len(true_tags)\n\n    # logging loss, f1 and report\n    metrics = {}\n    if mode == 'dev':\n        f1 = f1_score(true_tags, pred_tags, mode)\n        metrics['f1'] = f1\n    else:\n        bad_case(true_tags, pred_tags, sent_data)\n        f1_labels, f1 = f1_score(true_tags, pred_tags, mode)\n        metrics['f1_labels'] = f1_labels\n        metrics['f1'] = f1\n    metrics['loss'] = float(dev_losses) / len(dev_loader)\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.392094Z","iopub.execute_input":"2023-04-14T04:28:32.392379Z","iopub.status.idle":"2023-04-14T04:28:32.414507Z","shell.execute_reply.started":"2023-04-14T04:28:32.392354Z","shell.execute_reply":"2023-04-14T04:28:32.413397Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"### hoho\n\nl1 = [('H', 0, 1), ('B', 0, 1), ('H', 0, 1), ('B', 3, 3)]\nl2 = [('H', 0, 1), ('B', 0, 1)]\nset(l1) & set(l2)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.417347Z","iopub.execute_input":"2023-04-14T04:28:32.418486Z","iopub.status.idle":"2023-04-14T04:28:32.429490Z","shell.execute_reply.started":"2023-04-14T04:28:32.418442Z","shell.execute_reply":"2023-04-14T04:28:32.428289Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{('B', 0, 1), ('H', 0, 1)}"},"metadata":{}}]},{"cell_type":"code","source":"# utils.py\n\nimport logging\n\n\ndef set_logger(log_path):\n    \"\"\"Set the logger to log info in terminal and file `log_path`.\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n    Example:\n    ```\n    logging.info(\"Starting training...\")\n    ```\n    Args:\n        log_path: (string) where to log\n    \"\"\"\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n        logger.addHandler(stream_handler)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:32.431202Z","iopub.execute_input":"2023-04-14T04:28:32.431571Z","iopub.status.idle":"2023-04-14T04:28:32.438676Z","shell.execute_reply.started":"2023-04-14T04:28:32.431535Z","shell.execute_reply":"2023-04-14T04:28:32.437318Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# run.py\n\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom transformers.optimization import get_cosine_schedule_with_warmup, AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef dev_split(dataset_dir):\n    data = np.load(dataset_dir, allow_pickle = True)\n    words = data['words']\n    labels = data['labels']\n    x_train, x_dev, y_train, y_dev = train_test_split(words, labels, test_size = config.dev_split_size, random_state = RANDOM_STATE)\n    return x_train, x_dev, y_train, y_dev\n\ndef test():\n    data = np.load(config.test_dir, allow_pickle = True)\n    word_test = data['words']\n    label_test = data['labels']\n    test_dataset = NERDataset(word_test, label_test, config)\n    print('--------Dataset Build!--------')\n    test_loader = DataLoader(test_dataset, batch_size = config.batch_size, shuffle = False, collate_fn = test_dataset.collate_fn)\n    \n    print('--------Get Dataloader!--------')\n    if config.model_dir is not None:\n        model = BertNER.from_pretrained(config.model_dir)\n        model.to(config.device)\n        print(f'--------Load model from {config.model_dir}')\n    else:\n        print(f'--------No model to test!--------')\n        return\n    val_metrics = evaluate(test_loader, model, mode = 'test')\n    val_f1 = val_metrics['f1']\n    print(f\"test loss: {val_metrics['loss']}, f1 score: {val_f1}\")\n    val_f1_labels = val_metrics['f1_labels']\n    for label in config.labels:\n        print(f'f1 score of {label}: {val_f1_labels[label]}')\n        \ndef load_dev(mode):\n    if mode == 'train':\n        word_train, word_dev, label_train, label_dev = dev_split(config.train_dir)\n    elif mode == 'test':\n        train_data = np.load(config.train_dir, allow_pickle = True)\n        dev_data = np.load(config.test_dir, allow_pickle = True)\n        word_train = train_data['words']\n        label_train = train_data['labels']\n        word_dev = dev_data['words']\n        label_dev = dev_data['labels']\n    else:\n        word_train = None\n        label_train = None\n        word_dev = None\n        label_dev = None\n    return word_train, word_dev, label_train, label_dev\n\n\ndef run():\n#     set_logger(config.log_dir)\n    print(f'device: {config.device}')\n    \n    processor = Processor(config)\n    processor.process()\n    print('--------Process Done!--------')\n    \n    word_train, word_dev, label_train, label_dev = load_dev('train')\n    train_dataset = NERDataset(word_train, label_train, config)\n    dev_dataset = NERDataset(word_dev, label_dev, config)\n    print('--------Dataset Build!--------')\n    \n    train_size = len(train_dataset)\n    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle = True, collate_fn = train_dataset.collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size = config.batch_size, shuffle = True, collate_fn = train_dataset.collate_fn)\n    print('--------Get Dataloader!--------')\n    \n    model = BertNER.from_pretrained(config.roberta_model, num_labels = len(config.label2id))\n    model.to(config.device)\n    \n    if config.full_fine_tuning:\n        bert_optimizer = list(model.bert.named_parameters())\n        lstm_optimizer = list(model.bilstm.named_parameters())\n        classifier_optimizer = list(model.classifier.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {\n                'params': [p for n, p in bert_optimizer if not any(nd in n for nd in no_decay)],\n                'weight_decay': config.weight_decay\n            },\n            {\n                'params': [p for n, p in bert_optimizer if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0\n            },\n            {\n                'params': [p for n, p in lstm_optimizer if not any(nd in n for nd in no_decay)],\n                'weight_decay': config.weight_decay,\n                'lr': config.learning_rate * 5\n            },\n            {\n                'params': [p for n, p in lstm_optimizer if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': config.learning_rate * 5\n            },\n            {\n                'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],\n                'weight_decay': config.weight_decay,\n                'lr': config.learning_rate * 5\n            },\n            {\n                'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': config.learning_rate * 5\n            },\n            {\n                'params': model.crf.parameters(),\n                'lr': config.learning_rate * 5\n            }\n            \n        ]\n    else:\n        param_optimizer = list(model.classifier.named_parameters())\n        optimizer_grouped_parameters = [{'param': [p for n, p in param_optimizer]}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr = config.learning_rate, correct_bias = False)\n    train_steps_per_epoch = train_size // config.batch_size\n    scheduler = get_cosine_schedule_with_warmup(optimizer, \n                                                num_warmup_steps = (config.epoch_num // 10) * train_steps_per_epoch, \n                                                num_training_steps = config.epoch_num * train_steps_per_epoch)\n    print('--------Start Training!---------')\n    train(train_loader, dev_loader, model, optimizer, scheduler, config.model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T06:04:53.194483Z","iopub.execute_input":"2023-04-14T06:04:53.194859Z","iopub.status.idle":"2023-04-14T06:04:53.219722Z","shell.execute_reply.started":"2023-04-14T06:04:53.194826Z","shell.execute_reply":"2023-04-14T06:04:53.218530Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T04:28:38.198103Z","iopub.execute_input":"2023-04-14T04:28:38.198518Z","iopub.status.idle":"2023-04-14T05:26:13.619140Z","shell.execute_reply.started":"2023-04-14T04:28:38.198474Z","shell.execute_reply":"2023-04-14T05:26:13.617929Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"device: cuda\n--------Process Done!--------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e488fb9b44cb42ce947af6663de007ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6d555d15bf4060bbff1ff2aa83e744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b9f351a8b24efaa76cb26c2e42017e"}},"metadata":{}},{"name":"stdout","text":"--------Dataset Build!--------\n--------Get Dataloader!--------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622ebbd6ad224191b64b02e8994ac47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fc021ca9834457a3652f5858542e5a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertNER: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertNER were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['bilstm.weight_hh_l0_reverse', 'bilstm.weight_ih_l0', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1', 'bilstm.bias_hh_l0_reverse', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l1_reverse', 'classifier.weight', 'crf.transitions', 'bilstm.weight_hh_l1_reverse', 'bilstm.weight_hh_l1', 'crf.end_transitions', 'bilstm.bias_ih_l1', 'bilstm.bias_ih_l1_reverse', 'crf.start_transitions', 'bilstm.weight_hh_l0', 'bilstm.bias_hh_l1_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_ih_l0', 'classifier.bias', 'bilstm.weight_ih_l0_reverse']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"--------Start Training!---------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:15<00:00,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, train loss: 1551.621292315694\nEpoch: 1, dev loss: 562.4388858570773, f1 score: 0.45458265139116205\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, train loss: 405.37886482968975\nEpoch: 2, dev loss: 357.7667433794807, f1 score: 0.68920778454885\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, train loss: 258.2732638116717\nEpoch: 3, dev loss: 299.7866978364832, f1 score: 0.726805143422354\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, train loss: 190.40315792977614\nEpoch: 4, dev loss: 309.484723035027, f1 score: 0.7208061647895673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, train loss: 151.8444409512057\nEpoch: 5, dev loss: 281.1020076976103, f1 score: 0.757234726688103\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, train loss: 116.84070126372988\nEpoch: 6, dev loss: 319.55024943632236, f1 score: 0.738773916438891\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, train loss: 90.31626202170999\nEpoch: 7, dev loss: 328.2891177009134, f1 score: 0.7529648241206031\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, train loss: 68.75701175349774\nEpoch: 8, dev loss: 349.6932521146887, f1 score: 0.7623481781376519\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, train loss: 56.13403694228371\nEpoch: 9, dev loss: 374.48007112390854, f1 score: 0.7481869460112813\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, train loss: 45.27599221408957\nEpoch: 10, dev loss: 399.9840123793658, f1 score: 0.7537387836490529\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, train loss: 36.62137262577271\nEpoch: 11, dev loss: 430.8044007245232, f1 score: 0.7638399336512545\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, train loss: 30.42282790004617\nEpoch: 12, dev loss: 427.9679816750919, f1 score: 0.7537932429698564\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, train loss: 25.24538288179404\nEpoch: 13, dev loss: 471.9651278327493, f1 score: 0.757324712063043\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, train loss: 20.68653864593002\nEpoch: 14, dev loss: 458.92971891515396, f1 score: 0.7612540192926045\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, train loss: 17.811368344247146\nEpoch: 15, dev loss: 522.5648139504825, f1 score: 0.7726341663252766\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:14<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, train loss: 18.05860178856173\nEpoch: 16, dev loss: 526.9027324003332, f1 score: 0.7645744464757261\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:14<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, train loss: 13.517744693819052\nEpoch: 17, dev loss: 554.8559597239774, f1 score: 0.7657119307644757\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, train loss: 12.338699869590231\nEpoch: 18, dev loss: 606.3282919491039, f1 score: 0.7736692401462821\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, train loss: 12.028977837893041\nEpoch: 19, dev loss: 629.1843095667222, f1 score: 0.7674086175209311\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20, train loss: 10.977499744679669\nEpoch: 20, dev loss: 632.9486420575311, f1 score: 0.759153783563873\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 21, train loss: 9.668869371067967\nEpoch: 21, dev loss: 728.2501566269818, f1 score: 0.7591419816138918\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 22, train loss: 9.596506628659693\nEpoch: 22, dev loss: 724.0008374382468, f1 score: 0.7655774304850821\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 23, train loss: 10.424302570103812\nEpoch: 23, dev loss: 748.0362342385685, f1 score: 0.7691678593334698\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 24, train loss: 7.695328161661381\nEpoch: 24, dev loss: 767.0233324836282, f1 score: 0.7775716347144919\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 25, train loss: 7.025792099855127\nEpoch: 25, dev loss: 781.0002522188074, f1 score: 0.7733930024410088\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 26, train loss: 6.999601641110461\nEpoch: 26, dev loss: 810.7841024959789, f1 score: 0.7730061349693251\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 27, train loss: 5.6275073702972715\nEpoch: 27, dev loss: 822.353707706227, f1 score: 0.7772550623849458\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 28, train loss: 4.6737562311757905\nEpoch: 28, dev loss: 838.430658677045, f1 score: 0.7756856324191567\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 29, train loss: 4.381741212146117\nEpoch: 29, dev loss: 831.1436579087201, f1 score: 0.7786228160328881\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 30, train loss: 3.1760120014152906\nEpoch: 30, dev loss: 836.6103605382583, f1 score: 0.7808190985799547\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 31, train loss: 3.2808447167424872\nEpoch: 31, dev loss: 853.7201547061695, f1 score: 0.7826622843056696\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 32, train loss: 2.3656755894324175\nEpoch: 32, dev loss: 857.4136271757238, f1 score: 0.7794177941779418\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 33, train loss: 2.8027247218015563\nEpoch: 33, dev loss: 871.1861159380744, f1 score: 0.7866639394559215\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 34, train loss: 1.378218000035475\nEpoch: 34, dev loss: 857.6151912913602, f1 score: 0.7841770854683336\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 35, train loss: 1.9507526549765772\nEpoch: 35, dev loss: 901.0234572466682, f1 score: 0.7791675210170186\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 36, train loss: 1.061379325665263\nEpoch: 36, dev loss: 904.9159240722656, f1 score: 0.7781416291444945\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 37, train loss: 1.2170848586771748\nEpoch: 37, dev loss: 896.11840281767, f1 score: 0.7818219206251286\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 38, train loss: 1.3285952609511886\nEpoch: 38, dev loss: 914.7790132410386, f1 score: 0.7830578512396694\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 39, train loss: 1.0881922437806335\nEpoch: 39, dev loss: 914.0419589771944, f1 score: 0.7834839769926049\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 40, train loss: 0.6265108176190467\nEpoch: 40, dev loss: 924.0027762020336, f1 score: 0.7825550298292533\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 41, train loss: 0.4520463746766446\nEpoch: 41, dev loss: 935.8647622501148, f1 score: 0.7789646672144617\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 42, train loss: 0.6191177360295462\nEpoch: 42, dev loss: 940.8033186968635, f1 score: 0.7824302134646961\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 303/303 [01:13<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 43, train loss: 0.4184116191989911\nEpoch: 43, dev loss: 933.0171760110294, f1 score: 0.784217016029593\nBest val f1: {best_val_f1}\nTraining Finished!\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-04-14T05:26:13.621440Z","iopub.execute_input":"2023-04-14T05:26:13.621827Z","iopub.status.idle":"2023-04-14T05:26:14.676614Z","shell.execute_reply.started":"2023-04-14T05:26:13.621788Z","shell.execute_reply":"2023-04-14T05:26:14.675444Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Fri Apr 14 05:26:14 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   70C    P0    48W / 250W |   4129MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"test()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T06:05:59.374153Z","iopub.execute_input":"2023-04-14T06:05:59.375295Z","iopub.status.idle":"2023-04-14T06:06:12.237542Z","shell.execute_reply.started":"2023-04-14T06:05:59.375245Z","shell.execute_reply":"2023-04-14T06:06:12.236395Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"--------Dataset Build!--------\n--------Get Dataloader!--------\n--------Load model from /kaggle/working/experiments/clue/\n--------Bad Cases reserved !--------\ntest loss: 842.7020728701636, f1 score: 0.7873965626989178\nf1 score of address: 0.6493506493506493\nf1 score of book: 0.8133333333333332\nf1 score of company: 0.7917737789203084\nf1 score of game: 0.8431372549019608\nf1 score of government: 0.8061420345489443\nf1 score of movie: 0.8039867109634552\nf1 score of name: 0.8701973001038422\nf1 score of organization: 0.7941176470588234\nf1 score of position: 0.7699316628701595\nf1 score of scene: 0.7312348668280871\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}